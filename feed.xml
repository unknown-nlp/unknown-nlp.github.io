<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://unknownnlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unknownnlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-21T04:43:11+00:00</updated><id>https://unknownnlp.github.io/feed.xml</id><title type="html">Unknown NLP Papers</title><subtitle>Unknown NLP Papers - A group of nine natural language processing researchers and professionals who have reviewed over 150 papers since 2023. Our focus is on NLP research, paper reviews, and academic collaboration. </subtitle><entry><title type="html">ON THE GENERALIZATION OF SFT: A REINFORCEMENT LEARNING PERSPECTIVE WITH REWARD RECTIFICATION</title><link href="https://unknownnlp.github.io/blog/2025/on-the-generalization-of-sft-a-reinforcement-learning/" rel="alternate" type="text/html" title="ON THE GENERALIZATION OF SFT: A REINFORCEMENT LEARNING PERSPECTIVE WITH REWARD RECTIFICATION"/><published>2025-08-19T00:00:00+00:00</published><updated>2025-08-19T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/on-the-generalization-of-sft-a-reinforcement-learning</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/on-the-generalization-of-sft-a-reinforcement-learning/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-19</li> <li><strong>Reviewer</strong>: 전민진</li> <li><strong>Property</strong>: RL, SFT</li> </ul> <blockquote> <p>standard SFT gradient는 내재적으로 모델의 일반화 능력을 제한하는 reward를 갖고 있음. 이를 RL과 유사하게 loss 수식을 변경해 더 높은 성능을 내보자!</p> </blockquote> <h2 id="abstact">Abstact</h2> <ul> <li> <p>간단하지만 이론적으로 영감을 받은 향상된 SFT방식을 제안</p> <ul> <li>RL과 비교하여, 제한된 generalization 문제를 다룸</li> </ul> </li> <li> <p>수학적인 분석을 통해서, 기본 SFT gradient가 내재적으로 모델의 일반화 능력을 제한하는 reward를 갖고 있음을 밝힘</p> </li> <li> <p>이를 수정하여, Dynamic Fine-Tuning(DFT)를 제안</p> <ul> <li> <p>각 토큰에 대한 gradient updates를, 각 토큰의 확률로 objective function을 dynamic하게 rescaling해서 안정화</p> </li> <li> <p>특히, 코드 한 줄만 바꿔서 SFT보다 여러 데이터셋에서 훨씬 높은 성능을 보임</p> </li> <li> <p>또한, DFT는 offline RL설정과 비슷한 결과를 보이며, 효과적이면서 더 간단한 대안을 냄</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>expert demonstration dataset에 모델을 학습시키는 방법론인 SFT는 LLM을 새로운 태스크나 도메인에 adapt할 때 사용하는 기본적인 post-training방법론</p> <ul> <li> <p>가장 구현하기 쉽고, 전문가스러운 행동을 가장 빠르게 습득</p> </li> <li> <p>하지만, RL에 비해서 generalization 성능이 제한적이라는 한계가 존재</p> </li> </ul> </li> <li> <p>RL은 명시적인 reward나 verification signal을 사용, 모델이 좀 더 다양한 strategy를 탐색할 수 있게 해 더 높은 일반화 성능을 달성</p> <ul> <li> <p>하지만, computational cost가 상당함 + hyperparameter tuning에 민감 + reward signal을 쓸 수 있을 때만 활용 가능 ⇒ 이 모든 조건이 다 맞기는 쉽지 않음</p> </li> <li> <p>RL이 가능한 경우에도, SFT는 RL이 스스로 학습하기 어려운 정답 패턴을 빠르게 학습하는데는 여전히 유리</p> </li> </ul> </li> <li> <p>이러한 상보적인 특성 덕분에, 최근 연구들은 보통 SFT와 RL을 함께 사용해서 모델을 학습함</p> </li> </ul> <p>⇒ 하지만 SFT만으로 RL에서 얻는 효과를 어느정도 얻을 수 있다면? 아주 나이스함</p> <ul> <li> <p>SFT는 negative sample 혹은 reward, vefication model이 없는 경우에도 활용할 수 있음</p> </li> <li> <p>본 논문에서는, SFT와 RL사이의 근본적인 차이를 밝히는 수학적 분석을 통해서 제공하여 이러한 차이를 해소하려고 함</p> <ul> <li> <p>SFT의 gradient update가 (내재적으로 정의된 reward structure를 갖는) policy gradient method의 한 케이스로 해석될 수 있음을 보임</p> </li> <li> <p>저자들의 분석에 따르면, 이러한 implict reward가 극단적으로 희소하면서 expert action의 배정된 policy에 반비례함을 보임 ⇒ 그래디언트는 무한한 분산을 겪게 되어 안좋은 최적화 환경을 만듦</p> <ul> <li>정답 데이터의 확률이 땅바닥에 있으면 reward가 매우 커지는.. 그런 문제가 있음</li> </ul> </li> </ul> </li> <li> <p>이러한 수학적 분석에 기반, Dynamic Fine-tuning(DFT)를 제안</p> <ul> <li> <p>ill-posed reward structure의 근원을 해소</p> </li> <li> <p>간단하게, 각 토큰의 확률로 SFT objective를 rescaling ⇒ 예기치 못한 reward structure와 무한한 분산을 야기하는 inverse probability weighting를 효과적으로 중화 ⇒ 확률 의존적인 gradient estimator를 안정적이고 균일하게 가중된 업데이트로 변환</p> </li> </ul> </li> <li> <p>실험 결과, Qwen2.5-Math Modeles를 NuminaMath dataset에 학습하여 SFT보다 높은 성능을 보임.</p> <ul> <li>특히, SFT는 AIME 2025, Olympiad Bench 등을 포함한 어려운 데이터셋에 대해서 performance degradation 현상을 보이는 반면, DFT는 상당한 성능 향상을 보임</li> </ul> </li> <li> <p>또한, RL senarios안에서 비교</p> <ul> <li>RFT/RAFT, DPO보다 높은 성능을 보일 뿐만 아니라, GRPO, PPO와 같은 online RL과도 뒤지지 않는 성능을 보임</li> </ul> </li> <li> <p>DFT가 모델에 어떻게 다르게 영향을 미치는지 분석하기 위해, 학습 후의 확률 분포의 변화를 분석</p> <ul> <li>standard SFT의 경우 학습 데이터에 더 가깝게 맞추기 위해 토큰 확률을 일률적으로 높이는 반면에, DFT는 학습 세트에서 멀어지게도 만듦</li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>SFT과 RL의 trade-off는 language model alignment에서 핵심 주제</p> <ul> <li> <p>SFT는 expert demonstration을 따라하는데 있어 간단하고 효율적이여서 널리 사용됨</p> <ul> <li>overfitting와 RL보다 geneeralization이 좋지 않다는 한계가 존재</li> </ul> </li> <li> <p>최근에 SFT는 외우지만 RL은 일반화한다는 분석도 존재</p> <ul> <li> <p>RL 학습이 효과적이기 위해선 초반에 output format을 안정화해야하는데, 이 때 SFT가 필수적</p> </li> <li> <p>RL을 실제로 사용하려고 할 때, 쓰기 어려운 조건일 경우가 많음</p> <ul> <li>reward를 명시하기 어렵다던가.. computational cost가 제한적이라던가.. 등</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>최근엔 그 둘의 장점을 모두 활용하기 위해서 hybrid method에 집중을 하고 있음</p> <ul> <li> <p>SFT 후 RL-based refinement : 학습된 reward model(like InstructGPT)을 활용</p> </li> <li> <p>SFT와 RL의 학습 단계를 번갈아가면서 시행 : 안정성을 높이면서 성능도 높이기 위해</p> </li> <li> <p>DPO : reward model없이 postive answer, negative answer를 활용하여 imitation과 reinforcement signal을 하나의 loss에 녹여냄</p> </li> <li> <p>Negative-aware Fine-Tuning(NFT) : 모델이 틀린 부분을 학습하여 스스로 개선할 수 있도록 함</p> </li> </ul> </li> </ul> <p>⇒ 다 좋은 방법들이지만 reward signal, preference pairs or negative samples이 있어야 함</p> <p>⇒ traning pipeline을 합치는 방법론이지 positive expert demonstration만을 사용하는 SFT 자체를 향상시키는 방법론은 아님</p> <ul> <li> <p>SFT와 RL을 이론적으로 통합하려는 연구 라인도 존재</p> <ul> <li> <p>RLHF를 SFT의 reward-weighted로 reframe, 파이프라인을 단순화, explict reward에 의존하는건 동일</p> </li> <li> <p>SFT는 implicit reward를 가진 RL로 볼 수 있다는 것을 증명, vanishing KL constraint를 관리하기 위해 더 작은 lr를 사용하는 방법론을 제안</p> </li> <li> <p>positive, negative feedback에서 학습하는 것을 분석, 이 둘의 balance가 policy covergence에 어떻게 영향을 끼치는 지를 보임</p> </li> <li> <p>SFT를 RL의 lower bound로 reframe, data-generating policy에 기반해서 importance weighting을 도입해 향상 시킴</p> </li> </ul> </li> </ul> <p>⇒ 이러한 경구들은 SFT와 RL 사이의 연결고리를 weigthing의 관점에서 지적, SFT gradient과 off-line policy gradient 간의 간단한 수학적 equivalence를 보이는 것에는 한계를 보임</p> <ul> <li> <p>반대로 본 논문에서는 equivalence를 엄격하게 확립한 최초의 연구, 핵심적인 차이가 SFT에 존재하는 inverse-probability weighting term에 있음을 보임</p> </li> <li> <p>흥미롭게도, 본 저자의 방법은 잘 알려진 Focal Loss와 정반대되는 CE loss 설계를 도출</p> <ul> <li> <p>our modified CE : -plog(p)</p> </li> <li> <p>focal loss : -(1-p)^\gamma log(p)</p> </li> <li> <p>focal loss의 경우 소수 클래스에 대한 성능을 개선하기 위해, 잘 분류된 샘플의 가중치를 의도적으로 낮추지만, 본 논문에선 일반화를 개선하기 위해 잘못 분류된 샘플의 가중치를 의도적으로 낮추는 방식</p> <ul> <li>요즘엔 underfitting이 overfitting보다 더 문제니까.. 라고 함.</li> </ul> </li> </ul> </li> </ul> <h2 id="method">Method</h2> <h3 id="preliminaries">Preliminaries</h3> <ul> <li>Supervised Fine-Tuning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Reinforcement Learning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>\nabla \theta를 Expectation 안에 넣는 과정</strong></p> <ul> <li>여기서 적분 형태로 식을 재작성</li> </ul> </li> </ul> <p>(편의상 x에 대한 기대값은 잠시 생략)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 여기서 로그 미분 트릭 \nabla log(f(z))=\nabla f(z)/f(z), \nabla  log(f(z))f(z)=\nabla f(z)을 사용하면, 아래와 같이 변형 가능
</code></pre></div></div> <p>⇒ 아까 생략했던 x에 대한 기대값을 추가하면 4번식과 동일!</p> <h3 id="unify-sft-rl-gradient-expression">Unify SFT-RL Gradient Expression</h3> <ul> <li> <p>2번의 SFT gradient식은 fixed demonstration distribution</p> </li> <li> <p>이를 importance weight를 추가하여 on-policy expectation으로 변환해보자!</p> <ul> <li>expert (Dirac Delta) distribution과 모델 분포를 비교하는 importance weight를 도입</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>SFT gradient의 형태는 policy gradient equation(4번식)과 거의 유사</p> </li> <li> <p>즉, 여기서 conventinal SFT는 reward를 expert trajectory와 동일한지를 비교하는 indicator function으로 주면서, importance weigthing 1/\pi_\theta로 편향된 on-policy-gradient로 해석 가능</p> </li> </ul> <p>⇒ 만약 모델이 정답을 맞힐 확률이 매우 낮으면, 가중치가 폭증, 학습 과정을 불안정하게 만들고, 모델이 어쩌다 맞춘 exact-match demonstration에 과적합되도록 함.</p> <p>⇒ 학습 데이터의 특정 사례만 암기, 새로운 데이터에 대한 일반화 능력이 저하됨</p> <h3 id="proposed-method">Proposed Method</h3> <ul> <li> <p>이러한 skewed reward issue를 좀 완화하기 위해서, policy probability로 주어지는 1/w를 곱해서 reward를 dynamic하게 reweight</p> <ul> <li>stop-graident operator를 사용해서 reward scaling term w를 통해 gradient가 흐르지 않도록 함</li> </ul> </li> <li> <p>식을 좀 더 간단하게 쓰기 위해 정답인 경우(y*)에 대해서만 식을 작성하면 8번이 됨</p> <ul> <li>이제 gradient가 흐르지 않기 때문에, 수정된 SFT는 간단한 reweighted loss가 되고, DFT라고 명명</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>하지만 전체 trajectory에 대한 importance weight를 계산하는 것은 수학적으로 불가능하기 때문에, PPO처럼 token-level에서의 importance sampling을 적용</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>DFT에서의 reward는 모든 expert trajectory에 대해서 uniform하게 1이 됨</p> <ul> <li>이는 RLVR이 모든 정답 sample에 대해 동일한 reward를 부여하는 것과 비슷</li> </ul> </li> <li> <p>결과적으로, 특정 low-probability reference token에 과도하게 집중되는 현상을 피하게 하고, 추가적인 샘플링이나 reward model없이 좀 더 안정적이게 업데이트 하면서 향상된 일반화 성능을 이끔!</p> </li> <li> <p>코드 수정 한 줄만 하면 됨</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">shift_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">shift_labels</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">detach</span><span class="p">()</span>
</code></pre></div></div> <h2 id="experiments">Experiments</h2> <h3 id="main-experiment---sft-setting">Main experiment - SFT setting</h3> <ul> <li> <p>Setup and Implementation details</p> <ul> <li> <p>Dataset</p> <ul> <li>NuminaMath CoT dataset : 860K, 중국 고등학교 수학 문제와 미국 국제 수학 올림피아드 문제를 포함 ⇒ 여기서 100K sampling해서 학습 데이터로 사용</li> </ul> </li> <li> <p>Model</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, LLaMA-3.2-3B, LLaMA-3.1-8B, and
</code></pre></div> </div> <p>DeepSeekMath-7B-Base.</p> </li> <li> <p>Training details</p> <ul> <li> <p>AdamW optim 사용</p> </li> <li> <p>epoch 1</p> </li> <li> <p>lr은 Llama-3.1-8B만 2e-5, 나머지는 5e-5 사용</p> </li> <li> <p>batch는 256, maximum input length는 2048</p> </li> <li> <p>cosine decay schedule with warm-up ratio 0.1</p> </li> <li> <p>iw-SFT도 포함</p> <ul> <li>동기간에 나온 방법론</li> </ul> </li> </ul> </li> <li> <p>Evaluation settings</p> <ul> <li> <p>Math500, Minerva Math, OlympiadBench, AIME 2024, AMC 2023 사용</p> </li> <li> <p>모든 모델은 기본적으로 chat template, CoT prompting 사용</p> </li> <li> <p>실험은 16번 decoding run해서 평균, temperature는 0.1, maximum generation length는 4096</p> </li> </ul> </li> </ul> </li> <li> <p>Main result</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>DFT는 모든 베이스 모델에 대해서 SFT보다 높은 성능을 보임</p> </li> <li> <p>특히 DFT는 좀 더 어려운 벤치마크에서 강점을 보임 (generalization과 robustness가 뛰어남)</p> <ul> <li>SFT의 경우 OlympiadBench, AIME24, AMC23과 같은 어려운 데이터셋에서는 base모델보다 낮은 성능을 보이는 경우도 있는 반면, DFT는 이러한 데이터셋에서도 뛰어난 성능을 보임</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>DFT는 SFT와 비교했을 때 빠른 수렴 속도, 뛰어난 초기 성능, 높은 샘플 효율성을 보임</p> <ul> <li>학습이 수렴되는 속도가 빠르며, 학습 초기 10-20단계만에 SFT의 최종 성능을 뛰어 넘음 + 최고 성능에 도달할 때까지 더 적은 업데이트(=더 적은 학습 데이터)를 필요로 함</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>비슷한 시기에 나온 iw-SFT와 성능을 비교해도, 대부분의 경우에서 평균적으로 높은 성능을 보임</p> <ul> <li>특히, iw-SFT의 경우 특정 모델이나 벤치마크에서 성능이 불안정</li> </ul> </li> </ul> <p>(iw-SFT는 reference model을 써서 reweight하는 방법론)</p> <h3 id="exploratory-experiment---offline-rl-setting">Exploratory experiment - offline RL setting</h3> <ul> <li> <p>Data Preparation</p> <ul> <li> <p>DFT를 offline setting에 적용해봄</p> <ul> <li> <p>SFT setting에 비해서 reward의 sparse문제(확률이 낮은 정답에 너무 집중하는 문제)가 완화될 수 있음</p> </li> <li> <p>데이터는 RFT와 동일하게 함</p> </li> </ul> </li> <li> <p>rejection sampling fine-tuning (RFT) framework를 위해서 아래와 같은 방식으로 데이터 구축</p> <ul> <li> <p>100000개의 math question에 대한 답을 sampling, base model로부터 질문당 4개의 response를 생성</p> <ul> <li>(논문엔 10000개라고 하는데 그럼 140000개의 데이터가 나올 수가 없을듯.. 오타라고 생각)</li> </ul> </li> <li> <p>정답만 학습 데이터로 사용, 거의 140K개의 데이터를 사용</p> </li> </ul> </li> <li> <p>DPO를 위해서 100K개의 positive-negative preference pair를 구축</p> </li> </ul> </li> <li> <p>Training details</p> <ul> <li> <p>Qwen2.5-math-1.5B model로 실험</p> </li> <li> <p>DFT를 DPO, RFT, PPO, GRPO와 비교</p> </li> <li> <p>RFT와 DFT는 위와 동일한 config 사용</p> </li> <li> <p>DPO는 ms-swift framework를 사용, lr 1e-6, batch size 128, warmup ratio of 0.05</p> </li> <li> <p>PPO와 GRPO는 verl framework에서 학습, lr 1e-6, batch size 256, warmup ratio 0.1</p> <ul> <li>GRPO에서 group 크기는 4</li> </ul> </li> </ul> </li> <li> <p>Results</p> <ul> <li> <p>DFT는 SFT setting에서도 offline RL보다 높은 성능을 보임</p> </li> <li> <p>online RL보다는 살짝 밀리지만, DFT를 offline setting으로 하면 online RL보다도 높은 성능을 보임</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="ablation-and-investigation">Ablation and investigation</h3> <ul> <li> <p>Token probaility distribution</p> <ul> <li> <p>SFT는 토큰 확률을 균일하게 증가시키는 경향이 있음</p> <ul> <li>하지만 base와의 차이를 보면, 주로 확률이 낮은 토큰들을 조정한다는 것을 볼 수 있음</li> </ul> </li> <li> <p>DFT의 경우 일부 토큰들의 확률은 대폭 낮추면서 다른 토큰들의 확률은 높이는 경향 존재(양극화 효과)</p> </li> <li> <p>가장 낮은 확률 구간에 속하는 단어들은 보통 ‘the’, ‘let’, ‘,’, ‘.’ 등과 같은 접속사나 구두점이었음</p> </li> </ul> </li> </ul> <p>⇒ 견고한 학습을 위해서 모든 토큰을 동일한 신뢰도로 맞추려고 하면 안된다. LLM의 경우 핵심적 의미를 전달하기보다 문법적 기능을 수행하는 토큰들의 학습 우선순위를 낮추는게 유리할 수 있다</p> <ul> <li>학생들이 연결어보다는 실질적인 개념에 집중하도록 하는 것과 유사..</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Training hyper-parameters ablation</p> <ul> <li>lr, batch를 바꿔봐도 DFT 승!</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-on-the-generalization-of-sft-a-reinforcement-learning/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>SFT식을 조금 reformulate하면 policy gradient의 식처럼 바꿀 수 있음</p> <ul> <li>특수한 importance weight가 내재된 policy gradient의 형태</li> </ul> </li> <li> <p>SFT에 내재된 imporatence weight의 역수를 loss에 곱해줘서 학습 안정성을 높이면서 성능도 높이는 DFT방법론 제안</p> </li> <li> <p>실험 결과, SFT, SFT 변형식 뿐만 아니라 offline RL보다도 높은 성능을 보임</p> </li> <li> <p>추가적인 cost가 전혀 없다는 측면에서 한번쯤 SFT 대신 loss만 바꿔서 학습해볼만할지도?</p> <ul> <li>물론 본 논문에서의 실험은 수학 도메인에만 제한적…</li> </ul> </li> <li> <p>계속 이 policy gradient와 SFT사이의 오묘한 느낌.. 비슷하면서 다른듯.. 근데 결국 똑같아 보이는.. 느낌을 갖고 있었는데 좀 해소된거 같아서 좋았다!</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="alignment"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reinforcement-learning"/><category term="rl"/><category term="rlhf"/><category term="sft"/><summary type="html"><![CDATA[논문 리뷰 - RL, SFT 관련 연구]]></summary></entry><entry><title type="html">Spurious Rewards: Rethinking Training Signals in RLVR</title><link href="https://unknownnlp.github.io/blog/2025/spurious-rewards-rethinking-training-signals-in-rlvr/" rel="alternate" type="text/html" title="Spurious Rewards: Rethinking Training Signals in RLVR"/><published>2025-08-19T00:00:00+00:00</published><updated>2025-08-19T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/spurious-rewards-rethinking-training-signals-in-rlvr</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/spurious-rewards-rethinking-training-signals-in-rlvr/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-19</li> <li><strong>Reviewer</strong>: 건우 김</li> <li><strong>Property</strong>: RLVR</li> </ul> <h3 id="key-takeaways">Key Takeaways</h3> <ol> <li> <p><strong>Pretraining is important for RLVR:</strong> RLVR outcomes depend heavily on reasoning strategies already learned during pretraining.</p> </li> <li> <p><strong>Weak or spurious rewards can still work:</strong> Even random or incorrect rewards can improve performance by amplifying useful pre-existing behaviors (e.g., code reasoning).</p> </li> <li> <p><strong>Limited generalization:</strong> Gains observed in Qwen models do not necessarily transfer to other model families like Llama or OLMo.</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <h2 id="1-introduction">1. Introduction</h2> <p>최근에 RLVR이 language model reasoning을 향상시킴에 있어 큰 도움이 되었는데, 역설적으로 본 연구에서 처음으로 <strong><em>spurious rewards</em></strong>만으로도 특정 모델에서 강력한 mathematical reasoning을 이끌어낼 수 있음을 보여줌.</p> <ul> <li> <p>MATH-500에서 Incorrect labels을 이용해 학습하면 24.1% 성능 향상을 보여주고, 이는 ground truth reward를 사용했을때 29.1% 향상과 비슷한 수준임</p> </li> <li> <p>format reward, random reward → 13.8%, 21.4% performance increases, respectively</p> </li> </ul> <p>→ 위 발견은 RLVR이 성능을 개선하는 정확한 메커니즘을 아직 완전히 이해하지 못하고, 많은 경우 RLVR은 reward signal과는 별개로, <strong>pretraining 중에 학습된 모델의 innate abilities를 드러내는 방식으로 작동할 수 있다고 볼 수 있음</strong>.</p> <hr/> <p>weak and spurious rewards가 주는 성능 향상을 측정하기 위해 cross-model analysis를 진행했고, 그 결과 non-Qwen models (OLMo2, Llama3 variants)는 성능 향상이 거의 없거나 오히려 악화된 결과를 보여줌.</p> <p>→<strong>RLVR 성능 차이가 적어도 일부는 pretraining에서의 차이에서 비롯됨을 시사함</strong></p> <hr/> <p>본 연구는 <strong><em>pretraining 과정에서 주입된 reasoning patterns이 RLVR 학습에 큰 영향을 끼치는 것</em></strong>을 알아야한다는 것을 강조함.</p> <ul> <li> <p>Qwen models은 open weight and high performance이기에, RLVR 연구의 de facto choice가 되버림 (최근 RLVR 연구 대부분이 Qwen2.5-Math-7B 중심의 실험)</p> </li> <li> <p>본 연구에서 Qwen 계열 models은 spurious rewards 만으로도 큰 성능 향상을 얻을 수 있음을 보여줌</p> </li> </ul> <p>→ 향후 RLVR 연구는 가능하다면 <strong>다른 계열의 models에서 결과가 재현되는지</strong> 확인하는 것을 제안함.</p> <hr/> <h2 id="2-spurious-rewards-yield-significant-rlvr-gains">2. Spurious Rewards Yield Significant RLVR Gains</h2> <h3 id="21-experimental-setup">2.1 Experimental Setup</h3> <p>최근 RLVR 연구들을 따라, GRPO를 사용하여 Qwen2.5-Math를 finetune 수행하고 rollouts에 대해 binary reward를 부여함.</p> <p>해당 standard ground-truth reward를 대체하기 위해 점진적으로 약화된 reward functions을 다음과 같이 설계함 (<strong>얼마나 적은 supervision으로도 효과적인 RLVR 학습이 가능한지 한계를 확인하기 위함</strong>)</p> <ul> <li> <p><strong>Ground truth reward</strong>: standard RLVR approach (upper bound for reward supervision quality)</p> </li> <li> <p><strong>weak rewards</strong></p> <ul> <li> <p><strong>majority vote reward</strong>: ground truth labels을 사용하지 않고, RLVR 학습을 하지 않은 model을 활용해 pseudo-label 진행 (각 prompt에 대해 64개 response 추출후 majority answer를 선택하여, online RLVR training에 활용)</p> </li> <li> <p><strong>format reward</strong>: response의 수학적 정답 여부를 전혀 고려하지 않고, 최소한 하나의 non-empty \boxed{} expression이 있으면 reward 부여함. (<strong>괄호 안의 정답 여부 상관없음</strong>)</p> </li> </ul> </li> <li> <p><strong>spurious rewards</strong></p> <ul> <li> <p><strong>random reward</strong>: <em>no guidance</em> in the rewarding process를 해도 수학 성능이 향상되는지 확인함. 보상을 fixed probability \gamma로 설정하여 \gamma 확률로 reward 1을 부여함 (\gamma=0.5 사용)</p> </li> <li> <p><strong>incorrect reward</strong>: 의도적으로 incorrect supervision을 제공하여 incorrect answer에 대해서만 reward를 부여함 (majority voting으로 training data를 labeling한 뒤에, incorrect label이 붙은 subset만 선택하여 training data로 사용함 → 이렇게 얻어진 incorrect label은 model이 산출할 가능성이 높은 출력)</p> </li> </ul> </li> </ul> <hr/> <ul> <li>Training data: DeepScaleR (40,000 unique math problem-answer pairs)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Benchmark dataset (+Metric)</p> <ul> <li> <p>MATH-500 w/ pass@1 accuracy</p> </li> <li> <p>AMC w/ average@8 accuracy</p> </li> </ul> </li> </ul> <hr/> <h3 id="23-results">2.3 Results</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>MATH, AMC 모든 벤치마크에서 reward 종류와 상관없이 untuned baseline 대비 학습 초반 50 steps이후부터 유의미한 성능 개선을 보여줌</p> <ul> <li>(예외) Qwen2.5-Math-1.5B에서 Random reward는 비교적 느리게 학습되며, AMC에서는 4.9% 제한적 성능 향상을 보여줌</li> </ul> </li> <li> <p>MATH-500과 AMC에서 모두 Spurious rewards에 의한 성능 향상은 ground truth 기반의 RLVR과 차이가 크게 나지 않음을 보여줌</p> <ul> <li> <p>*<strong>*Additional Results</strong> (AIME 2024, 2025)**</p> <ul> <li> <p>AIME2024역시 ground truth와 spurious rewards랑 큰 차이를 보여주지 않음</p> </li> <li> <p>다만 AIME2025에서는 유의미한 차이가 나타나는데, 이는 AIME2025가 model의 지식 cutoff 이후에 작성된 문제들을 포함하기 때문 (그럼에도 불구하고 untuned baselines 대비 성능 향상 보임)</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong><em>→ 위 실험 결과는 적어도 open-source post-training pipeline 규모에서, RLVR은 새로운 reasoning capabilities를 가르치는 것이 아니라, 이미 base model의 latent capabilities를 trigger함을 보여줌</em></strong></p> <p>(뒤에 추가 실험 결과들은 위 가설을 뒷받침함)</p> <hr/> <h2 id="3-lack-of-generalization-to-other-models">3. (Lack of) Generalization to Other Models</h2> <p>Section 2에서 보여준 현상이 다른 모델을 학습할 때에도 적용되는지 확인하는 실험을 함.</p> <p>Models</p> <ul> <li> <p>Qwen2.5-7B, Qwen2.5-1.5B (수학 특화 모델이 아닌 general-purpose variants)</p> </li> <li> <p>Llama3.1-8B(-Instruct), Llama3.2-3B(-Instruct)</p> </li> <li> <p>OLMo2-7B, OLMo2-7B-SFT</p> </li> </ul> <p><strong>Spurious rewards can benefit Qwen2.5 models, but nearly always fail to improve non-Qwen models.</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>같은 계열의 models에서는 일반적으로 유사한 경향을 보여줌</p> <ul> <li> <p>Qwen2.5 models에서 random reward를 제외하면, MATH와 ACC에서 명확한 성능 향상을 보여줌</p> </li> <li> <p>OLMo models은 ground truth reward에 대해서만 효과 있음 (spurious rewards X)</p> </li> </ul> </li> </ul> <p>**→ 저자들은 같은 계열의 model들이 유사한 경향을 보여주는 이유는, pretraining data의 distribution이 유사하기 때문이라고 추정함 **</p> <ul> <li>작은 models일수록 spurious rewards의 gain이 낮음</li> </ul> <p><strong>→ 저자들은 bigger models이 pretraining 단계에서 더 많은 knowledge를 학습했고, spurious rewards가 그 지식을 이끌어낼 수 있기 때문이라고 추정함</strong></p> <ul> <li> <p>어떤 model 계열에서 잘 작동하는 reward signals이 다른 계열의 모델로 일반화되지는 않음</p> <ul> <li>Spurious rewards는 Qwen 계열의 models에서 일관된 성능 향상을 보여주고, 다른 계열의 models에서는 그렇지 않음</li> </ul> </li> </ul> <hr/> <p><strong>Practical warning : Proposed RLVR reward signals should be tested on diverse models!</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>최근 RLVR reasoning 연구들은 주로 Qwen model에 대해서 결론을 도출함 (test-time scaling, one-shot RL)</p> </li> <li> <p>TTRL과 One-Shot RL에 대해서 Qwen 계열 models을 포함하여 다른 계열의 models도 실험해본 결과,</p> <ul> <li> <p>proposed rewards는 Qwen 계열에서는 잘 작동함 (어떤 spurious reward 적용했는지 언급 x..)</p> </li> <li> <p>동일한 reward signal임에도 다른 models 계열에서는 성능 향상이 없음</p> </li> </ul> </li> </ul> <p><strong>→ 앞으로 Qwen-centric RLVR 연구는 non-Qwen models에 대해서도 validation이 필요함</strong></p> <hr/> <h2 id="4-what-makes-rlvr-with-spurious-rewards-work">4. What Makes RLVR with Spurious Rewards Work?</h2> <p>Section 3에서는 동일한 reward function을 사용함에도 불구하고 model에 따라 결과가 달라지는 것을 보여줬고, 이번 Section에서는 왜 이러한 discrepancy가 발생하는지 알아봄.</p> <p><strong>*hypothesis**</strong>: RLVR 결과의 차이가 각 model이 pretraining하는 동안 학습한 특정 reasoning strategies의 차이에서 발생함. (어떤 strategy는 RLVR에 의해 쉽게 이끌어낼 수 있고, 다른 strategy는 그렇지 않음)*</p> <ul> <li> <p>Qwen-Math는 효과적으로 활용하고 다른 model 계열은 그렇지 못하는, ‘generating code to assist in math reasoning’ strategy를 확인함 (Section 4.1)</p> </li> <li> <p>RLVR 학습 과정에서 code reasoning의 prevalence를 tracing하며, 위 hypothesis를 지지하는 evidence 발견함 (Section 4.2)</p> </li> <li> <p>Incorrect and Random rewards (Spurious rewards)의 reward signals의 origin에 대한 hypothesis를 제안함 (Section 4.4)</p> </li> </ul> <hr/> <h3 id="41-different-models-exhibit-pre-existing-discrepancies-in-reasoning-strategies">4.1 Different Models Exhibit Pre-existing Discrepancies in Reasoning Strategies</h3> <p>Qwen2.5-Math-7B와 OLMo2-7B의 behaviors discrepancy를 이해하기 위해, MATH-500에 대한 reasoning traces를 평가함.</p> <ul> <li>Qwen2.5-Math-7B는 code execution environment가 아님에도 불구하고, 자신의 thinking process를 돕기 위해 자주 Python 코드를 생성함 (65.0% of all responses) → <strong><em>Code Reasoning</em></strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Code Reasoning</strong> 현상은 단순히 model의 memorization이라고 볼 수 없는게, 문제에서 숫자만 바꿔도 accuracy를 유지하는 현상을 보여줌.</p> </li> <li> <p>문제가 다른 서술 형태로 재구성되면 Code Reasoning 현상을 보여주지 않고, 틀린 정답을 도출함</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ Qwen2.5-Math-7B가 pretraining 과정에서 많은 code-assisted math reasoning traces를 접했을 것이라 추정함</p> <ul> <li>Code 사용은 answer correctness를 강하게 예측하는 것을 보여줌</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Qwen2.5-Math-7B/1.5B 둘 다 Acc. w/Code가 Acc. w/Lang 보다 유의미하게 높은 성능 보여줌</p> <ul> <li>두 모델은 동일한 corpus로 pretraining 진행됨</li> </ul> </li> <li> <p>다른 계열의 models에서는 이러한 현상이 나타지 않음 → <strong>No-Code / Bad-Code</strong></p> <ul> <li> <p>No-Code: Llama, Qwen2.5-1.5B, OLMo2-7B는 Code Frequency가 0%</p> </li> <li> <p>Bad-Code: OLMo2-7B-SFT, Qwen2.5-7B는 Code Frequency가 90% 이상으로 매우 높지만, 오히려 성능 악화로 이어짐</p> </li> </ul> </li> </ul> <p><strong>→ Effective Code Reasoning은 RLVR 학습 이전에 Qwen2.5-Math models이 갖는 unique capability라고 볼 수 있음</strong></p> <hr/> <h3 id="42-rlvr-with-spurious-rewards-can-upweight-pre-existing-reasoning-strategies">4.2 RLVR with Spurious Rewards Can Upweight Pre-existing Reasoning Strategies</h3> <p>Section 4.1의 결과에 따라 RLVR training에 걸친 model의 reasoning behavior를 다음과 같이 분석함</p> <ol> <li> <p><strong>Accuracy</strong>: MATH-500의 평균 accuracy</p> </li> <li> <p><strong>Code reasoning frequency</strong>: model 응답에서 “python” string을 포함하는 비율</p> </li> </ol> <hr/> <p><strong>Performance is correlated with code reasoning frequency</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Qwen2.5-Math-7B는 RLVR training 이후에 초반 15 steps에 reward에 상관없이 code frequency가 대략 90%로 보여주며, accuracy improvements와 강한 상관관계를 보여줌 (뚜렷한가..?)</p> <ul> <li> <p>Random Reward는 비록 초반에 낮은 수치를 보여주지만, 후반부에 가서 95.6% 찍음</p> </li> <li> <p>Ground Truth를 reward로 RLVR을 수행할 때, code frequency는 급격히 증가하지만, model의 natural language reasoning accuracy가 올라감에 따라 감소하는 경향을 보여줌</p> </li> </ul> </li> </ul> <p><strong>→ RLVR 중에 model은 high-quality ground truth reward로부터 real knowledge를 학습함</strong></p> <ul> <li>Bad-Code 모델은 뚜렷한 상관관계를 보여주지는 않다고 주장하지만, 대체로 음의 상관관계 보여줌</li> </ul> <hr/> <p><strong>Reasoning strategy switches during RLVR</strong></p> <ul> <li> <p>Qwen2.5-Math-7B의 accuracy 향상을 세분화하여 분석하기 위해, 각 reward signal로 학습한 model의 성능을 다음과 같이 설정하여 분석함 (test prompt를 4개의 베타적인 subset으로 나눔)</p> <ol> <li> <p><strong>Code → Code</strong>: RLVR 전후 모두 code reasoning 사용</p> </li> <li> <p><strong>Code → Lang</strong>: 초기에는 code reasoning 사용, 이후에 natural language reasoning 사용</p> </li> <li> <p><strong>Lang → Code</strong>: 초기에는 natural language reasoning 사용, 이후에 code reasoning 사용</p> </li> <li> <p><strong>Lang → Lang</strong>: RLVR 전후 모두 natural language reasoning 사용</p> </li> </ol> </li> </ul> <p>**Partial Contribution Score **C_d: test set D의 subset d에 대한 부분 기여 점수를 이용하여 각 subset이 성능 향상에 기여한 정도를 정량화 시킴</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <hr/> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Frequency</strong></p> <ul> <li> <p>각 reward signal 별로 Qwen2.5-Math-7B의 reasoning strategy switches를 보면,</p> <ul> <li> <p>weak/spurious rewards에서 RLVR 이후에 code reasoning을 많이 사용함 (C → L cases는 적지만, L → C cases는 상대적으로 많음)</p> </li> <li> <p>Ground truth reward는 상반된 결과를 보여줌 (L → C cases &lt; C → L cases)</p> </li> </ul> </li> <li> <p>Section 4.2 결과와도 일맥상 통하는 결론</p> <ul> <li> <p>Bad-Code 모델에서 (Qwen2.5-7B, OLMo2-7B-SFT) meaningful reward (ground truth, majority vote)는 model로 하여금 bad code reasoning으로부터 멀어지게 함 (<em>위에 Figure 참조</em>)</p> </li> <li> <p>(No-Code 모델에서 RLVR은 reasoning strategy에서 meaningful changes를 보여주지 못하는데, 이는 <strong>pretraining 과정에서 해당 capability 자체가 학습되지 않았기 때문</strong>)</p> </li> </ul> </li> </ul> <hr/> <p><strong>Accuracy</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>위 Figure 보면, L → C subset의 accuracy가 모든 reward에서 RLVR 이후 큰 성능 향상을 보여줌</li> </ul> <p>→ Qwen2.5-Math-7B/1.5B의 성능 향상의 58.3% / 78.7%은 L→C case가 차지함</p> <ul> <li>Bad-Code model인 Qwen2.5-7B에서 성능 향상은 C → L case가 93.9% 차지함</li> </ul> <p>→ 이 model은 language reasoning accuracy가 code reasoning accuracy보다 높기 때문에, **RLVR training은 결국 model이 더 잘하는 strategy를 이용하도록 학습됨 **(즉, prior knowledge에 대한 dependency가 있다고 생각함)</p> <hr/> <h3 id="43-intervening-explicitly-on-code-reasoning-frequency">4.3 Intervening Explicitly on Code Reasoning Frequency</h3> <p>Section 4.2와 4.3에서 RLVR 동안 code reasoning frequency가 증가하고, 이는 test performance의 향상과 상관관계가 있다는 것을 실험적으로 보여줌. 이번 Section에서는 Code reasoning의 빈도를 more / less 명시적으로 유도하며, causal impact를 분석함</p> <p><strong>*Hypothesis**</strong>: spurious reward로 학습할 때, code reasoning 증가가 Qwen2.5-Math-7B의 성능 향상의 주된 원인중 하나가 맞다*</p> <p>→ 해당 hypothesis가 맞다면, code reasoning frequency에 intervention을 하는 것은 그에 상응하는 성능 향상 혹은 감소를 일으켜야함.</p> <hr/> <p>Code reasoning을 유도하면 Qwen2.5-Math의 성능이 향상되는 반면, 다른 모델들은 반대 경향을 보여줌.</p> <ul> <li> <p>Prompting과 RLVR 학습을 통해 code reasoning을 유도함</p> <ul> <li><strong>Prompting</strong>: “Let’s solve this using Python” 명시적으로 강제함</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- No-Code models은 성능 하락을 보이는데, 이는 해당 계열 models이 effective code reasoning behavior를 보여주지 않음 (Section 4.1 실험 결과와 일치)
</code></pre></div></div> <ul> <li><strong>RLVR</strong>: response에 “python” string을 포함할때만 + reward 부여함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Qwen2.5-Math-7B는 20 training steps부터 code reasoning frequency가 99% 이상 비율

- Qwen2.5-Math에서만 유의미한 성능 향상을 보여주고, 다른 모델에서는 그렇지 않은 결과를 보임
</code></pre></div></div> <hr/> <p>반대로 RLVR 중 code reasoning을 억제하면, Qwen2.5-Math-7B에서 성능이 줄어들고, 다른 models에서는 향상될 수 있음.</p> <p>위 Hypothesis에 대한 대우명제를 검증하는 실험을 진행함</p> <ul> <li> <p>대우: <em>“Code reasoning에 penalty를 주면 spurious reward로 인한 성능 향상이 감소할 수 있음”</em></p> </li> <li> <p>실험 세팅으로는 compound rewards를 설계함 (아래 두가지 조건 모두 만족할 때 reward 부여)</p> <ol> <li> <p>원래의 spurious rewards를 만족</p> </li> <li> <p>response에 “python” string이 없을 때</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>format reward와 no-code reward가 compound된 Figure (a)를 보면, Qwen2.5-Math-7B는 성능이 하락함 → <strong>hypothesis와 일치한 결과</strong> (<strong>웃긴게 여기만 제대로 일치함 ㅋㅋ</strong>)</p> </li> <li> <p>Incorrect reward에서 MATH-500은 Compound reward와 Original reward가 비슷한 수준으로 성능이 나오는 반면, 더 어려운 task인 AMC에서는 성능 향상의 폭이 줄어듬</p> </li> </ul> <p>→ code reasoning을 제거하면, spurious reward의 performance gain이 줄어듬</p> <ul> <li>ground truth reward에서는 성능 향상을 보여주는데, 이는 앞에서 code reasoning frequency가 줄어듬에 따라 real knowledge를 배워 성능이 개선되는 결과와 consistent함</li> </ul> <p>(<strong>???</strong>: 그런데, Incorrect reward와 GT reward에서 Qwen-Math-7B trend는 비슷한거 아닌가..? 최종 acc까지 비슷한 수준임. Format reward하고 일관된 trend가 보이지 않음..)</p> <ul> <li>Bad-code models (Qwen2.5-7B, OLMo2-7B-SFT)는 Compound reward가 original reward보다 더 높은 성능을 보여줌 → 이는, bad-code models이 weak code reasoning을 보이기 때문에, <strong>Compound reward가 model이 못하는 behavior의 weight를 명시적으로 낮춰 학습을 유도함</strong>.</li> </ul> <hr/> <h3 id="44-the-curious-cases-training-signals-from-incorrect-rewards-and-random-rewards">4.4 The Curious Cases: Training Signals from Incorrect Rewards and Random Rewards</h3> <p>그러면 Spurious Rewards가 어떻게 RLVR에서 meaningful training signal을 만들어내는가?</p> <hr/> <p><strong>Incorrect Rewards</strong></p> <p>저자들은 incorrect rewards가 effective training signals을 만드는 다음 두 가지 포인트를 가정함</p> <ol> <li> <p><em>많은 incorrect labels이 ground truth 값에 가까운 값으로 남아 있어, 대체로 올바른 reasoning에 positive reinforcement를 제공함</em></p> </li> <li> <p><em>Incorrect labels may function like format rewards</em></p> </li> </ol> <ul> <li>models은 생성된 response를 성공적으로 추출하고 평가하지 않으면 reward를 줄 수 없기 때문에, 일정 수준의 correct reasoning이 필요함</li> </ul> <p>→ 말로 가정만 하고… 정작 이를 검증하는 실험이 없네요….ㅋㅋ</p> <hr/> <p><strong>Random Rewards</strong></p> <p>혹자는 Rewarded answers 다수가 correct하기 때문에, Qwen2.5-Math의 성능이 개선되었다 볼 수 있음.</p> <p>→ GRPO는 reward의 평균을 0으로 normalize하기 때문에, rewarded answers가 대부분 정답이라면, penalty answers도 대부분 correct하기 때문에 (penalized responses 중에도 correct 다수 포함), 위에 말은 틀림.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Random rewards with varying probabilities consistently improve performance</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>GRPO training에서 *Bernoulli<strong>(\gamma)</strong> *variable로 random rewards를 보여함</p> </li> <li> <p>\gamma 가 0이 아닐 때는, 유의미한 성능 향상을 보여줬고 (15~20% 향상), 0일 때는 constant rewards가 learning signal을 만들지 않아 개선이 없음.</p> <ul> <li>\gamma가 0이면, Reward가 0이고, 그럼 모든 rollout의 Advantage 역시 0이고 → gradient=0이라 학습x</li> </ul> </li> </ul> <p><strong>GRPO clipping bias can induce random reward training signals</strong></p> <p>GRPO는 gradient updates에서 reward에 대해 normalized group-relative advantage를 계산함. 이때, batch rollouts에 대해 normalize를 하기 때문에, advantage의 expectation은 0임.</p> <p>→ 그런데, GRPO의 clipping mechanism 때문에, 실제로 advantage의 expectation은 0이 아님.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>\rho<em>t=\pi</em>{\theta}(y)/\pi_{old}(y) → 1-\epsilon &lt; \rho_t &lt; 1 + \epsilon</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>확률이 높은 token에 대해서는 clip에 걸리는 경우가 거의 없어, update는 대부분 +방향으로 이루어짐</p> </li> <li> <p>확률이 낮은 token에 대해서는 범위가 좁아 clip에 쉽게 걸려, 조금만 확률을 늘리면 penalty (-gradient)가 들어옴 → 낮은 확률 token에 대해서는 키우기가 어려움</p> <ul> <li> <p><strong>Example</strong></p> <ul> <li> <p>\pi_{old}(y)=0.85, \epsilon=0.2</p> <ul> <li>clip range = [0.85 x 0.8, 0.85 x 1.2] = [0.68, 1.02]</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>→ prob은 1을 넘을 수 없어 실제 상한은 1.0이므로 구간의 폭은 0.32</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - \pi_{old}(y)=0.02,\epsilon=0.2

    - clip range = [0.02 x 0.8, 0.02 x 1.2] = [0.016, 0.024]
</code></pre></div></div> <p>→ 구간 폭은 0.008 (매우 좁기때문에, 조금만 확률이 늘어나도 clip에 걸려 gradient가 0이 되거나 penalty 발생 → 확률을 키우기 어려움)</p> <p>→ clipping으로 인해 비대칭이 생겨, advantage expectation은 0이 아니고, model이 원래 자주 생성하는 token들로 probability mass가 쏠리는 현상 발생 (<strong>model의 prior knowledge를 강화하는 방향으로 bias 생김</strong>)</p> <ul> <li> <p>높은 token 확률은 clipping에 잘 안걸려 → +gradient 누적</p> </li> <li> <p>낮은 token 확률은 clipping에 잘 걸려 → -gradient 누적</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ 즉, random reward에서도 <strong>clipping bias가 prior knowledge에 기반한 behavior를 강화함</strong> (아래 실험 결과로도 보여줌)</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Dapo: An open-source llm reinforcement learning system at scale, 2025</p> <ul> <li>위 연구에서도 clipping bais가 RLVR에서 exploration을 줄이고 exploitation을 늘린다는 것을 발견함</li> </ul> <p>추가로 GRPO에서 clipping effect를 실험적으로 검증을 함</p> <ul> <li> <p>clipping 효과를 제거하기 위해, clipping bias를 없애는 설계를 구현함</p> <ol> <li> <p>loss calculation에서 clipping bias를 직접 제거</p> </li> <li> <p>training 및 rollout batch size를 조정하여 \pi<em>{\theta}=\pi</em>{old}를 보장하며, clipping이 발생하지 않도록 함</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Random w. Clipping Enable (GRPO)에서 random reward가 code reasoning frequency를 높임.</p> </li> <li> <p>w. Clipping Disabled case를 보면, 이러한 reasoning pattern trend가 사라짐</p> </li> </ul> <p>→ Clipping을 사용하면, 이에 따라 성능 향상으로 이어짐</p> <hr/> <h2 id="5-conclusion">5. Conclusion</h2> <p>본 연구에서는 Weak / Spurious Rewards를 사용한 RLVR이 Qwen-2.5-Math에서 유의미한 성능 개선을 보여주는데, 이는 existing reasoning patterns을 증폭함으로써 이루어진다는 것을 설명함.</p> <p>본 연구의 다양한 실험을 통해 아래 3가지 주요 포인트를 강조함.</p> <ol> <li> <p>Base model pretraining은 RLVR 결과에 큰 영향을 미침</p> </li> <li> <p>Spurious supervision도 유의미한 existing behaviors를 촉발할 때 reasoning을 향상시킬 수 있음</p> </li> <li> <p>특정 model 계열의 결과가 다른 계열의 model로 일반화 되지 않음</p> </li> </ol> <p>본 연구가 제시하는 기준은 다음과 같음</p> <ol> <li> <p>서로 다른 pretraining distribution을 가진 여러 model에 대해서 RLVR을 검증</p> </li> <li> <p>RL을 평가할 때, 다양한 reward를 baseline으로 두어 비교</p> </li> </ol> <h3 id="아쉬운점">아쉬운점..?</h3> <ul> <li> <p>전반적으로 잘 설계된 많은 실험과 흥미로운 결과들을 보여줘서 인정하지만, 몇몇 부분에서는 다소 비약이 있다고 생각함.</p> </li> <li> <p>Spurious Reward가 다른 계열의 모델에서는 잘 나타나지 않고, Qwen2.5-Math에서만 강하게 나타나는 것으로 보아, 해당 계열 model의 특수성일 수 있음. 즉, RLVR이 latent capability를 trigger한다는 주장은 일반화하기 어려움</p> </li> <li> <p>random reward가 왜 작동하는지 설명하는 부분에 있어서, 변인통제가 제대로 이루어지지 않음. 다른 bias로 인해 영향을 받았을 수도 있음</p> </li> <li> <p>Math 이외의 domain (ex. code, language reasoning) tasks에 대해서 어떻게 나타는지 분석이 아쉬움</p> </li> <li> <p>실험 전반의 RLVR training steps이 50~100 steps에서 saturation이 이러나는데, long-term stability 검증이 부족함 (catastrophic forgetting 혹은 overfitting issue에 어떻게 되는지)</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><category term="reinforcement-learning"/><category term="rlvr"/><category term="vision"/><summary type="html"><![CDATA[논문 리뷰 - RLVR 관련 연구]]></summary></entry><entry><title type="html">ON THE EXPRESSIVENESS OF SOFTMAX ATTENTION: A RECURRENT NEURAL NETWORK PERSPECTIVE</title><link href="https://unknownnlp.github.io/blog/2025/on-the-expressiveness-of-softmax-attention-a-recurrent/" rel="alternate" type="text/html" title="ON THE EXPRESSIVENESS OF SOFTMAX ATTENTION: A RECURRENT NEURAL NETWORK PERSPECTIVE"/><published>2025-08-12T00:00:00+00:00</published><updated>2025-08-12T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/on-the-expressiveness-of-softmax-attention-a-recurrent</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/on-the-expressiveness-of-softmax-attention-a-recurrent/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-12</li> <li><strong>Reviewer</strong>: Jaewon Cheon</li> <li><strong>Property</strong>: Efficient Transformer</li> </ul> <hr/> <hr/> <h3 id="선정-이유">선정 이유</h3> <ul> <li> <p>잘 쓴 논문은 아님; Rather Report 느낌</p> <ul> <li>오히려 논문으로서는 이런 논문들이 Contribution은 더 많음</li> </ul> </li> <li> <p>Research Field 소개에는 나쁘지 않은 것 같아서..</p> </li> </ul> <hr/> <h2 id="0-prerequisites">0. Prerequisites</h2> <blockquote> <p>RQ: <strong>RNNs(Linear Attention)</strong>은 왜 <strong>Attention</strong>보다 LM 성능이 낮을까?</p> </blockquote> <p>Prerequisites:</p> <ul> <li> <p>RNNs(Linear Attention) ?</p> </li> <li> <p>Isn’t RQ seems so obvious ?</p> </li> <li> <p>Is Attention - Softmax = RNNs(Linear Attention) ?</p> </li> <li> <p>RNNs(Linear Attention)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Inference Cost는 TF-based Language Models의 가장 큰 내재적 한계들 중 하나</p> <ul> <li> <p>KV Cache Complexity = O(N)</p> </li> <li> <p>Attention Complexity(per query) = O(N)</p> </li> </ul> </li> <li> <p>RNNs(Mamba, RWKV, Titans, …)는 Sequential Information을 고정된 State에 저장 → 가장 직관적인 한계 극복 방법</p> <ul> <li> <p>KV Cache Complexity = O(1)</p> </li> <li> <p>Attention Complexity(per query) = O(1)</p> </li> </ul> </li> <li> <p>But, Training Parallelism 및 Weak Performance가 해결해야 할 과제</p> <ul> <li> <p>Training Parallelism → 꽤 많이 해결되었음</p> </li> <li> <p>Performance → 근본적으로 해결이 안 될 것처럼 보임</p> </li> </ul> </li> <li> <p>Is Attn vs RNNs a obvious fight?</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>위와 같은 Intuition이 있음</p> <ul> <li> <p>Attention: q에 따라 필요한 정보를 모든 KV들에 대해 선택적으로 가져옴</p> </li> <li> <p>RNNs: q가 무엇이든 고정된 S에서만 정보를 가져올 수 있음</p> </li> </ul> </li> <li> <p>Attention w/o Softmax is RNN!</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Line 1: Attend Property는 (q<em>tK^T</em>{1:t})를 하기 때문에 발생</p> </li> <li> <p>Line 2 : Softmax 연산을 빼더라도, 여전히 q<em>t가 모든 K</em>{1:t}, V_{1:t}에 Attend하는 것은 동일</p> </li> <li> <p>Line 3 : Associativity</p> </li> <li> <p>Line 5 : 이는 고정된 State S_t에서 정보를 가져오는 것과 같음</p> </li> <li> <p>Line 4: Moreover, 이 S_t는 각 시점 1…t에서 만들어지는 KV의 Outer Product로 Update됨</p> <ul> <li>굉장히 간단한 RNN Update Rule</li> </ul> </li> <li> <p>즉, Softmax(유일한 Non-linearity)가 없는 Attention은 RNN → Linear Attention이라 명명</p> </li> <li> <p><strong>제 Attend Property는 어디갔나요?</strong></p> <ul> <li>Attention</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Linear Attention
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Attention과 RNN의 차이는 위와 같은 Intuition에서 오는 것이 아닌, Softmax에서 오는 것</p> <ul> <li>Challengeable</li> </ul> </li> </ul> <h2 id="1-intro">1. Intro</h2> <blockquote> <p>RQ: <strong>RNNs(Linear Attention)</strong>은 왜 <strong>Attention</strong>보다 LM 성능이 낮을까?</p> </blockquote> <p>Prerequisites:</p> <ul> <li> <p>RNNs(Linear Attention) ?</p> </li> <li> <p>Isn’t RQ seems so obvious?</p> </li> <li> <p>Is Attention - Softmax = RNNs(Linear Attention)?</p> </li> </ul> <h2 id="2-recurrent-style-softmax">2. Recurrent-style Softmax</h2> <ul> <li> <p>Objective : Softmax를 RNNs로 reformulate ⇒ 기존 RNNs의 요소들과 비교, Weakness 파악</p> </li> <li> <p>Softmax를 Approximate하지 않고, Linearize할 수는 없을까?</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>e^{Q\cdot K^T}연산을 분리해야 함 → 보통 아래와 같이 Kernel Trick을 써서 표현</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>이 때, Exponential에 Taylor series expansion을 사용하면 Approximate 없는 Linearize 가능</p> <ul> <li>Softmax를 RNN화했을 때, 어떤 일이 일어나는가?</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- **증명**

  - Taylor Expansion
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Prove
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>\otimes: n차 tensor product</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>결국 Recurrent Softmax는…</p> <ul> <li> <p>e^{Q⋅K}의 테일러 전개로부터 도출되는 1차항부터 무한차항까지의 RNN을 전부 더한 연산</p> </li> <li> <p>각 RNN 출력에는 계수 \frac{1}{n!}가 곱해짐</p> </li> <li> <p><strong>각 RNN의 hidden state 차원</strong></p> <ul> <li> <p>n차 항의 hidden state 크기: \mathbb{R}^{d^n\times e}</p> </li> <li> <p>d = K(Q) 차원, e = V 차원</p> </li> </ul> </li> </ul> </li> <li> <p>이는 아래와 같은 두 Term으로 나뉨</p> <ul> <li> <p>Q, K 벡터의 Infini-Sum with 높은 차수에 대해서는 매우 낮은 가중치를 곁들인… → 분자</p> </li> <li> <p>위 연산들의 합에 대한 Inverse G_t → 분모</p> </li> </ul> </li> </ul> <h2 id="4-numerator">4. Numerator</h2> <ul> <li> <p>분자의 Infini-Sum 중, n=1만 사용하면?</p> <ul> <li>Linear Attention과 동일!</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>결국 (Primitive) Linear Attention은 Softmax Attention의 First-order approximation이었던 셈</p> </li> <li> <p>무한 개의 State를 사용하는 것과 같은 Softmax Attention에 비해 하나의 State만 사용하고 있는 것으로 해석</p> </li> <li> <p>Or, Softmax Attention의 고차항 Interaction을 무시하는 Approximation으로도 해석할 수 있음</p> </li> <li> <p>그럼, 고차항의 Interaction과 그에 대한 State를 추가하면, RNN의 성능이 좋아지는가?</p> <ul> <li>⇒ YES!</li> </ul> </li> </ul> <h2 id="5-denominator">5. Denominator</h2> <ul> <li>G_t는 Recurrent Architecture에서 동치되는 무언가를 찾기 애매</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Original G_t는 Attention score의 Exact Normalization를 수행</p> <ul> <li> <p>Score의 확률분포화 ⇒ 크게 중요하지 않을 수도…? (i.e. Quite Attention, …)</p> </li> <li> <p>Output에 대해 곱해지는 어떤 값이라고 일반화를 하여 취급 ⇒ 비슷한 역할을 하는 요소가 존재</p> </li> </ul> </li> <li> <p>G_t as a Gate</p> <ul> <li>보통 RNNs에는 Output의 크기를 조절하는 Learnable Gate가 존재</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>하지만 결정적으로, Sequence length가 길어짐에 따라(많은 Vs가 추가됨에 따라) exploding을 막을 수 없다는 점에서 기존의 G_t와 다름</p> </li> <li> <p>G_t as a Norm</p> <ul> <li> <p>Adaptive Gating이 중요하다기보다는, exploding을 막을 수 있게 Normalize를 하는 것이 중요할 수 있음</p> <ul> <li>C.f. Normalize score vs Normalize output</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>그럼, G_t를 Attention Score의 Summation이 아니라, 거리 기반의 Normalization Term으로 바꿔도 Softmax의 성능이 유지되는가?</p> <ul> <li>⇒ YES!</li> </ul> </li> </ul> <h2 id="6-results">6. Results</h2> <ul> <li> <p>실험 세팅</p> <ul> <li> <p>Model: Llama-2 사용</p> <ul> <li> <p>사실 Setting만 사용</p> </li> <li> <p>300M ~ 2B</p> </li> </ul> </li> <li> <p>Dataset:</p> <ul> <li> <p>Pile</p> </li> <li> <p>Slimpajama</p> </li> <li> <p>Fineweb</p> </li> </ul> </li> <li> <p>Training</p> <ul> <li>NTP from scratch</li> </ul> </li> <li> <p><strong>그 외</strong></p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="g_t-as-a-norm">G_t as a Norm</h3> <ul> <li> <p>Numerator: exponential로 고정</p> </li> <li> <p>Denominator: Softmax(기존) / Norm(L2) / Gate(Learnt)</p> </li> <li> <p>Model size: 300M(…)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Gate의 경우 학습이 Unstable하고, 성능도 Softmax에 비해 많은 차이가 남</p> </li> <li> <p>Norm은 학습도 Stable, 성능도 Softmax와 비슷</p> <ul> <li>Softmax Denominator == Norm operator라고는 할 수 없겠지만, 적어도 오늘날 많이 사용되는 Gate로는 부족하다는 말을 할 수 있음</li> </ul> </li> <li> <p><strong>Scaling을 했을 때, 이러한 경향은 더 심해짐</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="taylor-series-expansion">Taylor series Expansion</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-on-the-expressiveness-of-softmax-attention-a-recurrent/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Order가 추가될수록 Full Softmax의 원래 성능을 회복하는 것을 확인</p> <ul> <li>10th Order 정도면 완벽히 성능 복원이 되는 것을 확인할 수 있음</li> </ul> </li> <li> <p>물론, 그렇다고 해서 10th Order Update을 구현하는 것은 Infeasible하겠지만…</p> <ul> <li>적어도 Full Softmax(O(N))에 Comparable한 Recursive formula(O(1))를 볼 수 있는 것은 신기한 부분</li> </ul> </li> </ul> <h2 id="7-conclusion">7. Conclusion</h2> <ul> <li> <p>Softmax는 무한차수의 RNN Summation + Norm Denominator이다! (Roughly)</p> </li> <li> <p>Limitation</p> <ul> <li> <p>Up to date RNNs들에 적용했을 때의 결과가 없음</p> </li> <li> <p>Model size와 Downstream task에서 제한되어있는…</p> </li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="efficient transformer"/><category term="language-model"/><category term="neural"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰 - Efficient Transformer 관련 연구]]></summary></entry><entry><title type="html">The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective</title><link href="https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward/" rel="alternate" type="text/html" title="The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective"/><published>2025-08-12T00:00:00+00:00</published><updated>2025-08-12T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-12</li> <li><strong>Reviewer</strong>: 준원 장</li> <li><strong>Property</strong>: Reinforcement Learning, Reward Model</li> </ul> <hr/> <hr/> <h1 id="the-accuracy-paradox-in-rlhf-when-better-reward-models-dont-yield-better-language-models">The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models</h1> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>우리는 왜 RL을 통해 alignment를 할까?</p> </li> <li> <p>SFT suffers from exposure bias</p> </li> <li> <p>SFT lacks the ability to optimize for sequence-level rewards</p> </li> <li> <p>RQ &amp; Our Common Myth</p> </li> <li> <p>더 Accurate을 줄 수 있는 RM이 더 effectiveness한 RM을 만들 수 있다.</p> </li> <li> <p>논문은 여러 실험을 통해 이를 반박하고자 함.</p> </li> </ul> <p>⇒ moderate한 accuracy를 가진 RM이 the most accuracy를 가진 RM보다 LM performance를 더 향상시킨다.</p> <p>⇒ RM accuracy랑 LM final performance는 correlation이 없다.</p> <h2 id="2-motivation-and-problem-setting--recap-rlhf-formula">2. Motivation and Problem Setting &amp; Recap RLHF Formula</h2> <h3 id="motivation">Motivation</h3> <ul> <li>LM performance를 maximize할 수 있는 reward model의 optimal accuracy range가 존재한다고 가정</li> </ul> <h3 id="rlhf-formula">RLHF Formula</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>triplet: <code class="language-plaintext highlighter-rouge">(x, y_w, y_l)</code></p> <ul> <li> <p>accepted response score: <code class="language-plaintext highlighter-rouge">s_w = r_θ(x, y_w)</code></p> </li> <li> <p>rejected response score: <code class="language-plaintext highlighter-rouge">s_l = r_θ(x, y_l)</code></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="problem-setting">Problem Setting</h3> <ul> <li> <p>RM strength가 LM performance에 미치는 영향을 탐색</p> <ul> <li>metric</li> </ul> </li> <li> <p>factuality</p> </li> <li> <p>relevance</p> </li> <li> <p>completeness</p> </li> <li> <p>P<em>{LM} = f (S</em>{RM}, τ )</p> <ul> <li> <p>P_{LM}: LM acc on task</p> </li> <li> <p>S_{LM}: RM acc on RM binary task</p> </li> <li> <p>τ: RL training time</p> </li> </ul> </li> </ul> <h2 id="3-experiment-and-results">3. Experiment and Results</h2> <h3 id="experimental-setting">Experimental Setting</h3> <ul> <li> <p>Models</p> <ul> <li> <p>LM: T5 (small, base, large)</p> </li> <li> <p>RM: Longformer-base-4096</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Datasets</p> <ul> <li>QA-FEEDBACK (3,853/500/948)</li> </ul> </li> </ul> <p>(Q, Gold, non-fact, …)가 존재</p> <ul> <li> <p>Training</p> <ul> <li> <p>PPO</p> </li> <li> <p>RM list</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Critic LM: T5-base</p> </li> <li> <p>Reward hacking 방지를 위해 KLD(↔ ref LM)가 너무 커지면 training interrupt (약간의 휴리스틱)</p> </li> </ul> <h3 id="results">Results</h3> <h3 id="are-high-accuracy-and-deeply-trained-reward-models-always-the-best">Are High-Accuracy and Deeply Trained Reward Models Always the Best?</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>moderate accuracy와 appropriate number of trained steps로 학습된 RM이 높은 LM performance로 이어진다.</p> <ul> <li> <p>relevance: mitigating the risk of overfitting</p> </li> <li> <p>factuality: prevent overfitting and ensure reliable outcomes</p> </li> </ul> </li> </ul> <p>consistent across the T5-base and T5-large models</p> <h3 id="how-do-best-and-most-accurate-reward-models-differ">How Do Best and Most Accurate Reward Models Differ?</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Relevance</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">high-variance</code></p> <ul> <li>Factuality</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">less-variance</code></p> <ul> <li>Completeness</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">low-score</code>, <code class="language-plaintext highlighter-rouge">high-variance</code></p> <p>⇒ 공통적으로 모든 task에 대해서 best-performance RM은 variance가 높음.</p> <p>직관적으로 생각해보면, 이 말은 곧 RM이 broader range of responses에 대한 평가를 가능하게 함 := exploration ⇒ improving the quality of the generated text</p> <p>(개인적으로, variance가 클수 밖에 없는 verifiable reward가 이래서 LM performance가 좋았나..라는 생각이 들음)</p> <h3 id="how-do-best-and-most-accurate-rewards-impact-models-ie-role-of-kld">How Do Best and Most Accurate Rewards Impact Models? (i.e., Role of KLD)</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Relevance</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">low-score</code>, <code class="language-plaintext highlighter-rouge">low-variance</code></p> <p>relevance측면에서 stable alignment</p> <ul> <li>Factuality</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">low-variance</code></p> <p>factuality 측면에서 consistent yet varied alignment (ref가 틀린 사실을 말하고 있다면)</p> <ul> <li>Completeness</li> </ul> <p>→ <code class="language-plaintext highlighter-rouge">high-score</code>,<code class="language-plaintext highlighter-rouge">high-variance</code></p> <p>flexible approach suitable for evaluating complex texts (이건 해석이 좀..)</p> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li>RM을 평가할때 accuracy 자체로만 평가하는 것의 한계를 실험적으로 명확히 보여준 논문</li> </ul> <h1 id="what-makes-a-reward-model-a-good-teacher-an-optimization-perspective">What Makes a Reward Model a Good Teacher? An Optimization Perspective</h1> <h2 id="1-introduction-1">1. Introduction</h2> <ul> <li> <p>이전 논문에서 RL 관점에서 RM의 성능을 평가할 때 accuracy만으로 판단하는 것의 한계를 지적하며, variance이 높은 RM이 오히려 더 나은 policy model performance으로 이어질 수 있음을 보여주었다.</p> </li> <li> <p>이 논문은 이 논의를 확장해 다음의 질문에 대한 대답을 하고자 함.</p> </li> </ul> <p><strong><em>“what makes a reward model a good teacher for RLHF?”</em></strong></p> <p>(수학적으로 많은 증명들이 있지만, 차치하고 논문에서 이야기하고 싶은 바는 아래와 같다.)</p> <ul> <li> <p>\pi_{\theta} (policy)에서 충분히 높은 확률로 rollout한 output에 대해서 얼만큼 잘 구분하는가 = reward variance</p> </li> <li> <p>r_G (ground truth reward: 우리가 올려야하는 reward)</p> </li> <li> <p>r_{rm} (proxy reward: policy model에 의해 학습되는 reward)</p> </li> </ul> <p>⇒ low reward variance는 policy gradient로 학습시 r_{rm}뿐만 아니라 r_G도 굉장히 느리게 update하게 만든다.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="2-preliminaries">2. Preliminaries</h2> <p><strong>Reward model training or selection</strong></p> <ul> <li> <table> <tbody> <tr> <td>우리의 목표: E<em>{y∼π</em>θ (·</td> <td>x)}[r_G(x, y)]</td> </tr> </tbody> </table> </li> </ul> <p>maximize the exp. ground truth reward</p> <ul> <li>Proxy 목표: r_{RM} : X × Y → [−1, 1]</li> </ul> <p><strong>Reward maximization via policy gradient</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Accuracy in RM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>RM에서의 accuracy란 r<em>G랑 r</em>{rm} 이 같은 ranking으로 예측하고 있냐?이다.</p> </li> <li> <p>즉,</p> </li> <li> <p>r_{G}: 0.9 (x1) &gt; 0.5 (x2) &gt; 0.2 (x3)</p> </li> <li> <p>r_{rm}: 0.54 (x1) &gt; 0.51 (x2) &gt; 0.49 (x3)</p> </li> </ul> <p>이면 r_{rm}의 acc는 1.0이다.</p> <ul> <li>일반적으로 RM의 acc는 off-policy bencmark (e.g., HH test set)에서 이루어짐. 그러나 RM이 적용되는 시점은 on-policy 시점 ⇒ 논문은 이를 모두 고려해서 분석을 진행.</li> </ul> <p><strong>Reward Variance in RM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>\pi_{\theta} (policy)하에서 발생하는 rollout을 RM이 얼만큼 잘 구분하는가?</li> </ul> <h2 id="3-theory-optimization-perspective-on-what-makes-a-good-reward-model">3. Theory: Optimization Perspective on What Makes a Good Reward Model</h2> <h3 id="technical-setting">Technical Setting</h3> <ul> <li>논문에서 증명을 위해 정의한 policy의 generation 식</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="low-reward-variance-implies-slow-reward-maximization">Low Reward Variance Implies Slow Reward Maximization</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>초기 reward 대비 \gamma만큼 기대보상을 올리는데 걸리는 시간 t는 reward variance의 -1/3 제곱에 비례한다.</li> </ul> <p>(논문에 증명있습니다!)</p> <ul> <li>reward variance 🔽 → RLHF loss 고차미분 🔽 = hessian 🔽 → gradient norm이 커지는거 방지 → 학습 방해</li> </ul> <h3 id="more-accurate-reward-models-are-not-necessarily-better-teachers">More Accurate Reward Models Are Not Necessarily Better Teachers</h3> <ul> <li> <p>어떤 초기 policy \pi_{\theta(0)}에 대해서도,</p> <ul> <li> <p>acc =1인 완벽한 보상 모델 r_{\mathrm{RM}} 존재</p> </li> <li> <p>acc ≤2/∣Y∣ 인 매우 부정확한 보상 모델 r’_{\mathrm{RM}} 존재</p> </li> </ul> </li> <li> <p>그런데 r<em>{\mathrm{RM}}을 쓰면 t</em>\gamma가 <strong>무한히 커질 수 있음</strong> (학습이 극도로 느림)</p> </li> <li> <table> <tbody> <tr> <td>반면 r’<em>{\mathrm{RM}}을 쓰면 t</em>\gamma = O(\pi_{\theta(0)}(y^\gamma</td> <td>x)^{-1})로 훨씬 짧을 수 있음</td> </tr> </tbody> </table> </li> </ul> <p>(전재 조건은 그래도 r’<em>{\mathrm{RM}}(x,y^\gamma) &gt; r’</em>{\mathrm{RM}}(x,y))</p> <h2 id="4-experiments">4. Experiments</h2> <ul> <li> <p>Ground truth reward.</p> </li> <li> <p>ArmoRM라는 모델이 주는 reward가 gt reward라고 가정</p> </li> <li> <p>Data.</p> </li> <li> <p>UltraFeedback (80: RM tr / 20: policy gradient)</p> </li> <li> <p>Ref.</p> </li> <li> <p>Pythia2.8B → AlpacaFarm SFT</p> </li> <li> <p>Reward model</p> </li> <li> <p>On-Policy Data: 100%, 75%, 50%, 25%, 0% on-policy data sampling해서 ArmoRM로 labeling.</p> </li> <li> <p>Off-Policy Data: UltraFeedback</p> </li> <li> <p>Policy Gradient</p> </li> <li> <p>RLOO</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Reward variance가 높을수록 proxy reward는 빠르게 증가함.</li> </ul> <p>하지만, reward variance가 높은 RM이라고 하더라도 해당 RM의 본질적인 불안정성 (GT의 acc를 정확하게 반영하지는 못함 = Reward Hacking)이 있기에 Ground Truth reward만큼을 True Reward를 못올림</p> <p>⇒ Epoch3가 Reward Hacking 지점</p> <ul> <li> <p>Accuracy가 높고 Reward variance가 낮은 RM은 학습도 느리며 실제 Ground Truth reward도 그렇게 많이 올리지는 못함</p> </li> <li> <p>우리가 ‘Ground Truth reward’를 100%로 반영하는 RM을 반영할 수 없으니 실제로는 reward variance가 높은 RM으로 on-policy training하고 끊어주는건 좋은 RL optimization이다.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Corr을 보면, reward variance는 학습하고자 하는 proxy말고 ground truth를 target하기에도 좋은 feature</p> </li> <li> <p>우리가 자주 봐왔던 off-policy acc (e.g., HH-test set)은 RL시 RM이 도움되는가?에 대한 대답이 되지 못함</p> </li> <li> <p>마지막 지표는 ‘initial policy에 대한 accuracy와 off-policy dataset에 대한 accuracy를 reward variance로 평균낸거’라는데 해석은 잘 못했습니다…</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>initial policy의 output에 대한 reward variance를 충분히 크게 하는 RM을 사용해야 한다.</li> </ul> <h2 id="5-conclusion">5. Conclusion</h2> <ul> <li> <p>on-policy training 상황에서 off-policy accuracy만으로 RM의 effectiveness를 평가하면 optimization에 악영향을 끼침을 보임</p> </li> <li> <p>reward variance는 RM의 optimization를 미리 가늠해볼 수 있는 좋은 지표</p> </li> </ul> <p>→ ground truth reward를 보장해주지는 않음.</p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="alignment"/><category term="language-model"/><category term="paper-review"/><category term="reinforcement learning"/><category term="reward model"/><category term="rlhf"/><summary type="html"><![CDATA[논문 리뷰 - Reinforcement Learning, Reward Model 관련 연구]]></summary></entry><entry><title type="html">What Makes a Reward Model a Good Teacher? An Optimization Perspective / The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models</title><link href="https://unknownnlp.github.io/blog/2025/what-makes-a-reward-model-a-good-teacher/" rel="alternate" type="text/html" title="What Makes a Reward Model a Good Teacher? An Optimization Perspective / The Accuracy Paradox in RLHF: When Better Reward Models Don’t Yield Better Language Models"/><published>2025-08-12T00:00:00+00:00</published><updated>2025-08-12T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/what-makes-a-reward-model-a-good-teacher</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/what-makes-a-reward-model-a-good-teacher/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-12</li> <li><strong>Reviewer</strong>: 준원 장</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <hr/> <hr/> <h2 id="1-intro">1. Intro</h2> <h2 id="2-method">2. Method</h2> <h2 id="3-how-it-works">3. How It Works?</h2> <h2 id="4-experiments">4. Experiments</h2> <h2 id="7-conclusion">7. Conclusion</h2>]]></content><author><name></name></author><category term="paper-reviews"/><category term="language-model"/><category term="paper-review"/><category term="reinforcement learning"/><category term="rlhf"/><summary type="html"><![CDATA[논문 리뷰 - Reinforcement Learning 관련 연구]]></summary></entry><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p>→ <strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></p> <p><strong>BD3-LMs</strong></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>\Delta^V: simplex 공간</p> </li> <li> <p>m \in \mathcal{V}: [MASK] token’s one-hot vector</p> </li> </ul> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></h3> <ul> <li> <p>p: denoising, q: noising</p> </li> <li> <p>s(j) = (j-1)/T, t(j) = j/T (이후에 j는 생략!)</p> <ul> <li>알파벳 순서대로 s가 앞 t가 뒤, s → t 과정은 noise를 더하는 과정</li> </ul> </li> <li> <p>D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</p> <ul> <li> <p>Q_t의 예시</p> <ul> <li> <p>Uniform replacement</p> </li> <li> <p><strong>Masking 기반</strong>: \beta_t 확률로 [MASK] 토큰으로 변경</p> </li> </ul> </li> </ul> </li> <li> <p>이상적인 diffusion model p<em>{\theta}는 q의 역방향이므로 D3PM에서는 아래 수식으로 p</em>{\theta}를 정의</p> <ul> <li> <p>1단계 denoising 과정 = 개별 토큰 위치에 대한 denoise는 독립 과정 = x^\ell 근사</p> </li> <li> <p>x^{\ell} (원본 텍스트)가 주어진다면 q를 활용해 x_t^\ell \rightarrow x_s^\ell을 완전히 복구할 수 있음.</p> </li> <li> <p>denoise 과정에서 x^\ell이 주어지지 않으므로 p로 근사: p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right)</p> </li> </ul> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><strong>Block Definition</strong></p> <ul> <li> <p>길이 L’이 되게 B개의 block으로 만들기 (x^b: x^{(b-1)L’:bL’} \in {1,…,B})</p> </li> <li> <p>Likelihood over block</p> </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><strong>Learning Objective</strong></p> <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><strong>Denoiser model</strong></p> <ul> <li> <table> <tbody> <tr> <td>Transformer x<em>\theta를 사용해 파라미터화: p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b})</td> </tr> </tbody> </table> <ul> <li> <p>given x^{&lt;b}: AR 특성 유지</p> </li> <li> <p>x^b 예측: Denosing</p> </li> </ul> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>x<em>\theta의 학습: block b 내에서 x</em>\theta^b(x_t^b, x^{&lt;b}) → L’ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><strong>K, V caching</strong></p> <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><strong>Training</strong></p> <ul> <li>모든 block은 x_\theta의 forward pass를 두 번 거쳐야 함 (x_t^b, x^b) → 계산의 효율화 필요</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input x_{t_b}^b 생성</p> </li> <li> <p>\left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}): 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 x^b_{\text{logit}} 계산</p> </li> </ol> <ul> <li> <p>Naive: B-times loop를 이용해 forward pass를 별도로 진행</p> </li> <li> <p>Vectorized 방식</p> <ul> <li> <p>x<em>{\text {noisy }}=x</em>{t<em>1}^1 \oplus x</em>{t<em>2}^2 \oplus \cdots \oplus x</em>{t_B}^B</p> </li> <li> <p>x_{\text{noisy}} \oplus x을 input으로 하여 한 번에 계산 How? attention mask를 이전 block만 조회하게끔 조절</p> </li> </ul> </li> </ul> <p><strong>Sampling</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> <ul> <li>\alpha_0=1 → linear scheduler→ \alpha_1=0</li> </ul> </li> <li> <p>목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</p> <ul> <li> <p><strong>Zero Masking Probabilities</strong>: clean sequence( x^\ell)에는 mask를 포함하지 않음. (이건 아래의 조건을 위해 필요한듯합니다.)</p> </li> <li> <p><strong>Carry-Over Unmasking</strong>: x_t^\ell \neq m인 경우 q\left(x_s^l=x_t^l \mid x_t^l \neq m\right)=1. 즉, unmaksed된 token은 다시 mask 되지 않음.</p> <ul> <li>Denoising model 단순화: p_\theta\left(x_s^{\ell}=x_t^{\ell} \mid x_t^{\ell} \neq m\right)=1</li> </ul> </li> <li> <p>\alpha<em>t = \prod</em>{\tau=1}^{t}(1 - \beta_\tau): t시점까지 mask되지 않고 살아남을 확률</p> </li> <li> <p><strong>why?</strong></p> <ul> <li> <p>t 시점에서 mask transition matrix (noising 과정에서 i→ j로 변환)</p> <ul> <li> <p>순서대로 mask는 mask 유지</p> </li> <li> <p>값을 그대로 가질 확률: \alpha_t</p> </li> <li> <p>token이 mask 될 확률: 1 - \alpha_t</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>marginal Q*{t</td> <td>s} (여기서 \alpha*{t</td> <td>s} = \alpha_t/\alpha_s)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> </ul> <p>전개…… \mathcal{L}_{\text{diffusion}}은 앞의 수식과 의미적으로 같습니다…..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 일단 여기까진 정의대로 가되 block 내 token 길이인 L'으로 확장

- KL divergence 정의에 의해 다음과 같이 전개 가능 (이건 ㄹㅇ KLD 정의)

- \log{q} 부분은 학습과 무관하므로 제외

  - q(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = m, x^{b,\ell}) = \frac{\alpha_s - \alpha_t}{1 - \alpha_t}

  - q(x_s^{b,\ell} = m | x_t^{b,\ell} = m, x^{b,\ell}) = \frac{1 - \alpha_s}{1 - \alpha_t}

  - q(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = x^{b,\ell}, x^{b,\ell}) = 1: 1이므로 뒤에 계산에서 제외

- x_t^{b,\ell}이 mask인 경우만 계산

- 뒤에 항은 mask → mask는 상수라서 계산에서 제외
</code></pre></div></div> <table> <tbody> <tr> <td>= \sum<em>{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \sum</em>{\ell=1}^{L’} \frac{\alpha<em>t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^{b,\ell}</td> <td>x_t^{b,\ell}, x^{&lt;b}) \right]</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>= \sum<em>{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b}) \right]</td> </tr> </tbody> </table> <p>T \rarr \infin, T(\alpha_t - \alpha_s) = \alpha’_t</p> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>L^\prime = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> <ul> <li> <p><strong>직관적 핵석</strong>: block의 길이가 1이라면 한 토큰 단위 AR과 같음. → ??? 그래도 한 토큰 단위로 일어나는 diffusion 과정이 있는데? → mask로 intitialize 후, 원하는 다음 token을 찾는 과정이란 점에선 동일.</p> </li> <li> <p>*<strong>*수식 ok**</strong></p> <ul> <li> <p>linear scheduler에서 \alpha’_t, \alpha_t의 정의는 위와 같음. 그 다음 전개 과정은 이해할 수 있을듯?</p> </li> <li> <p>Expanding 부분은 Expatation of q를 제거 하기 위한 과정 q가 mask transition을 전제로 하므로 경우 (mask/unmask) 두 가지 확률에 대해서 전개</p> </li> <li> <table> <tbody> <tr> <td>SUBS-parameterization 가정의 carry-over unmasking 특성으로 \log{p_\theta(x^b</td> <td>x_t^b=x^b, x^{&lt;b})} = 0</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>q(x_t^b=m</td> <td>x^b) = 1 - \alpha_t = 1 - (1 - t) = t</td> </tr> </tbody> </table> </li> <li> <p>t는 상관없으니깐 삭제!</p> </li> <li>최종 결과는 NLL 로스와 기대값이 같다!</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <table> <tbody> <tr> <td>tuned schedule: q(x_t^\ell = m</td> <td>x^\ell) = 1</td> </tr> </tbody> </table> <ul> <li> <p>해당 schedule에서는 AR의 목적함수와 완전히 동일</p> </li> <li> <p>PPL도 감소, NELBO의 분산도 감소</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 L^\ell \geq 1인 케이스로 확장하고 싶음!</p> <ul> <li> <p>NELBO는 이론적으로 t에 invariance (기존 연구 ref: T가 무한히 커질수록 \alpha값이 아닌 누적값에 의해서 기대값이 정의되기 때문… 이 이상의 이해는 포기)하기에 스케줄에 따른 기대값의 변화가 없어야 함.</p> </li> <li> <p>하지만 우리는 모든 연산을 한 번에 하는 것이 아닌 Batch 연산을 활용 → 이론적인 invariance가 깨짐</p> </li> </ul> </li> </ul> <p>→ Schedule에 따라 분산의 결과가 변하게 됨. → Schedule을 잘 만들어보자!</p> <ul> <li> <p>Batch size를 K라고 할 때, batch of sequence \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}], with each \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x)</p> </li> <li> <p><strong>NELBO estimator</strong></p> </li> <li> <p><strong>Variance of the gradient estimator</strong></p> </li> </ul> <h2 id="low-variance-noise-schedules-for-bd3-lms">LOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS</h2> <h3 id="intuition-avoid-extreme-mask-rates--clipped-schedules-for-low-variance-gradients">INTUITION: AVOID EXTREME MASK RATES &amp; CLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS</h3> <ul> <li> <p>이상적인 마스킹: 모델이 다양한 수준의 노이즈 [MASK]에서 원래대로 되돌리는 법을 배우는 것</p> </li> <li> <p>극단적인 마스킹</p> <ul> <li> <p>마스킹 토큰이 너무 적을 경우, 너무 쉬운 문제를 풀게 됨.</p> </li> <li> <p>모든 토큰이 마스킹 될 경우, 문맥 정보가 전혀 없음 빈도에 기반한 학습만 진행</p> </li> </ul> </li> </ul> <p>→ 극단적인 부분을 날린 CLIP을 이용하자</p> <p>→ sample mask rates: 1 - \alpha_t \sim \mathcal{U}[\beta, \omega] for 0 \leq \beta, \omega \leq 1</p> <h3 id="data-driven-clipped-schedules-across-block-sizes">DATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES</h3> <ul> <li> <p>Block size ( L’)에 따른 최적의 mask rate을 찾아보자.</p> </li> <li> <p>Gradient 분산을 최소화하기 위함이지만 아래 NELBO를 추정지로 하여 실험을 진행</p> <ul> <li> <p>forward pass만으로 계산 가능</p> </li> <li> <p>실험 결과들에서 NELBO와 기울기 분산이 같은 경향성을 보임을 확인</p> </li> </ul> </li> <li> <p>\beta, \omega에 대해 grid search 진행</p> </li> <li> <p>Table 2에서 PPL과 NELBO과 상관성 보임을 재차 확인 + L’에 따라 최적의 조합이 있음을 발견함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="experiments">EXPERIMENTS</h2> <ul> <li> <p>Pre-train: base BD3-LM ( L’=L) for 850K gradient steps (순수 diffusion?)</p> </li> <li> <p>Fine-tune</p> <ul> <li>150K gradient steps on One Billion Words dataset (LM1B) and OpenWebText (OWT)</li> </ul> </li> <li> <p>L’에 따라 다른 Clipped schedule 적용 (매 validation epoch 마다 최적의 \beta, \omega 조합을 찾음!)</p> </li> </ul> <h3 id="likelihood-evaluation">LIKELIHOOD EVALUATION</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>다른 MDLM 모델 대비 perplexity이 향상됨</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Zero-shot validation perplexity 결과 Pubmed는 AR보다도 잘함.</p> </li> <li> <p>대체로 다른 MDLM보단 PPL 값이 더 낮음.</p> </li> </ul> <h3 id="sample-quality-and-variable-length-sequence-generation">SAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>[EOS] 토큰을 생성하거나 sample quality가 급감 (the average entropy of the the last 256-token chunk is below 4)할 때까지 실험 진행</p> </li> <li> <p>SEDD 대비 최대 10배 더 긴 text 생성 가능함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>GPT-2를 이용해 generative perplexity 측정, 효율성을 보기 위해 the number of generation steps (NFEs)</p> </li> <li> <p>기존 Block Diffusion 대비해도 더 적은 step에서 높은 Gen PPL 달성</p> </li> <li> <p>정성 분석은 Appendix D에 있음. AR과 유사할 정도의 퀄리티, 다른 DLM보단 좋더라</p> </li> </ul> <h3 id="ablations">ABLATIONS</h3> <p><strong>SELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>L’이 작을수록 heavier mask가 효과적</li> </ul> <p><strong>EFFICIENCY OF TRAINING ALGORITHM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>concat 활용하여 처리할 경우, sparse attention mask 활용</p> </li> <li> <p>FlexAttention을 이용할 경우 Sparsity를 활용해 효율적 처리 가능</p> </li> <li> <p>20-25% 속도 향상 가능!</p> </li> </ul> <hr/> <ul> <li> <p>수학 공부 열심히 하자.</p> </li> <li> <p>결과에서 힘이 많이 빠지긴 한다.</p> </li> <li> <p>전개과정에서 이 정도는 해야 oral로 가는구나 벽느껴진다.</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="diffusion"/><category term="diffusionlm"/><category term="generative"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰 - DiffusionLM, LLM 관련 연구]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>많은 연구들이 LLM이 사전학습 단계에서 학습 데이터를 외우는 이슈에 대해서 보고하고 있는 한편, finetuning에 대해서 비슷한 연구는 놀라울 정도로 적다.</p> <p>하지만, finetuning도 당연히 모델 대부의 업데이트와 때때로는 구조적인 변화까지도 이루어지기 때문에, finetuning의 memorization level에 대한 연구도 필요하다.</p> <p>그렇다면, 존재하는 다양한 finetuning 방법에 따른 memorization of fineuning data의 영향력은 어떻게 되는가?</p> <p>해당 연구는 이를 시험하기 위해 우선 finetuning 방법을 크게 두 가지로 구분한다:</p> <ol> <li> <p>Parameter-based finetuning: 모델 파라 바꿈</p> </li> <li> <p>Prompt-based fine-tuning: 모델 파라 고정, soft token/prefix embedding…</p> </li> </ol> <p>결과적으로 두 카테고리를 고루 포함한 5가지 방법을 시험했고,</p> <p>평가는 다양한 MIAs(membership inference attacks )로 했고,</p> <p>데이터는 Wikitext, WebNLG, Xsum 세 가지로 했다 (좀 적긴하네요)</p> <p>간단하고 빠르게 다음으로 넘어갑시다</p> <h1 id="fine-tuning-methods">Fine-Tuning Methods</h1> <ul> <li> <p>Parameter-based fine-tuning</p> <ul> <li> <p>Model Head Tuning (FT head): fine-tunes only the final output layer</p> </li> <li> <p>Low-Rank Adaptation (LoRA) (Hu et al., 2021)</p> </li> </ul> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> <ul> <li> <p><strong>Prefix Tuning</strong></p> <ul> <li>각 attention layer의 key/value에 학습 가능한 prefix 벡터 추가.</li> </ul> </li> <li> <p><strong>Prompt Tuning</strong></p> <ul> <li>모델 입력 임베딩 앞에 학습 가능한 연속형 프롬프트 임베딩 추가.</li> </ul> </li> <li> <p><strong>P-tuning</strong></p> <ul> <li>별도의 신경망으로 학습한 연속형 프롬프트를 입력에 삽입.</li> </ul> </li> </ul> </li> </ul> <h1 id="memorization-and-mias">Memorization and MIAs</h1> <ul> <li> <p>사용된 MIA 기법과 점수 계산 방식:</p> <ol> <li><strong>LOSS</strong> (Yeom et al., 2018)</li> </ol> <ul> <li>Membership Score = 모델의 손실</li> </ul> </li> </ul> <p>\text{Score} = L(x, M_t)</p> <p>(손실이 낮을수록 멤버일 가능성 ↑)</p> <ol> <li> <p><strong>Reference-based (Ref)</strong> (Mireshghallah et al., 2022a)</p> <ul> <li>기준 모델 MrM_rMr와 비교하여 손실 차이 계산</li> </ul> </li> </ol> <p>\text{Score} = L(x, M_t) - L(x, M_r)</p> <ol> <li> <p><strong>Zlib Entropy (Zlib)</strong> (Carlini et al., 2021)</p> <ul> <li>손실을 zlib 엔트로피로 나눈 비율</li> </ul> </li> </ol> <p>\text{Score} = \frac{L(x, M_t)}{\text{zlib}(x)}</p> <ol> <li> <p><strong>Min-K%</strong> (Shi et al., 2024)</p> <ul> <li>토큰 확률이 낮은 하위 k% 토큰들의 평균 로그 likelihood</li> </ul> </li> </ol> <table> <tbody> <tr> <td>\text{Score} = \frac{1}{E} \sum*{x_i \in \text{Min-}K\%(x)} \log p(x_i</td> <td>x*{&lt;i})</td> </tr> </tbody> </table> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>데이터</p> <ul> <li> <p>Wikitext-2-raw-1</p> </li> <li> <p>WebNLG</p> <ul> <li>triple로 이루어짐 (Subject-Predicate-Object)</li> </ul> </li> <li> <p>Xsum: 요약</p> <ul> <li>finetuning에 5000개만 사용</li> </ul> </li> </ul> </li> <li> <p>평가</p> <ul> <li>training and test sets에서 샘플링</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모델</p> <ul> <li> <p>LLaMA 2-7B (Touvron et al., 2023)</p> </li> <li> <p>GPT2-series (Radford et al., 2019)</p> </li> <li> <p>LLaMA 3-1B</p> </li> </ul> </li> </ul> <p>→ 2025의 논문이라고 믿기지 않는군여!</p> <ul> <li> <p>Evaluation Metrics</p> <ul> <li> <p>PERF: validation PPL as the primary metric</p> </li> <li> <p>MIA: AUC-ROC</p> </li> </ul> </li> <li> <p>Implementation Details</p> <ul> <li> <p>15 epoch</p> </li> <li> <p>모든 세팅은 논문에 나온거 그대로 따라함</p> </li> <li> <p>4090이랑 H100 한대 사용</p> </li> </ul> </li> </ul> <h1 id="results-and-observations">Results and Observations</h1> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <blockquote> <p>Observation ♯1: (당연)</p> </blockquote> <p>Parameter-based fine-tuning demonstrates a higher tendency to explicitly memorize training data.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>모든 방법론은 validation PPL기준으로 성능 좋았음.</p> <p>하지만, prompt-based methods 는 parameter-based 보다 외우는 성능 떨어짐 (당연)</p> <blockquote> <p>Observation ♯2:</p> </blockquote> <p>Parameter-based fine-tuning exhibits increasing memorization over training epochs, while prompt-based fine-tuning maintains consistently low memorization throughout training.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the model’s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <ul> <li><strong>Prefix Tuning 수식 (Petrov et al., 2024)</strong></li> </ul> <p>t^{pt}<em>i = A^{pt}</em>{i0} W<em>V S_1 + (1 - A^{pt}</em>{i0})\; t_i</p> <ul> <li> <p>soft-prefix가 어텐션 가중치 A^{pt}를 ‘어디를 볼지’만 재조정, <strong>본래 토큰 간 상대 분포는 그대로</strong>.</p> </li> <li> <p>즉 <strong>새로운 attention 패턴을 학습</strong>하기보다는 <strong>기존 능력을 재활용</strong>.</p> </li> <li> <p>결과적으로 <strong>표현 공간의 이동(shift) &lt; 적음</strong> → 학습, 비학습 샘플 분포 차이가 작아 MIA가 어렵다.</p> <ul> <li>Petrov et al. (2024) prove that the presence of a prefix does not alter the relative distribution of the input but only shifts the attention to different content.</li> </ul> </li> </ul> <p>이 가설을 확인하기 위해:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7B를 세 세팅에서 비교함:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>LoRA는 membership and non-membership samples 사이 분포 차이가 큰데, prefix tuning은 미미하다는 것을 알 수 있음</p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>두 방법론이 최종적으로는 비슷한 PPL을 가졌음에도 불구하고, Learning trajactories는 꽤나 달랐음</p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>이는 아까도 이야기 했듯이, 후자가 internal sample distribution of the model을 바꾸는 것이 아니라 단순히 다운스트림 태스크에 쪼끔 더 나은 bias를 추가하는 정도임을 다시한번 보인다</p> <h1 id="discussion">Discussion</h1> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>모델 사이즈가 memorization에 중요한 영향력을 줄 것임.</p> <p>→ To what extent does model size influence memorization under different fine-tuning strategies?</p> <blockquote> <p>Observation ♯3</p> </blockquote> <p>Model size significantly enhances memorization in parameter-based fine-tuning methods, while prompt-based methods show minimal sensitivity and maintain consistently low memorization.</p> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p>LLaMA2-7B vs LLaMA3-1B</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>요약: 파라미터 바꾸는 애들은 모델 크기 커질수록 더 잘 외웠는데 반대는 미미하더라 (low sensitivity of prompt tuning to model scale)</p> <p>특히, gpt2의 경우나 1B 스케일에서 LoRA는 사실상 거의 못외움</p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation ♯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p>다운스트림 태스크의 종류에 따라서도 다를 수 있음. 이를 위 LLaMA2-7B를 다양한 방법을 통해 학습시키고 LOSS attack against에 대해서 각각을 평가해봄</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prompt-based 만 봤을 때, WebNLG가 다른 것들에 비해서 성능이 높다</p> <p>아마도 구조화된 pattern학습에는 유리한 것 같다</p> <h2 id="impact-of-lora-placement-on-memorization">Impact of LoRA Placement on Memorization</h2> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>AUC↑ ⇒ 기억(privacy risk)↑</p> <ol> <li><strong>Projection &gt; Attention</strong></li> </ol> <ul> <li>LoRA를 <strong>projection layer</strong>에만 적용할 때, 두 데이터셋 모두 네 가지 MIA 지표에서 <strong>AUC가 일관되게 상승</strong> → 기억이 더 강해짐.</li> </ul> <ol> <li><strong>Both layers = 기억 제일 강함</strong></li> </ol> <ul> <li>Attention + Projection 동시 적용 시 <strong>가장 높은 AUC</strong> → 최대 수준의 memorization.</li> </ul> <ol> <li><strong>메커니즘 해석</strong></li> </ol> <ul> <li> <p>Projection layer는 <strong>특징 변환, 정보 압축</strong>을 담당 → 학습 데이터의 구체적 패턴을 더 잘 ‘붙잡아 두는’ 위치.</p> </li> <li> <p>결과는 Meng et al. (ROME)의 Transformer 기억은 주로 projection 층에 집중한다는 가설을 재확인.</p> </li> </ul> <p>Practical한 관점에서…</p> <ul> <li> <p>프라이버시에 민감한 애플리케이션에서는 LoRA를 attention 층에만 삽입하거나 rank를 낮추어 위험을 완화.</p> </li> <li> <p>성능과의 트레이드오프가 필요할 때, 삽입 위치(attn vs proj)와 범위(단일 vs 복합 층)를 주요 조절 변수로 활용하면 효과적일 수 있겠다!</p> </li> </ul> <h1 id="limitation">Limitation</h1> <p>너무 많죠..하지만 저자가 이야기한 것만 말해보겠습니다.</p> <ol> <li> <p>larger model</p> </li> <li> <p>MoE 같은 다른 구조</p> </li> <li> <p>데이터 적음</p> </li> </ol>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="embedding"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근 LLM이 복잡한 reasoning task에서 괄목할만한 성능을 보이고 있으나, (모델에게) 편한 reasoning pattern에 의존하는 경향이 있음</p> <ul> <li>이를 Reaosning rigidity로 정의</li> </ul> </li> <li> <p>사용자의 명시적인 instruction이 있음에도 불구하고, 습관적인 reasoning trajectory를 생성, 오답으로 귀결</p> </li> <li> <p>이를 분석하기 위해 reasoning trap이라는 진단 데이터셋을 도입</p> <ul> <li> <p>deviation을 요구하도록 기존 데이터셋을 수정한 상태</p> </li> <li> <p>예를 들어, 모든 토끼는 불임이다. 토끼가 3쌍이 있고 토끼 한 쌍이 암수 각 1마리씩 총 2마리를 낳는다고 가정하면, 2세대를 거치면 몇마리의 토끼가 되는가? 라는 질문.</p> </li> </ul> </li> </ul> <p>⇒ 이를 통해서 모델이 습관적으로 쓰는 contamination된 pattern을 식별할 수 있음</p> <ul> <li> <p>모델이 주어진 instruction 을 무시하거나 왜곡하도록 함</p> </li> <li> <p>reasoning trap을 통해서 모덷이 습관적으로 사용하는 reasoning pattern을 발견, 분류</p> <ul> <li> <p>interpretation overload</p> </li> <li> <p>input distrust</p> </li> <li> <p>partial instruction attnetion</p> </li> </ul> </li> </ul> <p>⇒ 해당 데이터셋을 통해 LLM에 있는 reasoning rigidity를 해소하는 미래 연구를 용이하게 함</p> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM은 수학, 복잡한 코딩 문제, 퍼즐 풀이를 포함한 여러 어려운 태스크에서 주목할만한 성능을 보임</p> <ul> <li>특히 test-time scaling을 활용해 확장된 CoT prompting을 활용하는 reasoning model들이 큰 주목을 받고 있음</li> </ul> </li> <li> <p>하지만, 이러한 모델들에게 문제 행동, reasoning rigidity가 발견됨</p> <ul> <li>특히 긴 CoT reasoning으로 학습된 모델에게 나타남</li> </ul> </li> <li> <p>reasoning rigidity는 cognitive bias를 반영, 주어진 조건을 이해해도 자기 방식대로 override, 무시하고 문제를 푸는 현상을 뜻함</p> </li> </ul> <p>⇒ 이는 기존에 언급되어왔던 hallucinataion, prompt brittlness들을 해소해도 존재할 수 있음</p> <ul> <li> <p>hallucination : 틀린 정보를 생성하는 것</p> </li> <li> <p>prompt brittlness : 미묘한 prompt 차이에 따라 답변이 바뀜. 답변이 unstable한 현상</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>reasoning rigidity는 사용자가 서술한 제약이 중요한 도메인에서 큰 문제가 됨</p> <ul> <li>예를 들어, 수학이나 퍼즐 풀이의 경우, 뒤의 문제와 관계 없이 유저가 바로 정답으로 이어질 수 있는 조건을 줬을 경우, 이를 무시하면 완전히 오답이 될 수 밖에 없음</li> </ul> </li> </ul> <p>⇒ 사용자의 지시를 무의식 중에 편집하거나 무시(reasoning rigidity), 모델의 reasoning path전체가 오염, 오답으로 이어짐</p> <ul> <li> <p>이러한 현상은 아주 크리티컬하나, 본 논문에서 처음으로 문제를 정의</p> </li> <li> <p>reasoning rigidity를 식별할 수 있도록, 기존의 수학, 퍼즐 데이터셋을 활용해 reasoningtrap이라는 벤치마크를 제안</p> <ul> <li> <p>잘 알려진 cahllenges와 닮았으나, 조건이 수정되어서 답이 완전 바뀌는 문제들로 구성</p> </li> <li> <p>모델이 습관적으로 문제를 풀 경우 오답으로 이어지는 구조로 설계</p> </li> </ul> </li> <li> <p>ReasoningTrap으로 여러 모델을 평가한 결과, 여러 중요한 현상들을 발견</p> <ul> <li> <p>reasoning process의 중간 단계에서 contamination이 시작</p> </li> <li> <p>이러한 contamination은 명백하게 식별 가능, 반복되는 패턴을 가짐</p> </li> </ul> </li> <li> <p>또한, 이러한 contamination의 패턴을 3가지로 분류</p> <ul> <li>interpretation overload, input distrust, partial insturction attention</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> <ul> <li> <p>LLM의 reasoning ability를 향상시키기 위해 CoT를 길게 생성하도록 학습하는 방법론이 제안</p> </li> <li> <p>또한, Qwen3의 경우 reasoning과 non-reasoning mode를 둘다 지원하는 unified fusion architecture를 공개</p> <ul> <li>user가 모델이 긴 CoT를 생성하도록 할지 여부를 고를 수 있음</li> </ul> </li> </ul> </li> <li> <p>Instruction following of reasoning models</p> <ul> <li>여러 in-context examples 혹은 장황한 instruction을 넣으면 reasoning model들의 성능이 떨어진다는 것을 잘 알려짐</li> </ul> </li> </ul> <p>⇒ 즉, LRM이 user-provided example을 following하는 능력이 부족</p> <ul> <li> <p>본 연구는 이와 결은 같지만, 모델이 친숙한 reaosning pattern을 고집한다는 것에 초점을 둠</p> </li> <li> <p>Rigidity in reasoning models</p> <ul> <li> <p>몇몇 연구들이 LLM이 reasoning할 때 rigid pattern을 보인다는 것을 지적함</p> <ul> <li> <p>medical domain, educational domain</p> </li> <li> <p>우리의 연구는 더 큰 도메인이 수학, 퍼즐에 초첨</p> </li> </ul> </li> <li> <p>본 연구와 유사하게, 몇몇 논문들이 LLM이 rigidity를 탐구</p> <ul> <li>이러한 연구들은 LLM이 creative problem solving에 적용될 때 혹은 matha word problem의 unseen variant의 일반화 에 초점을 둠</li> </ul> </li> </ul> </li> <li> <p>Underlying reason for rigidity</p> <ul> <li> <p>몇몇 연구들이 왜 LLM이 이러한 rigidity를 가지는지에 대해 분석했고, training data 혹은 optimization 방식에 임베딩된 bias를 지적</p> </li> <li> <p>한 연구에서 RL로 학습된 모델들이 exploitation이 뛰어나고, 이로 인해 높은 성능은 달성했지만 역설적이게도 non-reasoning model에 비해 좁은 knowledge coverage를 보인다고 주장</p> </li> <li> <p>다른 연구에서는 training data에 내재된 bias때문이라고 함</p> </li> </ul> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>크게 2가지로 도메인으로 구성 : 수학(ConditionedMath), 퍼즐(PuzzleTrivial)</p> </li> <li> <p>각 데이터는 원래 Q-R-A tuple (q_orig, r_orig, a_orig)과 수정된 tuple (q_mod, r_mod, a_mod)로 구성</p> </li> <li> <p>총 164개의 데이터셋, 84개는 수학, 80개는 퍼즐</p> </li> <li> <p>ConditionedMath에 있는 모든 질문은 개념적으로 다르고, 겹치지 않고, human annotator에 의해 엄격하게 검증됨</p> </li> <li> <p>PuzzleTrival은 10개의 puzzle concept를 가짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5를 활용해서 제작</p> </li> <li> <p>원래 질문을 수정하고, 수정된 질문이 아래 조건에 부합하는지를 확인, 필터링</p> <ul> <li> <p>validity : 기존 condition과 모순되는지</p> </li> <li> <p>divergence : 기존 답, 풀이와 상이한지</p> </li> <li> <p>existence : 답이 있는지</p> </li> </ul> </li> </ul> <p>⇒ 문제를 수정할 때는 gpt-4o-mini활용, 필터링 할 때는 o4-mini를 사용</p> <ul> <li>220개의 원본 데이터를 5가지의 variant로 modified, 필터링 후에 최종 84개만 남음</li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzle은 조건을 수정하면 급격하게 단순해지거나 답이 여러개일 수 있음</p> </li> <li> <p>ambiguity를 줄이기 위해, “valid solution을 위해 가장 간단한 답을 찾아라”라는 문구를 instruction에 추가</p> </li> <li> <p>과정 자체는 위와 동일</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li> <p>시스템적으로 reasoning model의 contamination을 측정하기 위해서, Contamination ratio를 제안</p> <ul> <li> <p>친숙한 패턴에서 contaminated reasoning이 얼마나 차지하는지를 나타냄</p> </li> <li> <p>이를 자동적으로 식별하는 방법도 제안</p> </li> </ul> </li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>모델이 문제를 풀 때, 수정된 조건을 이해하고 풀었는지 이해하지 않고 풀었는지를 구분하기 위해 metric을 도입</p> </li> <li> <p>생성된 reasoning path를 단락별로 쪼개고, 각 단락을 textual representation으로 embedding</p> <ul> <li> <p>openAI의 text-embedding-small model을 사용</p> </li> <li> <p>단락은 double line break를 기준으로 분리</p> </li> </ul> </li> <li> <p>각 단락과 오리지널 문제의 reasoning path, 각 단락과 modified reasoning path와의 cosine 유사도를 계산, 둘을 비교해 original reasoning path와의 유사도가 더 높을 경우 1로 계산</p> <ul> <li>즉, 조건이 수정되었는데도 무시하고 습관처럼 reasoning을 했다는 뜻</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidity를 잘 관찰하기 위해, 모델이 수정된 조건을 이해했는데도 습관처럼 풀었는지 아니면 인지조차 하지 못했는지를 구분</p> <ul> <li>모델이 조건을 잘못 이해한 경우 / 조건을 잘 이해했으나 reasoning을 잘못한 경우</li> </ul> </li> <li> <p>이를 반영한 metric을 p-passs@k라고 정의, reasosning path에서 constraint를 인지하고 있는 경우에만 accuracy를 측정</p> </li> <li> <p>constraint를 인지했는지는 모델이 생성한 reasoning path중 첫 15개의 단락과 정답, 질문을 LLM에 넣고 판단하도록 함(p_i)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>question만 주어지는 현실적인 상황에서, generated reasosning이 원치 않지만 친숙한 pattern으로 contaminated됐는지 자동적으로 식별하는 것을 불가능</p> </li> <li> <p>그래서 간단하게, contamination의 종류를 분류해서, 각 type별 의심스러운 pattern을 식별</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Interpretation overload : 모델이 주어진 문제 조건을 거절하는 것으로 시작, 문제를 바로 해석하는 것보다 여러 방식으로 재해석. 보통 reasoning 중간 단계에서 발생, inconsistent 혹은 contraditory한 결론을 야기</p> </li> <li> <p>Input Distrust : 모델이 번역 오류, input error, typo존재 등을 가정함. 직관적으로 바로 문제를 풀 수 있음에도 부정하고 매우 복잡하게 풀게 됨.</p> </li> <li> <p>Partial Instruction Attention : 모델이 제공된 instruction의 일부분만을 선택적으로 집중</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrap을 여러 LLM에 inference</p> </li> <li> <p>실험은 CoT prompting을 사용, ‘Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}’ 포맷으로 질문을 전달</p> </li> <li> <p>table 2,3은 16번 sampling, 다른 실험은 4번 sampling</p> </li> <li> <p>수학 문제의 경우, exat matching으로 correctness 판단, puzzle의 경우 free-from sentence로 답이 구성되다 보니, LLM을 사용해서 정답과 모델 답변을 함께 제공해 correctness를 판단</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>실험 결과, 대부분 reason모드일 때보다 base모드에서 더 높은 성능을 보임</p> <ul> <li>즉, 길게 reasoning을 하면서 습관적인 reasoning pattern을 사용, 오답으로 이어지는 경우가 많다는 것</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Buget forcing : 버짓 마지막에 ‘Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;’를 추가하여 답을 바로 내도록 함</p> <ul> <li> <p>MATH500 : low 2000, medium 4000, high 6000 토큰 사용</p> </li> <li> <p>AIME : low 2000, medium 6000, high 10000</p> </li> </ul> </li> <li> <p>prompt hinting : 문제에 오타 없고 지시 그대로 하라는 prompt를 추가</p> </li> <li> <p>실험 결과, budget이 커질 수록 성능이 악화됨</p> </li> <li> <p>prompt로 hint를 줘도 여전히 reasoning rigidity가 존재</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>모델 크기에 따른 실험. base모델이 성능이 전반적으로 높게 나오는 편</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="detection"/><category term="embedding"/><category term="gpt"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근의 reasoning oriented model(LRM)은 여러 수학 데이터셋에서 높은 성능 달성을 보이나, natural instruction following에 대한 성능은 분석되지 않음</p> </li> <li> <p>본 논문에서는 이러한 LRM들의 instruction following 능력을 분석하기 위해 MathIF라는 데이터셋을 제안, math 도메인에서의 instruction following 성능을 평가</p> </li> <li> <p>실험 결과, reasoning을 효과적으로 하는 모델이 user direction에 따르는 것을 어려워 하는 현상 발견</p> <ul> <li> <p>긴 CoT dataset에 SFT하거나 RL로 학습한 모델이 답변 길이가 길어질수록 instruction following 능력이 떨어지는 현상 발견</p> </li> <li> <p>간단한 개입(CoT 마지막 부분에 instruction을 다시 붙여서 넣어줌)으로 instruction following 성능을 향상시킬 수 있음을 보임</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoning을 scaling하는 것은 reasoning ability를 향상시킴</p> <ul> <li>SFT or RLVR 사용</li> </ul> </li> <li> <p>LRM의 경우 간단한 instruction도 following하는 것을 어려워 한다는 것을 발견</p> </li> </ul> <p>⇒ reasoning-oriented learning을 하면 모델 자체의 reasoning ability는 향상돼도 controllability는 떨어지는게 아닐까?</p> <ul> <li>하지만 현재는 범용 목적의 instruction following(IF) 벤치마크만 존재</li> </ul> <p>⇒ 수학 도메인에서의 IF 벤치마크를 만들고 평가해보자!</p> <ul> <li> <p>실험 결과, instruction following과 reasoning capability사이의 일종의 trade-off가 존재</p> <ul> <li> <p>즉, SFT 혹은 RL로 reasoning ability를 향상시킨 모델은 reasoning 성능은 올라도 IF 성능은 떨어짐</p> </li> <li> <p>특히, CoT 길이가 길어질수록 IF 성능이 악화됨</p> </li> </ul> </li> <li> <p>contribution</p> <ul> <li> <p>MathIF, 첫번째로 수학 도메인에서 instruction following 능력을 시스템적으로 측정하는 벤치마크 데이터셋 도입</p> </li> <li> <p>23개의 LRM를 해당 벤치마크에 대해서 평가</p> </li> <li> <p>reasoning performance와 instruction-following사이의 trade-off가 있음을 실험적으로 보임</p> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> <ul> <li> <p>high-quality long CoT by distilling from more capable LRMs or combining primitive reasoning actions</p> <ul> <li> <p>s1 : 적은 양의 CoT data로도 reasoning ability를 향상시킴</p> </li> <li> <p>LIMO : 관련 도메인이 이미 pre-training때 포함되어 있다면, 최소한의 cognitive process를 담은 demonstration으로 reasoning capabilities를 발생시킬 수 있다고 서술</p> </li> </ul> </li> <li> <p>cold-RL</p> <ul> <li> <p>deepseek-R1-zero로 주목 받게 된 방법론</p> </li> <li> <p>SFT와 달리, cold-RL은 긴 CoT dataset에 의존하지 않고, final outcome으로 reward를 받아서 학습</p> </li> <li> <p>RL과정을 간단, 가속화 하기 위해서 dynamic sampling, process-reward, off-policy guidance, CoT preference optimziation 등이 제안됨</p> </li> </ul> </li> </ul> </li> <li> <p>Instruction-followiwng benchmark</p> <ul> <li> <p>이전의 벤치마크는 보통 user query의 completeness에 초점, proprietary language model에 의존해서 win rate를 측정하는 식으로 평가</p> </li> <li> <p>format constraint, multi-turn instruction, refutation instruction, compositional instruction을 따르는지를 평가</p> </li> <li> <p>하지만 대부분의 IF 벤치마크는 일반적인 도메인에 집중, 상대적으로 직관적인 query를 사용</p> </li> </ul> </li> </ul> <p>→ 이러한 도메인 차이와 long CoT의 부재는 LRM을 평가하는데에 방해가 됨</p> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> <ul> <li>toy experiment로 IFEval과 FollowBench에 대한 LRM과 Instruct 모델 성능 비교</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>확실히 LRM의 성능이 상대적으로 낮으나, 낮은 원인이 domain shift때문인지 IF성능 때문인지는 분명하지 않음</li> </ul> <p>⇒ 수학 도메인의 IF benchmark를 만들자!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- python으로 검증 가능한 constraint를 고려, 2-3개의 constraint를 합쳐서 instruction으로 부여하는 방안을 고려

- contraint를 얼마나 만족했는지를 평가하기 위해 Hard accuracy(HAcc), Soft accruacy(SAcc)로 측정
</code></pre></div></div> <ul> <li> <p>Constraint type</p> <ul> <li> <p>length, lexical, format, affix로 크게 4가지 type으로 분류, 그 안에 sub-type을 명시</p> </li> <li> <p>proprietary language model에 의존하지 않기 위해서 python으로 제약을 만족했는지 검증 가능하도록 설계</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Compositional Constraint</p> <ul> <li> <p>2-3개의 constraint를 조합해서 compositional constraint를 구축</p> </li> <li> <p>같이 존재할 수 없는 constraint나 같은 subtype끼리 있으면 filtering, 그 외의 조합에서 random sampling해서 데이터셋을 구축</p> </li> <li> <p>이러한 과정을 통해 30개의 dual-constraint와 15개의 triple-constraint를 구축</p> </li> </ul> </li> <li> <p>Math problem collection</p> <ul> <li> <p>GSM8K, MATH-500, Minerva, Olympiad에서 90개씩 sampling</p> <ul> <li>초등학교부터 올림피아드 수준의 문제까지 아우르도록 함</li> </ul> </li> <li> <p>각 데이터에 대해서 single, dual, triple constraint를 적용</p> </li> <li> <p>sanity check를 위해 사람이 직접 검수, math problem에 추가된 constraint가 모순되지 않는지 더블췍</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Evaluation metric</p> <ul> <li> <p>HAcc : constraint다 만족해야 1</p> </li> <li> <p>SAcc : contraint 개당 만족하면 1 아니면 0으로 계산, 평균</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>구체적인 언급 없으면 correctness는 contraint가 있는 상태에서 나온 답변으로 계산</li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>모든 LRM은 nucleus sampling(T=1.0, p=0.95)로 디코딩, 최대 답변 길이 16,384 토큰, vLLM 사용</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모든 LRM은 IF성능이 하락함</p> <ul> <li> <p>최고 성능을 낸 Qwen3-14B마저도 50.71밖에 안됨</p> </li> <li> <p>특히 deepseek-R1-distill-llama-70B나 open-reasonser-zero-32B의 경우 모델 크기에 비해서 매우 낮은 IF 성능을 보임</p> </li> </ul> </li> <li> <p>Qwen3 시리즈가 그나마 높은 IF 성능을 보임</p> </li> <li> <p>모델 크기가 IF 성능을 결정하진 않음</p> <ul> <li>같은 계열에서는 종종 경향성이 있으나, 다른 계열까지 한번에 봤을 때는 크기가 크다고 IF 성능이 보장되진 않음</li> </ul> </li> <li> <p>명시적인 reasoning seperation (<think>,</think>)가 있는 모델이 전반적으로 IF 성능이 높음</p> <ul> <li>Qwen2.5-Math-1.5B, 7B-Instruct, Qwen2.5-1.5B, 7B-SimpleRL-Zoo 친구들이 명시적인 reasoning token 없는 애들 ⇒ 성능이 쏘 처참</li> </ul> </li> <li> <p>instruction-following과 mathematical reasoning사이에 trade-off가 존재</p> <ul> <li>Diff를 보면 대부분의 모델이 constraint가 있을 때와 없을 때의 correctness차이가 큼</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>LRM모델이 constraint를 잘 따르는게 문제 난이도와 연관이 있는지를 살펴보기 위해 데이터셋 별로 IF성능을 표현</p> </li> <li> <p>문제가 어려울수록 constraint를 잘 만족하지 못한다는 것을 알 수 있음</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>제약이 많아질수록 IF 성능 하락, 특히 2개이상부터 크게 하락..</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>제약조건을 만족하면서 문제를 맞추는 경우는 크지 않음</p> </li> <li> <p>보통 제약조건 혹은 문제 하나만을 만족함 + 즉, 제약조건을 걸면 문제 풀이 성능이 하락</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>constraint가 있을 때와 없을 때의 성능 차이</p> </li> <li> <p>특히, GSM8K, Minerva에서 극심 ⇒ 문제 난이도와 상관 없이 contraint가 있으면 reasoning ability가 하락</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>CoT가 길수록 IF 성능 하락</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>IF가 낮았던 Qwen2.5를 대상으로 실험, 데이터는 deepscalar를 사용, QwQ로 CoT생성, 정답을 맞추면서 너무 길지 않은 애들만 필터링해서 학습에 사용</p> <ul> <li>format reward는 think token를 포함하는지 여부로 포함하면 답이 틀려도 0.1점 줌</li> </ul> </li> <li> <p>실험 결과, reasoning-orienteed 방법론이 reasoning성능은 향상시키지만 IF는 하락하는 것을 볼 수 있음</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 7</p> <ul> <li> <p>모델이 reasonign path를 종료하려고 할 때마다 wait를 걸어서 강제로 CoT길이를 늘림</p> </li> <li> <p>CoT길이가 길어질수록 constraint instruction과 멀어져서 constraint에 대한 acc가 떨어지는 것으로 추론</p> </li> </ul> <p>Table 5</p> <ul> <li>cold-RL에서 roll-out 길이를 조정하며 학습, 길어질수록 reasoning은 향상되나 IF는 떨어짐</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>간단하게 reasoning이 끝나갈 때 쯤에 wait을 넣고 constraint instruction을 반복해서 넣어준 경우의 성능을 측정</p> </li> <li> <p>IF성능은 향상되나 Correctness는 하락하는 것을 볼 수 있음</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented model들이 생각보다 instruction following 성능이 악화됨</p> </li> <li> <p>대부분 간단한 형식에 대한 제약인데도, 제약이 있을 때와 없을 때의 성능 차이가 큰게 충격적</p> </li> <li> <p>LLM이 정말 reasoning을 하는걸까? 그냥 답변 길이가 길어져서 발생하는 attention sink일까?</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title><link href="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/" rel="alternate" type="text/html" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 건우 김</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="abstract">Abstract</h1> <ul> <li>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</li> </ul> <p>→ 하지만 기존 advanced reasoning ability를 가진 LLM에게 prompt 기반의 search engine을 활용하도록 하는 것은 suboptimal임 (LLM이 search engine과 어떻게 상호작용해야 하는지 완전히 이해 못함)</p> <ul> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> <ul> <li>단계별 reasoning step에서 autonomously하게 multiple search queries를 생성하고 실시간으로 정보를 검색하도록 학습</li> </ul> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <strong>효과적으로 상호작용하는</strong> 능력이 필수적임</p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 **LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, 단순한 결과 기반 보상이 충분한지는 아직 불확실함.</p> </li> </ol> <p>→ 3번은 자기들도 모르면서 뭔가 싶네요 ㅋㅋ</p> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <h1 id="2-related-works">2. Related Works</h1> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <h1 id="3-search-r1">3. Search-R1</h1> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine R을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>r_{\phi}: output quality를 평가하는 reward function</p> </li> <li> <p>\pi_\theta: policy LLM</p> </li> <li> <p>\pi_{ref}: reference LLM</p> </li> <li> <p>x: dataset D에서 추출된 input sample</p> </li> <li> <p>y: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>D_{KL}: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 \pi_\theta가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <ul> <li> <table> <tbody> <tr> <td>retrieval interleaved reasoning via \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> <ul> <li>\bigotimes denotes interleaved retrieval-and-reasoning</li> </ul> </li> </ul> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <ul> <li>*<strong>*Formulation of RL with a Search Engine**</strong></li> </ul> <p>LLM에서 자주 사용하는 원래 기존 RL의 objective는 아래와 같이 정의됨</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>그런데, 위 formulation은 entire output sequence y가 \pi_{\theta}로부터 생성되었다는 가정이 있음. 이 가정은 model behavior가 internal reasoning과 external information retrieval을 모두 포함하는 상황에서 적용할 수 없음.</p> <p>따라서, RL objective를 serach engine R과 통합시키기 위해 아래와 같이 수정함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>위 수정된 objective에서는 trajectory y 는 interleaved reasoning steps과 retrieved content를 포함</p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>\pi_{\theta}: current policy</p> </li> <li> <p>\pi_{old}: previous policy</p> </li> <li> <p>I(y_t): token loss masking 연산으로, y_t가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <ul> <li> <table> <tbody> <tr> <td>y ~ \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> </li> </ul> <p>→ LLM은 x를 입력 받아 Search Engine R과의 interleaved 흐름을 통해 y를 생성</p> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li> <p><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></p> </li> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Case Study</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="4-main-results">4. Main Results</h1> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> </li> <li> <p>Natural Questions (NQ)</p> </li> <li> <p>TriviaQA</p> </li> <li> <p>PopQA</p> </li> <li> <p>Multi-Hop QA</p> </li> <li> <p>HotpotQA</p> </li> <li> <p>2WikiMultiHopQA</p> </li> <li> <p>Musique</p> </li> <li> <p>Bamboogle</p> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> <ol> <li> <p>Inference w/o Retrieval</p> </li> <li> <p>Direct Inference</p> </li> <li> <p>Chain-of-Thought</p> </li> <li> <p>Inference w/ Retrieval</p> </li> <li> <p>RAG</p> </li> <li> <p>IRCoT (Information Retrieval CoT)</p> </li> <li> <p>Search-o1 (using search engine tool)</p> </li> <li> <p>fine-tuning methods</p> </li> <li> <p>SFT</p> </li> <li> <p>R1: search engine없이 RL fine-tuning (Search-R1과 fair한 비교를 위해 동일 데이터로 RL을 학습하되 검색은 사용하지 않음)</p> </li> </ol> <h2 id="43-experimental-setup">4.3 Experimental Setup</h2> <ul> <li> <p>LLMs: Qwen-2.5-3B, Qwen-2.5-7B (Base / Instruct)</p> </li> <li> <p>Retrieval</p> <ul> <li> <p>Knowledge Source: 2018 Wikipedia dump (using E5 as retriever)</p> </li> <li> <p>number of retrieved documents: 3</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>training data: NQ + HotpotQA for Search-R1 and fine-tuning methods</p> </li> <li> <p>evaluation data: (in-domain, out-of-domain)</p> </li> </ul> </li> <li> <p>metric: EM</p> </li> <li> <p>Inference 설정</p> <ul> <li>Inference-style baseline은 Instruct 모델 사용 (Base 모델은 instruction을 따르지 못함)</li> </ul> </li> <li> <p>RL 설정</p> <ul> <li>별도 언급이 없으면 PPO 사용</li> </ul> </li> </ul> <h2 id="44-performance">4.4 Performance</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Search-R1은 baselines 대비 우수한 성능 보여줌</p> <ul> <li> <p>Qwen2.5-7B: 평균적으로 41% 향상</p> </li> <li> <p>Qwen2.5-3B: 평균적으로 20% 향상</p> </li> </ul> </li> </ul> <p>→ in-domain (NQ, HotpotQA)와 out-of-domain (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle) <strong>모두 일관되게 높음</strong></p> <ul> <li>검색 없이 추론만하는 R1보다도 Search-R1이 우수함</li> </ul> <p>→ <strong>Search가 LLM 추론에 external knowledge를 추가함으로써 도움되는 것을 보임</strong></p> <ul> <li>Base와 Instruct model 모두 일관되게 Search-R1 효과적임</li> </ul> <p>→ DeepSeek-R1-Zero style의 단순 outcome-based reward가 순수 Reasoning 뿐만 아니라 <strong>search를 포함한 complex reasoning scenarios에서도 효과적임을</strong> 보여줌</p> <ul> <li>**Model size가 클 수록 검색 활용 효과가 더 큼 **</li> </ul> <h1 id="5-analysis">5. Analysis</h1> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="51-different-rl-methods-ppo-vs-grpo">5.1 Different RL methods: PPO vs. GRPO</h2> <p>Search-R1에서 RL 방법으로 PPO와 GRPO 두 가지를 모두 실험함</p> <ol> <li> <p><strong>GRPO는 PPO보다 수렴 속도가 빠름</strong> → Figure2 (a)</p> </li> <li> <p>PPO는 critic model에 의존하기 때문에 효과적인 학습이 시작되려면 여러 단계의 워밍업이 필요하지만, GRPO는 baseline을 여러 샘플 평균으로 잡아 더 빠르게 수렴함</p> </li> <li> <p><strong>PPO는 학습 안정성이 더 높음</strong> → Figure2 (a)</p> </li> <li> <p><strong>GRPO는 일정 단계 이후 reward collapse</strong>가 나타나지만, <strong>PPO는 학습이 더 안정적으로 유지됨</strong></p> </li> <li> <p><strong>최종 train reward는 PPO와 GRPO 모두 유사함</strong></p> </li> <li> <p>수렴 속도와 안정성은 다르지만 최종 성능과 train reward는 큰 차이가 없음. 그래도 GRPO는 나중에 불안정해질 수 있기에 더 안정적인 PPO가 느리지만 적합함.</p> </li> </ol> <p><em>(다른 세팅에서도 동일한 현상이 관찰됨)</em></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="52-base-vs-instruct-llms">5.2 Base vs. Instruct LLMs</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Figure2 (b)에서 Instruction-tuned model은 Base model보다 더 빠르게 수렴하고 초기 성능도 더 높게 나오지만, <strong>최종 train reward는 두 모델 모두 거의 동일한 수준으로 수렴함</strong></li> </ul> <p>→ 이는 사전 instruction tuning이 초기 학습을 가속화하는데 도움이 되지만, <strong>RL만으로도 Base model이 충분히 따라잡을 수 있음을 보임</strong></p> <p>(다른 세팅에서도 동일한 현상이 관찰됨)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="53-response-length-and-valid-search-study">5.3 Response Length and Valid Search Study</h2> <p>Qwen2.5-7B-base 모델로 response length와 검색 호출 횟수 변화를 분석함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Figure2 (c)를 보면</p> <ul> <li> <p>초기 단계 (100 steps 전후)</p> <ul> <li> <p><strong>응답 길이가 급격히 줄고, train reward는 소폭 상승함</strong></p> </li> <li> <p>모델이 불필요한 군더더기 단어를 줄이고 task에 적응하기 시작함을 보여줌</p> </li> </ul> </li> <li> <p>후기 단계 (100 steps 이후)</p> <ul> <li> <p><strong>응답 길이와 train reward 모두 증가함</strong></p> </li> <li> <p>모델이 검색 호출을 더 자주 하면서 (Search Engine을 자주 호출하는 법 학습) 검색 결과가 추가되어 응답이 길어짐</p> </li> <li> <p>검색 결과를 효과적으로 활용하며 train reward도 크게 향상됨</p> </li> </ul> </li> </ul> </li> <li> <p>Figure2 (d)를 보면 <strong>학습이 진행될수록 LLM이 검색 엔진 호출을 더 많이 학습한다는 점이 드러남</strong></p> </li> </ul> <h2 id="54-study-of-retrieved-tokens-loss-masking">5.4 Study of Retrieved Tokens Loss Masking</h2> <p>Retrieved Token Loss Masking은 unintended optimization을 방지하기 위해 도입한 것임. Retrieved token loss masking의 효과를 추가로 분석해봄 (Qwen2.5-7B-base)</p> <ul> <li>Figure 3에 따르면, <strong>masking을 적용하면 원치 않는 최적화 효과를 줄이고 LLM 성능 향상이 더 커짐</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>w. mask와 w.o. mask를 비교한 결과 **masking을 적용한 경우가 항상 더 높은 성능을 기록함 **</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>*<strong>*Appendix**</strong></li> </ul> <p><strong>Number of Retrieved Passages Study in SEARCH-R1 Training</strong></p> <ul> <li>본 실험에서는 top-k를 3으로 설정했지만, 1,3,5 바꿔가며 이것의 effect를 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ top-k가 1,3,5 설정이 <strong>모두 training pattern이 비슷함</strong> (top-k=5가 초기 수렴 속도가 빠른 대신에 이후 train reward가 감소하며 학습 안정성 떨어짐을 보임)</p> <p><strong>Group Size Study in SEARCH-R1 (GRPO) Training</strong></p> <ul> <li>본 실험에서는 Search-R1 (GRPO)의 group size를 5로 설정했지만, group size가 어떤 영향을 미치는지 확인하고자 1,3,5로 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ Figure7을 보면 <strong>group size가 칼수록 수렴 속도 빨라지는 반면 RL의 불안정성 때문에 training collapse 위험도 증가</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ Table8을 보면 group size가 큰 경우 빠른 수렴과 더 높은 train reward가 있었지만, group size=1일 때 학습이 더 안정적이고 일반화 성능이 더 우수함 (out-of-domain에서 더 우수함)</p> <h1 id="6-conclusion">6. Conclusion</h1> <ul> <li> <p>본 연구에서는 LLM이 self-reasoning과 실시간 검색 엔진 상호작용을 교차적으로 수행할 수 있는 framework인 Search-R1 제안함</p> </li> <li> <p>기존의 multi-turn search를 위해 많은 prompt에 의존하는 RAG나 대규모 train data가 필요한 tool 사용 기반 접근법과 달리, <strong>Search-R1은 RL을 통해 모델이 자율적으로 검색 쿼리를 생성하고 검색된 정보를 전략적으로 활용할 수 있도록 최적화함</strong></p> </li> </ul> <p>Limitations</p> <ul> <li>Reward Design가 단순 결과 기반 보상이라 보다 디벨롭이 필요함</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="fine-tuning"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><category term="reinforcement learning"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[논문 리뷰 - Reinforcement Learning 관련 연구]]></summary></entry></feed>