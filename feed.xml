<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://alshedivat.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alshedivat.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T11:56:42+00:00</updated><id>https://alshedivat.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://alshedivat.github.io/al-folio/blog/2025/2025-08-10-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-10T00:00:00+00:00</published><updated>2025-08-10T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/2025-08-10-block-diffusion-interpolating-between-autoregressive-and-diffusion-language</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/2025-08-10-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p>→ <strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></p> <p><strong>BD3-LMs</strong></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/3acbc979-3f43-48f4-8683-229c6104ec76/45b8f6de-b2b2-47cc-ac8b-8d268aa97e24/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=ASIAZI2LB466R27ZVFKX%2F20250810%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250810T105949Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCIGh3%2FVK8MxTZGWx8EIBPmzCC5B9Ex0Tk3YZUSo3qieJ0AiAF76nYDUcXmMJxHcjpA3wWo6J0DMA7AzOJDqmdCd1GdiqIBAjT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIM6vTN%2FFvE7Q0qm3efKtwDoyRuNj8qCRSyr1D14k3QpH06%2BTqB1NJDMawNN%2Fd3eftUsqQMWsBUHlPW4fS7IZjzzsxfXyzTywvEGNXzOacqhROjhhqwIEHcQmuEzva%2Fq7sr93JFgFylhZNH5i6030B31Xrmy3QhFWx1ox%2BHmqyBwEvDp5fgDc3y0obpigKNRXJdQPnqk5BfkjeY%2B4z3pjbi9EzidjaW0Tgq5NWsoYiP%2BCql12BhwLFqnitpfIgAfwUIW1ga54TNzkYtm8BZIO3kA8BfobLJqGKsJ535ZeoBwII9VygJkhQVuQK7ToFr0re9yRxYUIjvu2wCZK7L89QVMHmGa1hWE1CD6zMz2RJziMDhNLOD2cbT%2BMaRDzP0ubhCrMQP5p5s7Fu6FhwOvKyh7xE0x3UUB6WgAI95izKGjf9nYb2gu1dufZ5iHcCUUcmErlAvM2fuPeZrNrw5zgHt6MTeJvU0T9XRCFrpT%2FM3cv8ZMfLPovzSSqm12eqSIQQCY1u%2BEkggeiklKUmowkQZIybTZfL2NiUVbRvsL6%2FsXCfY4iG%2BfCSl9kV4XQktZ7rgyzeiVBfsqM58ss1RtYvgohtBnEPkNbLMA%2FEDWXjlAwH29dZD9IvBTYTEBA6nmZ%2FCGeO33BuH%2FOvShbsw49PhxAY6pgEDK3W91VDJVlBdn20fWQTpstqSNvGyPdZ8A4z7%2BZRuMgND6myHrcaFSCQbeVvPvpt0sfYlo%2BwcXZOf1KenC1Itq1%2BOLMMD%2FQMYjBQIu8tJukAvzMAmw9sZxMd9x%2FBv9fbRAjhk9aHILudzdRO%2B8HNdfxWsYLTWnXmvvGA1NVxq5qkaIIEAmet7Tez8O7s51S18MeVQFYhtTZFOMCX06grZ5pIYrLUL&amp;X-Amz-Signature=6ab3db514270a66f8b9e5246e2f9f7726e598f39f34b022dd6db71bb2b000c01&amp;X-Amz-SignedHeaders=host&amp;x-amz-checksum-mode=ENABLED&amp;x-id=GetObject" alt="Image"/></p> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>\Delta^V: simplex 공간</p> </li> <li> <p>m \in \mathcal{V}: [MASK] token’s one-hot vector</p> </li> </ul> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></h3> <ul> <li> <p>p: denoising, q: noising</p> </li> <li> <p>s(j) = (j-1)/T, t(j) = j/T (이후에 j는 생략!)</p> </li> <li> <p>D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</p> </li> <li> <p>이상적인 diffusion model p_{\theta}는 q의 역방향이므로 D3PM에서는 아래 수식으로 p_{\theta}를 정의</p> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><strong>Block Definition</strong></p> <ul> <li> <p>길이 L’이 되게 B개의 block으로 만들기 (x^b: x^{(b-1)L’:bL’} \in {1,…,B})</p> </li> <li> <p>Likelihood over block</p> </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><strong>Learning Objective</strong></p> <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><strong>Denoiser model</strong></p> <ul> <li> <table> <tbody> <tr> <td>Transformer x_\theta를 사용해 파라미터화: p_\theta(x^b</td> <td>x_t^b, x^{&lt;b})</td> </tr> </tbody> </table> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>x_\theta의 학습: block b 내에서 x_\theta^b(x_t^b, x^{&lt;b}) → L’ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><strong>K, V caching</strong></p> <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><strong>Training</strong></p> <ul> <li>모든 block은 x_\theta의 forward pass를 두 번 거쳐야 함 (x_t^b, x^b) → 계산의 효율화 필요</li> </ul> <p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/3acbc979-3f43-48f4-8683-229c6104ec76/eb4721a8-a676-45e2-b3f2-97af33df40a3/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=ASIAZI2LB466R27ZVFKX%2F20250810%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250810T105949Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCIGh3%2FVK8MxTZGWx8EIBPmzCC5B9Ex0Tk3YZUSo3qieJ0AiAF76nYDUcXmMJxHcjpA3wWo6J0DMA7AzOJDqmdCd1GdiqIBAjT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIM6vTN%2FFvE7Q0qm3efKtwDoyRuNj8qCRSyr1D14k3QpH06%2BTqB1NJDMawNN%2Fd3eftUsqQMWsBUHlPW4fS7IZjzzsxfXyzTywvEGNXzOacqhROjhhqwIEHcQmuEzva%2Fq7sr93JFgFylhZNH5i6030B31Xrmy3QhFWx1ox%2BHmqyBwEvDp5fgDc3y0obpigKNRXJdQPnqk5BfkjeY%2B4z3pjbi9EzidjaW0Tgq5NWsoYiP%2BCql12BhwLFqnitpfIgAfwUIW1ga54TNzkYtm8BZIO3kA8BfobLJqGKsJ535ZeoBwII9VygJkhQVuQK7ToFr0re9yRxYUIjvu2wCZK7L89QVMHmGa1hWE1CD6zMz2RJziMDhNLOD2cbT%2BMaRDzP0ubhCrMQP5p5s7Fu6FhwOvKyh7xE0x3UUB6WgAI95izKGjf9nYb2gu1dufZ5iHcCUUcmErlAvM2fuPeZrNrw5zgHt6MTeJvU0T9XRCFrpT%2FM3cv8ZMfLPovzSSqm12eqSIQQCY1u%2BEkggeiklKUmowkQZIybTZfL2NiUVbRvsL6%2FsXCfY4iG%2BfCSl9kV4XQktZ7rgyzeiVBfsqM58ss1RtYvgohtBnEPkNbLMA%2FEDWXjlAwH29dZD9IvBTYTEBA6nmZ%2FCGeO33BuH%2FOvShbsw49PhxAY6pgEDK3W91VDJVlBdn20fWQTpstqSNvGyPdZ8A4z7%2BZRuMgND6myHrcaFSCQbeVvPvpt0sfYlo%2BwcXZOf1KenC1Itq1%2BOLMMD%2FQMYjBQIu8tJukAvzMAmw9sZxMd9x%2FBv9fbRAjhk9aHILudzdRO%2B8HNdfxWsYLTWnXmvvGA1NVxq5qkaIIEAmet7Tez8O7s51S18MeVQFYhtTZFOMCX06grZ5pIYrLUL&amp;X-Amz-Signature=5cf63f629c543e86d11c4b3feac5c6a5a59d961072eb63d65d4268a5d1e9c165&amp;X-Amz-SignedHeaders=host&amp;x-amz-checksum-mode=ENABLED&amp;x-id=GetObject" alt="Image"/></p> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input x_{t_b}^b 생성</p> </li> <li> <p>\left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}): 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 x^b_{\text{logit}} 계산</p> </li> </ol> <p><strong>Sampling</strong></p> <p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/3acbc979-3f43-48f4-8683-229c6104ec76/58c6f097-5e59-4fe3-ad19-c252725e2e29/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=ASIAZI2LB466R27ZVFKX%2F20250810%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250810T105950Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCIGh3%2FVK8MxTZGWx8EIBPmzCC5B9Ex0Tk3YZUSo3qieJ0AiAF76nYDUcXmMJxHcjpA3wWo6J0DMA7AzOJDqmdCd1GdiqIBAjT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIM6vTN%2FFvE7Q0qm3efKtwDoyRuNj8qCRSyr1D14k3QpH06%2BTqB1NJDMawNN%2Fd3eftUsqQMWsBUHlPW4fS7IZjzzsxfXyzTywvEGNXzOacqhROjhhqwIEHcQmuEzva%2Fq7sr93JFgFylhZNH5i6030B31Xrmy3QhFWx1ox%2BHmqyBwEvDp5fgDc3y0obpigKNRXJdQPnqk5BfkjeY%2B4z3pjbi9EzidjaW0Tgq5NWsoYiP%2BCql12BhwLFqnitpfIgAfwUIW1ga54TNzkYtm8BZIO3kA8BfobLJqGKsJ535ZeoBwII9VygJkhQVuQK7ToFr0re9yRxYUIjvu2wCZK7L89QVMHmGa1hWE1CD6zMz2RJziMDhNLOD2cbT%2BMaRDzP0ubhCrMQP5p5s7Fu6FhwOvKyh7xE0x3UUB6WgAI95izKGjf9nYb2gu1dufZ5iHcCUUcmErlAvM2fuPeZrNrw5zgHt6MTeJvU0T9XRCFrpT%2FM3cv8ZMfLPovzSSqm12eqSIQQCY1u%2BEkggeiklKUmowkQZIybTZfL2NiUVbRvsL6%2FsXCfY4iG%2BfCSl9kV4XQktZ7rgyzeiVBfsqM58ss1RtYvgohtBnEPkNbLMA%2FEDWXjlAwH29dZD9IvBTYTEBA6nmZ%2FCGeO33BuH%2FOvShbsw49PhxAY6pgEDK3W91VDJVlBdn20fWQTpstqSNvGyPdZ8A4z7%2BZRuMgND6myHrcaFSCQbeVvPvpt0sfYlo%2BwcXZOf1KenC1Itq1%2BOLMMD%2FQMYjBQIu8tJukAvzMAmw9sZxMd9x%2FBv9fbRAjhk9aHILudzdRO%2B8HNdfxWsYLTWnXmvvGA1NVxq5qkaIIEAmet7Tez8O7s51S18MeVQFYhtTZFOMCX06grZ5pIYrLUL&amp;X-Amz-Signature=f4ea61d54bf2b7a7b469db57d5ce92b4eaf77a64dd7a53856f1540e57f0cbd0f&amp;X-Amz-SignedHeaders=host&amp;x-amz-checksum-mode=ENABLED&amp;x-id=GetObject" alt="Image"/></p> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> </li> <li> <p>목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</p> </li> </ul> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>L^\prime = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> </li> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> </ul> <p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/3acbc979-3f43-48f4-8683-229c6104ec76/700e7811-3ef0-4009-9169-95751d4f79aa/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&amp;X-Amz-Credential=ASIAZI2LB466R27ZVFKX%2F20250810%2Fus-west-2%2Fs3%2Faws4_request&amp;X-Amz-Date=20250810T105950Z&amp;X-Amz-Expires=3600&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJGMEQCIGh3%2FVK8MxTZGWx8EIBPmzCC5B9Ex0Tk3YZUSo3qieJ0AiAF76nYDUcXmMJxHcjpA3wWo6J0DMA7AzOJDqmdCd1GdiqIBAjT%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAAaDDYzNzQyMzE4MzgwNSIM6vTN%2FFvE7Q0qm3efKtwDoyRuNj8qCRSyr1D14k3QpH06%2BTqB1NJDMawNN%2Fd3eftUsqQMWsBUHlPW4fS7IZjzzsxfXyzTywvEGNXzOacqhROjhhqwIEHcQmuEzva%2Fq7sr93JFgFylhZNH5i6030B31Xrmy3QhFWx1ox%2BHmqyBwEvDp5fgDc3y0obpigKNRXJdQPnqk5BfkjeY%2B4z3pjbi9EzidjaW0Tgq5NWsoYiP%2BCql12BhwLFqnitpfIgAfwUIW1ga54TNzkYtm8BZIO3kA8BfobLJqGKsJ535ZeoBwII9VygJkhQVuQK7ToFr0re9yRxYUIjvu2wCZK7L89QVMHmGa1hWE1CD6zMz2RJziMDhNLOD2cbT%2BMaRDzP0ubhCrMQP5p5s7Fu6FhwOvKyh7xE0x3UUB6WgAI95izKGjf9nYb2gu1dufZ5iHcCUUcmErlAvM2fuPeZrNrw5zgHt6MTeJvU0T9XRCFrpT%2FM3cv8ZMfLPovzSSqm12eqSIQQCY1u%2BEkggeiklKUmowkQZIybTZfL2NiUVbRvsL6%2FsXCfY4iG%2BfCSl9kV4XQktZ7rgyzeiVBfsqM58ss1RtYvgohtBnEPkNbLMA%2FEDWXjlAwH29dZD9IvBTYTEBA6nmZ%2FCGeO33BuH%2FOvShbsw49PhxAY6pgEDK3W91VDJVlBdn20fWQTpstqSNvGyPdZ8A4z7%2BZRuMgND6myHrcaFSCQbeVvPvpt0sfYlo%2BwcXZOf1KenC1Itq1%2BOLMMD%2FQMYjBQIu8tJukAvzMAmw9sZxMd9x%2FBv9fbRAjhk9aHILudzdRO%2B8HNdfxWsYLTWnXmvvGA1NVxq5qkaIIEAmet7Tez8O7s51S18MeVQFYhtTZFOMCX06grZ5pIYrLUL&amp;X-Amz-Signature=5bdf4d742efc53fad469b072da9b4ab09540645608e2412ecab16ef2e7f8b9c3&amp;X-Amz-SignedHeaders=host&amp;x-amz-checksum-mode=ENABLED&amp;x-id=GetObject" alt="Image"/></p> <ul> <li> <table> <tbody> <tr> <td>tuned schedule: q(x_t^\ell = m</td> <td>x^\ell) = 1</td> </tr> </tbody> </table> </li> </ul> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 L^\ell \geq 1인 케이스로 확장하고 싶음!</p> </li> <li> <p>Batch size를 K라고 할 때, batch of sequence \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}], with each \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x)</p> </li> <li> <p><strong>NELBO estimator</strong></p> </li> <li> <p><strong>Variance of the gradient estimator</strong></p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="diffusion"/><category term="diffusionlm"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰 - DiffusionLM, LLM 관련 연구]]></summary></entry><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p><span style="color:yellow_background">→ </span><span style="color:yellow_background"><strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></span></p> <p><br/></p> <p><span style="color:yellow_background"><strong>BD3-LMs</strong></span></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></span></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><br/></p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <p><br/></p> <p><br/></p> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> \[\mathcal{V}=\left\{\mathbf{x} \in\{0,1\}^V: \sum_i \mathbf{x}_i=1\right\} \subset \Delta^V\] <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>$ \Delta^V $: simplex 공간</p> </li> <li> <p>$ m \in \mathcal{V} $: [MASK] token’s one-hot vector</p> </li> </ul> <p><br/></p> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> \[\log p_{\theta}(\mathbf{x}) = \sum_{\ell=1}^{L} \log p_{\theta}(\mathbf{x}^{\ell} | \mathbf{x}^{&lt;\ell})\] <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><span style="color:yellow_background"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></span></h3> <ul> <li> <p>$ p $: denoising, $ q $: noising</p> </li> <li> <p>$ s(j) = (j-1)/T $, $ t(j) = j/T $ (이후에 j는 생략!)</p> <ul> <li>알파벳 순서대로 s가 앞 t가 뒤, s → t 과정은 noise를 더하는 과정</li> </ul> </li> <li> <p><span style="color:yellow_background">D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</span></p> \[\mathbf{x}^{\ell}: q\left(\mathbf{x}_t^{\ell} \mid \mathbf{x}_s^{\ell}\right)=\operatorname{Cat}\left(\mathbf{x}_t^{\ell} ; Q_t \mathbf{x}_s^{\ell}\right) \text { where } Q_t \in \mathbb{R}^{V \times V}\] <ul> <li> <p>$ Q_t $의 예시</p> <ul> <li>Uniform replacement</li> </ul> \[Q_t(i, j)= \begin{cases}1-\beta_t, &amp; j=i \\ \beta_t /(V-1), &amp; j \neq i\end{cases}\] <ul> <li><span style="color:yellow_background"><strong>Masking 기반</strong></span>: $ \beta_t $ 확률로 [MASK] 토큰으로 변경</li> </ul> </li> </ul> </li> <li> <p>이상적인 diffusion model $ p_{\theta} $는 $ q $의 역방향이므로 D3PM에서는 아래 수식으로 $ p_{\theta} $를 정의</p> \[p_\theta\left(\mathbf{x}_s \mid \mathbf{x}_t\right)=\prod_{\ell=1}^L p_\theta\left(\mathbf{x}_s^{\ell} \mid \mathbf{x}_t\right)=\sum_{\mathbf{x}}\left[\prod_{\ell=1}^L q\left(\mathbf{x}_s^{\ell} \mid \mathbf{x}_t^{\ell}, \mathbf{x}^{\ell}\right) p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right)\right]\] <ul> <li> <p>1단계 denoising 과정 = 개별 토큰 위치에 대한 denoise는 독립 과정 = $ x^\ell $ 근사</p> </li> <li> <p>$ x^{\ell} $ (원본 텍스트)가 주어진다면 q를 활용해 $ x_t^\ell \rightarrow x_s^\ell $을 완전히 복구할 수 있음.</p> </li> <li> <p>denoise 과정에서 $ x^\ell $이 주어지지 않으므로 $ p $로 근사: $ p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right) $</p> </li> </ul> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> </ul> \[\mathcal{L}(\mathbf{x} ; \theta)=\mathbb{E}_q\left[-\log p_\theta\left(\mathbf{x} \mid \mathbf{x}_{t(1)}\right)+\sum_{j=1}^T D_{\mathrm{KL}}\left[q\left(\mathbf{x}_{s(j)} \mid \mathbf{x}_{t(j)}, \mathbf{x}\right) \| p_\theta\left(\mathbf{x}_{s(j)} \mid \mathbf{x}_{t(j)}\right)\right]+D_{\mathrm{KL}}\left[q\left(\mathbf{x}_{t(T)} \mid \mathbf{x}\right) \| p_\theta\left(\mathbf{x}_{t(T)} \right)\right]\right]\] <ul> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <p><br/></p> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><span style="color:yellow_background"><strong>Block Definition</strong></span></p> <ul> <li> <p>길이 $ L’ $이 되게 $ B $개의 block으로 만들기 ($ x^b: x^{(b-1)L’:bL’} \in {1,…,B} $)</p> </li> <li> <p>Likelihood over block</p> \[\log p_\theta(\mathbf{x})=\sum_{b=1}^B \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}^{&lt;b}\right)\] </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> \[p_{\theta}(\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^{&lt;b}) = \sum_{\mathbf{x}^b} q(\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^b) p_{\theta}(\mathbf{x}^b | \mathbf{x}_t^b, \mathbf{x}^{&lt;b})\] <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Learning Objective</strong></span></p> \[- \log p_\theta(\mathbf{x}) \leq \mathcal{L}_{\text{BD}}(\mathbf{x}; \theta) := \sum_{b=1}^{B} \mathcal{L}(\mathbf{x}^b, \mathbf{x}^{&lt;b}; \theta)\] <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><br/></p> <p><span style="color:yellow_background"><strong>Denoiser model</strong></span></p> <ul> <li> <table> <tbody> <tr> <td>Transformer $ x_\theta $를 사용해 파라미터화: $ p_\theta(x^b</td> <td>x_t^b, x^{&lt;b}) $</td> </tr> </tbody> </table> <ul> <li> <p>given $ x^{&lt;b} $: AR 특성 유지</p> </li> <li> <p>$ x^b $ 예측: Denosing</p> </li> </ul> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>$ x_\theta $의 학습: block b 내에서 $ x_\theta^b(x_t^b, x^{&lt;b}) $ → $ L’ $ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><br/></p> <p><span style="color:yellow_background"><strong>K, V caching</strong></span></p> \[\mathbf{x}_{\text {logits }}^b, \mathbf{K}^b, \mathbf{V}^b \leftarrow \mathbf{x}_\theta^b\left(\mathbf{x}_t^b, \mathbf{K}^{1: b-1}, \mathbf{V}^{1: b-1}\right):=\mathbf{x}_\theta^b\left(\mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\] <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <p><br/></p> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><span style="color:yellow_background"><strong>Training</strong></span></p> <ul> <li>모든 block은 $ x_\theta $의 forward pass를 두 번 거쳐야 함 ($ x_t^b $, $ x^b $) → 계산의 효율화 필요</li> </ul> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input $ x_{t_b}^b $ 생성</p> </li> <li> <p>$ \left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}) $: 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 $ x^b_{\text{logit}} $ 계산</p> <ul> <li> <p>Naive: B-times loop를 이용해 forward pass를 별도로 진행</p> </li> <li> <p>Vectorized 방식</p> <ul> <li> <p>$ x_{\text {noisy }}=x_{t_1}^1 \oplus x_{t_2}^2 \oplus \cdots \oplus x_{t_B}^B $</p> </li> <li> <p>$ x_{\text{noisy}} \oplus x $을 input으로 하여 한 번에 계산 How? attention mask를 이전 block만 조회하게끔 조절</p> </li> </ul> </li> </ul> </li> </ol> <p><br/></p> <p><span style="color:yellow_background"><strong>Sampling</strong></span></p> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <p><br/></p> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> \[q\left(\mathbf{x}_t^{\ell} \mid \mathbf{x}^{\ell}\right)=\operatorname{Cat}\left(\mathbf{x}_t^{\ell} ; \alpha_t \mathbf{x}^{\ell}+\left(1-\alpha_t\right) \mathbf{m}\right)\] <ul> <li>$ \alpha_0=1 $ → linear scheduler→ $ \alpha_1=0 $</li> </ul> </li> <li> <p><span style="color:yellow_background">목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</span></p> <ul> <li> <p><strong>Zero Masking Probabilities</strong>: clean sequence( $ x^\ell $)에는 mask를 포함하지 않음. (이건 아래의 조건을 위해 필요한듯합니다.)</p> </li> <li> <p><strong>Carry-Over Unmasking</strong>: $ x_t^\ell \neq m $인 경우 $ q\left(x_s^l=x_t^l \mid x_t^l \neq m\right)=1 $. 즉, unmaksed된 token은 다시 mask 되지 않음.</p> <ul> <li>Denoising model 단순화: $ p_\theta\left(x_s^{\ell}=x_t^{\ell} \mid x_t^{\ell} \neq m\right)=1 $</li> </ul> </li> </ul> \[-\log p_\theta(\mathbf{x}) \leq \mathcal{L}_{\mathrm{BD}}(\mathbf{x} ; \theta):=\sum_{b=1}^B \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q \frac{\alpha_t^{\prime}}{1-\alpha_t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\] <ul> <li> <p>$ \alpha_t = \prod_{\tau=1}^{t}(1 - \beta_\tau) $: t시점까지 mask되지 않고 살아남을 확률</p> </li> <li> <p><span style="color:yellow_background">why?</span></p> <ul> <li> <p>t 시점에서 mask transition matrix (noising 과정에서 i→ j로 변환)</p> \[\left[Q_t\right]_{i j}= \begin{cases}1 &amp; \text { if } i=j=m \\ \alpha_t &amp; \text { if } i=j \neq m \\ 1-\alpha_t &amp; \text { if } j=m, i \neq m\end{cases}\] <ul> <li> <p>순서대로 mask는 mask 유지</p> </li> <li> <p>값을 그대로 가질 확률: $ \alpha_t $</p> </li> <li> <p>token이 mask 될 확률: $ 1 - \alpha_t $</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>marginal $ Q_{t</td> <td>s} $ (여기서 $ \alpha_{t</td> <td>s} = \alpha_t/\alpha_s $)</td> </tr> </tbody> </table> \[\left[Q_{t \mid s}\right]_{i j}= \begin{cases}1 &amp; \text { if } i=j=m \\ \alpha_{t \mid s} &amp; \text { if } i=j \neq m \\ 1-\alpha_{t \mid s} &amp; \text { if } j=m, i \neq m\end{cases}\] </li> </ul> <p>전개…… $ \mathcal{L}_{\text{diffusion}} $은 앞의 수식과 의미적으로 같습니다…..</p> \[\mathcal{L}_{\text{diffusion}} = \sum_{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \text{D}_{\text{KL}} \left[ q(\mathbf{x}_s^b|\mathbf{x}_t^b, \mathbf{x}^b) \Vert p_{\theta}(\mathbf{x}_s^b|\mathbf{x}_t^b, \mathbf{x}^{&lt;b}) \right] \right] \\= \sum_{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \sum_{\ell=1}^{L'} \text{D}_{\text{KL}} \left[ q(\mathbf{x}_s^{b,\ell}|\mathbf{x}_t^{b,\ell}, \mathbf{x}^{b,\ell}) \Vert p_{\theta}(\mathbf{x}_s^{b,\ell}|\mathbf{x}_t^b, \mathbf{x}^{&lt;b}) \right] \right]\] <ul> <li> <p>일단 여기까진 정의대로 가되 block 내 token 길이인 $ L’ $으로 확장</p> </li> <li> <p>KL divergence 정의에 의해 다음과 같이 전개 가능 (이건 ㄹㅇ KLD 정의)</p> \[\sum_{\ell=1}^{L^{\prime}} D_{\mathrm{KL}}\left(q \| p_\theta\right)=\sum_{\ell=1}^{L^{\prime}} \mathbb{E}_{q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)}\left[\log q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)-\log p_\theta\left(x_s^{b, l} \mid x_t^{b, l}, x^{&lt;b}\right)\right]\] </li> <li> <p>$ \log{q} $ 부분은 학습과 무관하므로 제외</p> \[\sum_{\ell=1}^{L^{\prime}} \mathbb{E}_{q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)}\left[-\log p_\theta\left(x_s^{b, l} \mid x_t^{b, l}, x^{&lt;b}\right)\right]\] <ul> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = x^{b,\ell}</td> <td>x_t^{b,\ell} = m, x^{b,\ell}) = \frac{\alpha_s - \alpha_t}{1 - \alpha_t} $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = m</td> <td>x_t^{b,\ell} = m, x^{b,\ell}) = \frac{1 - \alpha_s}{1 - \alpha_t} $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = x^{b,\ell}</td> <td>x_t^{b,\ell} = x^{b,\ell}, x^{b,\ell}) = 1 $: 1이므로 뒤에 계산에서 제외</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>$ x_t^{b,\ell} $이 mask인 경우만 계산</p> \[\frac{\alpha_s - \alpha_t}{1 - \alpha_t}[- \log p_\theta(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = m, x^{&lt;b})] + \frac{1 - \alpha_s}{1 - \alpha_t}[- \log p_\theta(x_s^{b,\ell} = m | x_t^{b,\ell} = m, x^{&lt;b})]\] </li> <li> <p>뒤에 항은 mask → mask는 상수라서 계산에서 제외</p> <table> <tbody> <tr> <td>$ = \sum_{b=1}^{B} \mathbb{E}<em>t \mathbb{E}_q T \left[ \sum</em>{\ell=1}^{L’} \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p_\theta(x^{b,\ell}</td> <td>x_t^{b,\ell}, x^{&lt;b}) \right] $</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$ = \sum_{b=1}^{B} \mathbb{E}<em>t \mathbb{E}_q T \left[ \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b}) \right] $</td> </tr> </tbody> </table> </li> </ul> <p>$ T \rarr \infin, T(\alpha_t - \alpha_s) = \alpha’_t $</p> </li> </ul> </li> </ul> <p><br/></p> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>$ L^\prime $ = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> <ul> <li> <p><span style="color:yellow_background"><strong>직관적 핵석</strong></span>: block의 길이가 1이라면 한 토큰 단위 AR과 같음. → ??? 그래도 한 토큰 단위로 일어나는 diffusion 과정이 있는데? → mask로 intitialize 후, 원하는 다음 token을 찾는 과정이란 점에선 동일.</p> </li> <li> <p><strong>수식 ok</strong></p> \[\begin{aligned}&amp;-\log p(\mathbf{x}) \leq \sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q\left[\frac{\alpha_t^{\prime}}{1-\alpha_t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] \\&amp; \because \alpha_t^{\prime}=-1 \text { and } \alpha_t=1-t, \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q {\left[\frac{1}{t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] } \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \frac{1}{t} \mathbb{E}_q\left[\log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] \\&amp; \text { Expanding } \mathbb{E}_q[.], \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \frac{1}{t} {\left[q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right) \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right)\right.} \\&amp;\left.\quad+q\left(\mathbf{x}_t^b=\mathbf{x}^b \mid \mathbf{x}^b\right) \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b=\mathbf{x}^b, \mathbf{x}^{&lt;b}\right)\right]\end{aligned}\] <ul> <li> <p>linear scheduler에서 $ \alpha’_t, \alpha_t $의 정의는 위와 같음. 그 다음 전개 과정은 이해할 수 있을듯?</p> </li> <li> <p>Expanding 부분은 Expatation of q를 제거 하기 위한 과정 q가 mask transition을 전제로 하므로 경우 (mask/unmask) 두 가지 확률에 대해서 전개</p> </li> <li> <table> <tbody> <tr> <td>SUBS-parameterization 가정의 carry-over unmasking 특성으로 $ \log{p_\theta(x^b</td> <td>x_t^b=x^b, x^{&lt;b})} = 0 $</td> </tr> </tbody> </table> <p>$$ \begin{aligned}-\log p_\theta(\mathbf{x}) &amp; \leq-\sum_{b=1}^L \mathbb{E}<em>{t \sim[0,1]} \frac{1}{t} q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right) \log p</em>\theta\left(\mathbf{x}^b \mid \mathbf{x}<em>t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right) \&amp; \because q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right)=t, \text { we get: } \&amp; =-\sum</em>{b=1}^L \mathbb{E}<em>{t \sim[0,1]} \log p</em>\theta\left(\mathbf{x}^b \mid \mathbf{x}<em>t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right)\&amp; = -\sum</em>{b=1}^{L} \log p_\theta(\mathbf{x}^b \mid \mathbf{m}, \mathbf{x}^{&lt;b})</p> </li> </ul> </li> </ul> </li> </ul> <p>\end{aligned} $$</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>		- $ q(x_t^b=m|x^b) = 1 - \alpha_t = 1 - (1 - t) = t $

		- $ t $는 상관없으니깐 삭제!

		- 최종 결과는 NLL 로스와 기대값이 같다!
</code></pre></div></div> <ul> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? $ \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) $ = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>tuned schedule: $ q(x_t^\ell = m</td> <td>x^\ell) $ = 1</td> </tr> </tbody> </table> <ul> <li> <p>해당 schedule에서는 AR의 목적함수와 완전히 동일</p> </li> <li> <p>PPL도 감소, NELBO의 분산도 감소</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  &lt;br/&gt;
</code></pre></div> </div> </li> </ul> </li> </ul> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 $ L^\ell \geq 1 $인 케이스로 확장하고 싶음!</p> <ul> <li> <p>NELBO는 이론적으로 t에 invariance (기존 연구 <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/eb0b13cc515724ab8015bc978fdde0ad-Paper-Conference.pdf">ref</a>: T가 무한히 커질수록 $ \alpha $값이 아닌 누적값에 의해서 기대값이 정의되기 때문… 이 이상의 이해는 포기)하기에 스케줄에 따른 기대값의 변화가 없어야 함.</p> </li> <li> <p>하지만 우리는 모든 연산을 한 번에 하는 것이 아닌 Batch 연산을 활용 → 이론적인 invariance가 깨짐</p> </li> </ul> <p>→ Schedule에 따라 분산의 결과가 변하게 됨. → Schedule을 잘 만들어보자!</p> </li> <li> <p>Batch size를 $ K $라고 할 때, batch of sequence $ \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}] $, with each $ \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x) $</p> </li> <li> <p><span style="color:yellow_background"><strong>NELBO estimator</strong></span></p> \[\mathcal{L}_{\text{BD}}(\mathbf{X};\theta) := l(\mathbf{X};\theta) = \frac{1}{K} \sum_{k=1}^{K} \sum_{b=1}^{B} \frac{\alpha'_{t(k,b)}}{1 - \alpha_{t(k,b)}} \log p_{\theta} \left( \mathbf{x}^{(k),b} \mid \mathbf{x}_{t(k,b)}^{(k),b}, \mathbf{x}^{(k),&lt;b} \right)\] </li> <li> <p><span style="color:yellow_background"><strong>Variance of the gradient estimator</strong></span></p> \[\text{Var}_{\mathbf{X},t} \left[ \nabla_{\theta}l(\mathbf{X};\theta) \right] \approx \frac{1}{M-1} \sum_{m=1}^{M} \left\| \nabla_{\theta}l(\mathbf{X}^m;\theta) - \frac{1}{M} \sum_{m=1}^{M} \nabla_{\theta}l(\mathbf{X}^m;\theta) \right\|_2^2\] </li> </ul> <p><br/></p> <h2 id="low-variance-noise-schedules-for-bd3-lms">LOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS</h2> <h3 id="intuition-avoid-extreme-mask-rates--clipped-schedules-for-low-variance-gradients">INTUITION: AVOID EXTREME MASK RATES &amp; CLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS</h3> <ul> <li> <p>이상적인 마스킹: 모델이 다양한 수준의 노이즈 [MASK]에서 원래대로 되돌리는 법을 배우는 것</p> </li> <li> <p>극단적인 마스킹</p> <ul> <li> <p>마스킹 토큰이 너무 적을 경우, 너무 쉬운 문제를 풀게 됨.</p> </li> <li> <p>모든 토큰이 마스킹 될 경우, 문맥 정보가 전혀 없음 빈도에 기반한 학습만 진행</p> </li> </ul> </li> </ul> <p>→ 극단적인 부분을 날린 CLIP을 이용하자</p> <p>→ sample mask rates: $ 1 - \alpha_t \sim \mathcal{U}[\beta, \omega] $ for $ 0 \leq \beta, \omega \leq 1 $</p> <p><br/></p> <h3 id="data-driven-clipped-schedules-across-block-sizes">DATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES</h3> <ul> <li> <p>Block size ( $ L’ $)에 따른 최적의 mask rate을 찾아보자.</p> </li> <li> <p>Gradient 분산을 최소화하기 위함이지만 아래 NELBO를 추정지로 하여 실험을 진행</p> \[\text{min}_{\beta,\omega} \text{Var}_{X,t}[\mathcal{L}(X; \theta, \beta, \omega)]\] <ul> <li> <p>forward pass만으로 계산 가능</p> </li> <li> <p>실험 결과들에서 NELBO와 기울기 분산이 같은 경향성을 보임을 확인</p> </li> </ul> </li> <li> <p>$ \beta, \omega $에 대해 grid search 진행</p> </li> <li> <p>Table 2에서 PPL과 NELBO과 상관성 보임을 재차 확인 + $ L’ $에 따라 최적의 조합이 있음을 발견함.</p> </li> </ul> <p><br/></p> <h2 id="experiments">EXPERIMENTS</h2> <ul> <li> <p>Pre-train: base BD3-LM ( $ L’=L $) for 850K gradient steps (순수 diffusion?)</p> </li> <li> <p>Fine-tune</p> <ul> <li>150K gradient steps on One Billion Words dataset (LM1B) and OpenWebText (OWT)</li> </ul> </li> <li> <p>$ L’ $에 따라 다른 Clipped schedule 적용 (매 validation epoch 마다 최적의 $ \beta, \omega $ 조합을 찾음!)</p> </li> </ul> <p><br/></p> <h3 id="likelihood-evaluation">LIKELIHOOD EVALUATION</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

		[//]: # (column is not supported)

	- 다른 MDLM 모델 대비 perplexity이 향상됨
</code></pre></div></div> <ul> <li> <p>Zero-shot validation perplexity 결과 Pubmed는 AR보다도 잘함.</p> </li> <li> <p>대체로 다른 MDLM보단 PPL 값이 더 낮음.</p> </li> </ul> <p><br/></p> <h3 id="sample-quality-and-variable-length-sequence-generation">SAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION</h3> <ul> <li> <p>[EOS] 토큰을 생성하거나 sample quality가 급감 (the average entropy of the the last 256-token chunk is below 4)할 때까지 실험 진행</p> </li> <li> <p>SEDD 대비 최대 10배 더 긴 text 생성 가능함.</p> </li> <li> <p>GPT-2를 이용해 generative perplexity 측정, 효율성을 보기 위해 the number of generation steps (NFEs)</p> </li> <li> <p>기존 Block Diffusion 대비해도 더 적은 step에서 높은 Gen PPL 달성</p> </li> <li> <p>정성 분석은 Appendix D에 있음. AR과 유사할 정도의 퀄리티, 다른 DLM보단 좋더라</p> </li> </ul> <p><br/></p> <h3 id="ablations">ABLATIONS</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

	&lt;span style='color:yellow_background'&gt;**SELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE**&lt;/span&gt;

			- $ L' $이 작을수록 heavier mask가 효과적

[//]: # (column is not supported)

	&lt;span style='color:yellow_background'&gt;**EFFICIENCY OF TRAINING ALGORITHM**&lt;/span&gt;

			- concat 활용하여 처리할 경우, sparse attention mask 활용

	- FlexAttention을 이용할 경우 Sparsity를 활용해 효율적 처리 가능

	- 20-25% 속도 향상 가능!
</code></pre></div></div> <p><br/></p> <p><br/></p> <hr/> <ul> <li> <p>수학 공부 열심히 하자.</p> </li> <li> <p>결과에서 힘이 많이 빠지긴 한다.</p> </li> <li> <p>전개과정에서 이 정도는 해야 oral로 가는구나 벽느껴진다.</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><category term="nlp"/><summary type="html"><![CDATA[논문 리뷰 - BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS]]></summary></entry><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p>→ <strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></p> <p><strong>BD3-LMs</strong></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>\Delta^V: simplex 공간</p> </li> <li> <p>m \in \mathcal{V}: [MASK] token’s one-hot vector</p> </li> </ul> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></h3> <ul> <li> <p>p: denoising, q: noising</p> </li> <li> <p>s(j) = (j-1)/T, t(j) = j/T (이후에 j는 생략!)</p> </li> <li> <p>D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</p> </li> <li> <p>이상적인 diffusion model p_{\theta}는 q의 역방향이므로 D3PM에서는 아래 수식으로 p_{\theta}를 정의</p> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><strong>Block Definition</strong></p> <ul> <li> <p>길이 L’이 되게 B개의 block으로 만들기 (x^b: x^{(b-1)L’:bL’} \in {1,…,B})</p> </li> <li> <p>Likelihood over block</p> </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><strong>Learning Objective</strong></p> <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><strong>Denoiser model</strong></p> <ul> <li> <table> <tbody> <tr> <td>Transformer x_\theta를 사용해 파라미터화: p_\theta(x^b</td> <td>x_t^b, x^{&lt;b})</td> </tr> </tbody> </table> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>x_\theta의 학습: block b 내에서 x_\theta^b(x_t^b, x^{&lt;b}) → L’ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><strong>K, V caching</strong></p> <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><strong>Training</strong></p> <ul> <li>모든 block은 x_\theta의 forward pass를 두 번 거쳐야 함 (x_t^b, x^b) → 계산의 효율화 필요</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input x_{t_b}^b 생성</p> </li> <li> <p>\left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}): 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 x^b_{\text{logit}} 계산</p> </li> </ol> <p><strong>Sampling</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> </li> <li> <p>목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</p> </li> </ul> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>L^\prime = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> </li> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <table> <tbody> <tr> <td>tuned schedule: q(x_t^\ell = m</td> <td>x^\ell) = 1</td> </tr> </tbody> </table> </li> </ul> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 L^\ell \geq 1인 케이스로 확장하고 싶음!</p> </li> <li> <p>Batch size를 K라고 할 때, batch of sequence \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}], with each \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x)</p> </li> <li> <p><strong>NELBO estimator</strong></p> </li> <li> <p><strong>Variance of the gradient estimator</strong></p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="diffusion"/><category term="diffusionlm"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰 - DiffusionLM, LLM 관련 연구]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>많은 연구들이 LLM이 사전학습 단계에서 학습 데이터를 외우는 이슈에 대해서 보고하고 있는 한편, finetuning에 대해서 비슷한 연구는 놀라울 정도로 적다.</p> <p>하지만, finetuning도 당연히 모델 대부의 업데이트와 때때로는 구조적인 변화까지도 이루어지기 때문에, finetuning의 memorization level에 대한 연구도 필요하다.</p> <p>그렇다면, 존재하는 다양한 finetuning 방법에 따른 memorization of fineuning data의 영향력은 어떻게 되는가?</p> <p><br/></p> <p>해당 연구는 이를 시험하기 위해 우선 finetuning 방법을 크게 두 가지로 구분한다:</p> <ol> <li> <p>Parameter-based finetuning: 모델 파라 바꿈</p> </li> <li> <p>Prompt-based fine-tuning: 모델 파라 고정, soft token/prefix embedding…</p> </li> </ol> <p><br/></p> <p>결과적으로 두 카테고리를 고루 포함한 5가지 방법을 시험했고,</p> <p>평가는 다양한 MIAs(membership inference attacks )로 했고,</p> <p>데이터는 Wikitext, WebNLG, Xsum 세 가지로 했다 (좀 적긴하네요)</p> <p><br/></p> <p>간단하고 빠르게 다음으로 넘어갑시다</p> <ul> <li> <p>Parameter-based fine-tuning</p> <ul> <li> <p>Model Head Tuning (FT head): fine-tunes only the final output layer</p> </li> <li> <p>Low-Rank Adaptation (LoRA) (Hu et al., 2021)</p> </li> </ul> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> <ul> <li> <p><strong>Prefix Tuning</strong></p> <ul> <li>각 attention layer의 key/value에 학습 가능한 prefix 벡터 추가.</li> </ul> </li> <li> <p><strong>Prompt Tuning</strong></p> <ul> <li>모델 입력 임베딩 앞에 학습 가능한 연속형 프롬프트 임베딩 추가.</li> </ul> </li> <li> <p><strong>P-tuning</strong></p> <ul> <li>별도의 신경망으로 학습한 연속형 프롬프트를 입력에 삽입.</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p>사용된 MIA 기법과 점수 계산 방식:</p> <ol> <li> <p><strong>LOSS</strong> (Yeom et al., 2018)</p> <ul> <li> <p>Membership Score = 모델의 손실</p> <p>$ \text{Score} = L(x, M_t) $</p> <p>(손실이 낮을수록 멤버일 가능성 ↑)</p> </li> </ul> </li> <li> <p><strong>Reference-based (Ref)</strong> (Mireshghallah et al., 2022a)</p> <ul> <li> <p>기준 모델 MrM_rMr와 비교하여 손실 차이 계산</p> <p>$ \text{Score} = L(x, M_t) - L(x, M_r) $</p> </li> </ul> </li> <li> <p><strong>Zlib Entropy (Zlib)</strong> (Carlini et al., 2021)</p> <ul> <li> <p>손실을 zlib 엔트로피로 나눈 비율</p> <p>$ \text{Score} = \frac{L(x, M_t)}{\text{zlib}(x)} $</p> </li> </ul> </li> <li> <p><strong>Min-K%</strong> (Shi et al., 2024)</p> <ul> <li> <p>토큰 확률이 낮은 하위 k% 토큰들의 평균 로그 likelihood</p> <table> <tbody> <tr> <td>$ \text{Score} = \frac{1}{E} \sum_{x_i \in \text{Min-}K\%(x)} \log p(x_i</td> <td>x_{&lt;i}) $</td> </tr> </tbody> </table> </li> </ul> </li> </ol> </li> </ul> <p><br/></p> <ul> <li> <p>데이터</p> <ul> <li> <p>Wikitext-2-raw-1</p> </li> <li> <p>WebNLG</p> <ul> <li>triple로 이루어짐 (Subject-Predicate-Object)</li> </ul> </li> <li> <p>Xsum: 요약</p> <ul> <li>finetuning에 5000개만 사용</li> </ul> </li> </ul> <p><br/></p> </li> <li> <p>평가</p> <ul> <li>training and test sets에서 샘플링</li> </ul> <p><br/></p> </li> <li> <p>모델</p> <ul> <li> <p>LLaMA 2-7B (Touvron et al., 2023)</p> </li> <li> <p>GPT2-series (Radford et al., 2019)</p> </li> <li> <p>LLaMA 3-1B</p> </li> </ul> <p>→ 2025의 논문이라고 믿기지 않는군여!</p> <p><br/></p> </li> <li> <p>Evaluation Metrics</p> <ul> <li> <p>PERF: validation PPL as the primary metric</p> </li> <li> <p>MIA: AUC-ROC</p> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p>Implementation Details</p> <ul> <li> <p>15 epoch</p> </li> <li> <p>모든 세팅은 논문에 나온거 그대로 따라함</p> </li> <li> <p>4090이랑 H100 한대 사용</p> </li> </ul> </li> </ul> <p><br/></p> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <p><br/></p> <p><br/></p> <blockquote> <p>Observation ♯1: (당연)</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter-based fine-tuning demonstrates a higher tendency to explicitly memorize training data.
</code></pre></div></div> <p><br/></p> <p>모든 방법론은 validation PPL기준으로 성능 좋았음.</p> <p>하지만, prompt-based methods 는 parameter-based 보다 외우는 성능 떨어짐 (당연)</p> <p><br/></p> <blockquote> <p>Observation ♯2:</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter-based fine-tuning exhibits increasing memorization over training epochs, while prompt-based fine-tuning maintains consistently low memorization throughout training.
</code></pre></div></div> <p><br/></p> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the model’s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <p><br/></p> <ul> <li> <p><strong>Prefix Tuning 수식 (Petrov et al., 2024)</strong></p> <p> $  t^{pt}<em>i = A^{pt}</em>{i0} W_V S_1 + (1 - A^{pt}_{i0})\; t_i $</p> <ul> <li> <p>soft-prefix가 어텐션 가중치 $ A^{pt} $를 ‘어디를 볼지’만 재조정, <strong>본래 토큰 간 상대 분포는 그대로</strong>.</p> </li> <li> <p>즉 <strong>새로운 attention 패턴을 학습</strong>하기보다는 <strong>기존 능력을 재활용</strong>.</p> </li> </ul> </li> <li> <p>결과적으로 <strong>표현 공간의 이동(shift) &lt; 적음</strong> → 학습, 비학습 샘플 분포 차이가 작아 MIA가 어렵다.</p> <ul> <li>Petrov et al. (2024) prove that the presence of a prefix does not alter the relative distribution of the input but only shifts the attention to different content.</li> </ul> </li> </ul> <p><br/></p> <p>이 가설을 확인하기 위해:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7B를 세 세팅에서 비교함:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <p>LoRA는 membership and non-membership samples 사이 분포 차이가 큰데, prefix tuning은 미미하다는 것을 알 수 있음</p> <p><br/></p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <p><br/></p> <p>두 방법론이 최종적으로는 비슷한 PPL을 가졌음에도 불구하고, Learning trajactories는 꽤나 달랐음</p> <p><br/></p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>이는 아까도 이야기 했듯이, 후자가 internal sample distribution of the model을 바꾸는 것이 아니라 단순히 다운스트림 태스크에 쪼끔 더 나은 bias를 추가하는 정도임을 다시한번 보인다</p> <p><br/></p> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>모델 사이즈가 memorization에 중요한 영향력을 줄 것임.</p> <p>→ To what extent does model size influence memorization under different fine-tuning strategies?</p> <p><br/></p> <p><br/></p> <blockquote> <p>Observation ♯3</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model size significantly enhances memorization in parameter-based fine-tuning methods, while prompt-based methods show minimal sensitivity and maintain consistently low memorization.
</code></pre></div></div> <p><br/></p> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p><br/></p> <p>LLaMA2-7B vs LLaMA3-1B</p> <p><br/></p> <h2 id="-요약-파라미터-바꾸는-애들은-모델-크기-커질수록-더-잘-외웠는데-반대는-미미하더라-low-sensitivity-of-prompt-tuning-to-model-scale">📝 요약 파라미터 바꾸는 애들은 모델 크기 커질수록 더 잘 외웠는데 반대는 미미하더라 (low sensitivity of prompt tuning to model scale)</h2> <p>특히, gpt2의 경우나 1B 스케일에서 LoRA는 사실상 거의 못외움</p> <p><br/></p> <p><br/></p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation ♯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p><br/></p> <p>다운스트림 태스크의 종류에 따라서도 다를 수 있음. 이를 위 LLaMA2-7B를 다양한 방법을 통해 학습시키고 LOSS attack against에 대해서 각각을 평가해봄</p> <p><br/></p> <p><br/></p> <p>Prompt-based 만 봤을 때, WebNLG가 다른 것들에 비해서 성능이 높다</p> <p>아마도 구조화된 pattern학습에는 유리한 것 같다</p> <p><br/></p> <h2 id="impact-of-lora-placement-on-memorization">Impact of LoRA Placement on Memorization</h2> <p>AUC↑ ⇒ 기억(privacy risk)↑</p> <p><br/></p> <ol> <li> <p><strong>Projection &gt; Attention</strong></p> <ul> <li>LoRA를 <strong>projection layer</strong>에만 적용할 때, 두 데이터셋 모두 네 가지 MIA 지표에서 <strong>AUC가 일관되게 상승</strong> → 기억이 더 강해짐.</li> </ul> </li> <li> <p><strong>Both layers = 기억 제일 강함</strong></p> <ul> <li>Attention + Projection 동시 적용 시 <strong>가장 높은 AUC</strong> → 최대 수준의 memorization.</li> </ul> </li> <li> <p><strong>메커니즘 해석</strong></p> <ul> <li> <p>Projection layer는 <strong>특징 변환, 정보 압축</strong>을 담당 → 학습 데이터의 구체적 패턴을 더 잘 ‘붙잡아 두는’ 위치.</p> </li> <li> <p>결과는 Meng et al. (ROME)의 Transformer 기억은 주로 projection 층에 집중한다는 가설을 재확인.</p> </li> </ul> <p><br/></p> <p><br/></p> </li> </ol> <p>Practical한 관점에서…</p> <ul> <li> <p>프라이버시에 민감한 애플리케이션에서는 LoRA를 attention 층에만 삽입하거나 rank를 낮추어 위험을 완화.</p> </li> <li> <p>성능과의 트레이드오프가 필요할 때, 삽입 위치(attn vs proj)와 범위(단일 vs 복합 층)를 주요 조절 변수로 활용하면 효과적일 수 있겠다!</p> </li> </ul> <p><br/></p> <p><br/></p> <p>너무 많죠..하지만 저자가 이야기한 것만 말해보겠습니다.</p> <ol> <li> <p>larger model</p> </li> <li> <p>MoE 같은 다른 구조</p> </li> <li> <p>데이터 적음</p> </li> </ol> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰 - Impact of Fine-Tuning Methods on Memorization in Large Language Models]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>많은 연구들이 LLM이 사전학습 단계에서 학습 데이터를 외우는 이슈에 대해서 보고하고 있는 한편, finetuning에 대해서 비슷한 연구는 놀라울 정도로 적다.</p> <p>하지만, finetuning도 당연히 모델 대부의 업데이트와 때때로는 구조적인 변화까지도 이루어지기 때문에, finetuning의 memorization level에 대한 연구도 필요하다.</p> <p>그렇다면, 존재하는 다양한 finetuning 방법에 따른 memorization of fineuning data의 영향력은 어떻게 되는가?</p> <p>해당 연구는 이를 시험하기 위해 우선 finetuning 방법을 크게 두 가지로 구분한다:</p> <ol> <li> <p>Parameter-based finetuning: 모델 파라 바꿈</p> </li> <li> <p>Prompt-based fine-tuning: 모델 파라 고정, soft token/prefix embedding…</p> </li> </ol> <p>결과적으로 두 카테고리를 고루 포함한 5가지 방법을 시험했고,</p> <p>평가는 다양한 MIAs(membership inference attacks )로 했고,</p> <p>데이터는 Wikitext, WebNLG, Xsum 세 가지로 했다 (좀 적긴하네요)</p> <p>간단하고 빠르게 다음으로 넘어갑시다</p> <h1 id="fine-tuning-methods">Fine-Tuning Methods</h1> <ul> <li> <p>Parameter-based fine-tuning</p> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> </li> </ul> <h1 id="memorization-and-mias">Memorization and MIAs</h1> <ul> <li>사용된 MIA 기법과 점수 계산 방식:</li> </ul> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>데이터</p> </li> <li> <p>평가</p> </li> <li> <p>모델</p> </li> <li> <p>Evaluation Metrics</p> </li> <li> <p>Implementation Details</p> </li> </ul> <h1 id="results-and-observations">Results and Observations</h1> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <blockquote> <p>Observation ♯1: (당연)</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>모든 방법론은 validation PPL기준으로 성능 좋았음.</p> <p>하지만, prompt-based methods 는 parameter-based 보다 외우는 성능 떨어짐 (당연)</p> <blockquote> <p>Observation ♯2:</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the model’s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <ul> <li> <p><strong>Prefix Tuning 수식 (Petrov et al., 2024)</strong></p> </li> <li> <p>결과적으로 <strong>표현 공간의 이동(shift) &lt; 적음</strong> → 학습, 비학습 샘플 분포 차이가 작아 MIA가 어렵다.</p> </li> </ul> <p>이 가설을 확인하기 위해:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7B를 세 세팅에서 비교함:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>LoRA는 membership and non-membership samples 사이 분포 차이가 큰데, prefix tuning은 미미하다는 것을 알 수 있음</p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>두 방법론이 최종적으로는 비슷한 PPL을 가졌음에도 불구하고, Learning trajactories는 꽤나 달랐음</p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>이는 아까도 이야기 했듯이, 후자가 internal sample distribution of the model을 바꾸는 것이 아니라 단순히 다운스트림 태스크에 쪼끔 더 나은 bias를 추가하는 정도임을 다시한번 보인다</p> <h1 id="discussion">Discussion</h1> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>모델 사이즈가 memorization에 중요한 영향력을 줄 것임.</p> <p>→ To what extent does model size influence memorization under different fine-tuning strategies?</p> <blockquote> <p>Observation ♯3</p> </blockquote> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p>LLaMA2-7B vs LLaMA3-1B</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>요약: 파라미터 바꾸는 애들은 모델 크기 커질수록 더 잘 외웠는데 반대는 미미하더라 (low sensitivity of prompt tuning to model scale)</p> <p>특히, gpt2의 경우나 1B 스케일에서 LoRA는 사실상 거의 못외움</p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation ♯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p>다운스트림 태스크의 종류에 따라서도 다를 수 있음. 이를 위 LLaMA2-7B를 다양한 방법을 통해 학습시키고 LOSS attack against에 대해서 각각을 평가해봄</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005-480.webp 480w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005-800.webp 800w,/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prompt-based 만 봤을 때, WebNLG가 다른 것들에 비해서 성능이 높다</p> <p>아마도 구조화된 pattern학습에는 유리한 것 같다</p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="embedding"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근 LLM이 복잡한 reasoning task에서 괄목할만한 성능을 보이고 있으나, (모델에게) 편한 reasoning pattern에 의존하는 경향이 있음</p> <ul> <li>이를 Reaosning rigidity로 정의</li> </ul> </li> <li> <p>사용자의 명시적인 instruction이 있음에도 불구하고, 습관적인 reasoning trajectory를 생성, 오답으로 귀결</p> <ul> <li>특히 수학과 퍼즐 분야에서 두드러짐</li> </ul> </li> <li> <p>이를 분석하기 위해 reasoning trap이라는 진단 데이터셋을 도입</p> <ul> <li> <p>deviation을 요구하도록 기존 데이터셋을 수정한 상태</p> </li> <li> <p>예를 들어, 모든 토끼는 불임이다. 토끼가 3쌍이 있고 토끼 한 쌍이 암수 각 1마리씩 총 2마리를 낳는다고 가정하면, 2세대를 거치면 몇마리의 토끼가 되는가? 라는 질문.</p> </li> </ul> <p>⇒ 이를 통해서 모델이 습관적으로 쓰는 contamination된 pattern을 식별할 수 있음</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - 모델이 주어진 instruction 을 무시하거나 왜곡하도록 함
</code></pre></div> </div> </li> <li> <p>reasoning trap을 통해서 모덷이 습관적으로 사용하는 reasoning pattern을 발견, 분류</p> <ul> <li> <p>interpretation overload</p> </li> <li> <p>input distrust</p> </li> <li> <p>partial instruction attnetion</p> </li> </ul> <p>⇒ 해당 데이터셋을 통해 LLM에 있는 reasoning rigidity를 해소하는 미래 연구를 용이하게 함</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM은 수학, 복잡한 코딩 문제, 퍼즐 풀이를 포함한 여러 어려운 태스크에서 주목할만한 성능을 보임</p> <ul> <li>특히 test-time scaling을 활용해 확장된 CoT prompting을 활용하는 reasoning model들이 큰 주목을 받고 있음</li> </ul> </li> <li> <p>하지만, 이러한 모델들에게 문제 행동, reasoning rigidity가 발견됨</p> <ul> <li>특히 긴 CoT reasoning으로 학습된 모델에게 나타남</li> </ul> </li> <li> <p>reasoning rigidity는 cognitive bias를 반영, 주어진 조건을 이해해도 자기 방식대로 override, 무시하고 문제를 푸는 현상을 뜻함</p> <p>⇒ 이는 기존에 언급되어왔던 hallucinataion, prompt brittlness들을 해소해도 존재할 수 있음</p> <ul> <li> <p>hallucination : 틀린 정보를 생성하는 것</p> </li> <li> <p>prompt brittlness : 미묘한 prompt 차이에 따라 답변이 바뀜. 답변이 unstable한 현상</p> </li> <li> <p>reasoning rigidity는 사용자가 서술한 제약이 중요한 도메인에서 큰 문제가 됨</p> </li> <li> <p>예를 들어, 수학이나 퍼즐 풀이의 경우, 뒤의 문제와 관계 없이 유저가 바로 정답으로 이어질 수 있는 조건을 줬을 경우, 이를 무시하면 완전히 오답이 될 수 밖에 없음</p> </li> </ul> <p>⇒ 사용자의 지시를 무의식 중에 편집하거나 무시(reasoning rigidity), 모델의 reasoning path전체가 오염, 오답으로 이어짐</p> <ul> <li>이러한 현상은 아주 크리티컬하나, 본 논문에서 처음으로 문제를 정의</li> </ul> </li> <li> <p>reasoning rigidity를 식별할 수 있도록, 기존의 수학, 퍼즐 데이터셋을 활용해 reasoningtrap이라는 벤치마크를 제안</p> <ul> <li> <p>잘 알려진 cahllenges와 닮았으나, 조건이 수정되어서 답이 완전 바뀌는 문제들로 구성</p> </li> <li> <p>모델이 습관적으로 문제를 풀 경우 오답으로 이어지는 구조로 설계</p> </li> </ul> </li> <li> <p>ReasoningTrap으로 여러 모델을 평가한 결과, 여러 중요한 현상들을 발견</p> <ul> <li> <p>reasoning process의 중간 단계에서 contamination이 시작</p> </li> <li> <p>이러한 contamination은 명백하게 식별 가능, 반복되는 패턴을 가짐</p> </li> </ul> </li> <li> <p>또한, 이러한 contamination의 패턴을 3가지로 분류</p> <ul> <li>interpretation overload, input distrust, partial insturction attention</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> <ul> <li> <p>LLM의 reasoning ability를 향상시키기 위해 CoT를 길게 생성하도록 학습하는 방법론이 제안</p> </li> <li> <p>또한, Qwen3의 경우 reasoning과 non-reasoning mode를 둘다 지원하는 unified fusion architecture를 공개</p> <ul> <li>user가 모델이 긴 CoT를 생성하도록 할지 여부를 고를 수 있음</li> </ul> </li> </ul> </li> <li> <p>Instruction following of reasoning models</p> <ul> <li> <p>여러 in-context examples 혹은 장황한 instruction을 넣으면 reasoning model들의 성능이 떨어진다는 것을 잘 알려짐</p> <p>⇒ 즉, LRM이 user-provided example을 following하는 능력이 부족</p> </li> <li> <p>본 연구는 이와 결은 같지만, 모델이 친숙한 reaosning pattern을 고집한다는 것에 초점을 둠</p> </li> </ul> </li> <li> <p>Rigidity in reasoning models</p> <ul> <li> <p>몇몇 연구들이 LLM이 reasoning할 때 rigid pattern을 보인다는 것을 지적함</p> <ul> <li> <p>medical domain, educational domain</p> </li> <li> <p>우리의 연구는 더 큰 도메인이 수학, 퍼즐에 초첨</p> </li> </ul> </li> <li> <p>본 연구와 유사하게, 몇몇 논문들이 LLM이 rigidity를 탐구</p> <ul> <li>이러한 연구들은 LLM이 creative problem solving에 적용될 때 혹은 matha word problem의 unseen variant의 일반화 에 초점을 둠</li> </ul> </li> </ul> </li> <li> <p>Underlying reason for rigidity</p> <ul> <li> <p>몇몇 연구들이 왜 LLM이 이러한 rigidity를 가지는지에 대해 분석했고, training data 혹은 optimization 방식에 임베딩된 bias를 지적</p> </li> <li> <p>한 연구에서 RL로 학습된 모델들이 exploitation이 뛰어나고, 이로 인해 높은 성능은 달성했지만 역설적이게도 non-reasoning model에 비해 좁은 knowledge coverage를 보인다고 주장</p> </li> <li> <p>다른 연구에서는 training data에 내재된 bias때문이라고 함</p> </li> </ul> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>크게 2가지로 도메인으로 구성 : 수학(ConditionedMath), 퍼즐(PuzzleTrivial)</p> </li> <li> <p>각 데이터는 원래 Q-R-A tuple (q_orig, r_orig, a_orig)과 수정된 tuple (q_mod, r_mod, a_mod)로 구성</p> </li> <li> <p>총 164개의 데이터셋, 84개는 수학, 80개는 퍼즐</p> </li> <li> <p>ConditionedMath에 있는 모든 질문은 개념적으로 다르고, 겹치지 않고, human annotator에 의해 엄격하게 검증됨</p> </li> <li> <p>PuzzleTrival은 10개의 puzzle concept를 가짐</p> </li> </ul> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5를 활용해서 제작</p> </li> <li> <p>원래 질문을 수정하고, 수정된 질문이 아래 조건에 부합하는지를 확인, 필터링</p> <ul> <li> <p>validity : 기존 condition과 모순되는지</p> </li> <li> <p>divergence : 기존 답, 풀이와 상이한지</p> </li> <li> <p>existence : 답이 있는지</p> </li> </ul> <p>⇒ 문제를 수정할 때는 gpt-4o-mini활용, 필터링 할 때는 o4-mini를 사용</p> </li> <li> <p>220개의 원본 데이터를 5가지의 variant로 modified, 필터링 후에 최종 84개만 남음</p> </li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzle은 조건을 수정하면 급격하게 단순해지거나 답이 여러개일 수 있음</p> </li> <li> <p>ambiguity를 줄이기 위해, “valid solution을 위해 가장 간단한 답을 찾아라”라는 문구를 instruction에 추가</p> </li> <li> <p>과정 자체는 위와 동일</p> </li> </ul> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li> <p>시스템적으로 reasoning model의 contamination을 측정하기 위해서, Contamination ratio를 제안</p> <ul> <li> <p>친숙한 패턴에서 contaminated reasoning이 얼마나 차지하는지를 나타냄</p> </li> <li> <p>이를 자동적으로 식별하는 방법도 제안</p> </li> </ul> </li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>모델이 문제를 풀 때, 수정된 조건을 이해하고 풀었는지 이해하지 않고 풀었는지를 구분하기 위해 metric을 도입</p> </li> <li> <p>생성된 reasoning path를 단락별로 쪼개고, 각 단락을 textual representation으로 embedding</p> <ul> <li> <p>openAI의 text-embedding-small model을 사용</p> </li> <li> <p>단락은 double line break를 기준으로 분리</p> </li> </ul> </li> <li> <p>각 단락과 오리지널 문제의 reasoning path, 각 단락과 modified reasoning path와의 cosine 유사도를 계산, 둘을 비교해 original reasoning path와의 유사도가 더 높을 경우 1로 계산</p> <ul> <li>즉, 조건이 수정되었는데도 무시하고 습관처럼 reasoning을 했다는 뜻</li> </ul> </li> </ul> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidity를 잘 관찰하기 위해, 모델이 수정된 조건을 이해했는데도 습관처럼 풀었는지 아니면 인지조차 하지 못했는지를 구분</p> <ul> <li>모델이 조건을 잘못 이해한 경우 / 조건을 잘 이해했으나 reasoning을 잘못한 경우</li> </ul> </li> <li> <p>이를 반영한 metric을 p-passs@k라고 정의, reasosning path에서 constraint를 인지하고 있는 경우에만 accuracy를 측정</p> </li> <li> <p>constraint를 인지했는지는 모델이 생성한 reasoning path중 첫 15개의 단락과 정답, 질문을 LLM에 넣고 판단하도록 함(p_i)</p> </li> </ul> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>question만 주어지는 현실적인 상황에서, generated reasosning이 원치 않지만 친숙한 pattern으로 contaminated됐는지 자동적으로 식별하는 것을 불가능</p> </li> <li> <p>그래서 간단하게, contamination의 종류를 분류해서, 각 type별 의심스러운 pattern을 식별</p> </li> <li> <p>Interpretation overload : 모델이 주어진 문제 조건을 거절하는 것으로 시작, 문제를 바로 해석하는 것보다 여러 방식으로 재해석. 보통 reasoning 중간 단계에서 발생, inconsistent 혹은 contraditory한 결론을 야기</p> </li> <li> <p>Input Distrust : 모델이 번역 오류, input error, typo존재 등을 가정함. 직관적으로 바로 문제를 풀 수 있음에도 부정하고 매우 복잡하게 풀게 됨.</p> </li> <li> <p>Partial Instruction Attention : 모델이 제공된 instruction의 일부분만을 선택적으로 집중</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrap을 여러 LLM에 inference</p> </li> <li> <p>실험은 CoT prompting을 사용, ‘Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}’ 포맷으로 질문을 전달</p> </li> <li> <p>table 2,3은 16번 sampling, 다른 실험은 4번 sampling</p> </li> <li> <p>수학 문제의 경우, exat matching으로 correctness 판단, puzzle의 경우 free-from sentence로 답이 구성되다 보니, LLM을 사용해서 정답과 모델 답변을 함께 제공해 correctness를 판단</p> </li> <li> <p>실험 결과, 대부분 reason모드일 때보다 base모드에서 더 높은 성능을 보임</p> <ul> <li> <p>즉, 길게 reasoning을 하면서 습관적인 reasoning pattern을 사용, 오답으로 이어지는 경우가 많다는 것</p> </li> <li> <p>Buget forcing : 버짓 마지막에 ‘Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;’를 추가하여 답을 바로 내도록 함</p> </li> <li> <p>MATH500 : low 2000, medium 4000, high 6000 토큰 사용</p> </li> <li> <p>AIME : low 2000, medium 6000, high 10000</p> </li> </ul> </li> <li> <p>prompt hinting : 문제에 오타 없고 지시 그대로 하라는 prompt를 추가</p> </li> <li> <p>실험 결과, budget이 커질 수록 성능이 악화됨</p> </li> <li> <p>prompt로 hint를 줘도 여전히 reasoning rigidity가 존재</p> </li> <li> <p>모델 크기에 따른 실험. base모델이 성능이 전반적으로 높게 나오는 편</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰 - Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근 LLM이 복잡한 reasoning task에서 괄목할만한 성능을 보이고 있으나, (모델에게) 편한 reasoning pattern에 의존하는 경향이 있음</p> </li> <li> <p>사용자의 명시적인 instruction이 있음에도 불구하고, 습관적인 reasoning trajectory를 생성, 오답으로 귀결</p> </li> <li> <p>이를 분석하기 위해 reasoning trap이라는 진단 데이터셋을 도입</p> </li> <li> <p>reasoning trap을 통해서 모덷이 습관적으로 사용하는 reasoning pattern을 발견, 분류</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM은 수학, 복잡한 코딩 문제, 퍼즐 풀이를 포함한 여러 어려운 태스크에서 주목할만한 성능을 보임</p> </li> <li> <p>하지만, 이러한 모델들에게 문제 행동, reasoning rigidity가 발견됨</p> </li> <li> <p>reasoning rigidity는 cognitive bias를 반영, 주어진 조건을 이해해도 자기 방식대로 override, 무시하고 문제를 푸는 현상을 뜻함</p> </li> <li> <p>reasoning rigidity는 사용자가 서술한 제약이 중요한 도메인에서 큰 문제가 됨</p> </li> <li> <p>reasoning rigidity를 식별할 수 있도록, 기존의 수학, 퍼즐 데이터셋을 활용해 reasoningtrap이라는 벤치마크를 제안</p> </li> <li> <p>ReasoningTrap으로 여러 모델을 평가한 결과, 여러 중요한 현상들을 발견</p> </li> <li> <p>또한, 이러한 contamination의 패턴을 3가지로 분류</p> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> </li> <li> <p>Instruction following of reasoning models</p> </li> <li> <p>Rigidity in reasoning models</p> </li> <li> <p>Underlying reason for rigidity</p> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>크게 2가지로 도메인으로 구성 : 수학(ConditionedMath), 퍼즐(PuzzleTrivial)</p> </li> <li> <p>각 데이터는 원래 Q-R-A tuple (q_orig, r_orig, a_orig)과 수정된 tuple (q_mod, r_mod, a_mod)로 구성</p> </li> <li> <p>총 164개의 데이터셋, 84개는 수학, 80개는 퍼즐</p> </li> <li> <p>ConditionedMath에 있는 모든 질문은 개념적으로 다르고, 겹치지 않고, human annotator에 의해 엄격하게 검증됨</p> </li> <li> <p>PuzzleTrival은 10개의 puzzle concept를 가짐</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5를 활용해서 제작</p> </li> <li> <p>원래 질문을 수정하고, 수정된 질문이 아래 조건에 부합하는지를 확인, 필터링</p> </li> <li> <p>220개의 원본 데이터를 5가지의 variant로 modified, 필터링 후에 최종 84개만 남음</p> </li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzle은 조건을 수정하면 급격하게 단순해지거나 답이 여러개일 수 있음</p> </li> <li> <p>ambiguity를 줄이기 위해, “valid solution을 위해 가장 간단한 답을 찾아라”라는 문구를 instruction에 추가</p> </li> <li> <p>과정 자체는 위와 동일</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li>시스템적으로 reasoning model의 contamination을 측정하기 위해서, Contamination ratio를 제안</li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>모델이 문제를 풀 때, 수정된 조건을 이해하고 풀었는지 이해하지 않고 풀었는지를 구분하기 위해 metric을 도입</p> </li> <li> <p>생성된 reasoning path를 단락별로 쪼개고, 각 단락을 textual representation으로 embedding</p> </li> <li> <p>각 단락과 오리지널 문제의 reasoning path, 각 단락과 modified reasoning path와의 cosine 유사도를 계산, 둘을 비교해 original reasoning path와의 유사도가 더 높을 경우 1로 계산</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidity를 잘 관찰하기 위해, 모델이 수정된 조건을 이해했는데도 습관처럼 풀었는지 아니면 인지조차 하지 못했는지를 구분</p> </li> <li> <p>이를 반영한 metric을 p-passs@k라고 정의, reasosning path에서 constraint를 인지하고 있는 경우에만 accuracy를 측정</p> </li> <li> <p>constraint를 인지했는지는 모델이 생성한 reasoning path중 첫 15개의 단락과 정답, 질문을 LLM에 넣고 판단하도록 함(p_i)</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>question만 주어지는 현실적인 상황에서, generated reasosning이 원치 않지만 친숙한 pattern으로 contaminated됐는지 자동적으로 식별하는 것을 불가능</p> </li> <li> <p>그래서 간단하게, contamination의 종류를 분류해서, 각 type별 의심스러운 pattern을 식별</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Interpretation overload : 모델이 주어진 문제 조건을 거절하는 것으로 시작, 문제를 바로 해석하는 것보다 여러 방식으로 재해석. 보통 reasoning 중간 단계에서 발생, inconsistent 혹은 contraditory한 결론을 야기</p> </li> <li> <p>Input Distrust : 모델이 번역 오류, input error, typo존재 등을 가정함. 직관적으로 바로 문제를 풀 수 있음에도 부정하고 매우 복잡하게 풀게 됨.</p> </li> <li> <p>Partial Instruction Attention : 모델이 제공된 instruction의 일부분만을 선택적으로 집중</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrap을 여러 LLM에 inference</p> </li> <li> <p>실험은 CoT prompting을 사용, ‘Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}’ 포맷으로 질문을 전달</p> </li> <li> <p>table 2,3은 16번 sampling, 다른 실험은 4번 sampling</p> </li> <li> <p>수학 문제의 경우, exat matching으로 correctness 판단, puzzle의 경우 free-from sentence로 답이 구성되다 보니, LLM을 사용해서 정답과 모델 답변을 함께 제공해 correctness를 판단</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>실험 결과, 대부분 reason모드일 때보다 base모드에서 더 높은 성능을 보임</p> </li> <li> <p>Buget forcing : 버짓 마지막에 ‘Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;’를 추가하여 답을 바로 내도록 함</p> </li> <li> <p>prompt hinting : 문제에 오타 없고 지시 그대로 하라는 prompt를 추가</p> </li> <li> <p>실험 결과, budget이 커질 수록 성능이 악화됨</p> </li> <li> <p>prompt로 hint를 줘도 여전히 reasoning rigidity가 존재</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>모델 크기에 따른 실험. base모델이 성능이 전반적으로 높게 나오는 편</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="detection"/><category term="embedding"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근의 reasoning oriented model(LRM)은 여러 수학 데이터셋에서 높은 성능 달성을 보이나, natural instruction following에 대한 성능은 분석되지 않음</p> </li> <li> <p>본 논문에서는 이러한 LRM들의 instruction following 능력을 분석하기 위해 MathIF라는 데이터셋을 제안, math 도메인에서의 instruction following 성능을 평가</p> </li> <li> <p>실험 결과, reasoning을 효과적으로 하는 모델이 user direction에 따르는 것을 어려워 하는 현상 발견</p> <ul> <li> <p>긴 CoT dataset에 SFT하거나 RL로 학습한 모델이 답변 길이가 길어질수록 instruction following 능력이 떨어지는 현상 발견</p> </li> <li> <p>간단한 개입(CoT 마지막 부분에 instruction을 다시 붙여서 넣어줌)으로 instruction following 성능을 향상시킬 수 있음을 보임</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoning을 scaling하는 것은 reasoning ability를 향상시킴</p> <ul> <li>SFT or RLVR 사용</li> </ul> </li> <li> <p>LRM의 경우 간단한 instruction도 following하는 것을 어려워 한다는 것을 발견</p> </li> </ul> <p>⇒ reasoning-oriented learning을 하면 모델 자체의 reasoning ability는 향상돼도 controllability는 떨어지는게 아닐까?</p> <ul> <li>하지만 현재는 범용 목적의 instruction following(IF) 벤치마크만 존재</li> </ul> <p>⇒ 수학 도메인에서의 IF 벤치마크를 만들고 평가해보자!</p> <ul> <li> <p>실험 결과, instruction following과 reasoning capability사이의 일종의 trade-off가 존재</p> <ul> <li> <p>즉, SFT 혹은 RL로 reasoning ability를 향상시킨 모델은 reasoning 성능은 올라도 IF 성능은 떨어짐</p> </li> <li> <p>특히, CoT 길이가 길어질수록 IF 성능이 악화됨</p> </li> </ul> </li> <li> <p>contribution</p> <ul> <li> <p>MathIF, 첫번째로 수학 도메인에서 instruction following 능력을 시스템적으로 측정하는 벤치마크 데이터셋 도입</p> </li> <li> <p>23개의 LRM를 해당 벤치마크에 대해서 평가</p> </li> <li> <p>reasoning performance와 instruction-following사이의 trade-off가 있음을 실험적으로 보임</p> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> <ul> <li> <p>high-quality long CoT by distilling from more capable LRMs or combining primitive reasoning actions</p> <ul> <li> <p>s1 : 적은 양의 CoT data로도 reasoning ability를 향상시킴</p> </li> <li> <p>LIMO : 관련 도메인이 이미 pre-training때 포함되어 있다면, 최소한의 cognitive process를 담은 demonstration으로 reasoning capabilities를 발생시킬 수 있다고 서술</p> </li> </ul> </li> <li> <p>cold-RL</p> <ul> <li> <p>deepseek-R1-zero로 주목 받게 된 방법론</p> </li> <li> <p>SFT와 달리, cold-RL은 긴 CoT dataset에 의존하지 않고, final outcome으로 reward를 받아서 학습</p> </li> <li> <p>RL과정을 간단, 가속화 하기 위해서 dynamic sampling, process-reward, off-policy guidance, CoT preference optimziation 등이 제안됨</p> </li> </ul> </li> </ul> </li> <li> <p>Instruction-followiwng benchmark</p> <ul> <li> <p>이전의 벤치마크는 보통 user query의 completeness에 초점, proprietary language model에 의존해서 win rate를 측정하는 식으로 평가</p> </li> <li> <p>format constraint, multi-turn instruction, refutation instruction, compositional instruction을 따르는지를 평가</p> </li> <li> <p>하지만 대부분의 IF 벤치마크는 일반적인 도메인에 집중, 상대적으로 직관적인 query를 사용</p> </li> </ul> <p>→ 이러한 도메인 차이와 long CoT의 부재는 LRM을 평가하는데에 방해가 됨</p> </li> </ul> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> <ul> <li> <p>toy experiment로 IFEval과 FollowBench에 대한 LRM과 Instruct 모델 성능 비교</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - 확실히 LRM의 성능이 상대적으로 낮으나, 낮은 원인이 domain shift때문인지 IF성능 때문인지는 분명하지 않음
</code></pre></div> </div> <p>⇒ 수학 도메인의 IF benchmark를 만들자!</p> <ul> <li> <p>python으로 검증 가능한 constraint를 고려, 2-3개의 constraint를 합쳐서 instruction으로 부여하는 방안을 고려</p> </li> <li> <p>contraint를 얼마나 만족했는지를 평가하기 위해 Hard accuracy(HAcc), Soft accruacy(SAcc)로 측정</p> </li> </ul> </li> </ul> </li> <li> <p>Constraint type</p> <ul> <li> <p>length, lexical, format, affix로 크게 4가지 type으로 분류, 그 안에 sub-type을 명시</p> </li> <li> <p>proprietary language model에 의존하지 않기 위해서 python으로 제약을 만족했는지 검증 가능하도록 설계</p> <ul> <li>Compositional Constraint</li> </ul> </li> <li> <p>2-3개의 constraint를 조합해서 compositional constraint를 구축</p> </li> <li> <p>같이 존재할 수 없는 constraint나 같은 subtype끼리 있으면 filtering, 그 외의 조합에서 random sampling해서 데이터셋을 구축</p> </li> <li> <p>이러한 과정을 통해 30개의 dual-constraint와 15개의 triple-constraint를 구축</p> </li> </ul> </li> <li> <p>Math problem collection</p> <ul> <li> <p>GSM8K, MATH-500, Minerva, Olympiad에서 90개씩 sampling</p> <ul> <li>초등학교부터 올림피아드 수준의 문제까지 아우르도록 함</li> </ul> </li> <li> <p>각 데이터에 대해서 single, dual, triple constraint를 적용</p> </li> <li> <p>sanity check를 위해 사람이 직접 검수, math problem에 추가된 constraint가 모순되지 않는지 더블췍</p> </li> <li> <p>Evaluation metric</p> </li> <li> <p>HAcc : constraint다 만족해야 1</p> </li> <li> <p>SAcc : contraint 개당 만족하면 1 아니면 0으로 계산, 평균</p> <ul> <li>구체적인 언급 없으면 correctness는 contraint가 있는 상태에서 나온 답변으로 계산</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>모든 LRM은 nucleus sampling(T=1.0, p=0.95)로 디코딩, 최대 답변 길이 16,384 토큰, vLLM 사용</p> </li> <li> <p>모든 LRM은 IF성능이 하락함</p> <ul> <li> <p>최고 성능을 낸 Qwen3-14B마저도 50.71밖에 안됨</p> </li> <li> <p>특히 deepseek-R1-distill-llama-70B나 open-reasonser-zero-32B의 경우 모델 크기에 비해서 매우 낮은 IF 성능을 보임</p> </li> </ul> </li> <li> <p>Qwen3 시리즈가 그나마 높은 IF 성능을 보임</p> </li> <li> <p>모델 크기가 IF 성능을 결정하진 않음</p> <ul> <li>같은 계열에서는 종종 경향성이 있으나, 다른 계열까지 한번에 봤을 때는 크기가 크다고 IF 성능이 보장되진 않음</li> </ul> </li> <li> <p>명시적인 reasoning seperation (<think>,</think>)가 있는 모델이 전반적으로 IF 성능이 높음</p> <ul> <li>Qwen2.5-Math-1.5B, 7B-Instruct, Qwen2.5-1.5B, 7B-SimpleRL-Zoo 친구들이 명시적인 reasoning token 없는 애들 ⇒ 성능이 쏘 처참</li> </ul> </li> <li> <p>instruction-following과 mathematical reasoning사이에 trade-off가 존재</p> <ul> <li>Diff를 보면 대부분의 모델이 constraint가 있을 때와 없을 때의 correctness차이가 큼</li> </ul> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

			- LRM모델이 constraint를 잘 따르는게 문제 난이도와 연관이 있는지를 살펴보기 위해 데이터셋 별로 IF성능을 표현

	- 문제가 어려울수록 constraint를 잘 만족하지 못한다는 것을 알 수 있음

[//]: # (column is not supported)

			- 제약이 많아질수록 IF 성능 하락, 특히 2개이상부터 크게 하락..
</code></pre></div></div> <ul> <li> <p>제약조건을 만족하면서 문제를 맞추는 경우는 크지 않음</p> </li> <li> <p>보통 제약조건 혹은 문제 하나만을 만족함 + 즉, 제약조건을 걸면 문제 풀이 성능이 하락</p> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

			- constraint가 있을 때와 없을 때의 성능 차이

	- 특히, GSM8K, Minerva에서 극심 ⇒ 문제 난이도와 상관 없이 contraint가 있으면 reasoning ability가 하락

[//]: # (column is not supported)

			- CoT가 길수록 IF 성능 하락
</code></pre></div></div> <ul> <li> <p>IF가 낮았던 Qwen2.5를 대상으로 실험, 데이터는 deepscalar를 사용, QwQ로 CoT생성, 정답을 맞추면서 너무 길지 않은 애들만 필터링해서 학습에 사용</p> <ul> <li>format reward는 think token를 포함하는지 여부로 포함하면 답이 틀려도 0.1점 줌</li> </ul> </li> <li> <p>실험 결과, reasoning-orienteed 방법론이 reasoning성능은 향상시키지만 IF는 하락하는 것을 볼 수 있음</p> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

		[//]: # (column is not supported)

	Figure 7
</code></pre></div></div> <ul> <li> <p>모델이 reasonign path를 종료하려고 할 때마다 wait를 걸어서 강제로 CoT길이를 늘림</p> </li> <li> <p>CoT길이가 길어질수록 constraint instruction과 멀어져서 constraint에 대한 acc가 떨어지는 것으로 추론</p> </li> </ul> <p>Table 5</p> <ul> <li> <p>cold-RL에서 roll-out 길이를 조정하며 학습, 길어질수록 reasoning은 향상되나 IF는 떨어짐</p> </li> <li> <p>간단하게 reasoning이 끝나갈 때 쯤에 wait을 넣고 constraint instruction을 반복해서 넣어준 경우의 성능을 측정</p> </li> <li> <p>IF성능은 향상되나 Correctness는 하락하는 것을 볼 수 있음</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented model들이 생각보다 instruction following 성능이 악화됨</p> </li> <li> <p>대부분 간단한 형식에 대한 제약인데도, 제약이 있을 때와 없을 때의 성능 차이가 큰게 충격적</p> </li> <li> <p>LLM이 정말 reasoning을 하는걸까? 그냥 답변 길이가 길어져서 발생하는 attention sink일까?</p> </li> </ul> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰 - Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근의 reasoning oriented model(LRM)은 여러 수학 데이터셋에서 높은 성능 달성을 보이나, natural instruction following에 대한 성능은 분석되지 않음</p> </li> <li> <p>본 논문에서는 이러한 LRM들의 instruction following 능력을 분석하기 위해 MathIF라는 데이터셋을 제안, math 도메인에서의 instruction following 성능을 평가</p> </li> <li> <p>실험 결과, reasoning을 효과적으로 하는 모델이 user direction에 따르는 것을 어려워 하는 현상 발견</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoning을 scaling하는 것은 reasoning ability를 향상시킴</p> </li> <li> <p>LRM의 경우 간단한 instruction도 following하는 것을 어려워 한다는 것을 발견</p> </li> </ul> <p>⇒ reasoning-oriented learning을 하면 모델 자체의 reasoning ability는 향상돼도 controllability는 떨어지는게 아닐까?</p> <ul> <li>하지만 현재는 범용 목적의 instruction following(IF) 벤치마크만 존재</li> </ul> <p>⇒ 수학 도메인에서의 IF 벤치마크를 만들고 평가해보자!</p> <ul> <li> <p>실험 결과, instruction following과 reasoning capability사이의 일종의 trade-off가 존재</p> </li> <li> <p>contribution</p> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> </li> <li> <p>Instruction-followiwng benchmark</p> </li> </ul> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> </li> <li> <p>Constraint type</p> </li> <li> <p>Compositional Constraint</p> </li> <li> <p>Math problem collection</p> </li> <li> <p>Evaluation metric</p> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>모든 LRM은 nucleus sampling(T=1.0, p=0.95)로 디코딩, 최대 답변 길이 16,384 토큰, vLLM 사용</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모든 LRM은 IF성능이 하락함</p> </li> <li> <p>Qwen3 시리즈가 그나마 높은 IF 성능을 보임</p> </li> <li> <p>모델 크기가 IF 성능을 결정하진 않음</p> </li> <li> <p>명시적인 reasoning seperation (<think>,</think>)가 있는 모델이 전반적으로 IF 성능이 높음</p> </li> <li> <p>instruction-following과 mathematical reasoning사이에 trade-off가 존재</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>제약조건을 만족하면서 문제를 맞추는 경우는 크지 않음</p> </li> <li> <p>보통 제약조건 혹은 문제 하나만을 만족함 + 즉, 제약조건을 걸면 문제 풀이 성능이 하락</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>IF가 낮았던 Qwen2.5를 대상으로 실험, 데이터는 deepscalar를 사용, QwQ로 CoT생성, 정답을 맞추면서 너무 길지 않은 애들만 필터링해서 학습에 사용</p> </li> <li> <p>실험 결과, reasoning-orienteed 방법론이 reasoning성능은 향상시키지만 IF는 하락하는 것을 볼 수 있음</p> </li> </ul> <p>Figure 7</p> <ul> <li> <p>모델이 reasonign path를 종료하려고 할 때마다 wait를 걸어서 강제로 CoT길이를 늘림</p> </li> <li> <p>CoT길이가 길어질수록 constraint instruction과 멀어져서 constraint에 대한 acc가 떨어지는 것으로 추론</p> </li> </ul> <p>Table 5</p> <ul> <li>cold-RL에서 roll-out 길이를 조정하며 학습, 길어질수록 reasoning은 향상되나 IF는 떨어짐</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003-480.webp 480w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003-800.webp 800w,/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/al-folio/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>간단하게 reasoning이 끝나갈 때 쯤에 wait을 넣고 constraint instruction을 반복해서 넣어준 경우의 성능을 측정</p> </li> <li> <p>IF성능은 향상되나 Correctness는 하락하는 것을 볼 수 있음</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented model들이 생각보다 instruction following 성능이 악화됨</p> </li> <li> <p>대부분 간단한 형식에 대한 제약인데도, 제약이 있을 때와 없을 때의 성능 차이가 큰게 충격적</p> </li> <li> <p>LLM이 정말 reasoning을 하는걸까? 그냥 답변 길이가 길어져서 발생하는 attention sink일까?</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title><link href="https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/" rel="alternate" type="text/html" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 건우 김</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <ul> <li> <p>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</p> <p>→ 하지만 기존 advanced reasoning ability를 가진 LLM에게 prompt 기반의 search engine을 활용하도록 하는 것은 suboptimal임 (LLM이 search engine과 어떻게 상호작용해야 하는지 완전히 이해 못함)</p> </li> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> <ul> <li>단계별 reasoning step에서 autonomously하게 multiple search queries를 생성하고 실시간으로 정보를 검색하도록 학습</li> </ul> </li> </ul> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <span style="color:yellow_background"><strong>효과적으로 상호작용하는</strong></span> 능력이 필수적임</p> <p><br/></p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p><br/></p> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 <span style="color:red">**LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</span></p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p><br/></p> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 <span style="color:red">검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</span></p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 <span style="color:red">LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할</span> 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, <span style="color:red">단순한 결과 기반 보상이 충분한지는 아직 불확실함</span>.</p> <p>→ 3번은 자기들도 모르면서 뭔가 싶네요 ㅋㅋ</p> </li> </ol> <p><br/></p> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <p><br/></p> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine $ R $을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <ul> <li> <p>$ r_{\phi} $: output quality를 평가하는 reward function</p> </li> <li> <p>$ \pi_\theta $: policy LLM</p> </li> <li> <p>$ \pi_{ref} $: reference LLM</p> </li> <li> <p>$ x $: dataset $ D $에서 추출된 input sample</p> </li> <li> <p>$ y $: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>$ D_{KL} $: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 $ \pi_\theta $가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- retrieval interleaved reasoning via $ \pi_{\theta}(.|x;R) =\pi_{ref}(.|x)\bigotimes R $

	- $ \bigotimes $ denotes interleaved retrieval-and-reasoning
</code></pre></div></div> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <ul> <li> <p><strong>Formulation of RL with a Search Engine</strong></p> <p>LLM에서 자주 사용하는 원래 기존 RL의 objective는 아래와 같이 정의됨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  그런데, 위 formulation은 entire output sequence $ y $가 $ \pi_{\theta} $로부터 생성되었다는 가정이 있음. 이 가정은 model behavior가 internal reasoning과 external information retrieval을 모두 포함하는 상황에서 적용할 수 없음.
</code></pre></div> </div> <p>따라서, RL objective를 serach engine $ R $과 통합시키기 위해 아래와 같이 수정함</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  위 수정된 objective에서는 trajectory $ y  $는 interleaved reasoning steps과 retrieved content를 포함
</code></pre></div> </div> </li> </ul> <p><br/></p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <p><br/></p> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <ul> <li> <p>$ \pi_{\theta} $: current policy</p> </li> <li> <p>$ \pi_{old} $: previous policy</p> </li> <li> <p>$ I(y_t) $: token loss masking 연산으로, $ y_t $가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><br/></p> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-  $ y $ ~ $ \pi_{\theta}(.|x;R) =\pi_{ref}(.|x)\bigotimes R $

→ LLM은 x를 입력 받아 Search Engine $ R $과의 interleaved 흐름을 통해 y를 생성
</code></pre></div></div> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></li> </ul> <p><br/></p> <ul> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <p><br/></p> <p><strong>Case Study</strong></p> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <p><br/></p> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> <ol> <li> <p>Natural Questions (NQ)</p> </li> <li> <p>TriviaQA</p> </li> <li> <p>PopQA</p> </li> </ol> </li> <li> <p>Multi-Hop QA</p> <ol> <li> <p>HotpotQA</p> </li> <li> <p>2WikiMultiHopQA</p> </li> <li> <p>Musique</p> </li> <li> <p>Bamboogle</p> </li> </ol> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> <ol> <li> <p>Inference w/o Retrieval</p> <ol> <li> <p>Direct Inference</p> </li> <li> <p>Chain-of-Thought</p> </li> </ol> </li> <li> <p>Inference w/ Retrieval</p> <ol> <li> <p>RAG</p> </li> <li> <p>IRCoT (Information Retrieval CoT)</p> </li> <li> <p>Search-o1 (using search engine tool)</p> </li> </ol> </li> <li> <p>fine-tuning methods</p> <ol> <li> <p>SFT</p> </li> <li> <p>R1: search engine없이 RL fine-tuning (Search-R1과 fair한 비교를 위해 동일 데이터로 RL을 학습하되 검색은 사용하지 않음)</p> </li> </ol> </li> </ol> <h2 id="43-experimental-setup">4.3 Experimental Setup</h2> <ul> <li> <p>LLMs: Qwen-2.5-3B, Qwen-2.5-7B (Base / Instruct)</p> </li> <li> <p>Retrieval</p> <ul> <li> <p>Knowledge Source: 2018 Wikipedia dump (using E5 as retriever)</p> </li> <li> <p>number of retrieved documents: 3</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>training data: NQ + HotpotQA for Search-R1 and fine-tuning methods</p> </li> <li> <p>evaluation data: (in-domain, out-of-domain)</p> </li> </ul> </li> <li> <p>metric: EM</p> </li> <li> <p>Inference 설정</p> <ul> <li>Inference-style baseline은 Instruct 모델 사용 (Base 모델은 instruction을 따르지 못함)</li> </ul> </li> <li> <p>RL 설정</p> <ul> <li>별도 언급이 없으면 PPO 사용</li> </ul> </li> </ul> <h2 id="44-performance">4.4 Performance</h2> <ul> <li> <p>Search-R1은 baselines 대비 우수한 성능 보여줌</p> <ul> <li> <p>Qwen2.5-7B: 평균적으로 41% 향상</p> </li> <li> <p>Qwen2.5-3B: 평균적으로 20% 향상</p> </li> </ul> <p>→ in-domain (NQ, HotpotQA)와 out-of-domain (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle) <strong>모두 일관되게 높음</strong></p> </li> <li> <p>검색 없이 추론만하는 R1보다도 Search-R1이 우수함</p> <p>→ <strong>Search가 LLM 추론에 external knowledge를 추가함으로써 도움되는 것을 보임</strong></p> </li> <li> <p>Base와 Instruct model 모두 일관되게 Search-R1 효과적임</p> <p>→ DeepSeek-R1-Zero style의 단순 outcome-based reward가 순수 Reasoning 뿐만 아니라 <strong>search를 포함한 complex reasoning scenarios에서도 효과적임을</strong> 보여줌</p> </li> <li> <p>**Model size가 클 수록 검색 활용 효과가 더 큼 **</p> </li> </ul> <p><br/></p> <h2 id="51-different-rl-methods-ppo-vs-grpo">5.1 Different RL methods: PPO vs. GRPO</h2> <p>Search-R1에서 RL 방법으로 PPO와 GRPO 두 가지를 모두 실험함</p> <ol> <li> <p><strong>GRPO는 PPO보다 수렴 속도가 빠름</strong> → Figure2 (a)</p> <ol> <li>PPO는 critic model에 의존하기 때문에 효과적인 학습이 시작되려면 여러 단계의 워밍업이 필요하지만, GRPO는 baseline을 여러 샘플 평균으로 잡아 더 빠르게 수렴함</li> </ol> </li> <li> <p><strong>PPO는 학습 안정성이 더 높음</strong> → Figure2 (a)</p> <ol> <li><span style="color:red_background"><strong>GRPO는 일정 단계 이후 reward collapse</strong></span>가 나타나지만, <span style="color:yellow_background"><strong>PPO는 학습이 더 안정적으로 유지됨</strong></span></li> </ol> </li> <li> <p><strong>최종 train reward는 PPO와 GRPO 모두 유사함</strong></p> <ol> <li>수렴 속도와 안정성은 다르지만 최종 성능과 train reward는 큰 차이가 없음. 그래도 GRPO는 나중에 불안정해질 수 있기에 더 안정적인 PPO가 느리지만 적합함.</li> </ol> </li> </ol> <p><br/></p> <p><em>(다른 세팅에서도 동일한 현상이 관찰됨)</em></p> <h2 id="52-base-vs-instruct-llms">5.2 Base vs. Instruct LLMs</h2> <ul> <li> <p>Figure2 (b)에서 Instruction-tuned model은 Base model보다 더 빠르게 수렴하고 초기 성능도 더 높게 나오지만, <strong>최종 train reward는 두 모델 모두 거의 동일한 수준으로 수렴함</strong></p> <p>→ 이는 사전 instruction tuning이 초기 학습을 가속화하는데 도움이 되지만, <strong>RL만으로도 Base model이 충분히 따라잡을 수 있음을 보임</strong></p> <p><br/></p> </li> </ul> <p>(다른 세팅에서도 동일한 현상이 관찰됨)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 5.3 Response Length and Valid Search Study
</code></pre></div></div> <p>Qwen2.5-7B-base 모델로 response length와 검색 호출 횟수 변화를 분석함</p> <ul> <li> <p>Figure2 (c)를 보면</p> <ul> <li> <p>초기 단계 (100 steps 전후)</p> <ul> <li> <p><strong>응답 길이가 급격히 줄고, train reward는 소폭 상승함</strong></p> </li> <li> <p>모델이 불필요한 군더더기 단어를 줄이고 task에 적응하기 시작함을 보여줌</p> </li> </ul> </li> <li> <p>후기 단계 (100 steps 이후)</p> <ul> <li> <p><strong>응답 길이와 train reward 모두 증가함</strong></p> </li> <li> <p>모델이 검색 호출을 더 자주 하면서 (Search Engine을 자주 호출하는 법 학습) 검색 결과가 추가되어 응답이 길어짐</p> </li> <li> <p>검색 결과를 효과적으로 활용하며 train reward도 크게 향상됨</p> </li> </ul> </li> </ul> </li> <li> <p>Figure2 (d)를 보면 <strong>학습이 진행될수록 LLM이 검색 엔진 호출을 더 많이 학습한다는 점이 드러남</strong></p> </li> </ul> <p><br/></p> <h2 id="54-study-of-retrieved-tokens-loss-masking">5.4 Study of Retrieved Tokens Loss Masking</h2> <p>Retrieved Token Loss Masking은 unintended optimization을 방지하기 위해 도입한 것임. Retrieved token loss masking의 효과를 추가로 분석해봄 (Qwen2.5-7B-base)</p> <ul> <li> <p>Figure 3에 따르면, <strong>masking을 적용하면 원치 않는 최적화 효과를 줄이고 LLM 성능 향상이 더 커짐</strong></p> <ul> <li> <p>w. mask와 w.o. mask를 비교한 결과 **masking을 적용한 경우가 항상 더 높은 성능을 기록함 **</p> </li> <li> <p><strong>Appendix</strong></p> </li> </ul> <p><strong>Number of Retrieved Passages Study in SEARCH-R1 Training</strong></p> <ul> <li> <p>본 실험에서는 top-k를 3으로 설정했지만, 1,3,5 바꿔가며 이것의 effect를 분석함</p> <p>→ top-k가 1,3,5 설정이 <strong>모두 training pattern이 비슷함</strong> (top-k=5가 초기 수렴 속도가 빠른 대신에 이후 train reward가 감소하며 학습 안정성 떨어짐을 보임)</p> </li> </ul> <p><br/></p> <p><strong>Group Size Study in SEARCH-R1 (GRPO) Training</strong></p> <ul> <li> <p>본 실험에서는 Search-R1 (GRPO)의 group size를 5로 설정했지만, group size가 어떤 영향을 미치는지 확인하고자 1,3,5로 분석함</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      → Figure7을 보면 &lt;span style='color:red_background'&gt;**group size가 칼수록 수렴 속도 빨라지는 반면 RL의 불안정성 때문에 training collapse 위험도 증가**&lt;/span&gt;

      → Table8을 보면 group size가 큰 경우 빠른 수렴과 더 높은 train reward가 있었지만, group size=1일 때 학습이 더 안정적이고 일반화 성능이 더 우수함 (out-of-domain에서 더 우수함)
</code></pre></div> </div> <p><br/></p> </li> </ul> </li> <li> <p>본 연구에서는 LLM이 self-reasoning과 실시간 검색 엔진 상호작용을 교차적으로 수행할 수 있는 framework인 Search-R1 제안함</p> </li> <li> <p>기존의 multi-turn search를 위해 많은 prompt에 의존하는 RAG나 대규모 train data가 필요한 tool 사용 기반 접근법과 달리, <strong>Search-R1은 RL을 통해 모델이 자율적으로 검색 쿼리를 생성하고 검색된 정보를 전략적으로 활용할 수 있도록 최적화함</strong></p> </li> </ul> <p>Limitations</p> <ul> <li>Reward Design가 단순 결과 기반 보상이라 보다 디벨롭이 필요함</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰 - Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning]]></summary></entry></feed>