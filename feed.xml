<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://unknownnlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unknownnlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-11T11:39:09+00:00</updated><id>https://unknownnlp.github.io/feed.xml</id><title type="html">Unknown NLP Papers</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p>→ <strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></p> <p><strong>BD3-LMs</strong></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>\Delta^V: simplex 공간</p> </li> <li> <p>m \in \mathcal{V}: [MASK] token’s one-hot vector</p> </li> </ul> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></h3> <ul> <li> <p>p: denoising, q: noising</p> </li> <li> <p>s(j) = (j-1)/T, t(j) = j/T (이후에 j는 생략!)</p> <ul> <li>알파벳 순서대로 s가 앞 t가 뒤, s → t 과정은 noise를 더하는 과정</li> </ul> </li> <li> <p>D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</p> <ul> <li> <p>Q_t의 예시</p> <ul> <li> <p>Uniform replacement</p> </li> <li> <p><strong>Masking 기반</strong>: \beta_t 확률로 [MASK] 토큰으로 변경</p> </li> </ul> </li> </ul> </li> <li> <p>이상적인 diffusion model p<em>{\theta}는 q의 역방향이므로 D3PM에서는 아래 수식으로 p</em>{\theta}를 정의</p> <ul> <li> <p>1단계 denoising 과정 = 개별 토큰 위치에 대한 denoise는 독립 과정 = x^\ell 근사</p> </li> <li> <p>x^{\ell} (원본 텍스트)가 주어진다면 q를 활용해 x_t^\ell \rightarrow x_s^\ell을 완전히 복구할 수 있음.</p> </li> <li> <p>denoise 과정에서 x^\ell이 주어지지 않으므로 p로 근사: p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right)</p> </li> </ul> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><strong>Block Definition</strong></p> <ul> <li> <p>길이 L’이 되게 B개의 block으로 만들기 (x^b: x^{(b-1)L’:bL’} \in {1,…,B})</p> </li> <li> <p>Likelihood over block</p> </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><strong>Learning Objective</strong></p> <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><strong>Denoiser model</strong></p> <ul> <li> <table> <tbody> <tr> <td>Transformer x<em>\theta를 사용해 파라미터화: p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b})</td> </tr> </tbody> </table> <ul> <li> <p>given x^{&lt;b}: AR 특성 유지</p> </li> <li> <p>x^b 예측: Denosing</p> </li> </ul> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>x<em>\theta의 학습: block b 내에서 x</em>\theta^b(x_t^b, x^{&lt;b}) → L’ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><strong>K, V caching</strong></p> <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><strong>Training</strong></p> <ul> <li>모든 block은 x_\theta의 forward pass를 두 번 거쳐야 함 (x_t^b, x^b) → 계산의 효율화 필요</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input x_{t_b}^b 생성</p> </li> <li> <p>\left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}): 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 x^b_{\text{logit}} 계산</p> </li> </ol> <ul> <li> <p>Naive: B-times loop를 이용해 forward pass를 별도로 진행</p> </li> <li> <p>Vectorized 방식</p> <ul> <li> <p>x<em>{\text {noisy }}=x</em>{t<em>1}^1 \oplus x</em>{t<em>2}^2 \oplus \cdots \oplus x</em>{t_B}^B</p> </li> <li> <p>x_{\text{noisy}} \oplus x을 input으로 하여 한 번에 계산 How? attention mask를 이전 block만 조회하게끔 조절</p> </li> </ul> </li> </ul> <p><strong>Sampling</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> <ul> <li>\alpha_0=1 → linear scheduler→ \alpha_1=0</li> </ul> </li> <li> <p>목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</p> <ul> <li> <p><strong>Zero Masking Probabilities</strong>: clean sequence( x^\ell)에는 mask를 포함하지 않음. (이건 아래의 조건을 위해 필요한듯합니다.)</p> </li> <li> <p><strong>Carry-Over Unmasking</strong>: x_t^\ell \neq m인 경우 q\left(x_s^l=x_t^l \mid x_t^l \neq m\right)=1. 즉, unmaksed된 token은 다시 mask 되지 않음.</p> <ul> <li>Denoising model 단순화: p_\theta\left(x_s^{\ell}=x_t^{\ell} \mid x_t^{\ell} \neq m\right)=1</li> </ul> </li> <li> <p>\alpha<em>t = \prod</em>{\tau=1}^{t}(1 - \beta_\tau): t시점까지 mask되지 않고 살아남을 확률</p> </li> <li> <p><strong>why?</strong></p> <ul> <li> <p>t 시점에서 mask transition matrix (noising 과정에서 i→ j로 변환)</p> <ul> <li> <p>순서대로 mask는 mask 유지</p> </li> <li> <p>값을 그대로 가질 확률: \alpha_t</p> </li> <li> <p>token이 mask 될 확률: 1 - \alpha_t</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>marginal Q*{t</td> <td>s} (여기서 \alpha*{t</td> <td>s} = \alpha_t/\alpha_s)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> </ul> <p>전개…… \mathcal{L}_{\text{diffusion}}은 앞의 수식과 의미적으로 같습니다…..</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 일단 여기까진 정의대로 가되 block 내 token 길이인 L'으로 확장

- KL divergence 정의에 의해 다음과 같이 전개 가능 (이건 ㄹㅇ KLD 정의)

- \log{q} 부분은 학습과 무관하므로 제외

  - q(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = m, x^{b,\ell}) = \frac{\alpha_s - \alpha_t}{1 - \alpha_t}

  - q(x_s^{b,\ell} = m | x_t^{b,\ell} = m, x^{b,\ell}) = \frac{1 - \alpha_s}{1 - \alpha_t}

  - q(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = x^{b,\ell}, x^{b,\ell}) = 1: 1이므로 뒤에 계산에서 제외

- x_t^{b,\ell}이 mask인 경우만 계산

- 뒤에 항은 mask → mask는 상수라서 계산에서 제외
</code></pre></div></div> <table> <tbody> <tr> <td>= \sum<em>{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \sum</em>{\ell=1}^{L’} \frac{\alpha<em>t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^{b,\ell}</td> <td>x_t^{b,\ell}, x^{&lt;b}) \right]</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>= \sum<em>{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b}) \right]</td> </tr> </tbody> </table> <p>T \rarr \infin, T(\alpha_t - \alpha_s) = \alpha’_t</p> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>L^\prime = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> <ul> <li> <p><strong>직관적 핵석</strong>: block의 길이가 1이라면 한 토큰 단위 AR과 같음. → ??? 그래도 한 토큰 단위로 일어나는 diffusion 과정이 있는데? → mask로 intitialize 후, 원하는 다음 token을 찾는 과정이란 점에선 동일.</p> </li> <li> <p>*<strong>*수식 ok**</strong></p> <ul> <li> <p>linear scheduler에서 \alpha’_t, \alpha_t의 정의는 위와 같음. 그 다음 전개 과정은 이해할 수 있을듯?</p> </li> <li> <p>Expanding 부분은 Expatation of q를 제거 하기 위한 과정 q가 mask transition을 전제로 하므로 경우 (mask/unmask) 두 가지 확률에 대해서 전개</p> </li> <li> <table> <tbody> <tr> <td>SUBS-parameterization 가정의 carry-over unmasking 특성으로 \log{p_\theta(x^b</td> <td>x_t^b=x^b, x^{&lt;b})} = 0</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>q(x_t^b=m</td> <td>x^b) = 1 - \alpha_t = 1 - (1 - t) = t</td> </tr> </tbody> </table> </li> <li> <p>t는 상관없으니깐 삭제!</p> </li> <li>최종 결과는 NLL 로스와 기대값이 같다!</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <table> <tbody> <tr> <td>tuned schedule: q(x_t^\ell = m</td> <td>x^\ell) = 1</td> </tr> </tbody> </table> <ul> <li> <p>해당 schedule에서는 AR의 목적함수와 완전히 동일</p> </li> <li> <p>PPL도 감소, NELBO의 분산도 감소</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 L^\ell \geq 1인 케이스로 확장하고 싶음!</p> <ul> <li> <p>NELBO는 이론적으로 t에 invariance (기존 연구 ref: T가 무한히 커질수록 \alpha값이 아닌 누적값에 의해서 기대값이 정의되기 때문… 이 이상의 이해는 포기)하기에 스케줄에 따른 기대값의 변화가 없어야 함.</p> </li> <li> <p>하지만 우리는 모든 연산을 한 번에 하는 것이 아닌 Batch 연산을 활용 → 이론적인 invariance가 깨짐</p> </li> </ul> </li> </ul> <p>→ Schedule에 따라 분산의 결과가 변하게 됨. → Schedule을 잘 만들어보자!</p> <ul> <li> <p>Batch size를 K라고 할 때, batch of sequence \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}], with each \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x)</p> </li> <li> <p><strong>NELBO estimator</strong></p> </li> <li> <p><strong>Variance of the gradient estimator</strong></p> </li> </ul> <h2 id="low-variance-noise-schedules-for-bd3-lms">LOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS</h2> <h3 id="intuition-avoid-extreme-mask-rates--clipped-schedules-for-low-variance-gradients">INTUITION: AVOID EXTREME MASK RATES &amp; CLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS</h3> <ul> <li> <p>이상적인 마스킹: 모델이 다양한 수준의 노이즈 [MASK]에서 원래대로 되돌리는 법을 배우는 것</p> </li> <li> <p>극단적인 마스킹</p> <ul> <li> <p>마스킹 토큰이 너무 적을 경우, 너무 쉬운 문제를 풀게 됨.</p> </li> <li> <p>모든 토큰이 마스킹 될 경우, 문맥 정보가 전혀 없음 빈도에 기반한 학습만 진행</p> </li> </ul> </li> </ul> <p>→ 극단적인 부분을 날린 CLIP을 이용하자</p> <p>→ sample mask rates: 1 - \alpha_t \sim \mathcal{U}[\beta, \omega] for 0 \leq \beta, \omega \leq 1</p> <h3 id="data-driven-clipped-schedules-across-block-sizes">DATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES</h3> <ul> <li> <p>Block size ( L’)에 따른 최적의 mask rate을 찾아보자.</p> </li> <li> <p>Gradient 분산을 최소화하기 위함이지만 아래 NELBO를 추정지로 하여 실험을 진행</p> <ul> <li> <p>forward pass만으로 계산 가능</p> </li> <li> <p>실험 결과들에서 NELBO와 기울기 분산이 같은 경향성을 보임을 확인</p> </li> </ul> </li> <li> <p>\beta, \omega에 대해 grid search 진행</p> </li> <li> <p>Table 2에서 PPL과 NELBO과 상관성 보임을 재차 확인 + L’에 따라 최적의 조합이 있음을 발견함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="experiments">EXPERIMENTS</h2> <ul> <li> <p>Pre-train: base BD3-LM ( L’=L) for 850K gradient steps (순수 diffusion?)</p> </li> <li> <p>Fine-tune</p> <ul> <li>150K gradient steps on One Billion Words dataset (LM1B) and OpenWebText (OWT)</li> </ul> </li> <li> <p>L’에 따라 다른 Clipped schedule 적용 (매 validation epoch 마다 최적의 \beta, \omega 조합을 찾음!)</p> </li> </ul> <h3 id="likelihood-evaluation">LIKELIHOOD EVALUATION</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>다른 MDLM 모델 대비 perplexity이 향상됨</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Zero-shot validation perplexity 결과 Pubmed는 AR보다도 잘함.</p> </li> <li> <p>대체로 다른 MDLM보단 PPL 값이 더 낮음.</p> </li> </ul> <h3 id="sample-quality-and-variable-length-sequence-generation">SAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>[EOS] 토큰을 생성하거나 sample quality가 급감 (the average entropy of the the last 256-token chunk is below 4)할 때까지 실험 진행</p> </li> <li> <p>SEDD 대비 최대 10배 더 긴 text 생성 가능함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>GPT-2를 이용해 generative perplexity 측정, 효율성을 보기 위해 the number of generation steps (NFEs)</p> </li> <li> <p>기존 Block Diffusion 대비해도 더 적은 step에서 높은 Gen PPL 달성</p> </li> <li> <p>정성 분석은 Appendix D에 있음. AR과 유사할 정도의 퀄리티, 다른 DLM보단 좋더라</p> </li> </ul> <h3 id="ablations">ABLATIONS</h3> <p><strong>SELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>L’이 작을수록 heavier mask가 효과적</li> </ul> <p><strong>EFFICIENCY OF TRAINING ALGORITHM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>concat 활용하여 처리할 경우, sparse attention mask 활용</p> </li> <li> <p>FlexAttention을 이용할 경우 Sparsity를 활용해 효율적 처리 가능</p> </li> <li> <p>20-25% 속도 향상 가능!</p> </li> </ul> <hr/> <ul> <li> <p>수학 공부 열심히 하자.</p> </li> <li> <p>결과에서 힘이 많이 빠지긴 한다.</p> </li> <li> <p>전개과정에서 이 정도는 해야 oral로 가는구나 벽느껴진다.</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="diffusion"/><category term="diffusionlm"/><category term="generative"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰 - DiffusionLM, LLM 관련 연구]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>많은 연구들이 LLM이 사전학습 단계에서 학습 데이터를 외우는 이슈에 대해서 보고하고 있는 한편, finetuning에 대해서 비슷한 연구는 놀라울 정도로 적다.</p> <p>하지만, finetuning도 당연히 모델 대부의 업데이트와 때때로는 구조적인 변화까지도 이루어지기 때문에, finetuning의 memorization level에 대한 연구도 필요하다.</p> <p>그렇다면, 존재하는 다양한 finetuning 방법에 따른 memorization of fineuning data의 영향력은 어떻게 되는가?</p> <p>해당 연구는 이를 시험하기 위해 우선 finetuning 방법을 크게 두 가지로 구분한다:</p> <ol> <li> <p>Parameter-based finetuning: 모델 파라 바꿈</p> </li> <li> <p>Prompt-based fine-tuning: 모델 파라 고정, soft token/prefix embedding…</p> </li> </ol> <p>결과적으로 두 카테고리를 고루 포함한 5가지 방법을 시험했고,</p> <p>평가는 다양한 MIAs(membership inference attacks )로 했고,</p> <p>데이터는 Wikitext, WebNLG, Xsum 세 가지로 했다 (좀 적긴하네요)</p> <p>간단하고 빠르게 다음으로 넘어갑시다</p> <h1 id="fine-tuning-methods">Fine-Tuning Methods</h1> <ul> <li> <p>Parameter-based fine-tuning</p> <ul> <li> <p>Model Head Tuning (FT head): fine-tunes only the final output layer</p> </li> <li> <p>Low-Rank Adaptation (LoRA) (Hu et al., 2021)</p> </li> </ul> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> <ul> <li> <p><strong>Prefix Tuning</strong></p> <ul> <li>각 attention layer의 key/value에 학습 가능한 prefix 벡터 추가.</li> </ul> </li> <li> <p><strong>Prompt Tuning</strong></p> <ul> <li>모델 입력 임베딩 앞에 학습 가능한 연속형 프롬프트 임베딩 추가.</li> </ul> </li> <li> <p><strong>P-tuning</strong></p> <ul> <li>별도의 신경망으로 학습한 연속형 프롬프트를 입력에 삽입.</li> </ul> </li> </ul> </li> </ul> <h1 id="memorization-and-mias">Memorization and MIAs</h1> <ul> <li> <p>사용된 MIA 기법과 점수 계산 방식:</p> <ol> <li><strong>LOSS</strong> (Yeom et al., 2018)</li> </ol> <ul> <li>Membership Score = 모델의 손실</li> </ul> </li> </ul> <p>\text{Score} = L(x, M_t)</p> <p>(손실이 낮을수록 멤버일 가능성 ↑)</p> <ol> <li> <p><strong>Reference-based (Ref)</strong> (Mireshghallah et al., 2022a)</p> <ul> <li>기준 모델 MrM_rMr와 비교하여 손실 차이 계산</li> </ul> </li> </ol> <p>\text{Score} = L(x, M_t) - L(x, M_r)</p> <ol> <li> <p><strong>Zlib Entropy (Zlib)</strong> (Carlini et al., 2021)</p> <ul> <li>손실을 zlib 엔트로피로 나눈 비율</li> </ul> </li> </ol> <p>\text{Score} = \frac{L(x, M_t)}{\text{zlib}(x)}</p> <ol> <li> <p><strong>Min-K%</strong> (Shi et al., 2024)</p> <ul> <li>토큰 확률이 낮은 하위 k% 토큰들의 평균 로그 likelihood</li> </ul> </li> </ol> <table> <tbody> <tr> <td>\text{Score} = \frac{1}{E} \sum*{x_i \in \text{Min-}K\%(x)} \log p(x_i</td> <td>x*{&lt;i})</td> </tr> </tbody> </table> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>데이터</p> <ul> <li> <p>Wikitext-2-raw-1</p> </li> <li> <p>WebNLG</p> <ul> <li>triple로 이루어짐 (Subject-Predicate-Object)</li> </ul> </li> <li> <p>Xsum: 요약</p> <ul> <li>finetuning에 5000개만 사용</li> </ul> </li> </ul> </li> <li> <p>평가</p> <ul> <li>training and test sets에서 샘플링</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모델</p> <ul> <li> <p>LLaMA 2-7B (Touvron et al., 2023)</p> </li> <li> <p>GPT2-series (Radford et al., 2019)</p> </li> <li> <p>LLaMA 3-1B</p> </li> </ul> </li> </ul> <p>→ 2025의 논문이라고 믿기지 않는군여!</p> <ul> <li> <p>Evaluation Metrics</p> <ul> <li> <p>PERF: validation PPL as the primary metric</p> </li> <li> <p>MIA: AUC-ROC</p> </li> </ul> </li> <li> <p>Implementation Details</p> <ul> <li> <p>15 epoch</p> </li> <li> <p>모든 세팅은 논문에 나온거 그대로 따라함</p> </li> <li> <p>4090이랑 H100 한대 사용</p> </li> </ul> </li> </ul> <h1 id="results-and-observations">Results and Observations</h1> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <blockquote> <p>Observation ♯1: (당연)</p> </blockquote> <p>Parameter-based fine-tuning demonstrates a higher tendency to explicitly memorize training data.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>모든 방법론은 validation PPL기준으로 성능 좋았음.</p> <p>하지만, prompt-based methods 는 parameter-based 보다 외우는 성능 떨어짐 (당연)</p> <blockquote> <p>Observation ♯2:</p> </blockquote> <p>Parameter-based fine-tuning exhibits increasing memorization over training epochs, while prompt-based fine-tuning maintains consistently low memorization throughout training.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the model’s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <ul> <li><strong>Prefix Tuning 수식 (Petrov et al., 2024)</strong></li> </ul> <p>t^{pt}<em>i = A^{pt}</em>{i0} W<em>V S_1 + (1 - A^{pt}</em>{i0})\; t_i</p> <ul> <li> <p>soft-prefix가 어텐션 가중치 A^{pt}를 ‘어디를 볼지’만 재조정, <strong>본래 토큰 간 상대 분포는 그대로</strong>.</p> </li> <li> <p>즉 <strong>새로운 attention 패턴을 학습</strong>하기보다는 <strong>기존 능력을 재활용</strong>.</p> </li> <li> <p>결과적으로 <strong>표현 공간의 이동(shift) &lt; 적음</strong> → 학습, 비학습 샘플 분포 차이가 작아 MIA가 어렵다.</p> <ul> <li>Petrov et al. (2024) prove that the presence of a prefix does not alter the relative distribution of the input but only shifts the attention to different content.</li> </ul> </li> </ul> <p>이 가설을 확인하기 위해:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7B를 세 세팅에서 비교함:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>LoRA는 membership and non-membership samples 사이 분포 차이가 큰데, prefix tuning은 미미하다는 것을 알 수 있음</p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>두 방법론이 최종적으로는 비슷한 PPL을 가졌음에도 불구하고, Learning trajactories는 꽤나 달랐음</p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>이는 아까도 이야기 했듯이, 후자가 internal sample distribution of the model을 바꾸는 것이 아니라 단순히 다운스트림 태스크에 쪼끔 더 나은 bias를 추가하는 정도임을 다시한번 보인다</p> <h1 id="discussion">Discussion</h1> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>모델 사이즈가 memorization에 중요한 영향력을 줄 것임.</p> <p>→ To what extent does model size influence memorization under different fine-tuning strategies?</p> <blockquote> <p>Observation ♯3</p> </blockquote> <p>Model size significantly enhances memorization in parameter-based fine-tuning methods, while prompt-based methods show minimal sensitivity and maintain consistently low memorization.</p> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p>LLaMA2-7B vs LLaMA3-1B</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>요약: 파라미터 바꾸는 애들은 모델 크기 커질수록 더 잘 외웠는데 반대는 미미하더라 (low sensitivity of prompt tuning to model scale)</p> <p>특히, gpt2의 경우나 1B 스케일에서 LoRA는 사실상 거의 못외움</p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation ♯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p>다운스트림 태스크의 종류에 따라서도 다를 수 있음. 이를 위 LLaMA2-7B를 다양한 방법을 통해 학습시키고 LOSS attack against에 대해서 각각을 평가해봄</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prompt-based 만 봤을 때, WebNLG가 다른 것들에 비해서 성능이 높다</p> <p>아마도 구조화된 pattern학습에는 유리한 것 같다</p> <h2 id="impact-of-lora-placement-on-memorization">Impact of LoRA Placement on Memorization</h2> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>AUC↑ ⇒ 기억(privacy risk)↑</p> <ol> <li><strong>Projection &gt; Attention</strong></li> </ol> <ul> <li>LoRA를 <strong>projection layer</strong>에만 적용할 때, 두 데이터셋 모두 네 가지 MIA 지표에서 <strong>AUC가 일관되게 상승</strong> → 기억이 더 강해짐.</li> </ul> <ol> <li><strong>Both layers = 기억 제일 강함</strong></li> </ol> <ul> <li>Attention + Projection 동시 적용 시 <strong>가장 높은 AUC</strong> → 최대 수준의 memorization.</li> </ul> <ol> <li><strong>메커니즘 해석</strong></li> </ol> <ul> <li> <p>Projection layer는 <strong>특징 변환, 정보 압축</strong>을 담당 → 학습 데이터의 구체적 패턴을 더 잘 ‘붙잡아 두는’ 위치.</p> </li> <li> <p>결과는 Meng et al. (ROME)의 Transformer 기억은 주로 projection 층에 집중한다는 가설을 재확인.</p> </li> </ul> <p>Practical한 관점에서…</p> <ul> <li> <p>프라이버시에 민감한 애플리케이션에서는 LoRA를 attention 층에만 삽입하거나 rank를 낮추어 위험을 완화.</p> </li> <li> <p>성능과의 트레이드오프가 필요할 때, 삽입 위치(attn vs proj)와 범위(단일 vs 복합 층)를 주요 조절 변수로 활용하면 효과적일 수 있겠다!</p> </li> </ul> <h1 id="limitation">Limitation</h1> <p>너무 많죠..하지만 저자가 이야기한 것만 말해보겠습니다.</p> <ol> <li> <p>larger model</p> </li> <li> <p>MoE 같은 다른 구조</p> </li> <li> <p>데이터 적음</p> </li> </ol>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="embedding"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근 LLM이 복잡한 reasoning task에서 괄목할만한 성능을 보이고 있으나, (모델에게) 편한 reasoning pattern에 의존하는 경향이 있음</p> <ul> <li>이를 Reaosning rigidity로 정의</li> </ul> </li> <li> <p>사용자의 명시적인 instruction이 있음에도 불구하고, 습관적인 reasoning trajectory를 생성, 오답으로 귀결</p> <ul> <li>특히 수학과 퍼즐 분야에서 두드러짐</li> </ul> </li> <li> <p>이를 분석하기 위해 reasoning trap이라는 진단 데이터셋을 도입</p> <ul> <li> <p>deviation을 요구하도록 기존 데이터셋을 수정한 상태</p> </li> <li> <p>예를 들어, 모든 토끼는 불임이다. 토끼가 3쌍이 있고 토끼 한 쌍이 암수 각 1마리씩 총 2마리를 낳는다고 가정하면, 2세대를 거치면 몇마리의 토끼가 되는가? 라는 질문.</p> </li> </ul> </li> </ul> <p>⇒ 이를 통해서 모델이 습관적으로 쓰는 contamination된 pattern을 식별할 수 있음</p> <ul> <li> <p>모델이 주어진 instruction 을 무시하거나 왜곡하도록 함</p> </li> <li> <p>reasoning trap을 통해서 모덷이 습관적으로 사용하는 reasoning pattern을 발견, 분류</p> <ul> <li> <p>interpretation overload</p> </li> <li> <p>input distrust</p> </li> <li> <p>partial instruction attnetion</p> </li> </ul> </li> </ul> <p>⇒ 해당 데이터셋을 통해 LLM에 있는 reasoning rigidity를 해소하는 미래 연구를 용이하게 함</p> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM은 수학, 복잡한 코딩 문제, 퍼즐 풀이를 포함한 여러 어려운 태스크에서 주목할만한 성능을 보임</p> <ul> <li>특히 test-time scaling을 활용해 확장된 CoT prompting을 활용하는 reasoning model들이 큰 주목을 받고 있음</li> </ul> </li> <li> <p>하지만, 이러한 모델들에게 문제 행동, reasoning rigidity가 발견됨</p> <ul> <li>특히 긴 CoT reasoning으로 학습된 모델에게 나타남</li> </ul> </li> <li> <p>reasoning rigidity는 cognitive bias를 반영, 주어진 조건을 이해해도 자기 방식대로 override, 무시하고 문제를 푸는 현상을 뜻함</p> </li> </ul> <p>⇒ 이는 기존에 언급되어왔던 hallucinataion, prompt brittlness들을 해소해도 존재할 수 있음</p> <ul> <li> <p>hallucination : 틀린 정보를 생성하는 것</p> </li> <li> <p>prompt brittlness : 미묘한 prompt 차이에 따라 답변이 바뀜. 답변이 unstable한 현상</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>reasoning rigidity는 사용자가 서술한 제약이 중요한 도메인에서 큰 문제가 됨</p> <ul> <li>예를 들어, 수학이나 퍼즐 풀이의 경우, 뒤의 문제와 관계 없이 유저가 바로 정답으로 이어질 수 있는 조건을 줬을 경우, 이를 무시하면 완전히 오답이 될 수 밖에 없음</li> </ul> </li> </ul> <p>⇒ 사용자의 지시를 무의식 중에 편집하거나 무시(reasoning rigidity), 모델의 reasoning path전체가 오염, 오답으로 이어짐</p> <ul> <li> <p>이러한 현상은 아주 크리티컬하나, 본 논문에서 처음으로 문제를 정의</p> </li> <li> <p>reasoning rigidity를 식별할 수 있도록, 기존의 수학, 퍼즐 데이터셋을 활용해 reasoningtrap이라는 벤치마크를 제안</p> <ul> <li> <p>잘 알려진 cahllenges와 닮았으나, 조건이 수정되어서 답이 완전 바뀌는 문제들로 구성</p> </li> <li> <p>모델이 습관적으로 문제를 풀 경우 오답으로 이어지는 구조로 설계</p> </li> </ul> </li> <li> <p>ReasoningTrap으로 여러 모델을 평가한 결과, 여러 중요한 현상들을 발견</p> <ul> <li> <p>reasoning process의 중간 단계에서 contamination이 시작</p> </li> <li> <p>이러한 contamination은 명백하게 식별 가능, 반복되는 패턴을 가짐</p> </li> </ul> </li> <li> <p>또한, 이러한 contamination의 패턴을 3가지로 분류</p> <ul> <li>interpretation overload, input distrust, partial insturction attention</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> <ul> <li> <p>LLM의 reasoning ability를 향상시키기 위해 CoT를 길게 생성하도록 학습하는 방법론이 제안</p> </li> <li> <p>또한, Qwen3의 경우 reasoning과 non-reasoning mode를 둘다 지원하는 unified fusion architecture를 공개</p> <ul> <li>user가 모델이 긴 CoT를 생성하도록 할지 여부를 고를 수 있음</li> </ul> </li> </ul> </li> <li> <p>Instruction following of reasoning models</p> <ul> <li>여러 in-context examples 혹은 장황한 instruction을 넣으면 reasoning model들의 성능이 떨어진다는 것을 잘 알려짐</li> </ul> </li> </ul> <p>⇒ 즉, LRM이 user-provided example을 following하는 능력이 부족</p> <ul> <li> <p>본 연구는 이와 결은 같지만, 모델이 친숙한 reaosning pattern을 고집한다는 것에 초점을 둠</p> </li> <li> <p>Rigidity in reasoning models</p> <ul> <li> <p>몇몇 연구들이 LLM이 reasoning할 때 rigid pattern을 보인다는 것을 지적함</p> <ul> <li> <p>medical domain, educational domain</p> </li> <li> <p>우리의 연구는 더 큰 도메인이 수학, 퍼즐에 초첨</p> </li> </ul> </li> <li> <p>본 연구와 유사하게, 몇몇 논문들이 LLM이 rigidity를 탐구</p> <ul> <li>이러한 연구들은 LLM이 creative problem solving에 적용될 때 혹은 matha word problem의 unseen variant의 일반화 에 초점을 둠</li> </ul> </li> </ul> </li> <li> <p>Underlying reason for rigidity</p> <ul> <li> <p>몇몇 연구들이 왜 LLM이 이러한 rigidity를 가지는지에 대해 분석했고, training data 혹은 optimization 방식에 임베딩된 bias를 지적</p> </li> <li> <p>한 연구에서 RL로 학습된 모델들이 exploitation이 뛰어나고, 이로 인해 높은 성능은 달성했지만 역설적이게도 non-reasoning model에 비해 좁은 knowledge coverage를 보인다고 주장</p> </li> <li> <p>다른 연구에서는 training data에 내재된 bias때문이라고 함</p> </li> </ul> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>크게 2가지로 도메인으로 구성 : 수학(ConditionedMath), 퍼즐(PuzzleTrivial)</p> </li> <li> <p>각 데이터는 원래 Q-R-A tuple (q_orig, r_orig, a_orig)과 수정된 tuple (q_mod, r_mod, a_mod)로 구성</p> </li> <li> <p>총 164개의 데이터셋, 84개는 수학, 80개는 퍼즐</p> </li> <li> <p>ConditionedMath에 있는 모든 질문은 개념적으로 다르고, 겹치지 않고, human annotator에 의해 엄격하게 검증됨</p> </li> <li> <p>PuzzleTrival은 10개의 puzzle concept를 가짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5를 활용해서 제작</p> </li> <li> <p>원래 질문을 수정하고, 수정된 질문이 아래 조건에 부합하는지를 확인, 필터링</p> <ul> <li> <p>validity : 기존 condition과 모순되는지</p> </li> <li> <p>divergence : 기존 답, 풀이와 상이한지</p> </li> <li> <p>existence : 답이 있는지</p> </li> </ul> </li> </ul> <p>⇒ 문제를 수정할 때는 gpt-4o-mini활용, 필터링 할 때는 o4-mini를 사용</p> <ul> <li>220개의 원본 데이터를 5가지의 variant로 modified, 필터링 후에 최종 84개만 남음</li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzle은 조건을 수정하면 급격하게 단순해지거나 답이 여러개일 수 있음</p> </li> <li> <p>ambiguity를 줄이기 위해, “valid solution을 위해 가장 간단한 답을 찾아라”라는 문구를 instruction에 추가</p> </li> <li> <p>과정 자체는 위와 동일</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li> <p>시스템적으로 reasoning model의 contamination을 측정하기 위해서, Contamination ratio를 제안</p> <ul> <li> <p>친숙한 패턴에서 contaminated reasoning이 얼마나 차지하는지를 나타냄</p> </li> <li> <p>이를 자동적으로 식별하는 방법도 제안</p> </li> </ul> </li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>모델이 문제를 풀 때, 수정된 조건을 이해하고 풀었는지 이해하지 않고 풀었는지를 구분하기 위해 metric을 도입</p> </li> <li> <p>생성된 reasoning path를 단락별로 쪼개고, 각 단락을 textual representation으로 embedding</p> <ul> <li> <p>openAI의 text-embedding-small model을 사용</p> </li> <li> <p>단락은 double line break를 기준으로 분리</p> </li> </ul> </li> <li> <p>각 단락과 오리지널 문제의 reasoning path, 각 단락과 modified reasoning path와의 cosine 유사도를 계산, 둘을 비교해 original reasoning path와의 유사도가 더 높을 경우 1로 계산</p> <ul> <li>즉, 조건이 수정되었는데도 무시하고 습관처럼 reasoning을 했다는 뜻</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidity를 잘 관찰하기 위해, 모델이 수정된 조건을 이해했는데도 습관처럼 풀었는지 아니면 인지조차 하지 못했는지를 구분</p> <ul> <li>모델이 조건을 잘못 이해한 경우 / 조건을 잘 이해했으나 reasoning을 잘못한 경우</li> </ul> </li> <li> <p>이를 반영한 metric을 p-passs@k라고 정의, reasosning path에서 constraint를 인지하고 있는 경우에만 accuracy를 측정</p> </li> <li> <p>constraint를 인지했는지는 모델이 생성한 reasoning path중 첫 15개의 단락과 정답, 질문을 LLM에 넣고 판단하도록 함(p_i)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>question만 주어지는 현실적인 상황에서, generated reasosning이 원치 않지만 친숙한 pattern으로 contaminated됐는지 자동적으로 식별하는 것을 불가능</p> </li> <li> <p>그래서 간단하게, contamination의 종류를 분류해서, 각 type별 의심스러운 pattern을 식별</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Interpretation overload : 모델이 주어진 문제 조건을 거절하는 것으로 시작, 문제를 바로 해석하는 것보다 여러 방식으로 재해석. 보통 reasoning 중간 단계에서 발생, inconsistent 혹은 contraditory한 결론을 야기</p> </li> <li> <p>Input Distrust : 모델이 번역 오류, input error, typo존재 등을 가정함. 직관적으로 바로 문제를 풀 수 있음에도 부정하고 매우 복잡하게 풀게 됨.</p> </li> <li> <p>Partial Instruction Attention : 모델이 제공된 instruction의 일부분만을 선택적으로 집중</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrap을 여러 LLM에 inference</p> </li> <li> <p>실험은 CoT prompting을 사용, ‘Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}’ 포맷으로 질문을 전달</p> </li> <li> <p>table 2,3은 16번 sampling, 다른 실험은 4번 sampling</p> </li> <li> <p>수학 문제의 경우, exat matching으로 correctness 판단, puzzle의 경우 free-from sentence로 답이 구성되다 보니, LLM을 사용해서 정답과 모델 답변을 함께 제공해 correctness를 판단</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>실험 결과, 대부분 reason모드일 때보다 base모드에서 더 높은 성능을 보임</p> <ul> <li>즉, 길게 reasoning을 하면서 습관적인 reasoning pattern을 사용, 오답으로 이어지는 경우가 많다는 것</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Buget forcing : 버짓 마지막에 ‘Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;’를 추가하여 답을 바로 내도록 함</p> <ul> <li> <p>MATH500 : low 2000, medium 4000, high 6000 토큰 사용</p> </li> <li> <p>AIME : low 2000, medium 6000, high 10000</p> </li> </ul> </li> <li> <p>prompt hinting : 문제에 오타 없고 지시 그대로 하라는 prompt를 추가</p> </li> <li> <p>실험 결과, budget이 커질 수록 성능이 악화됨</p> </li> <li> <p>prompt로 hint를 줘도 여전히 reasoning rigidity가 존재</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>모델 크기에 따른 실험. base모델이 성능이 전반적으로 높게 나오는 편</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="detection"/><category term="embedding"/><category term="gpt"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근의 reasoning oriented model(LRM)은 여러 수학 데이터셋에서 높은 성능 달성을 보이나, natural instruction following에 대한 성능은 분석되지 않음</p> </li> <li> <p>본 논문에서는 이러한 LRM들의 instruction following 능력을 분석하기 위해 MathIF라는 데이터셋을 제안, math 도메인에서의 instruction following 성능을 평가</p> </li> <li> <p>실험 결과, reasoning을 효과적으로 하는 모델이 user direction에 따르는 것을 어려워 하는 현상 발견</p> <ul> <li> <p>긴 CoT dataset에 SFT하거나 RL로 학습한 모델이 답변 길이가 길어질수록 instruction following 능력이 떨어지는 현상 발견</p> </li> <li> <p>간단한 개입(CoT 마지막 부분에 instruction을 다시 붙여서 넣어줌)으로 instruction following 성능을 향상시킬 수 있음을 보임</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoning을 scaling하는 것은 reasoning ability를 향상시킴</p> <ul> <li>SFT or RLVR 사용</li> </ul> </li> <li> <p>LRM의 경우 간단한 instruction도 following하는 것을 어려워 한다는 것을 발견</p> </li> </ul> <p>⇒ reasoning-oriented learning을 하면 모델 자체의 reasoning ability는 향상돼도 controllability는 떨어지는게 아닐까?</p> <ul> <li>하지만 현재는 범용 목적의 instruction following(IF) 벤치마크만 존재</li> </ul> <p>⇒ 수학 도메인에서의 IF 벤치마크를 만들고 평가해보자!</p> <ul> <li> <p>실험 결과, instruction following과 reasoning capability사이의 일종의 trade-off가 존재</p> <ul> <li> <p>즉, SFT 혹은 RL로 reasoning ability를 향상시킨 모델은 reasoning 성능은 올라도 IF 성능은 떨어짐</p> </li> <li> <p>특히, CoT 길이가 길어질수록 IF 성능이 악화됨</p> </li> </ul> </li> <li> <p>contribution</p> <ul> <li> <p>MathIF, 첫번째로 수학 도메인에서 instruction following 능력을 시스템적으로 측정하는 벤치마크 데이터셋 도입</p> </li> <li> <p>23개의 LRM를 해당 벤치마크에 대해서 평가</p> </li> <li> <p>reasoning performance와 instruction-following사이의 trade-off가 있음을 실험적으로 보임</p> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> <ul> <li> <p>high-quality long CoT by distilling from more capable LRMs or combining primitive reasoning actions</p> <ul> <li> <p>s1 : 적은 양의 CoT data로도 reasoning ability를 향상시킴</p> </li> <li> <p>LIMO : 관련 도메인이 이미 pre-training때 포함되어 있다면, 최소한의 cognitive process를 담은 demonstration으로 reasoning capabilities를 발생시킬 수 있다고 서술</p> </li> </ul> </li> <li> <p>cold-RL</p> <ul> <li> <p>deepseek-R1-zero로 주목 받게 된 방법론</p> </li> <li> <p>SFT와 달리, cold-RL은 긴 CoT dataset에 의존하지 않고, final outcome으로 reward를 받아서 학습</p> </li> <li> <p>RL과정을 간단, 가속화 하기 위해서 dynamic sampling, process-reward, off-policy guidance, CoT preference optimziation 등이 제안됨</p> </li> </ul> </li> </ul> </li> <li> <p>Instruction-followiwng benchmark</p> <ul> <li> <p>이전의 벤치마크는 보통 user query의 completeness에 초점, proprietary language model에 의존해서 win rate를 측정하는 식으로 평가</p> </li> <li> <p>format constraint, multi-turn instruction, refutation instruction, compositional instruction을 따르는지를 평가</p> </li> <li> <p>하지만 대부분의 IF 벤치마크는 일반적인 도메인에 집중, 상대적으로 직관적인 query를 사용</p> </li> </ul> </li> </ul> <p>→ 이러한 도메인 차이와 long CoT의 부재는 LRM을 평가하는데에 방해가 됨</p> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> <ul> <li>toy experiment로 IFEval과 FollowBench에 대한 LRM과 Instruct 모델 성능 비교</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>확실히 LRM의 성능이 상대적으로 낮으나, 낮은 원인이 domain shift때문인지 IF성능 때문인지는 분명하지 않음</li> </ul> <p>⇒ 수학 도메인의 IF benchmark를 만들자!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- python으로 검증 가능한 constraint를 고려, 2-3개의 constraint를 합쳐서 instruction으로 부여하는 방안을 고려

- contraint를 얼마나 만족했는지를 평가하기 위해 Hard accuracy(HAcc), Soft accruacy(SAcc)로 측정
</code></pre></div></div> <ul> <li> <p>Constraint type</p> <ul> <li> <p>length, lexical, format, affix로 크게 4가지 type으로 분류, 그 안에 sub-type을 명시</p> </li> <li> <p>proprietary language model에 의존하지 않기 위해서 python으로 제약을 만족했는지 검증 가능하도록 설계</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Compositional Constraint</p> <ul> <li> <p>2-3개의 constraint를 조합해서 compositional constraint를 구축</p> </li> <li> <p>같이 존재할 수 없는 constraint나 같은 subtype끼리 있으면 filtering, 그 외의 조합에서 random sampling해서 데이터셋을 구축</p> </li> <li> <p>이러한 과정을 통해 30개의 dual-constraint와 15개의 triple-constraint를 구축</p> </li> </ul> </li> <li> <p>Math problem collection</p> <ul> <li> <p>GSM8K, MATH-500, Minerva, Olympiad에서 90개씩 sampling</p> <ul> <li>초등학교부터 올림피아드 수준의 문제까지 아우르도록 함</li> </ul> </li> <li> <p>각 데이터에 대해서 single, dual, triple constraint를 적용</p> </li> <li> <p>sanity check를 위해 사람이 직접 검수, math problem에 추가된 constraint가 모순되지 않는지 더블췍</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Evaluation metric</p> <ul> <li> <p>HAcc : constraint다 만족해야 1</p> </li> <li> <p>SAcc : contraint 개당 만족하면 1 아니면 0으로 계산, 평균</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>구체적인 언급 없으면 correctness는 contraint가 있는 상태에서 나온 답변으로 계산</li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>모든 LRM은 nucleus sampling(T=1.0, p=0.95)로 디코딩, 최대 답변 길이 16,384 토큰, vLLM 사용</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모든 LRM은 IF성능이 하락함</p> <ul> <li> <p>최고 성능을 낸 Qwen3-14B마저도 50.71밖에 안됨</p> </li> <li> <p>특히 deepseek-R1-distill-llama-70B나 open-reasonser-zero-32B의 경우 모델 크기에 비해서 매우 낮은 IF 성능을 보임</p> </li> </ul> </li> <li> <p>Qwen3 시리즈가 그나마 높은 IF 성능을 보임</p> </li> <li> <p>모델 크기가 IF 성능을 결정하진 않음</p> <ul> <li>같은 계열에서는 종종 경향성이 있으나, 다른 계열까지 한번에 봤을 때는 크기가 크다고 IF 성능이 보장되진 않음</li> </ul> </li> <li> <p>명시적인 reasoning seperation (<think>,</think>)가 있는 모델이 전반적으로 IF 성능이 높음</p> <ul> <li>Qwen2.5-Math-1.5B, 7B-Instruct, Qwen2.5-1.5B, 7B-SimpleRL-Zoo 친구들이 명시적인 reasoning token 없는 애들 ⇒ 성능이 쏘 처참</li> </ul> </li> <li> <p>instruction-following과 mathematical reasoning사이에 trade-off가 존재</p> <ul> <li>Diff를 보면 대부분의 모델이 constraint가 있을 때와 없을 때의 correctness차이가 큼</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>LRM모델이 constraint를 잘 따르는게 문제 난이도와 연관이 있는지를 살펴보기 위해 데이터셋 별로 IF성능을 표현</p> </li> <li> <p>문제가 어려울수록 constraint를 잘 만족하지 못한다는 것을 알 수 있음</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>제약이 많아질수록 IF 성능 하락, 특히 2개이상부터 크게 하락..</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>제약조건을 만족하면서 문제를 맞추는 경우는 크지 않음</p> </li> <li> <p>보통 제약조건 혹은 문제 하나만을 만족함 + 즉, 제약조건을 걸면 문제 풀이 성능이 하락</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>constraint가 있을 때와 없을 때의 성능 차이</p> </li> <li> <p>특히, GSM8K, Minerva에서 극심 ⇒ 문제 난이도와 상관 없이 contraint가 있으면 reasoning ability가 하락</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>CoT가 길수록 IF 성능 하락</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>IF가 낮았던 Qwen2.5를 대상으로 실험, 데이터는 deepscalar를 사용, QwQ로 CoT생성, 정답을 맞추면서 너무 길지 않은 애들만 필터링해서 학습에 사용</p> <ul> <li>format reward는 think token를 포함하는지 여부로 포함하면 답이 틀려도 0.1점 줌</li> </ul> </li> <li> <p>실험 결과, reasoning-orienteed 방법론이 reasoning성능은 향상시키지만 IF는 하락하는 것을 볼 수 있음</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 7</p> <ul> <li> <p>모델이 reasonign path를 종료하려고 할 때마다 wait를 걸어서 강제로 CoT길이를 늘림</p> </li> <li> <p>CoT길이가 길어질수록 constraint instruction과 멀어져서 constraint에 대한 acc가 떨어지는 것으로 추론</p> </li> </ul> <p>Table 5</p> <ul> <li>cold-RL에서 roll-out 길이를 조정하며 학습, 길어질수록 reasoning은 향상되나 IF는 떨어짐</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>간단하게 reasoning이 끝나갈 때 쯤에 wait을 넣고 constraint instruction을 반복해서 넣어준 경우의 성능을 측정</p> </li> <li> <p>IF성능은 향상되나 Correctness는 하락하는 것을 볼 수 있음</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented model들이 생각보다 instruction following 성능이 악화됨</p> </li> <li> <p>대부분 간단한 형식에 대한 제약인데도, 제약이 있을 때와 없을 때의 성능 차이가 큰게 충격적</p> </li> <li> <p>LLM이 정말 reasoning을 하는걸까? 그냥 답변 길이가 길어져서 발생하는 attention sink일까?</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title><link href="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/" rel="alternate" type="text/html" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 건우 김</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="abstract">Abstract</h1> <ul> <li>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</li> </ul> <p>→ 하지만 기존 advanced reasoning ability를 가진 LLM에게 prompt 기반의 search engine을 활용하도록 하는 것은 suboptimal임 (LLM이 search engine과 어떻게 상호작용해야 하는지 완전히 이해 못함)</p> <ul> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> <ul> <li>단계별 reasoning step에서 autonomously하게 multiple search queries를 생성하고 실시간으로 정보를 검색하도록 학습</li> </ul> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <strong>효과적으로 상호작용하는</strong> 능력이 필수적임</p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 **LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, 단순한 결과 기반 보상이 충분한지는 아직 불확실함.</p> </li> </ol> <p>→ 3번은 자기들도 모르면서 뭔가 싶네요 ㅋㅋ</p> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <h1 id="2-related-works">2. Related Works</h1> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <h1 id="3-search-r1">3. Search-R1</h1> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine R을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>r_{\phi}: output quality를 평가하는 reward function</p> </li> <li> <p>\pi_\theta: policy LLM</p> </li> <li> <p>\pi_{ref}: reference LLM</p> </li> <li> <p>x: dataset D에서 추출된 input sample</p> </li> <li> <p>y: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>D_{KL}: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 \pi_\theta가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <ul> <li> <table> <tbody> <tr> <td>retrieval interleaved reasoning via \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> <ul> <li>\bigotimes denotes interleaved retrieval-and-reasoning</li> </ul> </li> </ul> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <ul> <li>*<strong>*Formulation of RL with a Search Engine**</strong></li> </ul> <p>LLM에서 자주 사용하는 원래 기존 RL의 objective는 아래와 같이 정의됨</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>그런데, 위 formulation은 entire output sequence y가 \pi_{\theta}로부터 생성되었다는 가정이 있음. 이 가정은 model behavior가 internal reasoning과 external information retrieval을 모두 포함하는 상황에서 적용할 수 없음.</p> <p>따라서, RL objective를 serach engine R과 통합시키기 위해 아래와 같이 수정함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>위 수정된 objective에서는 trajectory y 는 interleaved reasoning steps과 retrieved content를 포함</p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>\pi_{\theta}: current policy</p> </li> <li> <p>\pi_{old}: previous policy</p> </li> <li> <p>I(y_t): token loss masking 연산으로, y_t가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <ul> <li> <table> <tbody> <tr> <td>y ~ \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> </li> </ul> <p>→ LLM은 x를 입력 받아 Search Engine R과의 interleaved 흐름을 통해 y를 생성</p> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li> <p><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></p> </li> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Case Study</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="4-main-results">4. Main Results</h1> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> </li> <li> <p>Natural Questions (NQ)</p> </li> <li> <p>TriviaQA</p> </li> <li> <p>PopQA</p> </li> <li> <p>Multi-Hop QA</p> </li> <li> <p>HotpotQA</p> </li> <li> <p>2WikiMultiHopQA</p> </li> <li> <p>Musique</p> </li> <li> <p>Bamboogle</p> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> <ol> <li> <p>Inference w/o Retrieval</p> </li> <li> <p>Direct Inference</p> </li> <li> <p>Chain-of-Thought</p> </li> <li> <p>Inference w/ Retrieval</p> </li> <li> <p>RAG</p> </li> <li> <p>IRCoT (Information Retrieval CoT)</p> </li> <li> <p>Search-o1 (using search engine tool)</p> </li> <li> <p>fine-tuning methods</p> </li> <li> <p>SFT</p> </li> <li> <p>R1: search engine없이 RL fine-tuning (Search-R1과 fair한 비교를 위해 동일 데이터로 RL을 학습하되 검색은 사용하지 않음)</p> </li> </ol> <h2 id="43-experimental-setup">4.3 Experimental Setup</h2> <ul> <li> <p>LLMs: Qwen-2.5-3B, Qwen-2.5-7B (Base / Instruct)</p> </li> <li> <p>Retrieval</p> <ul> <li> <p>Knowledge Source: 2018 Wikipedia dump (using E5 as retriever)</p> </li> <li> <p>number of retrieved documents: 3</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>training data: NQ + HotpotQA for Search-R1 and fine-tuning methods</p> </li> <li> <p>evaluation data: (in-domain, out-of-domain)</p> </li> </ul> </li> <li> <p>metric: EM</p> </li> <li> <p>Inference 설정</p> <ul> <li>Inference-style baseline은 Instruct 모델 사용 (Base 모델은 instruction을 따르지 못함)</li> </ul> </li> <li> <p>RL 설정</p> <ul> <li>별도 언급이 없으면 PPO 사용</li> </ul> </li> </ul> <h2 id="44-performance">4.4 Performance</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Search-R1은 baselines 대비 우수한 성능 보여줌</p> <ul> <li> <p>Qwen2.5-7B: 평균적으로 41% 향상</p> </li> <li> <p>Qwen2.5-3B: 평균적으로 20% 향상</p> </li> </ul> </li> </ul> <p>→ in-domain (NQ, HotpotQA)와 out-of-domain (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle) <strong>모두 일관되게 높음</strong></p> <ul> <li>검색 없이 추론만하는 R1보다도 Search-R1이 우수함</li> </ul> <p>→ <strong>Search가 LLM 추론에 external knowledge를 추가함으로써 도움되는 것을 보임</strong></p> <ul> <li>Base와 Instruct model 모두 일관되게 Search-R1 효과적임</li> </ul> <p>→ DeepSeek-R1-Zero style의 단순 outcome-based reward가 순수 Reasoning 뿐만 아니라 <strong>search를 포함한 complex reasoning scenarios에서도 효과적임을</strong> 보여줌</p> <ul> <li>**Model size가 클 수록 검색 활용 효과가 더 큼 **</li> </ul> <h1 id="5-analysis">5. Analysis</h1> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="51-different-rl-methods-ppo-vs-grpo">5.1 Different RL methods: PPO vs. GRPO</h2> <p>Search-R1에서 RL 방법으로 PPO와 GRPO 두 가지를 모두 실험함</p> <ol> <li> <p><strong>GRPO는 PPO보다 수렴 속도가 빠름</strong> → Figure2 (a)</p> </li> <li> <p>PPO는 critic model에 의존하기 때문에 효과적인 학습이 시작되려면 여러 단계의 워밍업이 필요하지만, GRPO는 baseline을 여러 샘플 평균으로 잡아 더 빠르게 수렴함</p> </li> <li> <p><strong>PPO는 학습 안정성이 더 높음</strong> → Figure2 (a)</p> </li> <li> <p><strong>GRPO는 일정 단계 이후 reward collapse</strong>가 나타나지만, <strong>PPO는 학습이 더 안정적으로 유지됨</strong></p> </li> <li> <p><strong>최종 train reward는 PPO와 GRPO 모두 유사함</strong></p> </li> <li> <p>수렴 속도와 안정성은 다르지만 최종 성능과 train reward는 큰 차이가 없음. 그래도 GRPO는 나중에 불안정해질 수 있기에 더 안정적인 PPO가 느리지만 적합함.</p> </li> </ol> <p><em>(다른 세팅에서도 동일한 현상이 관찰됨)</em></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="52-base-vs-instruct-llms">5.2 Base vs. Instruct LLMs</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Figure2 (b)에서 Instruction-tuned model은 Base model보다 더 빠르게 수렴하고 초기 성능도 더 높게 나오지만, <strong>최종 train reward는 두 모델 모두 거의 동일한 수준으로 수렴함</strong></li> </ul> <p>→ 이는 사전 instruction tuning이 초기 학습을 가속화하는데 도움이 되지만, <strong>RL만으로도 Base model이 충분히 따라잡을 수 있음을 보임</strong></p> <p>(다른 세팅에서도 동일한 현상이 관찰됨)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="53-response-length-and-valid-search-study">5.3 Response Length and Valid Search Study</h2> <p>Qwen2.5-7B-base 모델로 response length와 검색 호출 횟수 변화를 분석함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Figure2 (c)를 보면</p> <ul> <li> <p>초기 단계 (100 steps 전후)</p> <ul> <li> <p><strong>응답 길이가 급격히 줄고, train reward는 소폭 상승함</strong></p> </li> <li> <p>모델이 불필요한 군더더기 단어를 줄이고 task에 적응하기 시작함을 보여줌</p> </li> </ul> </li> <li> <p>후기 단계 (100 steps 이후)</p> <ul> <li> <p><strong>응답 길이와 train reward 모두 증가함</strong></p> </li> <li> <p>모델이 검색 호출을 더 자주 하면서 (Search Engine을 자주 호출하는 법 학습) 검색 결과가 추가되어 응답이 길어짐</p> </li> <li> <p>검색 결과를 효과적으로 활용하며 train reward도 크게 향상됨</p> </li> </ul> </li> </ul> </li> <li> <p>Figure2 (d)를 보면 <strong>학습이 진행될수록 LLM이 검색 엔진 호출을 더 많이 학습한다는 점이 드러남</strong></p> </li> </ul> <h2 id="54-study-of-retrieved-tokens-loss-masking">5.4 Study of Retrieved Tokens Loss Masking</h2> <p>Retrieved Token Loss Masking은 unintended optimization을 방지하기 위해 도입한 것임. Retrieved token loss masking의 효과를 추가로 분석해봄 (Qwen2.5-7B-base)</p> <ul> <li>Figure 3에 따르면, <strong>masking을 적용하면 원치 않는 최적화 효과를 줄이고 LLM 성능 향상이 더 커짐</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>w. mask와 w.o. mask를 비교한 결과 **masking을 적용한 경우가 항상 더 높은 성능을 기록함 **</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>*<strong>*Appendix**</strong></li> </ul> <p><strong>Number of Retrieved Passages Study in SEARCH-R1 Training</strong></p> <ul> <li>본 실험에서는 top-k를 3으로 설정했지만, 1,3,5 바꿔가며 이것의 effect를 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ top-k가 1,3,5 설정이 <strong>모두 training pattern이 비슷함</strong> (top-k=5가 초기 수렴 속도가 빠른 대신에 이후 train reward가 감소하며 학습 안정성 떨어짐을 보임)</p> <p><strong>Group Size Study in SEARCH-R1 (GRPO) Training</strong></p> <ul> <li>본 실험에서는 Search-R1 (GRPO)의 group size를 5로 설정했지만, group size가 어떤 영향을 미치는지 확인하고자 1,3,5로 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ Figure7을 보면 <strong>group size가 칼수록 수렴 속도 빨라지는 반면 RL의 불안정성 때문에 training collapse 위험도 증가</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ Table8을 보면 group size가 큰 경우 빠른 수렴과 더 높은 train reward가 있었지만, group size=1일 때 학습이 더 안정적이고 일반화 성능이 더 우수함 (out-of-domain에서 더 우수함)</p> <h1 id="6-conclusion">6. Conclusion</h1> <ul> <li> <p>본 연구에서는 LLM이 self-reasoning과 실시간 검색 엔진 상호작용을 교차적으로 수행할 수 있는 framework인 Search-R1 제안함</p> </li> <li> <p>기존의 multi-turn search를 위해 많은 prompt에 의존하는 RAG나 대규모 train data가 필요한 tool 사용 기반 접근법과 달리, <strong>Search-R1은 RL을 통해 모델이 자율적으로 검색 쿼리를 생성하고 검색된 정보를 전략적으로 활용할 수 있도록 최적화함</strong></p> </li> </ul> <p>Limitations</p> <ul> <li>Reward Design가 단순 결과 기반 보상이라 보다 디벨롭이 필요함</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="fine-tuning"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><category term="reinforcement learning"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[논문 리뷰 - Reinforcement Learning 관련 연구]]></summary></entry><entry><title type="html">Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title><link href="https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of/" rel="alternate" type="text/html" title="Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> <li><strong>Reviewer</strong>: 준원 장</li> <li><strong>Property</strong>: Reasoning</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>Test-time scaling is trending, but <strong>longer reasoning is not always better.</strong></p> </li> <li> <p>Reasoning와 accuracy가 항상 상관관계를 이루지 않는다는 최신연구 존재 (Xie et al., 2025; Jin et al., 2024; Wu et al., 2025)</p> </li> </ul> <p>(연구들 안봤지만 (1) 모델이 작거나 (2) 32K, 64K까지 inference안해봐서 그럴거 같다고 생각.. o3도 아예 못푸는 lcb pro 수준이라면 예외)</p> <ul> <li> <p>여튼, 이러한 흐름에 따라 최근에 나온 용어</p> <ul> <li>Overthinking phenomenon; simple problems에도 verbose outputs을 생성하는 현상</li> </ul> </li> <li> <p>그래서 논문은 DeepSeek-1.5B-Distill과 DeepScaler-1.5B-Preview를 가지고 reasoning length와 accuracy를 가지고 체계적인 분석을 수행하겠다!</p> </li> </ul> <h2 id="2-related-work">2. Related Work</h2> <p>⇒ lengthy reasoning 문제를 관측하고, 이를 해결하기 위한 학습방법론들</p> <ul> <li> <p><strong>Concise thinking</strong></p> <ul> <li>reward나 pre-sampling (뭔진 몰겠음..)으로 RL/SFT-training때 good accuracy를 유지하면서 상대적으로 짧은 reasoning path를 생성하는 연구 계열</li> </ul> </li> <li> <p><strong>Adaptive thinking</strong></p> <ul> <li>(prompting 위주) 문제 난이도·모델 확신도에 따라 토큰 예산을 동적으로 조정하거나 조기 종료</li> </ul> </li> </ul> <p>→ lengthy reasoning path가 high accuracy를 보장하지 않는다는 실험적인 결과를 보이는 경우가 많음</p> <ul> <li> <p><strong>Optimal Thinking</strong></p> <ul> <li>reasoning path가 길어지면 성능이 처음엔 오르다가 다시 떨어진다는 점을 이론·실험으로 입증</li> </ul> </li> </ul> <p>(…이게 이론으로 입증이.. 되나..?)</p> <h2 id="3-experimental-setting">3. Experimental Setting</h2> <ul> <li> <p>Model</p> <ul> <li> <p>DeepSeek-1.5B-Distill (Denoted as R1-Distill)</p> </li> <li> <p>DeepScaler-1.5B-Preview (Denoted as R1-Preview)</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>GSM8K</p> </li> <li> <p>MATH</p> </li> </ul> </li> <li> <p>Params.</p> <ul> <li> <p>temperature T = 1.0 (most calibrated)</p> </li> <li> <p>top-p = 1</p> </li> </ul> </li> <li> <p>Notations</p> <ul> <li> <p>question: q</p> </li> <li> <h1 id="of-completions-n">of completions: N</h1> </li> <li> <p>{(o(q)<em>i , l(q)_i , c(q)_i )}^{N −1}</em>{i=0}</p> <ul> <li> <p>o(): output</p> </li> <li> <p>l(): length</p> </li> <li> <p>l(): correctness 여부 {0,1}</p> </li> </ul> </li> </ul> </li> </ul> <h2 id="4-sample-level-analysis">4. Sample-Level Analysis</h2> <p>→ q는 고정하고 길이가 다른 10개 completion을 비교해 length와 accuracy의 직접 상관을 조사</p> <ul> <li>난이도에 대한 변인을 고정하고 length ↔ accuracy 관계만 볼 수 있음</li> </ul> <h3 id="non-linear-relationship-of-sample-length-and-correctness">Non-Linear Relationship of Sample Length and Correctness</h3> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>L_r, Acc_r: r번째로 짧은 reasoning path의 평균 length/accuracy</li> </ul> <p>(모든 q에 대해 10개씩 다 생성하고 가장 짧은 completion의 길이 평균: L_0)</p> <ul> <li> <p>consistent non-monotonic trend 관찰</p> <ul> <li>Distill 기준 어느정도는 길어야 best acc, 너무 길어지면 decline</li> </ul> </li> </ul> <p>(준원 뇌피셜: 일단 R1은 (1) MATH 관련 데이터는 외워서 풀것 같기 때문에 temp=1.0, top_p=1로 줘서 decoding path 길어지면 degen 발생했을것으로 예상 (2) GSM8K 유사 난이도는 거의 외웠을것이고 + 상대적으로 쉽기 때문에 1~1.5K thinking budget내로는 거의 비슷할거 같음..)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>초록: q에 대한 정답 completion중 가장 짧은 거</p> </li> <li> <p>파랑: q에 대한 정답 completion중 가장 긴거</p> </li> <li> <p>빨강: q에 대한 오답 completion중 가장 짧은 거</p> </li> <li> <p>노랑: q에 대한 정답 completion중 가장 긴거</p> </li> <li> <p>R1-Preview는 MATH, GSM8K 모두 80% 이상의 질문에서 가장 짧은 샘플로 정답을 생성할 수 있음을 보임</p> </li> <li> <p>most length한 completion중에 correct response도 있지만 incorrect response도 존재 (논문 해석 이상..)</p> </li> </ul> <h2 id="5-question-level-analysis">5. Question-Level Analysis</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>단순하게 문제 난이도를 틀림 여부로 볼때, incorrect response가 어떤 조합에서든 response 길이가 더 길었음</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>N-completion별로 difficulty를 분류</p> <ul> <li> <p>Easy: model이 10 completion 모두 정답률 100%</p> </li> <li> <p>Medium: model이 10 completion에서 정답률 0% &lt; acc &lt; 100%</p> </li> <li> <p>Hard: model이 10 completion에서 정답률 0%</p> </li> </ul> </li> </ul> <p>⇒ 그러나 (1) 문제가 어려워서 lengthy한지 (2) length해져서 틀린건지 판단이 어려움</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Q^{easy}<em>{\cap} = Q^{easy}</em>{i} \cap Q^{easy}_{j}</p> </li> <li> <p>Q^{easy}<em>{i/j} = Q^{easy}</em>{i} / Q^{easy}_{j} &gt; M_i 에서만 쉬운 문제</p> </li> <li> <p>Q^{easy}<em>{j/i} = Q^{easy}</em>{j} / Q^{easy}_{i} &gt; M_j에서만 쉬운 문제</p> </li> <li> <p>보편적으로 쉬운 문제가 아니라 another model’s advantage set (다른모델에서 쉬운 문제)에서 오히려 lengthy generation을 보임</p> </li> <li> <p>signficant로 보면 M_i → M_j-Adv Set을 풀때 보다 lengthy해짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>hard question에서는 Q^{hard}_{\cap}에서 보다 another model’s advantage set에서 lengthy해질 것을 기대했으나 그렇진 않음</p> <ul> <li>hard question에서 clear pattern은 없음</li> </ul> </li> </ul> <p>→ 문제가 너무 어려운 경우 모델이 어려운 문제의 난이도 증가를 인식하고 이에 대응하는 데 어려움을 겪을 수 있음 (e.g., 문제 난이도를 과소평가하여 짧게 생성)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>(어떻게 실험했는지는 모르겠는데..) token length가 짧아질수록 accuracy가 올라간다.</p> </li> <li> <p>위에 실험을 기반으로 token legnth가 짧으니 확률적으로 당연히 accuracy가 높은 답변일수록 PPL도 낮을 것</p> </li> </ul> <h2 id="6-effect-of-length-preference-optimization">6. Effect of Length Preference Optimization</h2> <ul> <li>지금까지 지적된 문제들을 해결하기 위해 correct/length-balanced reward-based RL등이 소개되었음</li> </ul> <p>(N» samples, ground truth label이 필요)</p> <ul> <li> <p>이를 위해 이전에 drive-out한 직관들을 가지고 간단한 실험을 진행.</p> <ul> <li> <p>LM을 가지고 2개의 completion을 생성</p> </li> <li> <p>(데이터셋이 쉬웠으니) 정답 유무에 상관없이 짧은 response가 정답일 확률이 높을거라는 가정하에 짧은 response에 preference가 가해지도록 SimPO</p> </li> <li> <p>MATH/GSM8K training set, 8K rollout</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>training step을 반복할수록 accuracy 변동폭은 적으나 average token length 30%에서 60% 감소</li> </ul> <p>(length decrease가 정답의 단축 때문인지, 오답의 단축 때문인지, 아니면 둘 다 때문인지…?)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>SimPO가 진행됨에 따라 incorrect response의 생성이 줄어들었다.</li> </ul> <p>→ (준원) 해석을 해보면,</p> <ul> <li> <p>어짜피 맞을 문제는 2 completion 다 짧을거였으니 그 중에서도 짧게 생성하도록 model 학습</p> </li> <li> <p>틀린 문제는 2 completion 다 길게 생성했을것이나 (그 중 하나는 조금이라도 짧게 생성했을테니) 학습이 됨에 따라 조금씩 짧게 생성하도록 했을 것</p> </li> </ul> <h2 id="7-conclusion--limitation">7. Conclusion &amp; Limitation</h2> <ul> <li> <p>generation length와 final answer correctness에 대해서 심도 있는 분석</p> <ul> <li>변인 통제도 신경썼고, takeaway도 많음</li> </ul> </li> <li> <p>LM의 크기가 너무 작고, benchmark가 너무 쉬움…</p> <ul> <li>큰 LM도 어려운 문제에 대해서 lengthy generation을 하면서 잘 못푸는 모습 (reflexion x)을 많이 관찰했는데, 관련 내용이 있었으면 좋았을듯..</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰 - Reasoning 관련 연구]]></summary></entry><entry><title type="html">Reasoning Models Can Be Effective Without Thinking</title><link href="https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking/" rel="alternate" type="text/html" title="Reasoning Models Can Be Effective Without Thinking"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM을 이용해 복잡한 문제를 풀 때, 보통 우리는 “긴 chains of thoughts”를 생성하고 그것을 이용해 reflection, backtracking, self-validation 등을 수행하곤 한다 (“Thinking”). 이러한 reasoning path는 일반적으로 reward를 이용한 강화학습, 혹은 distilled reasoning trace를 이용한 finetuning을 통해서 획득되며, 이 <strong>explicit한 reasoning path가 실제로 성능에 많은 도움이 된다고 믿어져왔다</strong>. 이 때문에 inference-time compute scaling이 주된 paradigm이기도.</p> <p>하지만 저자들은 이에 대한 근본적인 질문을 던진다:</p> <ul> <li>정말로 explicit Thinking process가 상위 reasoning을 위해 필요한가?</li> </ul> <p>그리고 저자들은 사실 정교한 reasoning path은 그닥 중요하지 않다는 사실을 다양한 실험을 통해서 증명한다.</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>저자들은 DeepSeek-R1-Distill-Qwen을 이용해서 Thinking과 이들이 사용하는 NoThinking — 응답에 가짜 Thinking 블록을 미리 채워 넣고, 모델이 그 이후부터 이어서 답변하도록 하는 방법 — 을 비교해보았을 때, 오히려 NoThinking이 훨씬 더 성능이 좋다는 것을 보인다 (pass@k metrics).</p> <p>NoThinking은 2.0~5.1배 적은 토큰을 사용하면서도, k=1을 제외하고는 Thinking과 비슷하거나 더 좋은 성능을 보인다.</p> <p>또한, 두 접근법의 토큰 사용량을 동일하게 통제했을 때, NoThinking은 특히 low-budget 구간에서 pass@1가 Thinking보다 높았으며, k가 커질수록 성능 차이는 더 커졌다 (Figure 2).</p> <p>효율성을 체계적으로 평가하기 위해 pass@k와 평균 토큰 사용량 간의 Pareto frontier를 분석한 결과, NoThinking은 항상 Thinking보다 우수한 결과를 보였다. 이는 명시적인 추론 과정을 거치지 않더라도 NoThinking이 더 좋은 accuracy-cost tradeoffs를 가진다는 것을 의미한다.</p> <p>pass@k에서 NoThinking이 좋은 성능을 보였다는 것은, Thinking을 사용한 순차적 추론이 아닌, parallel scaling이 가능하다는 것이다. 저자들은 여러 응답을 병렬로 샘플링하고, best-of-N으로 최종 응답을 고르는 방법론을 제안한다.</p> <p>이들이 고려한 task는 두 가지이다:</p> <ol> <li> <p>tasks with perfect verifiers (e.g., formal theorem proving): 자동으로 정답 여부를 확인할 수 있는 경우</p> </li> <li> <p>tasks without verifiers (e.g., general problem solving): simple confidence-based selection strategies를 사용해야하는 경우</p> </li> </ol> <p>verifiers가 있는 경우, NoThinking이 Thinking을 가뿐히 능가했다 ( both with and without parallel scaling). 특히, 지연 시간을 7배 단축하고 총 토큰 사용량을 4배나 줄였다는 점이 이점. verifiers가 없는 경우에도 NoThinking 준수한 성능을 보인다. 예를 들어, Thinking을 9× lower latency + improved accuracy on OlympiadBench (Math)로 능가함. (Figure 3)</p> <p>요약하자면, 이 연구는 현재의 추론 모델들이 학습 과정에서 구조화된 추론 형식을 따르도록 훈련되었음에도 불구하고, 높은 성능을 내기 위해 반드시 명시적인 thinking 과정이 필요하지 않다는 사실을 처음으로 보여주고 있다.</p> <p>또한, NoThinking 방식을 병렬로 처리하면, 순차적 추론보다 더 좋은 latency-accuracy tradeoffs가 가능함을 보인다.</p> <p>전반적으로, 이 연구는 긴 thinking 과정이 과연 정말로 필요한 것인가에 대한 의문에 대한 답을 일부분 보여주고 있다고 할 수 있다!</p> <h1 id="2-related-work-and-background">2. Related Work and Background</h1> <h3 id="test-time-scaling-for-language-models">Test-Time Scaling for Language Models</h3> <ul> <li> <p><strong>Sequential approaches</strong></p> <ul> <li> <p>OpenAI o1, DeepSeek R1, Qwen QwQ.</p> </li> <li> <p>긴 chain-of-thought(CoT) 응답을 한 번의 순방향 패스에서 생성하며, 백트래킹과 검증 포함.</p> </li> <li> <p><strong>한계</strong>: 강화 학습(RL)이나 iterative self-improvement 등 비용이 큰 학습 과정 필요.</p> </li> </ul> </li> <li> <p><strong>Parallel approaches</strong></p> <ul> <li> <p>여러 후보 출력을 생성하고 선택적으로 응답 집계.</p> </li> <li> <p>ex. Best-of-N 샘플링, search-guided 디코딩 (ex. 몬테카를로 트리 탐색(MCTS))</p> </li> </ul> </li> <li> <p><strong>NoThinking의 차별점</strong></p> <ul> <li> <p>기존 연구는 명시적 thinking이 필수라고 가정했으나, NoThinking은 thinking을 생략해도 됨</p> </li> <li> <p>추가 학습, 보상, 감독 없이 경쟁력 있는 성능</p> </li> <li> <p>Best-of-N을 활용했지만 샘플링 기법 혁신이 아니라 cost-effective baseline for low-budget settings 제공이 목적</p> </li> </ul> </li> </ul> <h3 id="efficient-reasoning">Efficient Reasoning</h3> <p>recent work has explored various strategies to make reasoning in LLMs more efficient.</p> <ul> <li> <p><strong>추론 시퀀스 길이 최적화</strong></p> <ul> <li>생성되는 reasoning 시퀀스의 길이를 줄이거나 불필요한 단계를 제거해 간결한 추론을 유도.</li> </ul> </li> <li> <p><strong>강화 학습 기반 CoT 최적화</strong></p> <ul> <li> <p>강화 학습을 활용해 CoT 길이를 최적화하고 효율적인 reasoning을 학습.</p> </li> <li> <p>ex: 길이에 따라 보상을 설계해 모델이 적절한 길이의 reasoning을 생성하도록 유도 (Aggarwal &amp; Welleck, Luo, Shen, Arora, Qu 등).</p> </li> </ul> </li> <li> <p><strong>Best-of-N 샘플링을 활용한 파인튜닝</strong></p> <ul> <li>Best-of-N 방식으로 생성한 다양한 길이의 reasoning을 파인튜닝에 활용해 concise reasoning을 학습.</li> </ul> </li> <li> <p><strong>출력 방식 수정으로 reasoning 간결화</strong></p> <ul> <li>LLM이 reasoning을 latent representations 기반으로 생성하도록 학습해 더 간결한 reasoning을 유도</li> </ul> </li> <li> <p><strong>학습 없는 전략적 기준 설정</strong></p> <ul> <li>별도의 학습 없이, 프롬프트나 샘플 선택 criteria만으로 추론 전략을 가이드하는 training-free 방식</li> </ul> </li> <li> <p><strong>추론 단계 수 제한</strong></p> <ul> <li>프롬프트에 토큰 예산을 명시하거나, reasoning 단계를 적게 생성하도록 모델에 직접 지시해 추론을 간결화</li> </ul> </li> <li> <p><strong>동적 입력 라우팅으로 reasoning 복잡성 제어</strong></p> <ul> <li>입력 데이터를 작업 난이도에 따라 동적으로 라우팅해 복잡성을 조절하고, 쉬운 문제는 간단히 처리하고 어려운 문제만 깊이 추론</li> </ul> </li> </ul> <h1 id="3-nothinking-provides-better-accuracy-budget-tradeoffs-than-thinking">3. NoThinking Provides Better Accuracy-budget Tradeoffs than Thinking</h1> <p>Section 3.1: define Thinking and NoThinking</p> <p>Section 3.2: describe experimental setup</p> <p>Section 3.3: present experimental results</p> <p>Section 3.4: Discussions and Analyses</p> <h2 id="31-method">3.1 Method</h2> <p>대부분의 모델들은 보통 비슷한 구조로 generation을 한다:</p> <ul> <li> <table> <tbody> <tr> <td>reasoning process within the thinking box, marked by &lt;</td> <td>beginning of thinking</td> <td>&gt; and &lt;</td> <td>end of thinking</td> <td>&gt;, followed by the final answer.</td> </tr> </tbody> </table> </li> </ul> <p>이 구조에 기반해서 Thinking and NoThinking을 다음과 같이 만듦:</p> <ul> <li> <p>**Thinking: **the reasoning process within the thinking box, the final solution, and the final answer (Figure 1 (blue)).</p> </li> <li> <p>**NoThinking: ** explicit reasoning process 무시하고 바로 final solution and answer 만들기. thinking box를 decoding 할 때 빈칸으로 하도록 강제 (Figure 1 (orange)).</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;|</span><span class="n">beginning</span> <span class="n">of</span> <span class="n">thinking</span><span class="o">|&gt;</span>
<span class="n">Okay</span><span class="p">,</span> <span class="n">I</span> <span class="n">think</span> <span class="n">I</span> <span class="n">have</span> <span class="n">finished</span> <span class="n">thinking</span><span class="p">.</span>
<span class="o">&lt;|</span><span class="n">end</span> <span class="n">of</span> <span class="n">thinking</span><span class="o">|&gt;</span>
</code></pre></div></div> <table> <tbody> <tr> <td>token usage를 제어하기 위해 budget forcing technique from Muennighoff et al. (2025)을 사용 — 모델이 token budget에 도달하면, 강제로 Final Answer 만들도록 함. 만약 아직 thinking box 안에 있었다면, &lt;</td> <td>end of thinking</td> <td>&gt; 을 final answer tag 이전에 붙여서 만듬.</td> </tr> </tbody> </table> <h2 id="32-evaluation-setup">3.2 Evaluation Setup</h2> <ul> <li> <p><strong>Models</strong></p> <ul> <li> <p>DeepSeek-R1-Distill-Qwen-32B</p> </li> <li> <p>Qwen-32B-Instruct</p> </li> <li> <p>(Appendix) R1-series models at smaller scales (7B and 14B)</p> </li> </ul> </li> <li> <p><strong>Tasks and Benchmarks</strong></p> <ul> <li> <p>**Mathematical problem solving: **</p> <ul> <li> <p>For standard problem solving: AIME 2024, AIME 2025, and AMC 2023</p> </li> <li> <p>For more advanced reasoning: OlympiadBench</p> </li> </ul> </li> <li> <p><strong>Coding</strong>: LiveCodeBench</p> </li> <li> <p><strong>Formal theorem proving</strong>:</p> <ul> <li> <p>MiniF2F — for formal mathematical reasoning,</p> </li> <li> <p>ProofNet — for logic and theorem proving.</p> </li> </ul> </li> </ul> </li> <li> <p>**Metrics: **pass@k</p> <ul> <li> <p>k = {1, 2, 4, 8, 16, 32} for theorem proving datasets (MiniF2F and ProofNet)</p> </li> <li> <p>k = {1, 2, 4, 8, 16, 32, 64} for smaller datasets (AIME24, AIME25, AMC23)</p> </li> <li> <p>k = {1, 2, 4, 8, 16} for larger datasets (OlympiaddBench, LiveCodeBench).</p> </li> <li> <p>for formal theorem-proving benchmarks: pass@32 is the standard</p> </li> <li> <p>for math and coding: pass@1 (i.e., accuracy) is most commonly used.</p> </li> </ul> </li> </ul> <h2 id="33-results">3.3 Results</h2> <h3 id="thinking-vs-nothinking-vs-qwen-instruct-without-token-budget-controlled">Thinking vs. NoThinking vs. Qwen Instruct without token budget controlled</h3> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>budget forcing없이 세 경우를 비교한 결과:</p> <ul> <li> <p>MiniF2F and ProofNet에서 NoThinking은 모든 K에 대해서 Thinking과 비슷했으며, 둘은 Qwen-Instruct보다 성능 훨씬 좋았음</p> <ul> <li>NoThinking이 3.3–3.7x 더 적은 토큰을 사용하는데도!</li> </ul> </li> <li> <p>다른 데이터셋에서는 k = 1일 때는 NoThinking의 성능이 훨씬 떨어지지만, k가 커질수록 갭이 작아짐</p> </li> <li> <p>결과적으로, NoThinking은 가장 큰 k일때, 2.0–5.1x fewer tokens을 사용하는데도, Thinking의 성능을 넘거나 거의 근사함.</p> </li> <li> <p>Qwen-Instruct의 관점에서:</p> <ul> <li> <p>For AIME24, AIME25, and LiveCodeBench에서 Thinking and NoThinking이 훨씬 성능 좋음</p> </li> <li> <p>AMC23 and OlympiadBench에서는 Thinking and NoThinking과 비슷</p> </li> </ul> </li> </ul> <h3 id="thinking-vs-nothinking-with-token-budget-controlled">Thinking vs. NoThinking with token budget controlled</h3> <p>위에서 확인했듯,Thinking이 NoThinking보다 대부분의 데이터셋에서 성능이 더 좋음. 하지만, 결과적으로 Thinking이 더 많은 토큰을 사용하기 때문에 같은 토큰 수를 사용할 때 어떤 것이 더 성능이 좋은가를 비교함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>결과적으로 NoThinking generally outperforms Thinking.</p> <p>특히, low-budget setting (e.g., fewer than ≈ 3, 000 tokens)에서 NoThinking은 모든 k에서 더 좋은 성능을 보였고, k가 커질수록 차이는 커졌음. 좀 더 토큰 제한을 늘렸을 때 (e.g., around 3, 500 tokens), Thinking이 pass@1에서는 더 좋았으나, k = 2부터는 다시 NoThinking이 더 좋은 성능을 보임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 6는 해당 데이터셋에서 사용한 가장 큰 k와 1, 그리고 token usage를 plot하면서 위의 결과를 더 잘 보여줌.</p> <ul> <li> <p>pass@k</p> <ul> <li>NoThinking이 항상 더 좋았음</li> </ul> </li> <li> <p>pass@1</p> <ul> <li> <p>NoThinking이 low-budget regime에서는 더 좋고 high-budget regime에서는 더 나빴음</p> </li> <li> <p>LiveCodeBench은 예외. 아마도 thinking box를 없애는 것이 token usage를 그렇게 많이 줄이지 못했기 때문이라고 예상됨</p> </li> <li> <p>data contamination의 위험을 예상하고, 절대 학습에 사용되지 않았을 AIME 2025를 추가함</p> </li> <li> <p>모든 new and established benchmarks의 결과가 이 트렌드가 artifacts of memorization이 아닌, generalizable model behavior임을 보여줌</p> </li> </ul> </li> </ul> <p>[요약]</p> <ul> <li> <p>reasoning models의 핵심인 thinking box를 없애도, 여전히 효과 좋음</p> </li> <li> <p>3.3–3.7x 적은 토큰을 사용하는데도 비슷한 성능 나옴</p> </li> <li> <p>비슷한 수의 토큰이라면 thinking보다 성능 좋음</p> </li> </ul> <h2 id="34-discussions-and-analyses">3.4 Discussions and Analyses</h2> <h3 id="task-specific-differences-in-nothinking-performance">Task-Specific Differences in NoThinking Performance</h3> <p>Section 3.3에서 나름 일관적인 트렌드가 보이긴 하지만, 각 벤치마크 결과를 자세히 살펴보면 조금 동작이 다름</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Figure 4,</p> <ul> <li> <p>AMC 2023는 모든 세팅에서 거의 performance gap없이 convergence를 보임. 아마도 saturation이 예상됨</p> </li> <li> <p>MiniF2F and ProofNet pass@1에서 NoThinking은 Thinking에 비해 훨씬 더 적은 토큰을 사용하면서 비슷한 성능을 냄. 하지만, 이는 단순히 task simplicity 이슈로 해석되면 안됨! 검증 결과, OpenAI’s o1과 같은 엄청 강한 모델은 MiniF2F에서 30% accuracy 밖에 안됐고, ProofNet은 모든 방법론에서 성능 낮았음. 즉, 왜 어떤 벤치마크에서는 NoThinking이 잘되었는가는 open question for future work이라는 것</p> </li> </ul> <h3 id="how-increasing-k-affects-nothinking-performance">How Increasing k Affects NoThinking Performance</h3> <p>왜 k가 늘어날수록 NoThinking이 더 좋은 성능을 보이는지 대략적인 이유를 찾아보기 위해 생성된 답변의 diversity를 측정함 — by computing the entropy of the answer distribution for each question.</p> <p>높은 mean entropy는 당연히 더 높은 overall diversity를 의미하고, lower standard deviation은 더 일관적인 것을 의미. 실험은 token budget이 제한된 환경에서 진행</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>엔트로피의 관점에서는 특별한 차이를 찾지 못함. 어떨 땐 NoThinking이 어떨 땐 Thinking이 더 높음</p> </li> <li> <p>variance의 관점에서 NoThinking은 항상 더 낮은 값을 보임 — 더 uniform하게 답을 내고 있다는 것.</p> </li> </ul> <p>이러한 다양성의 일관성 증가가 k가 커질수록 더 좋은 pass@k를 보이는 이유와 연관이 되어있을 것이라 예상은 한다만, 이를 이용해 성능 차이를 완전히 설명하기는 어렵다고언급</p> <h1 id="4-nothinking-makes-parallel-test-time-compute-more-effective">4. NoThinking Makes Parallel Test-Time Compute More Effective</h1> <p>우리는 지금까지 NoThinking이 k가 늘어날수록 더 이점이 늘어난다고 이야기함. 즉, NoThinking을 활용하면 parallel scaling method를 더 잘 사용할 수 있다는 것!</p> <p>Section 4에서는 accuracy vs. latency의 관점에서 어떻게 Thinking보다 더 좋은 세팅을 만들 수 있는지 논한다.</p> <h2 id="41-motivation-and-methods">4.1 Motivation and Methods</h2> <h3 id="parallel-scaling-v-sequential-scaling">Parallel Scaling v. Sequential Scaling</h3> <ul> <li> <p>Parallel scaling:</p> <ul> <li> <p>low latency: 여러 샘플을 동시에 생성하므로 지연 시간이 줄어듦 — 이는 API 호출이든 로컬 모델 서비스든 동일함.</p> </li> <li> <p>전체 지연 시간은 가장 오래 걸린 개별 샘플의 생성 시간을 기준으로 측정함.</p> </li> <li> <p>NoThinking은 low-budget 구간에서 더 좋은 성능을 보이고, k가 커질수록 성능이 향상되기 때문에, 단순한 best-of-N 방식을 사용했을 때도 정확도와 지연 시간 측면에서 더 우수한 성능을 달성할 수 있음.</p> </li> <li> <p>실제로 budget forcing과 병렬 샘플링을 적용한 Thinking, 그리고 sequential scaling 하의 full Thinking(Thinking without budget forcing)과 비교했을 때도 NoThinking이 더 나은 성능을 보였음.</p> </li> </ul> </li> </ul> <h3 id="methods">Methods</h3> <ul> <li> <p><strong>Parallel sampling</strong></p> <ul> <li> <p>병렬 샘플링은 N개의 독립적인 해답을 집계해 단일 예측을 생성하는 best-of-N 방식을 필요로 함.</p> </li> <li> <p>N개의 예측 P = {p₁, ···, pₙ}이 있을 때, best-of-N은 최종 출력을 P 중 하나로 선택함.</p> </li> </ul> </li> <li> <p><strong>검증 가능한 작업(MiniF2F, ProofNet)</strong></p> <ul> <li>Lean 컴파일러와 같은 perfect verifier f를 사용해 각 예측 p ∈ P의 정답 여부를 확인하고 최종 정답을 선택함.</li> </ul> </li> <li> <p><strong>verifier가 없는 작업</strong></p> <ul> <li> <p>**Confidence-based **</p> <ul> <li> <p>Kang et al. (2025)를 따라 self-certainty 지표를 사용하여 경량의 confidence 기반 선택 방식을 적용.</p> </li> <li> <p>self-certainty는 예측된 토큰 분포와 균등 분포 간 KL divergence를 계산해 모델의 확신도를 수치화함.</p> </li> <li> <p>예측 집합 P의 self-certainty 점수 c₁, …, cₙ를 계산한 뒤, 동일 연구에서 소개된 Borda voting 방식을 통해 최종 답변 선택.</p> </li> <li> <p>equivalence checking이 불가능한 벤치마크(LiveCodeBench)에서는 self-certainty가 가장 높은 응답을 최종 선택함.</p> </li> </ul> </li> <li> <p>**Majority voting **</p> <ul> <li> <p>정확한 정답이 존재하는 과제(수학 문제 풀이, 과학 문제)에서는 이전 연구를 따라 majority vote 기반의 결과를 보고함.</p> </li> <li> <p>예측 집합 P에서 추출한 답변 모음 {aᵢ}로부터 cons@n = argmaxₐ ∑₁ⁿ 1(aᵢ = a)로 majority vote.</p> </li> <li> <p>k &lt; N인 경우, 전체 N개 예측에서 무작위로 k개를 샘플링해 컨센를 계산하고, Monte Carlo simulation으로 여러 번 반복해 정확도를 평균하여 cons@k를 추정함.</p> </li> </ul> </li> </ul> </li> <li> <p><strong>Metrics</strong></p> <ul> <li>지연 시간(latency)은 각 데이터셋과 N회 반복 실험에서 생성된 토큰 수의 최댓값을 평균하여 정의함.</li> </ul> </li> </ul> <h2 id="42-results">4.2 Results</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Tasks without verifiers</strong></p> <ul> <li> <p>Figure 7에서는 confidence-based 방식을 사용한 결과를 시각화했고, Table 2에는 선택된 실험의 ablation 결과를 제시함.</p> </li> <li> <p>Table 2에서는 Section 4.1에서 논의한 Best-of-N 방법을 비교했으며, 전반적으로 confidence-based 선택이 majority voting보다 더 우수한 성능을 보임.</p> </li> <li> <p>병렬 스케일링을 활용할 경우, 샘플 중 가장 좋은 예측을 선택해 pass@k 성능을 달성할 수 있으므로 pass@k 정확도를 pass@1의 상한으로 보고 Table 2에 포함.</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Perfect Verifiers</strong></p> <ul> <li> <p>병렬 스케일링과 결합한 NoThinking은 기존 sequential 접근법에 비해 훨씬 낮은 지연 시간+토큰 수로 유사하거나 더 나은 정확도를 달성.</p> </li> <li> <p>Figure 7의 첫 두 플롯에서 보듯 NoThinking은 Thinking과 동등하거나 더 나은 성능을 보이면서도 지연 시간이 훨씬 낮음.</p> </li> <li> <p>병렬 스케일링 없이도 NoThinking은 Thinking과 비슷한 정확도를 훨씬 짧은 지연 시간으로 달성함.</p> </li> <li> <p>병렬 스케일링과 결합할 때, NoThinking은 budget forcing과 병렬 스케일링 없이 수행한 Thinking과 유사한 정확도를 유지하면서 지연 시간을 7배 줄임.</p> </li> <li> <p>특히 MiniF2F와 ProofNet 데이터셋에서 NoThinking은 출력 토큰 수를 4배 줄이면서도 같은 정확도를 달성함.</p> </li> </ul> </li> <li> <p>**Simple Best-of-N Methods **</p> <ul> <li> <p>NoThinking은 병렬 스케일링과 confidence-based 선택을 결합했을 때, 대부분의 벤치마크에서 Thinking을 일관되게 능가함.</p> </li> <li> <p>Figure 7의 마지막 다섯 플롯은 여러 벤치마크에서 토큰 사용량을 통제한 상황에서 Thinking과 NoThinking의 confidence-based 선택 결과를 보여줌.</p> </li> <li> <p>연구는 주로 low-budget 환경에 초점을 맞추었음. 이유는</p> <ol> <li> <p>효율적인 추론에 대한 주된 관심사와 부합하고,</p> </li> <li> <p>최대 토큰 수가 너무 크면 지나치게 길고 비논리적인 출력(“babbling”)이 발생해 비교 가치가 떨어지고 지연 시간만 증가하기 때문.</p> </li> </ol> </li> <li> <p>병렬 스케일링 자체는 Thinking과 NoThinking 모두에서 pass@1 성능을 개선하지만, 모든 수학 벤치마크에서 NoThinking은 Thinking보다 항상 더 좋은 accuracy–budget tradeoffs를 가짐</p> </li> <li> <p>특히 예산 제한이 없는 full Thinking과 비교해도, NoThinking은 더 높은 pass@1 점수(55.79 vs. 54.1)를 기록하면서 지연 시간을 9배 단축함.</p> </li> </ul> </li> <li> <p>**LiveCodeBench **</p> <ul> <li> <p>NoThinking은 LiveCodeBench에서 less effective: 이는 confidence-based 선택이 정확한 일치 기준이 필요한 코딩 작업에서는 한계가 있기 때문으로 보임.</p> </li> <li> <p>이 경우 정확한 일치 기반 투표가 불가능해 self-certainty가 가장 높은 응답을 선택했는데, 이는 신뢰도가 낮아 성능이 떨어짐.</p> </li> <li> <p>Table 2에 따르면 이러한 방식은 투표 기반 방법이 가능한 다른 작업들과 비교해 일관되게 낮은 성능을 보임.</p> </li> </ul> </li> </ul> <p>[요약]</p> <ul> <li> <p>NoThinking의 pass@k 성능은 k가 증가할수록 더욱 좋아지며, 병렬 스케일링을 통해 pass@1 성능을 유사하거나 훨씬 더 낮은 지연 시간(최대 9배 감소)으로 달성할 수 있음.</p> </li> <li> <p>Verifiers가 있는 작업에서는 정확도는 비슷하거나 더 높이면서 총 토큰 사용량을 최대 4배까지 줄일 수 있음.</p> </li> </ul> <h1 id="conclusion">Conclusion</h1> <ul> <li> <p>이 연구는 동일한 모델이 긴 thinking chain 없이도, k가 증가함에 따라 pass@k에서 Thinking 방식과 동등하거나 더 나은 성능을 훨씬 적은 토큰으로 달성할 수 있음을 보여줌</p> </li> <li> <p>동일한 토큰 예산 하에서도, NoThinking은 대부분의 k 값에서 기존 Thinking 결과를 지속적으로 능가함.</p> </li> <li> <p>NoThinking을 Best-of-N 선택 방법과 결합하면, 기존 Thinking 방식으로는 달성하기 어려운 accuracy–budget tradeoffs 달성!</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">See What You Are Told: Visual Attention Sink in Large Multimodal Models</title><link href="https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink/" rel="alternate" type="text/html" title="See What You Are Told: Visual Attention Sink in Large Multimodal Models"/><published>2025-06-24T00:00:00+00:00</published><updated>2025-06-24T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-24</li> <li><strong>Reviewer</strong>: 조영재</li> <li><strong>Property</strong>: Multimodal</li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM의 발전과 함께 Multimodal 모델들도 많이 등장하고 있음 (VQA, image captioning, visual reasoning, …)</p> </li> <li> <p>LMM에서도 LLM 처럼 똑같이 attention 매커니즘을 따름. 예를 들어 ‘bird’를 말하고자 할때 model은 해당 이미지에 관련있는 visual token에 대해 집중함. (직관적으로) text와 visual token이 매칭됨.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>근데 실제로는 text와 visual 간의 관계가 unrelated 되는 경우도 관측됨. attention map을 통해 attention을 더 면밀히 살펴보았을때, Example 1을 보면, 위에 bird를 말하는데 ‘빨간 네모’ 처럼 bird와 무관한 곳에 높은 attention이 관측. 다른 예제들도 마찬가지로 텍스트와 무관한 곳에 높은 어텐션이 관측됨. 이게 왜 발생하는지 궁금해서 해당 연구가 시작됨</p> </li> <li> <p>해당 연구의 발견</p> </li> <li> <p>이러한 attention 맵의 오류는 몇몇 tokens 들이 massive activation of specific dimensions in the hidden states에서 일어남을 찾음. 이것은 LLM에서 특정 limited semantic meaning(e.g. “BOS”, “.”, “\n”)에 large attention이 부여되는 “attention sink”의 개념과 유사해보임.</p> </li> <li> <p>추가로 실험을 해보니 이러한 visual sink token 들은 없애도 모델 답변의 quality에 영향을 주지 않음.</p> </li> <li> <p>최근에 vlm에서 attention이 text에 비해 이미지에 부족하게 할당된다는 사전 연구도 있었음. 그래서 우리는 attention budget의 개념으로 visual sink token들에 가는 attention을 아껴서 다른 visual token들에 redistribute를 하고자 함(Visual Attention Redistribuion (VAR))</p> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li>Visual attention in large multimodal models.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>LMM이 특정 몇개의 토큰에 과도하게 attention을 부여한다는 연구가 있었고, 이를 활용해 contrastive decoding으로 해결하려는 시도가 있었음(빈 이미지와 질문을 넣었을 때 모델의 답변 logit을 빼서 bias를 없애는 방식, 위 이미지 참고 (https://arxiv.org/pdf/2405.17820)). 혹은 강제적으로 text에 가던 attention을 visual에 가게끔 만드는 시도.</p> </li> <li> <p>Attention sink in language models</p> </li> <li> <p>기존 LLM에서도 attention sink는 2024 년도 부터 제기되던 문제. 특히 BOS 같은 토큰은 AR 특성상 뒤에 모든 token들의 attention이 쏠리게 되어 의미는 적지만 attention이 높음 (c.f. StreamingLLM이란 연구에서는 attention sink가 걸린 토큰의 KV를 고정시켜 efficiency를 갖기도 함)</p> </li> <li> <p>이러한 attention sink가 특히 특정 dimenstion의 hidden state에서 발생! 여기의 attention을 다른곳에 재분배해 정교한 답변을 얻으려는 llm연구도 있었음. This work는 이 개념을 VLM에 적용한 느낌</p> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>트랜스포머 공식</p> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="visual-attention-sink">Visual Attention Sink</h2> <p>Figure1보면 attention이 우리의 직관대로 잘 따라가긴 하지만 <strong>고정된 어떤 background spot</strong>에 굳이 필요없는 limited semanic meaning에 높은 attention이 배정되어 있음</p> <h3 id="how-to-distinguish-irrelevant-visual-tokens">How to distinguish irrelevant visual tokens?</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>irrelevant visual token에서 두 가지 특성이 나타남. (1) figure 1에서 보듯 이미지에 same irrelevant visual token에 고정적으로 등장. (2) BOS 토큰이랑 유사하게 같은 dimension에서 등장 (Fig 2)</p> <h3 id="irrelevant-visual-tokens-have-high-activation-in-specific-dimensions">Irrelevant visual tokens have high activation in specific dimensions</h3> <ul> <li> <p>Fig2의 BOS랑 빨간<img/> 를 보면 같은 dimension에서 attention 값이 튀는 것을 볼 수 있음. 이는 LLM 각자가 같은 고유한 특성이라고 함. 예를들어 LLaVA-1.5-7B가 사용한 LLaMA2 백본은 모두 고정적으로 {1415, 2533} 의 dimension에서 위와같은 형태를 보임. (pretrain 과정에서 쏠리는 거라 finetuning을 해도 sink dimension은 계속 고정되어있다고 함)</p> </li> <li> <p>특정 토큰이 갖는 sink dimension value <strong>Φ(x)</strong>를 아래와 같이 정의</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>쉽게 말해서 1415, 2333 등과 같은 sink dimension에서 토큰이 갖게 되는 튀는 값을 나타냄. (Fig 2 참고)</p> <ul> <li>visual sink token을 구분하기 위해 20보다 <strong>Φ(x)</strong>가 큰 토큰들은 다 visual sink token으로 분류. 이를 통해 irrelevant visual token(sink dimension에서 attention 값이 튀는 애들)과 relevant visual token(튀지 않는 애들)을 구분함.** **</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Fig3 (a)를 통해 본인들이 정의한 high sink dimension value 들이 높은 attention 값을 가지는 애들이였으며, (b) 실제로 visual sink 들을 mask하고 하니 안할때보다 성능이 높았음. (c) attention contribution도 측정했을 때 (실제로 text 답변 만드는 logit에 기여하는 정도) 는 작았음. (d) 를 봐도 w/o sinks 가 noise를 잡아내며 대부분의 sink 들은 background에 존재</li> </ul> <hr/> <h2 id="surplus-attentions-in-visual-attention-sink--can-we-recycle-them">Surplus attentions in visual attention sink : can we recycle them?</h2> <p>(1) image centric-head 를 먼저 뽑고 (2) 해당 head에서 sink token들에 가던 attention을 보아서 non sink token 에게 분배할 예정</p> <ul> <li> <p>Image centric-head</p> <ul> <li> <p>먼저 visual token에 대한 attention weight의 sum 이 0.2 보다 작은 head는 다 버림</p> </li> <li> <p>visual non-sink ratio 정의 (전체 이미지에 대한 attention 분의 non visual sink token 에 대한 attention). 즉, 이미지를 해석하는데 실제로 필요한 애들의 비율</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Redistributing attention weights</p> <ul> <li>sink 토큰들에 대해 decrease 시키고</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>이것들을 모아서 attention budget에 넣어줌 (오메가)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>그리고 아래 식을 통해 attention sink에 attention 을 빼앗겼던 부분에 더 높은 가중치를 주어 redistribution</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="experiments">Experiments</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>(1) VL-task</p> <p>(2) visual hallucination</p> <p>(3) vision centric (spatial relationship between objects)</p> <h3 id="ablation-studies">Ablation studies</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>(Table 4) visual non sink ratio 를 정의해서 로 보다 큰 애들의 head 만 살렸었는데 이 과정이 필수적이였음.</p> </li> <li> <p>(Table 5) visual token 내에서만 attention redistribution이 성능이 제일 높음.</p> </li> </ul> <h3 id="appendix">Appendix</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="discussion">Discussion</h3> <ul> <li> <p>latency가 별로 없으면서 정말 많은 VL task에서 성능이 다 오른건 신기! 다만 해당 방법론 역시 projector을 이용한 vision language model에서만 적용 가능할 것으로 보임. (one-to-one 매칭, instructVL, instructBLIP같은 resampler는 적용 안됨)</p> </li> <li> <p>마지막 table5에서 budet을 아껴서 text에 줬을 때 성능이 안오른건 의외. 직관적으로 필요 없는 잉여물을 준다고 해서 오르는건 아닌것같음</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="language-model"/><category term="llm"/><category term="multimodal"/><category term="paper-review"/><category term="reasoning"/><category term="vision"/><summary type="html"><![CDATA[논문 리뷰 - Multimodal 관련 연구]]></summary></entry><entry><title type="html">Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</title><link href="https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in/" rel="alternate" type="text/html" title="Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models"/><published>2025-06-17T00:00:00+00:00</published><updated>2025-06-17T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-17</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: Text Generation, DiffusionLM</li> </ul> <p>https://huggingface.co/blog/ProCreations/diffusion-language-model?utm_source=chatgpt.com</p> <h2 id="introduction">Introduction</h2> <p><strong>도입 배경</strong></p> <ul> <li> <p>LLM의 엄청난 성장 → Chain-of-Thought (CoT)와 같은 Reasoning이 핵심 기법으로 부상</p> </li> <li> <p>CoT는 중간 추론 단계를** autoregressive 방식<strong>으로 생성하여 **LLM의 추론 능력을 향상</strong>시킴</p> </li> <li> <p>하지만 <strong>기존 CoT의 한계점</strong>들이 존재</p> <ul> <li> <p>중간 단계의 오류가 최종 답변에 영향을 미침</p> </li> <li> <p>자기 교정(self-correction) 능력의 부족</p> </li> <li> <p>효율성에 대한 우려</p> </li> </ul> </li> </ul> <p><strong>Diffusion Model의 등장</strong></p> <ul> <li> <p>Vision 영역에서의 성공에 이어 텍스트 처리 분야에서도 주목받기 시작</p> </li> <li> <p><strong>Why?</strong> Autoregressive model 대비 고유한 강점을 보유</p> <ul> <li> <p>global planning ability</p> </li> <li> <p>self correction</p> </li> <li> <p>효율성 개선 가능성 (이건 조금 확인이 필요함. 적절한 ref는 아닌 것 같음. https://arxiv.org/pdf/2310.16834)</p> </li> </ul> </li> <li> <p><strong>Pre-trained diffusion language model</strong> → Plaid, SEDD 등 (최근에는 Llama3-8B 정도 수준의 LlaDA 모델 등장)</p> <ul> <li> <p>GPT-2 수준의 성능 달성 (DoT는 Neurips 2024 논문)</p> </li> <li> <p>Scaling law의 적용 가능성 확인</p> </li> </ul> </li> </ul> <p><strong>RQ</strong></p> <blockquote> <p><strong>Can diffusion language models also leverage the CoT-style technique to gain enhanced complex reasoning abilities?</strong></p> </blockquote> <p><strong>Diffusion of Thoughts (DoT) 제안</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Diffusion model에 특화된 inherent chain-of-thought 방법 제안</strong></p> <ul> <li>일련의 latent variables를 스텝별로 업데이트 → 각 추론 단계들이 시간에 따라 병렬적으로 diffuse</li> </ul> </li> <li> <p>핵심 특징</p> <ul> <li> <p><strong>Multi-pass variant DoT</strong>: causal bias를 막기 위해 한 번에 하나의 thought를 생성하는 데 초점</p> </li> <li> <p><strong>Classifier-free guidance 사용</strong>: gradient-based classifier guidance 대신 더 신뢰할 수 있는 제어 신호 제공</p> </li> <li> <p><strong>Training-time sampling</strong>: Self-correcting 능력 향상</p> </li> <li> <p><strong>Conditional ODE Solver</strong>: continuous diffusion model의 inference 가속</p> </li> </ul> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p><strong>기본 개념</strong></p> <ul> <li> <p>Forward process</p> <ul> <li> <table> <tbody> <tr> <td>q(\mathbf{z}_t</td> <td>\mathbf{z}_{t-1}), t-1 시점의 representation에 noise를 주입</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>Reverse process</p> <ul> <li> <p>z_{t}를 denoising하여 z_0를 복구하는 것이 목표</p> </li> <li> <table> <tbody> <tr> <td>z<em>t: p</em>{\theta}(\mathbf{z}<em>{0:T}) := p(\mathbf{z}_T)\prod</em>{t=1}^T p<em>{\theta}(\mathbf{z}</em>{t-1}</td> <td>\mathbf{z}_t) 로 원본 데이터 복원</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>Text generation을 위한 diffusion 모델의 종류</p> <ol> <li>Continuous diffusion models</li> </ol> <ul> <li> <p>mapping function을 활용 (실수 → 토큰화)</p> </li> <li> <p>discrete text w → continuous space using \text{EMB}(w) → <strong>rounding</strong> (inverse operation)</p> </li> <li> <p>forward perturbations: q(\mathbf{z}<em>{t} \vert \mathbf{z}</em>{t-1}) = \mathcal{N}(\mathbf{z}<em>{t};\sqrt{1-\beta_t}\mathbf{z}</em>{t-1}, {\beta}_t \mathbf{I}), where \beta_t \in (0, 1)</p> </li> </ul> <ol> <li>Discrete diffusion models</li> </ol> <ul> <li> <p>문제 자체를 integer program으로 풀기</p> </li> <li> <p>z_t를 ont-hot vectors in {0, 1}^K로 표현. K는 vocab size</p> </li> <li> <table> <tbody> <tr> <td>q(\mathbf{z}_t</td> <td>\mathbf{z}_{t-1})을 transition matrix로 표현 → uniform 분포나 [mask]로 전부 변경하는 단계</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p><strong>Seq2Seq Generation (e.g. DiffuSeq)</strong></p> <ul> <li> <p>입력-출력 sequence를 single sequence로 처리: \mathbf{w}^{z}=\mathbf{w}^{[x; y]}</p> </li> <li> <p>Left-aligned mask [\mathbf{0};\mathbf{1}]로 x, y를 구분</p> </li> <li> <p><strong>Partial noising</strong>: mask value가 1인 부분에만 noise 적용</p> </li> </ul> <h2 id="diffusion-of-thoughts">Diffusion-of-Thoughts</h2> <ul> <li> <p>Notation: s (problem statement), a (answer), p_{\theta}^{LM} (language model)</p> </li> <li> <table> <tbody> <tr> <td>Answer-only generation model: \mathbf{a}\sim p_\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>CoT: \mathbf{a}\sim p*\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{r}*{1\dots n})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>implicit CoT: \mathbf{a}\sim p*\theta^{\textit{iCoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}*{1\dots n})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>DoT: \mathbf{a}\sim p_\theta^{\textit{DoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}_t)</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="dot-modeling">DoT Modeling</h3> <ul> <li> <p><strong>Gradient-based token guidance의 한계</strong></p> <ul> <li> <p>정확한 conditioning 실패. 특히 수학적 추론과 같이 정확한 숫자와 토큰이 필요한 곳에서 치명적</p> </li> <li> <p>예시: “Two trains” → “<strong>Three</strong> trains”</p> </li> </ul> </li> </ul> <p><strong>→ DiffuSeq-style classifier-free conditioning 채택</strong></p> <ul> <li> <p>모든 rationale들이 backward diffusion process에서 병렬적으로 생성</p> </li> <li> <p>모든 conditional token, 여기서는 s는 고정, r_{1…n}에만 noise 추가</p> </li> </ul> <p>→ continuous 방식의 DiffuSeq-style이 가진 장점이 무엇인가?</p> <ul> <li> <p>Gradient-based token guidance는 별도의 classifier를 학습 (최근 모델에서는 LLM 내부의 연산을 활용하기도), 여기서 얻은 정보를 condition으로 하여 p의 사후 확률을 조절하는 간접적인 방식</p> </li> <li> <p>DiffuSeq 방식은 모델 자체에서 condition (이전 z)를 denoising하는 과정에서 샘플 분포 자체를 확률적으로 조절하는 것으로 더 확실한 변화가 가능</p> </li> </ul> <p><strong>Multi-pass DoT (MP-DoT)</strong></p> <ul> <li> <p>Causal inductive bias 도입 thought-by-thought 방식으로 rationales을 생성하는 방법 제안</p> </li> <li> <p><strong>Process</strong>:</p> <ol> <li> <table> <tbody> <tr> <td>\mathbf{r}<em>1\sim p</em>{\theta}^{\textit{DoT}}(\mathbf{r}_1</td> <td>\mathbf{s}, \mathbf{z}^{r_1}_t)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>\mathbf{r}<em>2\sim p</em>{\theta}^{\textit{DoT}}(\mathbf{r}_2</td> <td>[\mathbf{s};\mathbf{r}_1], \mathbf{z}^{r_2}_t)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>\mathbf{a}\sim p_{\theta}^{\textit{DoT}}(\mathbf{a}</td> <td>[\mathbf{s};\mathbf{r}_1;…;\mathbf{r}_n], \mathbf{z}^{r_n}_t)</td> </tr> </tbody> </table> </li> </ol> </li> <li> <p>이후 rationale이 이전 rationale들을 더 강한 condition signal로 이용할 수 있음.</p> </li> </ul> <h3 id="training">Training</h3> <p><strong>Scheduled Sampling</strong></p> <ul> <li> <p>Diffusion 모델이 denoising을 하는 과정에서 이미 self-correcting 능력이 있다고 할 수 있음. → Sampling 과정을 통해 이를 더욱 발전</p> </li> <li> <p>Training과 inference 간의 <strong>exposure bias</strong>가 error를 발생시킨다고 생각</p> </li> <li> <p>Any timesteps: s, t, u that satisfy 1 &lt; s &lt; t &lt; u &lt; T</p> <ul> <li> <table> <tbody> <tr> <td>Training stage: \mathbf{z}_t \sim q(\mathbf{z}_t</td> <td>\mathbf{z}_0) (oracle data에서 diffuse)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Inference stage: q(z*t</td> <td>z*{\theta}(z_u;u))</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p><strong>해결책</strong>: 추론 단계를 모방하기 위해 \epsilon_i 확률로 다음과 같이 forward step에서 만들어진 z를 활용</p> <ul> <li> <table> <tbody> <tr> <td>u \in {t+1, …, T}, \hat{z<em>0} = z</em>{\theta}(z_u;u) → q(z_t</td> <td>\hat{z_0})</td> </tr> </tbody> </table> </li> <li>\epsilon<em>i는 1에서 \epsilon</em>{min}로 선형 감소</li> </ul> </li> </ul> <p><strong>Coupled Sampling</strong></p> <ul> <li> <p>Multi-pass DoT에서 rationale에 쌓이는 error accumulation 문제 해결</p> </li> <li> <p><strong>Training 시 현재 thought뿐만 아니라 이전 thought들에도 확률적으로 noise 추가</strong></p> <ul> <li> <p>\mathbf{z}<em>0=\text{EMB}([\mathbf{s};\mathbf{r}</em>{1};\mathbf{r}_{2}]) 과정에서 일반적으로 r_1에만 noise 적용</p> </li> <li> <p>일정 확률로 [r_1;r_2] 모두에 noise 적용</p> </li> </ul> </li> </ul> <p><strong>Training Objective</strong></p> <p>DoT 모델에 대해 두 가지 학습 방법을 사용</p> <ul> <li> <p>from scratch</p> </li> <li> <p>fine-tuning from pre-trained diffusion model</p> </li> </ul> <p><strong>공통 Objective function:</strong> Negative variational lower bound 최소화</p> <ul> <li> <p>z_t를 denoising 함으로써 z_0를 복원하는 것을 배우는 것</p> </li> <li> <p><strong>Prior loss</strong></p> <ul> <li> <p>p_{\theta}(z_T): 최종 noise에서 모델의 분포</p> </li> <li> <table> <tbody> <tr> <td>q(z_T</td> <td>w^z): 노이즈를 추가하는 과정에서 만들어진 최종 z의 분포</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p>→ 이상적으론 둘이 동일해져야 하며 prior loss는 0이 되어야 함.</p> <p>→ 더 직관적으로) 충분히 많은 noise를 주입하면 최종 noise 분포 \mathcal{N(0, I)}가 되어야 함.</p> <ul> <li> <p><strong>Diffusion loss</strong>: 각 단계에서 얼마나 noise를 잘 제거하는가에 대한 탐색</p> <ul> <li> <table> <tbody> <tr> <td>우리가 궁금한 것: <strong>p를 통한 denoising이 잘 된 것이 맞을까? == **</strong>p<em>{\theta}(z</em>{t-1}</td> <td>z_t)*<strong>* 분포를 잘 구했는가?</strong></td> </tr> </tbody> </table> </li> <li> <p>우리가 아는 것, z_t (현재 주어진 정보), z_0 (원본)</p> </li> <li> <p>Posterior를 활용, 다음의 분포를 이용해 p_{\theta}를 검정</p> </li> <li>더 직관적으로 z_{t-1}의 분포가 얼마나 noise, denoise 과정에서 동일한가</li> </ul> </li> <li> <p><strong>Rounding loss</strong>: 복원력 z_0 → \text{w}^z</p> </li> </ul> <h3 id="inference-strategy">Inference Strategy</h3> <ul> <li> <p>diffusion 모델의 추론 flexibility는 큰 장점 → 어려운 문제일수록 더 많은 reasoning time을 가져야 함. → backward timestep T를 크게 가져가자! (이거 안되는 게 있나? 논문에서 autoregressive 방법에서 토큰 수를 조절하는 것은 더 어렵다고 주장.)</p> </li> <li> <p><strong>문제</strong>: Continuous diffusion의 높은 timestep 요구사항 (예: Plaid 4096 timesteps)</p> </li> </ul> <p>→ ODE solver를 conditional form을 활용해 accelerate</p> <ul> <li>이게 최종식인데 미분방정식 얘기가 나와서 아직은 모르겠습니다….</li> </ul> <p><strong>Self-consistency Integration</strong></p> <ul> <li> <p>Multiple sampling을 통한 다양한 reasoning pathway 생성</p> </li> <li> <p>동일 문제 s에 대해 다양한 (r_{i;1…n}, a_i)를 구함. (Diffusion 모델의 강점: noise seed만 다르게 해도 됨!)</p> </li> <li> <p>Majority vote:</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="evaluation">Evaluation</h2> <h3 id="experimental-setup">Experimental Setup</h3> <p><strong>데이터셋 및 메트릭</strong></p> <ul> <li> <p><strong>Simple reasoning</strong>:</p> <ul> <li> <p>4×4, 5×5 digit multiplication (BIG-bench)</p> </li> <li> <p>Boolean logic reasoning (DyVal)</p> </li> </ul> </li> <li> <p><strong>Complex reasoning</strong>: GSM8K grade school math problems</p> </li> </ul> <p><strong>Base Model</strong></p> <ul> <li> <p><strong>From scratch</strong>: Following DiifuSeq (12-layer Transformer encoder, 124M)</p> </li> <li> <p><strong>Pre-trained model for fine-tuning</strong>:</p> <ul> <li> <p>Plaid (1.3B): OpenWebText에서 훈련, GPT-2 수준 perplexity</p> </li> <li> <p>SEDD-small (170M), SEDD-medium (424M)</p> </li> </ul> </li> </ul> <p><strong>Baseline</strong></p> <ul> <li> <p>Answer-only, CoT, Implicit CoT</p> </li> <li> <p>GPT-2 (small 124M, medium 355M, large 774M)</p> </li> <li> <p>ChatGPT (gpt-3.5-turbo-1106) with 5-shot CoT</p> </li> </ul> <p><strong>구현 세부사항</strong></p> <ul> <li> <p>Tokenization: 모든 digit을 개별 토큰으로 처리</p> </li> <li> <p>MP-DoT: 마지막 thought 뒤에 <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> 토큰 추가 (모델이 rationale 수 결정)</p> </li> <li> <p>8 * V100-32G</p> </li> <li> <p>Training:</p> <ul> <li> <p>scheduled sampling: \epsilon_{min}=0.95</p> </li> <li> <p>coupled sampling: \gamma (0.01, noise 추가할 확률), k (1, 이전 step)</p> </li> <li> <p>self-consistency: m (20)</p> </li> </ul> </li> <li> <p>Inference:</p> <ul> <li>temperature 0.5, default timestep T: 64</li> </ul> </li> </ul> <h2 id="results">Results</h2> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Digit Multiplication &amp; Boolean Logic</strong></p> <ul> <li> <p>DoT: 100% 정확도 달성, 이는 CoT를 활용하면 달성할 수 있는 수준</p> </li> <li> <p>속도에서 GPT-2 CoT 대비 최대 27배 빠름.</p> </li> <li> <p>**Optimal sampling timestep: 1 for multiplication, 2 for boolean logic **(very EZ)</p> </li> <li> <p>ChatGPT와 Implicit CoT도 100% accuracy 달성 실패</p> </li> </ul> <p>→ 간단한 작업에서 DoT의 효율성과 정확성 동시 달성</p> <p><strong>Results on Complex Reasoning (GSM8K)</strong></p> <ul> <li> <p><strong>From-scratch training</strong>: ~5% accuracy (pre-trained capability의 중요성 확인)</p> </li> <li> <p><strong>Fine-tuned DoT</strong>: 엄청난 성능 향상</p> <ul> <li> <p>SEDD-medium DoT &gt; similar-sized GPT2-medium CoT (10%까지 차이)</p> </li> <li> <p>DoT-SEDD-medium (424M) &gt; GPT2-medium (355M) + CoT</p> </li> </ul> </li> <li> <p><strong>Multi-pass DoT</strong></p> <ul> <li> <p>Plaid에서 single-pass 대비 약간의 성능 향상, 효율성은 single-pass가 우수</p> </li> <li> <p>(성능도 낮고 throughput도 낮아서 이건 왜 한 것인가…)</p> </li> </ul> </li> <li> <p><strong>Self-consistency</strong>: DoT 모델에서 큰 성능 향상</p> </li> </ul> <p><strong>Ablation Study</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Sampling 방법은 효과적</p> </li> <li> <p>Continue pre-training (Gradient-based token guidance)를 활용할 경우 0.5%가 됨. (pre-trained 모델보다도 감소)</p> </li> <li> <p><strong>Gradient-based conditioning 실패 사례</strong>:</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Conditional ODE solver</strong>: 4096 → 8 timestep 정도에서 결과 수렴</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Reasoning-Efficiency Trade-off</strong></p> <ul> <li> <p>left-to-right 방식의 reasoning을 개선하기 위한 여러 아이디어 (prompting, decoding 등)</p> </li> <li> <p>Diffusion도 이러한 reasoning capabilities를 증가시키기 위한 하나의 방안</p> </li> </ul> <p>→ 더 많은 timesteps → 더 많은 reasoning → 효율성 감소</p> <ul> <li>Efficiency trade-off를 확인</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>DoT는 timestep, 나머지 방법은 output token의 수를 step으로 생각</p> </li> <li> <p><strong>Simple task (5×5)</strong>: 1 reasoning step으로 100% accuracy, 추가 computation 불필요</p> </li> <li> <p><strong>Complex task (GSM8K)</strong>: Timestep 증가에 따른 지속적인 성능 향상</p> <ul> <li> <p>DoT-SEDD-medium은 outperform</p> </li> <li> <p>DoT-SEDD-small의 경우, 32 timestep에서 GPT2-medium CoT 능가, T=64에서 최고 성능 달성</p> </li> </ul> </li> <li> <p>**Diffsuion 모델의 장점: **작업 난이도에 따라 timesteps을 자유롭게 조절할 수 있음.</p> </li> <li> <p><strong>Autoregressive 한계</strong>: CoT와 Implicit CoT는 token-by-token prediction 특성상 명확한 조절이 어려움.</p> </li> </ul> <p><strong>Self-consistency in DoT</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>sampling 수 증가에 따른 지속적 개선</p> </li> <li> <p>Autoregressive model과 달리 decoding algorithm 없이도 자연스러운 diversity 제공 (Noise는 다르니깐!)</p> </li> </ul> <p><strong>Self-correction Capability</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Autoregressive CoT와 명확히 다른 self-correction 능력을 보임</li> </ul> <p><strong>Case 1: Fast thinking</strong></p> <ul> <li> <p>쉬운 문제의 경우, 모든 올바른 thought를 single reasoning step으로 도출</p> </li> <li> <p>두 번째 Step에서 정확한 최종 답변 생성</p> </li> </ul> <p><strong>Case 2: Slow thinking</strong></p> <ul> <li> <p>조금 더 어려운 문제 초기에는 에러가 발생</p> </li> <li> <p>후속 단계에서 점진적 refinement를 통한 정확한 답변 도출</p> </li> <li> <p>초기에 문제의 대략적인 outline을 잡고 refine &amp; improve하는 것은 사람의 복잡한 작업 수행 방식과 유사</p> </li> </ul> <p><strong>Case 3: Non-sequential correction</strong></p> <ul> <li> <p>Step 4: 잘못된 중간 thought <code class="language-plaintext highlighter-rouge">&lt;2*3=4&gt;</code> 존재하지만 이후 thought와 답변은 정확</p> </li> <li> <p>Step 5: 잘못된 중간 thought 교정</p> </li> <li> <p><strong>핵심 특징</strong>: Left-to-right paradigm을 벗어나 좌우의 정보를 바탕으로 수정</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Diffusion model에 특화된 최초의 CoT reasoning 방법 DoT 제안</p> </li> <li> <p>Scheduled sampling, coupled sampling, conditional ODE solver 등 고유 기법 개발</p> </li> <li> <p>Mathematical reasoning task에서 comprehensive evaluation 수행</p> </li> </ul> <hr/> <ul> <li> <p><strong>Current limitations</strong>: Pre-trained diffusion model의 scale과 generalization 한계 (제한된 모델 크기)</p> </li> <li> <p><strong>Future promise</strong>: 더 강력한 pre-trained model과 함께 autoregressive LLM에 필적하는 성능 기대</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="diffusion"/><category term="diffusionlm"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><category term="text generation"/><category term="transformer"/><category term="vision"/><summary type="html"><![CDATA[논문 리뷰 - Text Generation, DiffusionLM 관련 연구]]></summary></entry><entry><title type="html">DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</title><link href="https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for/" rel="alternate" type="text/html" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-10</li> <li><strong>Reviewer</strong>: 건우 김</li> </ul> <h1 id="abstract">Abstract</h1> <ul> <li>최근에 post-training을 위한 RL에서 <strong>GRPO</strong>와 같이 low-resource settings에서 가능성을 보여줌</li> </ul> <p><strong>→ GRPO는 solution-level의 scalar reward signals에 의존하는 경우가 많아, sampling된 문장들간의 semantic diversity를 제대로 반영하지 못함</strong></p> <p>→ 이는 서로 다른 reasoning path를 갖는 response들이 구분되지 않는 동일한 reward를 받는 (<strong>diversity-quality inconsistency</strong>) 문제가 있음</p> <ul> <li> <p>위 문제를 해결하기 위해 reward computation 과정에서 semantic diversity를 직접적으로 반영하는 방법인 <strong>Diversity-aware Reward Adjustment (DRA)</strong>를 제안함</p> </li> <li> <p>DRA는 Submodular Mutual Information (SMI)를 활용하여</p> <ol> <li> <p>중복된 response의 reward는 감소시킴</p> </li> <li> <p>diverse response의 reward는 증폭시킴</p> </li> </ol> </li> <li> <p>5개 Mathematical Reasoning benchmark에서 recent methods 대비 outperform 성능 보여줌</p> </li> </ul> <p>(단 7,000개 sample로만 fine-tuning을 하고, $55 training cost로 평균 acc 58.2% sota 달성)</p> <h1 id="1-introduction">1. Introduction</h1> <p>DeepSeek-R1-Zero (Guo et al., 2025)에서 기존 LLM에 SFT를 적용하는 것에서 벗어나, base LM에 바로 RL을 적용할 수 있는 R1-Zero training pipeline을 제안함.</p> <p>→ Group Relative Policy Optimization (GRPO) 알고리즘 덕분에 가능한 방법</p> <p>GRPO는 PPO와 다르게 critic model 없이 주어진 prompt에 대해 여러 sampling된 completions의 relative performance에 대한 advantage를 평가함.</p> <p>하지만 최근에 공개된 GRPO 및 그 variants (e.g,. DR. GRPO)들은 일반적으로 정답 여부와 같은 <strong>solution-level의 scalar reward signals에만 의존하는 경향이 있어, 같은 정답이라도 diverse reasoning path의 차이를 반영하지 못함</strong>.</p> <p>→ 이는 semantic하게 다른 completions들이 올바르거나 틀린 경우 모두 거의 동일한 rewards를 받아, 의미 있는 reasoning 차이를 반영하지 못하는 <strong>indistinguishable advantage estimates</strong>를 생성하는 문제가 있음</p> <ul> <li>*<strong>*Example1 (Correct Reward)**</strong></li> </ul> <p>GRPO training process의 examples을 준비함. → 저자들의 key motivation of research</p> <ul> <li>LLM은 diverse answer를 생성할 수 있지만, 이런 answers들은 비슷한 수준의 reward score를 받음</li> </ul> <p>→ 즉, solution-level judgements는 different reasoning paths를 구별하지 못함</p> <p>아래는 question에 동일한 정답을 생성했지만 reasoning path가 완전히 다른 두가지 응답에 대한 케이스 (그럼에도 불구하고 reward는 비슷함)</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>*<strong>*Example2 (Incorrect Reward)**</strong></li> </ul> <p>이번 예시는 Question에 대해 두가지 응답이 모두 Incorrect인 반면, reasoning path는 서로 다름 → 그럼에도 불구하고 reward score는 둘 다 비슷하게 낮음</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>→ 또한, 이는 resource-constrained settings에서 더 문제가 될 수 있음</p> <ul> <li> <p>각 prompt당 sampling할 수 있는 completions의 개수가 적기 때문에, <strong>reward가 높은 outputs에 대한 exploitation만 reinforce하며, alternative하고 potentially valid한 reasoning paths에 대한 exploration을 유도하지 못함.</strong></p> </li> <li> <p>(비유) 선생님이 수학 문제를 모두 정확하게 푼 학생들에게 100점을 주는 케이스. 정답이 맞더라도, 학생의 이해도나 사고 방식을 드러낼 수 있는 문제를 푸는 다양한 방식은 평가되지 않고, 오답일 경우에도 그 과정에서 드러나는 다양한 추론 접근을 평가하지 않고 단순히 같은 감점을 받음.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>이 문제를 해결하기 위해서 저자들은 <strong>Diversity-aware Reward Adjustment (DRA)</strong>를 제안함.</p> <p>이는 학습 과정에서 sampling된 completions 간의 <em>semantic diversity를 직접적으로 모델링하는 방식으로</em> 그룹 내 다른 <em>completions과의 semantic similarity를 기반으로 각 output의 reward를 reweight</em>함.</p> <ul> <li><strong>diverse completions에는 더 높은 weight, 중복된 completion에는 더 낮은 weight 부여</strong></li> </ul> <h1 id="2-method">2. Method</h1> <h3 id="preliminary">Preliminary</h3> <p>LM의 generation은 token-level Markov Decision Process로 볼 수 있음. 각 generation step t에서 state s<em>t는 input question q와 지금까지 생성된 partial output sequence o</em>{&lt;t}의 concatenation이기에, sates는 다음과 같음 s<em>t=[q;o</em>{&lt;t}].</p> <table> <tbody> <tr> <td>policy \pi*{\theta}(.</td> <td>s_t)는 vocab set A에서 next token o_t를 선택하고, 이는 deterministic transition을 유도하여 next state s*{t+1}=[s_t;o_t]로 이동함.</td> </tr> </tbody> </table> <p>GRPO는 각 question q에 대해 여러 개의 responses C={o_1,…o_G}를 sampling하고, 각 response에 대해 reward를 계산함 R={R(q,o_1), … , R(q,o_G)}</p> <p>계산된 reward R을 이용해 advantage A_{i,t}를 아래와 같이 계산함 (normalize)</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>GRPO의 objective function J<em>{GRPO}(\pi</em>{\theta})를 optimize함</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>이후 연구인 DR.GRPO (Liu et al., 2025)에서는 token efficiency를 개선하기 위해 <strong>GRPO의 objective function에서 ‘response length’ term과 Advantage에서 std로 normalize해주는 term을 지움</strong></p> <ul> <li><strong>DR.GRPO (Zichen Liu, et al 2025)</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>GRPO의 두 가지 편향</p> <ol> <li> <p><strong>Response-level length bias</strong></p> </li> <li> <table> <tbody> <tr> <td>각 response에 대해 평균을 구할때, 1/</td> <td>o_i</td> <td>가 곱해지는데,</td> </tr> </tbody> </table> </li> <li> <p>Correct response인 경우 advantage가 양수이면 shorter response에 대해서 greater gradient updates를 야기함 → 이 Policy는 correct answer에 대해 brevity favor있음</p> </li> <li> <table> <tbody> <tr> <td>Incorrect response인 경우 advantage가 음수 longer response는</td> <td>o_i</td> <td>가 커지기 때문에 penalized를 더 받음 → 이 Policy는 Incorrect answer에 대해 길게 말하는 favor 있음</td> </tr> </tbody> </table> </li> </ol> <table> <tbody> <tr> <td>⇒ 쉽게 말하면 GRPO의 개별 advantage를 A_{i,t}/</td> <td>o_i</td> <td>로 보면</td> </tr> </tbody> </table> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - positive advantage 에 대해서는 동일한 reward라도 |o_i|가 작을수록 update가 커지기에 짧은 response에 강한 signal을 주고

  - negative advantage 에 대해서 역시 동일한 reward라도 |o_i|가 클수록 update가 작아지기 때문에 긴 response에 강한 signal을 줌 (=긴 오답은 under-penalized)
</code></pre></div></div> <p><strong>→ 즉, GRPO는 정답은 짧게, 오답은 길게 말하게끔 biased policy를 유도함</strong></p> <ol> <li> <p><strong>Question-level difficulty bias</strong></p> </li> <li> <p>advantage를 특정 question 내의 group average와 std로 normalization을 하기 때문에, std가 작으면 상대적으로 해당 question에 대한 training signal (weight update)가 과도하게 커짐</p> </li> </ol> <p>→ 일반적인 RL에서는 batch 단위로 normalization되어 bias를 상쇄시키지만, GRPO는 question 단위로 처리되어 그렇지 못함</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>⇒ 위와 같은 문제는 LM의 response가 길어지는 이유가 reasoning capability 때문인지 아니면 bias 때문인지 구분하기가 어려워짐. 이에 따라 <strong>unbiased optimization method인 DR.GRPO 소개함</strong></p> <p><strong>Question1</strong>: 위에서 분명 Correct response에 대해서는 짧아지는데 왜 response가 길어진다고 표현하는지?</p> <p>⇒ (건피셜) Correct response 개수보다 Incorrect response의 수가 더 많기에, 대부분의 response는 RL training에서 오답이라 길게 생성하는 경향이 생김</p> <p><strong>Question2</strong>: GRPO를 보면 학습이 진행될수록 reasoning accuracy가 올라가는데, 그러면 bias에 따라 response length가 짧아져야 하는거 아닌가?</p> <p>⇒ (건피셜) complex tasks에서는 아무리 오래 학습시켜도 높은 acc에 도달하는 모델이 없어서 그렇지 않을까..?</p> <p>DR.GRPO는 GRPO의 optimization bias를 없애기 위해 아래 두가지 terms을 없앰</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>R1-Zero와 비슷한 minimal recipe로 학습한 Oat-Zero-7B 성능</p> <ul> <li> <p>minimal recipe</p> <ul> <li> <p>base model: Qwen2.5-Math-7B</p> </li> <li> <p>training dataset: MATH dataset의 level3~5</p> </li> <li> <p>GPU: 8xA100 27hrs</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>아래 (우측) 그림을 보면</p> <ul> <li> <p>(solid lines) DR.GRPO는 reasoning accuracy가 지속적으로 오르는 반면에, GRPO는 그렇지 않음 (불안정함)</p> </li> <li> <p>(dashed lines) DR.GRPO는 response length가 짧고 안정적인 반면에, GRPO는 response length가 계속 길어짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="diversity-quality-inconsistency">Diversity-Quality Inconsistency</h3> <p>GRPO와 DR.GRPO의 reward signal은 <strong>solution-level correctness</strong>만 사용하기 때문에, 각 completion에 대해 <strong>sparse scalar judgement</strong>를 계산함.</p> <p>→ 이러한 scalar reward는 동일하거나 유사한 결과를 산출하는 diverse reasoning-path를 고려하지 않기 때문에, Diversity-Quality Inconsistency가 발생함.</p> <p>위에 Example 말고, 보다 실증적인 방식으로 다음 statement (”<strong><em>reward alone fails to reflect the underlying variability in reasoning strategies</em></strong>”) 를 검증하기 위해 embedding distances로 측정된 completions의 structural dissimilarity를 계산함.</p> <ul> <li> <p>Spearman’s rank correlation을 사용하여 sampled completions 사이에서 reward difference와 semantic distance를 측정함 →semantic distance가 커질수록 reward 차이도 커지는가?</p> <ul> <li>3000개 prompt 뽑아서 p-value 측정</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Figure2는 Spearman’s rank correlation의 p-values의 분포를 보여주는데, 대부분의 p-value가 significance level인 0.05 보다 큰 값을 보여주며, 실제로 80% 이상의 prompt에 대해 statistically significant correlation이 없음을 확인할 수 있음</li> </ul> <p>→ 즉, reward가 semantic diversity와 상관이 없다는 것을 실험적으로 보여줌</p> <p>\tilde{a} \text{이렇게}</p> <h3 id="diversity-aware-reward-adjustment">Diversity-aware Reward Adjustment</h3> <p>Diversity-Quality Inconsistency 문제를 해결하기 위해, 각 sample의 relative diversity/redundancy에 따라 reward를 reweight하는 방법을 제안함.</p> <p><strong>→ diverse completions은 더 높은 weight, 중복된 response는 낮은 weight</strong></p> <p>먼저 기존의 reward R(q,o_i)를 diversity-aware adjusted reward \tilde{R}(q,o_i) (틸다 표시 어떻게 하나요…) 으로 대체함</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>SMI({o_i},C \ {o_i})는 completion o_i와 나머지 group C \ o_i 간의 Submodular Mutual Information을 나타냄</p> </li> <li> <p>Submodular functions은 diminishing returns 특성을 갖으며, diversity와 redundancy를 모델링할 수 있음</p> <ul> <li><strong>*diminishing returns**</strong>: 어떤 집합에 요소를 하나씩 더 추가할 때, 이미 비슷한 요소가 많을 수록 그 요소가 추가로 가져오는 가치(정보 기여도)는 줄어드는 성질*</li> </ul> </li> <li> <p>SMI는 두 집합 간의 shared information을 정량화하며 (Iyer et al., 2021a,b)에서는 아래와 같이 정의함</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>s(o_i,j)=s(j,o_i)라고 가정하며, SMI({o_i},C \ {o_i})는 o_i와 나머지 elements간의 total semmetric simialrity를 계산함</p> </li> <li> <p>즉, o_i가 나머지 completions과 유사할수록 SMI가 커져 reward 값이 낮게 reweight되고, o_i가 다를수록 SMI가 작아져 reward 값이 크게 reweight 됨.</p> </li> <li> <p>각 completion의 embedding은 small LM으로 얻고, s()는 cosine similarity를 사용함</p> </li> <li> <p>SMI를 쉽게 말하면 “<strong>특정 completion 하나가 group 내 다른 completion과 얼마나 겹치는가</strong>”를 수치로 나타내는 값</p> <ul> <li> <p><strong>중복이 많으면 (high redundancy) → SMI가 큼 → reward가 작아짐</strong></p> </li> <li> <p><strong>중복이 적으면 (high diversity) → SMI가 작음 → reward가 커짐</strong></p> </li> </ul> </li> <li> <p>Submodular 함수는 수학 개념으로 “새로운 element가 기존에 비슷한게 많을수록 기여도가 줄어드는 성질”을 갖고 있음</p> </li> </ul> <p>ex) 유사한 completions이 9개가 있는 상황에서, 10번째 비슷한 completion은 information을 별로 추가하지 않음. 반면, 완전히 다른 completion이 등장하면 information 기여도가 커짐</p> <p>→ 이렇게 새로운 reward를 구하는 연산은 Pytorch에서 효과적으로 처리될 수 있음</p> <ul> <li>\Sigma{L_{ij}} term이 simiarlity matrix L의 ith row의 합</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li><strong>Pytorch Code for DAR</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="3-experiment">3. Experiment</h2> <h3 id="31-experimental-setup">3.1 Experimental Setup</h3> <p>**Training Dataset: **</p> <ul> <li>s1 dataset + DeepScaleR dataset with mixed problem difficulties</li> </ul> <p>**Evaluation Dataset: **</p> <ul> <li>five mathematical reasoning benchmarks (AIME24, MATH-500, AMC23, Minerva, OlympiadBench)</li> </ul> <p><strong>Baselines</strong>:</p> <ul> <li> <p>general purpose large model: Llama-3.1-70B-Instruct, o1-preivew</p> </li> <li> <p>Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL</p> </li> <li> <p>Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS</p> </li> </ul> <p><strong>Implementations:</strong></p> <ul> <li> <p>본 연구는 DRA의 proof-of-concept만 검증하는 것이 목적이기에 DeepSeek-R1-Distill-Qwen-1.5B를 base model로 두어 학습시킴</p> </li> <li> <p>4 x A100 (40GB) GPUs</p> </li> </ul> <h3 id="32-empirical-analysis">3.2 Empirical Analysis</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Main Results</strong></p> <ul> <li> <p>DRA-DR.GRPO는 avg acc가 58.2%로 가장 높게 나옴 (DRA-GRPO역시 비슷한 수준으로 높게 나옴)</p> <ul> <li>AMC23에서 특히 높은 성능을 보여줌 (85.0%)</li> </ul> </li> <li> <p>DRA-GRPO와 DRA-DR.GRPO는 fine-tuning samples을 7,000개 밖에 사용하지 않았음에도 불구하고 40,000개 사용한 DeepScaleR-1.5B-preview보다 높은 성능 보여줌</p> </li> </ul> <p>→ Low-resource settings에서도 효과적임</p> <ul> <li><strong>DeepScaleR-1.5B-preview</strong></li> </ul> <p>나름 GOAT급의 성능 보여주는 잘나가는 모델</p> <p>Untitled</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Ablation Study</strong></p> <ul> <li>Base model인 DeepSeek-R1-Distill-Qwen-1.5B와 비교하여 DRA-GRPO, DRA-DR.GRPO는 각각 7.8%, 9.3% 성능 향상되고 단순 RL (GRPO, DR.GRPO) 대비 1.9%, 2.2% 향상</li> </ul> <p>→ 이게 왜 Ablation study라 말하는거지ㅋㅋ</p> <p><strong>Efficiency</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>DRA는 completions을 encoding 해야하기에 over-head가 존재하지만, 별로 크지 않음.</p> <p>→ 저자들이 실험에 사용한 GPU스펙인 (A100-40GB)에서는 어차피 DRA 없이도 mini-batch를 늘리는 것이 불가능해서 DRA 적용하는 것이 별 문제가 되지 않다고 하는데…. → 🐶 🔊 라고 생각합니다</p> <p><strong>Training Cost</strong></p> <p>500 steps 학습시켜 12.5hr 소요됨 ⇒ $55 비용</p> <p>→ 다른 방법대비 효율적임</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="33-discussion">3.3 Discussion</h3> <p><strong>Exploration-exploitation Balance</strong></p> <p>DRA는 Exploration-exploitation balance를 policy gradient 안에 직접 통합하여 적용함</p> <ul> <li>Base reward는 high score를 받는 completion을 reinforce함</li> </ul> <p>→ <strong>Exploitation 유도</strong></p> <ul> <li>Diversity weighting은 semantically novel completion에 learning signal을 amplify</li> </ul> <p>→ <strong>Exploration 유도</strong></p> <p>이러한 탐색은 low-resource settings (prompt당 sampling할 수 있는 응답 수가 제한 적인 경우)에서 중요함</p> <p>→ DRA는 mode collapse를 방지하고 더 넓은 reasoning strategies를 유도함</p> <p><strong>Ad-hoc vs Post-hoc Diversity</strong></p> <p>generated completions간의 diversity를 모델링하는 방법은 크게 Ad-hoc, Post-hoc 방식이 있음</p> <ol> <li> <p><strong>Ad-hoc</strong></p> </li> <li> <p>generation 단계에서 다양성을 유도함 (temperature 조절, decoding 설정 변경)</p> </li> <li> <p>이렇게 하면 각 completion이 독립적으로 sampling되어 → 응답 간 correlation을 명시적으로 모델링할 수 없음 (completion이 서로 얼마나 다른지 명시적으로 알 수 없음)</p> </li> <li> <p><strong>Post-hoc (본 연구에서 채택한 방법)</strong></p> </li> <li> <p>diversity를 reward signal에 바로 통합</p> </li> <li> <p>Semantic redundancy를 평가하여 policy가 효율적으로 learning을 조정할 수 있게 해줌</p> </li> </ol> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li>GRPO 형식의 RL에서 completions 간의 semantic diversity를 모델링할 수 있는 DRA 알고리즘 제안함</li> </ul> <p>→ 기존 scalar reward의 문제인 exploration-exploitation imbalance issue를 효과적으로 완화함</p> <ul> <li> <p>두가지 한계점이 있음</p> <ol> <li> <p>small-scale models (1.5B) / small group sizes (e.g 6 completions per prompt)</p> </li> <li> <p>diversity를 측정할 때 사용된 sentence embeddings은 외부 model에 의존하는 구조</p> </li> </ol> </li> <li> <p>이런 쪽도 재밌다!ㅋㅋ</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="embedding"/><category term="fine-tuning"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry></feed>