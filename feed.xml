<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://alshedivat.github.io/al-folio/feed.xml" rel="self" type="application/atom+xml"/><link href="https://alshedivat.github.io/al-folio/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T10:33:26+00:00</updated><id>https://alshedivat.github.io/al-folio/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: ìƒì—½</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion ëª¨ë¸ì´ ê°€ì§„ í•œê³„ì </strong></p> <ol> <li> <p>í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ diffusion ëª¨ë¸ì˜ ê²½ìš° <strong>ê³ ì •ëœ ê¸¸ì´ì˜ ë‹µë³€</strong>ë§Œì„ ìƒì„±.</p> </li> <li> <p>Bidirectional contextë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— <strong>KV ìºì‹œì™€ ê°™ì´ AR ì¶”ë¡ ì—ì„œ íš¨ìœ¨ì ì¸ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)ì—ì„œ <strong>ì—¬ì „íˆ ë‚®ì€ ì„±ëŠ¥</strong>ì„ ë³´ì„.</p> </li> </ol> <p><span style="color:yellow_background">â†’ </span><span style="color:yellow_background"><strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></span></p> <p><br/></p> <p><span style="color:yellow_background"><strong>BD3-LMs</strong></span></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Blockì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì€ AR ê³¼ì •ìœ¼ë¡œ ëª¨ë¸ë§</p> </li> <li> <p><strong>Intra-Block</strong>: ì´ì „ blockì´ ì£¼ì–´ì§ˆ ê²½ìš°, í˜„ì¬ block ë‚´ë¶€ëŠ” discrete diffusion ê³¼ì •ìœ¼ë¡œ ëª¨ë¸ë§</p> </li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Block Diffusion ëª¨ë¸ì´ ê°€ì§„ Two challengesë¥¼ ë°œê²¬: í•µì‹¬!!</strong></span></p> <ul> <li> <p>Block diffusionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ë‘ ë²ˆì˜ forward passê°€ í•„ìš”í•¨. â†’ ê³„ì‚°ëŸ‰ ì¦ê°€</p> </li> <li> <p>ë†’ì€ gradient varianceë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜</p> </li> </ul> <p>â†’ ì§€ê¸ˆì€ ì´í•´ê°€ ì–´ë ¤ìš°ë‹ˆ ë’¤ì—ì„œ í™•ì¸</p> <p><br/></p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model ì œì•ˆ. <strong>ê¸°ì¡´ diffusion ëª¨ë¸ê³¼ ë‹¬ë¦¬ variable-length generationê³¼ KV cachingì„ ì§€ì›</strong></p> </li> <li> <p>í•™ìŠµ ì‹œ í† í° ë°°ì¹˜ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ block diffusion modelì„ ìœ„í•œ <strong>í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ ì œì•ˆ</strong> (Challenge 1)</p> </li> <li> <p>Gradient varianceê°€ diffusion ëª¨ë¸ ì„±ëŠ¥ì˜ ì œí•œ ìš”ì†Œì„ì„ ë°í˜ + ë°ì´í„° ê¸°ë°˜ <strong>ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„</strong>ë¡œ í•´ê²° (Challenge 2)</p> </li> <li> <p><strong>ì„±ëŠ¥ í–¥ìƒ</strong>!</p> </li> </ul> <p><br/></p> <p><br/></p> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> \[\mathcal{V}=\left\{\mathbf{x} \in\{0,1\}^V: \sum_i \mathbf{x}_i=1\right\} \subset \Delta^V\] <ul> <li> <p>scalar discrete random variables with V categories as â€˜one-hotâ€™ column</p> </li> <li> <p>$ \Delta^V $: simplex ê³µê°„</p> </li> <li> <p>$ m \in \mathcal{V} $: [MASK] tokenâ€™s one-hot vector</p> </li> </ul> <p><br/></p> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> \[\log p_{\theta}(\mathbf{x}) = \sum_{\ell=1}^{L} \log p_{\theta}(\mathbf{x}^{\ell} | \mathbf{x}^{&lt;\ell})\] <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><span style="color:yellow_background"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></span></h3> <ul> <li> <p>$ p $: denoising, $ q $: noising</p> </li> <li> <p>$ s(j) = (j-1)/T $, $ t(j) = j/T $ (ì´í›„ì— jëŠ” ìƒëµ!)</p> <ul> <li>ì•ŒíŒŒë²³ ìˆœì„œëŒ€ë¡œ sê°€ ì• tê°€ ë’¤, s â†’ t ê³¼ì •ì€ noiseë¥¼ ë”í•˜ëŠ” ê³¼ì •</li> </ul> </li> <li> <p><span style="color:yellow_background">D3PM framework: që¥¼ Markov forward process, ê°ê°ì˜ í† í°ì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ì•„ë˜ì˜ ì‹ì„ ì§„í–‰</span></p> \[\mathbf{x}^{\ell}: q\left(\mathbf{x}_t^{\ell} \mid \mathbf{x}_s^{\ell}\right)=\operatorname{Cat}\left(\mathbf{x}_t^{\ell} ; Q_t \mathbf{x}_s^{\ell}\right) \text { where } Q_t \in \mathbb{R}^{V \times V}\] <ul> <li> <p>$ Q_t $ì˜ ì˜ˆì‹œ</p> <ul> <li>Uniform replacement</li> </ul> \[Q_t(i, j)= \begin{cases}1-\beta_t, &amp; j=i \\ \beta_t /(V-1), &amp; j \neq i\end{cases}\] <ul> <li><span style="color:yellow_background"><strong>Masking ê¸°ë°˜</strong></span>: $ \beta_t $ í™•ë¥ ë¡œ [MASK] í† í°ìœ¼ë¡œ ë³€ê²½</li> </ul> </li> </ul> </li> <li> <p>ì´ìƒì ì¸ diffusion model $ p_{\theta} $ëŠ” $ q $ì˜ ì—­ë°©í–¥ì´ë¯€ë¡œ D3PMì—ì„œëŠ” ì•„ë˜ ìˆ˜ì‹ìœ¼ë¡œ $ p_{\theta} $ë¥¼ ì •ì˜</p> \[p_\theta\left(\mathbf{x}_s \mid \mathbf{x}_t\right)=\prod_{\ell=1}^L p_\theta\left(\mathbf{x}_s^{\ell} \mid \mathbf{x}_t\right)=\sum_{\mathbf{x}}\left[\prod_{\ell=1}^L q\left(\mathbf{x}_s^{\ell} \mid \mathbf{x}_t^{\ell}, \mathbf{x}^{\ell}\right) p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right)\right]\] <ul> <li> <p>1ë‹¨ê³„ denoising ê³¼ì • = ê°œë³„ í† í° ìœ„ì¹˜ì— ëŒ€í•œ denoiseëŠ” ë…ë¦½ ê³¼ì • = $ x^\ell $ ê·¼ì‚¬</p> </li> <li> <p>$ x^{\ell} $ (ì›ë³¸ í…ìŠ¤íŠ¸)ê°€ ì£¼ì–´ì§„ë‹¤ë©´ që¥¼ í™œìš©í•´ $ x_t^\ell \rightarrow x_s^\ell $ì„ ì™„ì „íˆ ë³µêµ¬í•  ìˆ˜ ìˆìŒ.</p> </li> <li> <p>denoise ê³¼ì •ì—ì„œ $ x^\ell $ì´ ì£¼ì–´ì§€ì§€ ì•Šìœ¼ë¯€ë¡œ $ p $ë¡œ ê·¼ì‚¬: $ p_\theta\left(\mathbf{x}^{\ell} \mid \mathbf{x}_t\right) $</p> </li> </ul> </li> <li> <p><strong>Negative ELBO (NELBO)ë¥¼ ì´ìš©í•´ í•™ìŠµ</strong></p> </li> </ul> \[\mathcal{L}(\mathbf{x} ; \theta)=\mathbb{E}_q\left[-\log p_\theta\left(\mathbf{x} \mid \mathbf{x}_{t(1)}\right)+\sum_{j=1}^T D_{\mathrm{KL}}\left[q\left(\mathbf{x}_{s(j)} \mid \mathbf{x}_{t(j)}, \mathbf{x}\right) \| p_\theta\left(\mathbf{x}_{s(j)} \mid \mathbf{x}_{t(j)}\right)\right]+D_{\mathrm{KL}}\left[q\left(\mathbf{x}_{t(T)} \mid \mathbf{x}\right) \| p_\theta\left(\mathbf{x}_{t(T)} \right)\right]\right]\] <ul> <li> <p>1, 2í•­: noise, denoise ê³¼ì •ì—ì„œì˜ ìƒ˜í”Œì˜ ì¼ì¹˜ ì •ë„</p> </li> <li> <p>3í•­ ì–¼ë§ˆë‚˜ noiseë¥¼ ì˜ ë§Œë“¤ì—ˆëŠ”ê°€</p> </li> </ul> <p><br/></p> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><span style="color:yellow_background"><strong>Block Definition</strong></span></p> <ul> <li> <p>ê¸¸ì´ $ Lâ€™ $ì´ ë˜ê²Œ $ B $ê°œì˜ blockìœ¼ë¡œ ë§Œë“¤ê¸° ($ x^b: x^{(b-1)Lâ€™:bLâ€™} \in {1,â€¦,B} $)</p> </li> <li> <p>Likelihood over block</p> \[\log p_\theta(\mathbf{x})=\sum_{b=1}^B \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}^{&lt;b}\right)\] </li> </ul> <p>block ë‚´ì—ì„œ reverse diffusion í”„ë¡œì„¸ìŠ¤ ì ìš©</p> \[p_{\theta}(\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^{&lt;b}) = \sum_{\mathbf{x}^b} q(\mathbf{x}_s^b | \mathbf{x}_t^b, \mathbf{x}^b) p_{\theta}(\mathbf{x}^b | \mathbf{x}_t^b, \mathbf{x}^{&lt;b})\] <ul> <li>blockì´ constraintì¸ ê²ƒì„ ì œì™¸í•˜ë©´ preliminariesì˜ ìˆ˜ì‹ê³¼ ë™ì¼!</li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Learning Objective</strong></span></p> \[- \log p_\theta(\mathbf{x}) \leq \mathcal{L}_{\text{BD}}(\mathbf{x}; \theta) := \sum_{b=1}^{B} \mathcal{L}(\mathbf{x}^b, \mathbf{x}^{&lt;b}; \theta)\] <p>NELBOë¥¼ ì ìš©í•´ ìœ„ì™€ ê°™ì´ í•™ìŠµ ëª©ì í•¨ìˆ˜ ì •ì˜, ì´ê²ƒë„ Sumì„ ì œì™¸í•˜ê³¤ ì „ë¶€ ê°™ìŒ!</p> <p><br/></p> <p><span style="color:yellow_background"><strong>Denoiser model</strong></span></p> <ul> <li> <table> <tbody> <tr> <td>Transformer $ x_\theta $ë¥¼ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„°í™”: $ p_\theta(x^b</td> <td>x_t^b, x^{&lt;b}) $</td> </tr> </tbody> </table> <ul> <li> <p>given $ x^{&lt;b} $: AR íŠ¹ì„± ìœ ì§€</p> </li> <li> <p>$ x^b $ ì˜ˆì¸¡: Denosing</p> </li> </ul> </li> <li> <p>Blockë“¤ì— ëŒ€í•´ ë³‘ë ¬ì  í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•¨ (block-causal attention mask)</p> </li> <li>$ x_\theta $ì˜ í•™ìŠµ: block b ë‚´ì—ì„œ $ x_\theta^b(x_t^b, x^{&lt;b}) $ â†’ $ Lâ€™ $ ê¸¸ì´ì˜ ê²°ê³¼ ì˜ˆì¸¡</li> </ul> <p>â†’ ì•„ë˜ K, V ìºì‹œ ìˆ˜ì‹ì„ ë³´ì‹œë©´ ëª¨ë¸ì„ ì´í•´í•˜ê¸° ì‰¬ì›€!</p> <p><br/></p> <p><span style="color:yellow_background"><strong>K, V caching</strong></span></p> \[\mathbf{x}_{\text {logits }}^b, \mathbf{K}^b, \mathbf{V}^b \leftarrow \mathbf{x}_\theta^b\left(\mathbf{x}_t^b, \mathbf{K}^{1: b-1}, \mathbf{V}^{1: b-1}\right):=\mathbf{x}_\theta^b\left(\mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\] <ul> <li>recomputingì„ ë§‰ê¸° ìœ„í•œ block ë‹¨ìœ„ caching</li> </ul> <p><br/></p> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><span style="color:yellow_background"><strong>Training</strong></span></p> <ul> <li>ëª¨ë“  blockì€ $ x_\theta $ì˜ forward passë¥¼ ë‘ ë²ˆ ê±°ì³ì•¼ í•¨ ($ x_t^b $, $ x^b $) â†’ ê³„ì‚°ì˜ íš¨ìœ¨í™” í•„ìš”</li> </ul> <ol> <li> <p>Block ë³„ë¡œ noise level sampling</p> </li> <li> <p>ê° blockì— ëŒ€í•´ noisy input $ x_{t_b}^b $ ìƒì„±</p> </li> <li> <p>$ \left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}) $: ì›ë³¸ xë¥¼ ì´ìš©í•´ K, V cache ë¯¸ë¦¬ ë‹¤ ê³„ì‚°í•˜ê¸°</p> </li> <li> <p>ëª¨ë“  bì— ëŒ€í•´ $ x^b_{\text{logit}} $ ê³„ì‚°</p> <ul> <li> <p>Naive: B-times loopë¥¼ ì´ìš©í•´ forward passë¥¼ ë³„ë„ë¡œ ì§„í–‰</p> </li> <li> <p>Vectorized ë°©ì‹</p> <ul> <li> <p>$ x_{\text {noisy }}=x_{t_1}^1 \oplus x_{t_2}^2 \oplus \cdots \oplus x_{t_B}^B $</p> </li> <li> <p>$ x_{\text{noisy}} \oplus x $ì„ inputìœ¼ë¡œ í•˜ì—¬ í•œ ë²ˆì— ê³„ì‚° How? attention maskë¥¼ ì´ì „ blockë§Œ ì¡°íšŒí•˜ê²Œë” ì¡°ì ˆ</p> </li> </ul> </li> </ul> </li> </ol> <p><br/></p> <p><span style="color:yellow_background"><strong>Sampling</strong></span></p> <ul> <li> <p>Block ë‹¨ìœ„ì˜ ìˆœì°¨ì  ìƒ˜í”Œë§, K, V ìºì‹± ê°€ëŠ¥ â† ARì˜ ì¥ì </p> </li> <li> <p>arbitrary length ìƒì„± ê°€ëŠ¥ â† ARì˜ ì¥ì </p> </li> <li> <p>block ë‚´ë¶€ì—ì„  Parallelí•˜ê²Œ ìƒì„± ê°€ëŠ¥ â† Diffusionì˜ ì¥ì </p> </li> </ul> <p><br/></p> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>ìµœê·¼ ê°€ì¥ í° íš¨ê³¼ë¥¼ ë³´ì´ê³  ìˆëŠ” masking noise processë¥¼ ì ìš©</p> </li> <li> <p>Per-token noise process</p> \[q\left(\mathbf{x}_t^{\ell} \mid \mathbf{x}^{\ell}\right)=\operatorname{Cat}\left(\mathbf{x}_t^{\ell} ; \alpha_t \mathbf{x}^{\ell}+\left(1-\alpha_t\right) \mathbf{m}\right)\] <ul> <li>$ \alpha_0=1 $ â†’ linear schedulerâ†’ $ \alpha_1=0 $</li> </ul> </li> <li> <p><span style="color:yellow_background">ëª©ì  í•¨ìˆ˜ (Sahoo et al. (2024b)ì˜ SUBS-parameterization denoising ëª¨ë¸ ì² í•™ì„ ë”°ë¦„!!)</span></p> <ul> <li> <p><strong>Zero Masking Probabilities</strong>: clean sequence( $ x^\ell $)ì—ëŠ” maskë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ. (ì´ê±´ ì•„ë˜ì˜ ì¡°ê±´ì„ ìœ„í•´ í•„ìš”í•œë“¯í•©ë‹ˆë‹¤.)</p> </li> <li> <p><strong>Carry-Over Unmasking</strong>: $ x_t^\ell \neq m $ì¸ ê²½ìš° $ q\left(x_s^l=x_t^l \mid x_t^l \neq m\right)=1 $. ì¦‰, unmaksedëœ tokenì€ ë‹¤ì‹œ mask ë˜ì§€ ì•ŠìŒ.</p> <ul> <li>Denoising model ë‹¨ìˆœí™”: $ p_\theta\left(x_s^{\ell}=x_t^{\ell} \mid x_t^{\ell} \neq m\right)=1 $</li> </ul> </li> </ul> \[-\log p_\theta(\mathbf{x}) \leq \mathcal{L}_{\mathrm{BD}}(\mathbf{x} ; \theta):=\sum_{b=1}^B \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q \frac{\alpha_t^{\prime}}{1-\alpha_t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\] <ul> <li> <p>$ \alpha_t = \prod_{\tau=1}^{t}(1 - \beta_\tau) $: tì‹œì ê¹Œì§€ maskë˜ì§€ ì•Šê³  ì‚´ì•„ë‚¨ì„ í™•ë¥ </p> </li> <li> <p><span style="color:yellow_background">why?</span></p> <ul> <li> <p>t ì‹œì ì—ì„œ mask transition matrix (noising ê³¼ì •ì—ì„œ iâ†’ jë¡œ ë³€í™˜)</p> \[\left[Q_t\right]_{i j}= \begin{cases}1 &amp; \text { if } i=j=m \\ \alpha_t &amp; \text { if } i=j \neq m \\ 1-\alpha_t &amp; \text { if } j=m, i \neq m\end{cases}\] <ul> <li> <p>ìˆœì„œëŒ€ë¡œ maskëŠ” mask ìœ ì§€</p> </li> <li> <p>ê°’ì„ ê·¸ëŒ€ë¡œ ê°€ì§ˆ í™•ë¥ : $ \alpha_t $</p> </li> <li> <p>tokenì´ mask ë  í™•ë¥ : $ 1 - \alpha_t $</p> </li> </ul> </li> <li> <table> <tbody> <tr> <td>marginal $ Q_{t</td> <td>s} $ (ì—¬ê¸°ì„œ $ \alpha_{t</td> <td>s} = \alpha_t/\alpha_s $)</td> </tr> </tbody> </table> \[\left[Q_{t \mid s}\right]_{i j}= \begin{cases}1 &amp; \text { if } i=j=m \\ \alpha_{t \mid s} &amp; \text { if } i=j \neq m \\ 1-\alpha_{t \mid s} &amp; \text { if } j=m, i \neq m\end{cases}\] </li> </ul> <p>ì „ê°œâ€¦â€¦ $ \mathcal{L}_{\text{diffusion}} $ì€ ì•ì˜ ìˆ˜ì‹ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ê°™ìŠµë‹ˆë‹¤â€¦..</p> \[\mathcal{L}_{\text{diffusion}} = \sum_{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \text{D}_{\text{KL}} \left[ q(\mathbf{x}_s^b|\mathbf{x}_t^b, \mathbf{x}^b) \Vert p_{\theta}(\mathbf{x}_s^b|\mathbf{x}_t^b, \mathbf{x}^{&lt;b}) \right] \right] \\= \sum_{b=1}^{B} \mathbb{E}_t \mathbb{E}_q T \left[ \sum_{\ell=1}^{L'} \text{D}_{\text{KL}} \left[ q(\mathbf{x}_s^{b,\ell}|\mathbf{x}_t^{b,\ell}, \mathbf{x}^{b,\ell}) \Vert p_{\theta}(\mathbf{x}_s^{b,\ell}|\mathbf{x}_t^b, \mathbf{x}^{&lt;b}) \right] \right]\] <ul> <li> <p>ì¼ë‹¨ ì—¬ê¸°ê¹Œì§„ ì •ì˜ëŒ€ë¡œ ê°€ë˜ block ë‚´ token ê¸¸ì´ì¸ $ Lâ€™ $ìœ¼ë¡œ í™•ì¥</p> </li> <li> <p>KL divergence ì •ì˜ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œ ê°€ëŠ¥ (ì´ê±´ ã„¹ã…‡ KLD ì •ì˜)</p> \[\sum_{\ell=1}^{L^{\prime}} D_{\mathrm{KL}}\left(q \| p_\theta\right)=\sum_{\ell=1}^{L^{\prime}} \mathbb{E}_{q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)}\left[\log q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)-\log p_\theta\left(x_s^{b, l} \mid x_t^{b, l}, x^{&lt;b}\right)\right]\] </li> <li> <p>$ \log{q} $ ë¶€ë¶„ì€ í•™ìŠµê³¼ ë¬´ê´€í•˜ë¯€ë¡œ ì œì™¸</p> \[\sum_{\ell=1}^{L^{\prime}} \mathbb{E}_{q\left(x_s^{b, l} \mid x_t^{b, l}, x^{b, l}\right)}\left[-\log p_\theta\left(x_s^{b, l} \mid x_t^{b, l}, x^{&lt;b}\right)\right]\] <ul> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = x^{b,\ell}</td> <td>x_t^{b,\ell} = m, x^{b,\ell}) = \frac{\alpha_s - \alpha_t}{1 - \alpha_t} $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = m</td> <td>x_t^{b,\ell} = m, x^{b,\ell}) = \frac{1 - \alpha_s}{1 - \alpha_t} $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ q(x_s^{b,\ell} = x^{b,\ell}</td> <td>x_t^{b,\ell} = x^{b,\ell}, x^{b,\ell}) = 1 $: 1ì´ë¯€ë¡œ ë’¤ì— ê³„ì‚°ì—ì„œ ì œì™¸</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>$ x_t^{b,\ell} $ì´ maskì¸ ê²½ìš°ë§Œ ê³„ì‚°</p> \[\frac{\alpha_s - \alpha_t}{1 - \alpha_t}[- \log p_\theta(x_s^{b,\ell} = x^{b,\ell} | x_t^{b,\ell} = m, x^{&lt;b})] + \frac{1 - \alpha_s}{1 - \alpha_t}[- \log p_\theta(x_s^{b,\ell} = m | x_t^{b,\ell} = m, x^{&lt;b})]\] </li> <li> <p>ë’¤ì— í•­ì€ mask â†’ maskëŠ” ìƒìˆ˜ë¼ì„œ ê³„ì‚°ì—ì„œ ì œì™¸</p> <table> <tbody> <tr> <td>$ = \sum_{b=1}^{B} \mathbb{E}<em>t \mathbb{E}_q T \left[ \sum</em>{\ell=1}^{Lâ€™} \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p_\theta(x^{b,\ell}</td> <td>x_t^{b,\ell}, x^{&lt;b}) \right] $</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$ = \sum_{b=1}^{B} \mathbb{E}<em>t \mathbb{E}_q T \left[ \frac{\alpha_t - \alpha_s}{1 - \alpha_t} \log p</em>\theta(x^b</td> <td>x_t^b, x^{&lt;b}) \right] $</td> </tr> </tbody> </table> </li> </ul> <p>$ T \rarr \infin, T(\alpha_t - \alpha_s) = \alphaâ€™_t $</p> </li> </ul> </li> </ul> <p><br/></p> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>$ L^\prime $ = 1ì¸ ê²½ìš°, MASKED BD3-LMSì˜ ëª©ì í•¨ìˆ˜ëŠ” autoregressive NLLê³¼ ë™ë“±í•¨.</p> <ul> <li> <p><span style="color:yellow_background"><strong>ì§ê´€ì  í•µì„</strong></span>: blockì˜ ê¸¸ì´ê°€ 1ì´ë¼ë©´ í•œ í† í° ë‹¨ìœ„ ARê³¼ ê°™ìŒ. â†’ ??? ê·¸ë˜ë„ í•œ í† í° ë‹¨ìœ„ë¡œ ì¼ì–´ë‚˜ëŠ” diffusion ê³¼ì •ì´ ìˆëŠ”ë°? â†’ maskë¡œ intitialize í›„, ì›í•˜ëŠ” ë‹¤ìŒ tokenì„ ì°¾ëŠ” ê³¼ì •ì´ë€ ì ì—ì„  ë™ì¼.</p> </li> <li> <p><strong>ìˆ˜ì‹ ok</strong></p> \[\begin{aligned}&amp;-\log p(\mathbf{x}) \leq \sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q\left[\frac{\alpha_t^{\prime}}{1-\alpha_t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] \\&amp; \because \alpha_t^{\prime}=-1 \text { and } \alpha_t=1-t, \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \mathbb{E}_q {\left[\frac{1}{t} \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] } \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \frac{1}{t} \mathbb{E}_q\left[\log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b, \mathbf{x}^{&lt;b}\right)\right] \\&amp; \text { Expanding } \mathbb{E}_q[.], \\&amp;=-\sum_{b=1}^L \mathbb{E}_{t \sim[0,1]} \frac{1}{t} {\left[q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right) \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right)\right.} \\&amp;\left.\quad+q\left(\mathbf{x}_t^b=\mathbf{x}^b \mid \mathbf{x}^b\right) \log p_\theta\left(\mathbf{x}^b \mid \mathbf{x}_t^b=\mathbf{x}^b, \mathbf{x}^{&lt;b}\right)\right]\end{aligned}\] <ul> <li> <p>linear schedulerì—ì„œ $ \alphaâ€™_t, \alpha_t $ì˜ ì •ì˜ëŠ” ìœ„ì™€ ê°™ìŒ. ê·¸ ë‹¤ìŒ ì „ê°œ ê³¼ì •ì€ ì´í•´í•  ìˆ˜ ìˆì„ë“¯?</p> </li> <li> <p>Expanding ë¶€ë¶„ì€ Expatation of që¥¼ ì œê±° í•˜ê¸° ìœ„í•œ ê³¼ì • qê°€ mask transitionì„ ì „ì œë¡œ í•˜ë¯€ë¡œ ê²½ìš° (mask/unmask) ë‘ ê°€ì§€ í™•ë¥ ì— ëŒ€í•´ì„œ ì „ê°œ</p> </li> <li> <table> <tbody> <tr> <td>SUBS-parameterization ê°€ì •ì˜ carry-over unmasking íŠ¹ì„±ìœ¼ë¡œ $ \log{p_\theta(x^b</td> <td>x_t^b=x^b, x^{&lt;b})} = 0 $</td> </tr> </tbody> </table> <p>$$ \begin{aligned}-\log p_\theta(\mathbf{x}) &amp; \leq-\sum_{b=1}^L \mathbb{E}<em>{t \sim[0,1]} \frac{1}{t} q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right) \log p</em>\theta\left(\mathbf{x}^b \mid \mathbf{x}<em>t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right) \&amp; \because q\left(\mathbf{x}_t^b=\mathbf{m} \mid \mathbf{x}^b\right)=t, \text { we get: } \&amp; =-\sum</em>{b=1}^L \mathbb{E}<em>{t \sim[0,1]} \log p</em>\theta\left(\mathbf{x}^b \mid \mathbf{x}<em>t^b=\mathbf{m}, \mathbf{x}^{&lt;b}\right)\&amp; = -\sum</em>{b=1}^{L} \log p_\theta(\mathbf{x}^b \mid \mathbf{m}, \mathbf{x}^{&lt;b})</p> </li> </ul> </li> </ul> </li> </ul> <p>\end{aligned} $$</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>		- $ q(x_t^b=m|x^b) = 1 - \alpha_t = 1 - (1 - t) = t $

		- $ t $ëŠ” ìƒê´€ì—†ìœ¼ë‹ˆê¹ ì‚­ì œ!

		- ìµœì¢… ê²°ê³¼ëŠ” NLL ë¡œìŠ¤ì™€ ê¸°ëŒ€ê°’ì´ ê°™ë‹¤!
</code></pre></div></div> <ul> <li> <p>í•™ìŠµ ëª©í‘œì˜ ê¸°ëŒ€ê°’ì´ ê°™ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  perplexity gap (=ë†’ì€ í•™ìŠµ variance)ê°€ ì¡´ì¬í•¨ì„ í™•ì¸</p> </li> <li> <table> <tbody> <tr> <td>ì™œ ê·¸ëŸ´ê¹Œ? $ \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) $ = 0.5 ê¸°ë³¸ì ìœ¼ë¡œ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” tokenì˜ ìˆ˜ê°€ ì ˆë°˜ìœ¼ë¡œ ì¤„ê¸° ë•Œë¬¸ì— varianceê°€ ì»¤ì§€ëŠ” ê²ƒ</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>tuned schedule: $ q(x_t^\ell = m</td> <td>x^\ell) $ = 1</td> </tr> </tbody> </table> <ul> <li> <p>í•´ë‹¹ scheduleì—ì„œëŠ” ARì˜ ëª©ì í•¨ìˆ˜ì™€ ì™„ì „íˆ ë™ì¼</p> </li> <li> <p>PPLë„ ê°ì†Œ, NELBOì˜ ë¶„ì‚°ë„ ê°ì†Œ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  &lt;br/&gt;
</code></pre></div> </div> </li> </ul> </li> </ul> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case studyë¥¼ ë„˜ì–´ $ L^\ell \geq 1 $ì¸ ì¼€ì´ìŠ¤ë¡œ í™•ì¥í•˜ê³  ì‹¶ìŒ!</p> <ul> <li> <p>NELBOëŠ” ì´ë¡ ì ìœ¼ë¡œ tì— invariance (ê¸°ì¡´ ì—°êµ¬ <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/eb0b13cc515724ab8015bc978fdde0ad-Paper-Conference.pdf">ref</a>: Tê°€ ë¬´í•œíˆ ì»¤ì§ˆìˆ˜ë¡ $ \alpha $ê°’ì´ ì•„ë‹Œ ëˆ„ì ê°’ì— ì˜í•´ì„œ ê¸°ëŒ€ê°’ì´ ì •ì˜ë˜ê¸° ë•Œë¬¸â€¦ ì´ ì´ìƒì˜ ì´í•´ëŠ” í¬ê¸°)í•˜ê¸°ì— ìŠ¤ì¼€ì¤„ì— ë”°ë¥¸ ê¸°ëŒ€ê°’ì˜ ë³€í™”ê°€ ì—†ì–´ì•¼ í•¨.</p> </li> <li> <p>í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ëª¨ë“  ì—°ì‚°ì„ í•œ ë²ˆì— í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ Batch ì—°ì‚°ì„ í™œìš© â†’ ì´ë¡ ì ì¸ invarianceê°€ ê¹¨ì§</p> </li> </ul> <p>â†’ Scheduleì— ë”°ë¼ ë¶„ì‚°ì˜ ê²°ê³¼ê°€ ë³€í•˜ê²Œ ë¨. â†’ Scheduleì„ ì˜ ë§Œë“¤ì–´ë³´ì!</p> </li> <li> <p>Batch sizeë¥¼ $ K $ë¼ê³  í•  ë•Œ, batch of sequence $ \text{X} = [x^{(1)},x^{(1)},â€¦,x^{(K)}] $, with each $ \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x) $</p> </li> <li> <p><span style="color:yellow_background"><strong>NELBO estimator</strong></span></p> \[\mathcal{L}_{\text{BD}}(\mathbf{X};\theta) := l(\mathbf{X};\theta) = \frac{1}{K} \sum_{k=1}^{K} \sum_{b=1}^{B} \frac{\alpha'_{t(k,b)}}{1 - \alpha_{t(k,b)}} \log p_{\theta} \left( \mathbf{x}^{(k),b} \mid \mathbf{x}_{t(k,b)}^{(k),b}, \mathbf{x}^{(k),&lt;b} \right)\] </li> <li> <p><span style="color:yellow_background"><strong>Variance of the gradient estimator</strong></span></p> \[\text{Var}_{\mathbf{X},t} \left[ \nabla_{\theta}l(\mathbf{X};\theta) \right] \approx \frac{1}{M-1} \sum_{m=1}^{M} \left\| \nabla_{\theta}l(\mathbf{X}^m;\theta) - \frac{1}{M} \sum_{m=1}^{M} \nabla_{\theta}l(\mathbf{X}^m;\theta) \right\|_2^2\] </li> </ul> <p><br/></p> <h2 id="low-variance-noise-schedules-for-bd3-lms">LOW-VARIANCE NOISE SCHEDULES FOR BD3-LMS</h2> <h3 id="intuition-avoid-extreme-mask-rates--clipped-schedules-for-low-variance-gradients">INTUITION: AVOID EXTREME MASK RATES &amp; CLIPPED SCHEDULES FOR LOW-VARIANCE GRADIENTS</h3> <ul> <li> <p>ì´ìƒì ì¸ ë§ˆìŠ¤í‚¹: ëª¨ë¸ì´ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ë…¸ì´ì¦ˆ [MASK]ì—ì„œ ì›ë˜ëŒ€ë¡œ ë˜ëŒë¦¬ëŠ” ë²•ì„ ë°°ìš°ëŠ” ê²ƒ</p> </li> <li> <p>ê·¹ë‹¨ì ì¸ ë§ˆìŠ¤í‚¹</p> <ul> <li> <p>ë§ˆìŠ¤í‚¹ í† í°ì´ ë„ˆë¬´ ì ì„ ê²½ìš°, ë„ˆë¬´ ì‰¬ìš´ ë¬¸ì œë¥¼ í’€ê²Œ ë¨.</p> </li> <li> <p>ëª¨ë“  í† í°ì´ ë§ˆìŠ¤í‚¹ ë  ê²½ìš°, ë¬¸ë§¥ ì •ë³´ê°€ ì „í˜€ ì—†ìŒ ë¹ˆë„ì— ê¸°ë°˜í•œ í•™ìŠµë§Œ ì§„í–‰</p> </li> </ul> </li> </ul> <p>â†’ ê·¹ë‹¨ì ì¸ ë¶€ë¶„ì„ ë‚ ë¦° CLIPì„ ì´ìš©í•˜ì</p> <p>â†’ sample mask rates: $ 1 - \alpha_t \sim \mathcal{U}[\beta, \omega] $ for $ 0 \leq \beta, \omega \leq 1 $</p> <p><br/></p> <h3 id="data-driven-clipped-schedules-across-block-sizes">DATA-DRIVEN CLIPPED SCHEDULES ACROSS BLOCK SIZES</h3> <ul> <li> <p>Block size ( $ Lâ€™ $)ì— ë”°ë¥¸ ìµœì ì˜ mask rateì„ ì°¾ì•„ë³´ì.</p> </li> <li> <p>Gradient ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•¨ì´ì§€ë§Œ ì•„ë˜ NELBOë¥¼ ì¶”ì •ì§€ë¡œ í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰</p> \[\text{min}_{\beta,\omega} \text{Var}_{X,t}[\mathcal{L}(X; \theta, \beta, \omega)]\] <ul> <li> <p>forward passë§Œìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼ë“¤ì—ì„œ NELBOì™€ ê¸°ìš¸ê¸° ë¶„ì‚°ì´ ê°™ì€ ê²½í–¥ì„±ì„ ë³´ì„ì„ í™•ì¸</p> </li> </ul> </li> <li> <p>$ \beta, \omega $ì— ëŒ€í•´ grid search ì§„í–‰</p> </li> <li> <p>Table 2ì—ì„œ PPLê³¼ NELBOê³¼ ìƒê´€ì„± ë³´ì„ì„ ì¬ì°¨ í™•ì¸ + $ Lâ€™ $ì— ë”°ë¼ ìµœì ì˜ ì¡°í•©ì´ ìˆìŒì„ ë°œê²¬í•¨.</p> </li> </ul> <p><br/></p> <h2 id="experiments">EXPERIMENTS</h2> <ul> <li> <p>Pre-train: base BD3-LM ( $ Lâ€™=L $) for 850K gradient steps (ìˆœìˆ˜ diffusion?)</p> </li> <li> <p>Fine-tune</p> <ul> <li>150K gradient steps on One Billion Words dataset (LM1B) and OpenWebText (OWT)</li> </ul> </li> <li> <p>$ Lâ€™ $ì— ë”°ë¼ ë‹¤ë¥¸ Clipped schedule ì ìš© (ë§¤ validation epoch ë§ˆë‹¤ ìµœì ì˜ $ \beta, \omega $ ì¡°í•©ì„ ì°¾ìŒ!)</p> </li> </ul> <p><br/></p> <h3 id="likelihood-evaluation">LIKELIHOOD EVALUATION</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

		[//]: # (column is not supported)

	- ë‹¤ë¥¸ MDLM ëª¨ë¸ ëŒ€ë¹„ perplexityì´ í–¥ìƒë¨
</code></pre></div></div> <ul> <li> <p>Zero-shot validation perplexity ê²°ê³¼ PubmedëŠ” ARë³´ë‹¤ë„ ì˜í•¨.</p> </li> <li> <p>ëŒ€ì²´ë¡œ ë‹¤ë¥¸ MDLMë³´ë‹¨ PPL ê°’ì´ ë” ë‚®ìŒ.</p> </li> </ul> <p><br/></p> <h3 id="sample-quality-and-variable-length-sequence-generation">SAMPLE QUALITY AND VARIABLE-LENGTH SEQUENCE GENERATION</h3> <ul> <li> <p>[EOS] í† í°ì„ ìƒì„±í•˜ê±°ë‚˜ sample qualityê°€ ê¸‰ê° (the average entropy of the the last 256-token chunk is below 4)í•  ë•Œê¹Œì§€ ì‹¤í—˜ ì§„í–‰</p> </li> <li> <p>SEDD ëŒ€ë¹„ ìµœëŒ€ 10ë°° ë” ê¸´ text ìƒì„± ê°€ëŠ¥í•¨.</p> </li> <li> <p>GPT-2ë¥¼ ì´ìš©í•´ generative perplexity ì¸¡ì •, íš¨ìœ¨ì„±ì„ ë³´ê¸° ìœ„í•´ the number of generation steps (NFEs)</p> </li> <li> <p>ê¸°ì¡´ Block Diffusion ëŒ€ë¹„í•´ë„ ë” ì ì€ stepì—ì„œ ë†’ì€ Gen PPL ë‹¬ì„±</p> </li> <li> <p>ì •ì„± ë¶„ì„ì€ Appendix Dì— ìˆìŒ. ARê³¼ ìœ ì‚¬í•  ì •ë„ì˜ í€„ë¦¬í‹°, ë‹¤ë¥¸ DLMë³´ë‹¨ ì¢‹ë”ë¼</p> </li> </ul> <p><br/></p> <h3 id="ablations">ABLATIONS</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

	&lt;span style='color:yellow_background'&gt;**SELECTING NOISE SCHEDULES TO REDUCE TRAINING VARIANCE**&lt;/span&gt;

			- $ L' $ì´ ì‘ì„ìˆ˜ë¡ heavier maskê°€ íš¨ê³¼ì 

[//]: # (column is not supported)

	&lt;span style='color:yellow_background'&gt;**EFFICIENCY OF TRAINING ALGORITHM**&lt;/span&gt;

			- concat í™œìš©í•˜ì—¬ ì²˜ë¦¬í•  ê²½ìš°, sparse attention mask í™œìš©

	- FlexAttentionì„ ì´ìš©í•  ê²½ìš° Sparsityë¥¼ í™œìš©í•´ íš¨ìœ¨ì  ì²˜ë¦¬ ê°€ëŠ¥

	- 20-25% ì†ë„ í–¥ìƒ ê°€ëŠ¥!
</code></pre></div></div> <p><br/></p> <p><br/></p> <hr/> <ul> <li> <p>ìˆ˜í•™ ê³µë¶€ ì—´ì‹¬íˆ í•˜ì.</p> </li> <li> <p>ê²°ê³¼ì—ì„œ í˜ì´ ë§ì´ ë¹ ì§€ê¸´ í•œë‹¤.</p> </li> <li> <p>ì „ê°œê³¼ì •ì—ì„œ ì´ ì •ë„ëŠ” í•´ì•¼ oralë¡œ ê°€ëŠ”êµ¬ë‚˜ ë²½ëŠê»´ì§„ë‹¤.</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><category term="nlp"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/impact-of-fine-tuning-methods-on-memorization-in-large-language-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>ë§ì€ ì—°êµ¬ë“¤ì´ LLMì´ ì‚¬ì „í•™ìŠµ ë‹¨ê³„ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ì™¸ìš°ëŠ” ì´ìŠˆì— ëŒ€í•´ì„œ ë³´ê³ í•˜ê³  ìˆëŠ” í•œí¸, finetuningì— ëŒ€í•´ì„œ ë¹„ìŠ·í•œ ì—°êµ¬ëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ ì ë‹¤.</p> <p>í•˜ì§€ë§Œ, finetuningë„ ë‹¹ì—°íˆ ëª¨ë¸ ëŒ€ë¶€ì˜ ì—…ë°ì´íŠ¸ì™€ ë•Œë•Œë¡œëŠ” êµ¬ì¡°ì ì¸ ë³€í™”ê¹Œì§€ë„ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì—, finetuningì˜ memorization levelì— ëŒ€í•œ ì—°êµ¬ë„ í•„ìš”í•˜ë‹¤.</p> <p>ê·¸ë ‡ë‹¤ë©´, ì¡´ì¬í•˜ëŠ” ë‹¤ì–‘í•œ finetuning ë°©ë²•ì— ë”°ë¥¸ memorization of fineuning dataì˜ ì˜í–¥ë ¥ì€ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?</p> <p><br/></p> <p>í•´ë‹¹ ì—°êµ¬ëŠ” ì´ë¥¼ ì‹œí—˜í•˜ê¸° ìœ„í•´ ìš°ì„  finetuning ë°©ë²•ì„ í¬ê²Œ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„í•œë‹¤:</p> <ol> <li> <p>Parameter-based finetuning: ëª¨ë¸ íŒŒë¼ ë°”ê¿ˆ</p> </li> <li> <p>Prompt-based fine-tuning: ëª¨ë¸ íŒŒë¼ ê³ ì •, soft token/prefix embeddingâ€¦</p> </li> </ol> <p><br/></p> <p>ê²°ê³¼ì ìœ¼ë¡œ ë‘ ì¹´í…Œê³ ë¦¬ë¥¼ ê³ ë£¨ í¬í•¨í•œ 5ê°€ì§€ ë°©ë²•ì„ ì‹œí—˜í–ˆê³ ,</p> <p>í‰ê°€ëŠ” ë‹¤ì–‘í•œ MIAs(membership inference attacks )ë¡œ í–ˆê³ ,</p> <p>ë°ì´í„°ëŠ” Wikitext, WebNLG, Xsum ì„¸ ê°€ì§€ë¡œ í–ˆë‹¤ (ì¢€ ì ê¸´í•˜ë„¤ìš”)</p> <p><br/></p> <p>ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤</p> <ul> <li> <p>Parameter-based fine-tuning</p> <ul> <li> <p>Model Head Tuning (FT head): fine-tunes only the final output layer</p> </li> <li> <p>Low-Rank Adaptation (LoRA) (Hu et al., 2021)</p> </li> </ul> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> <ul> <li> <p><strong>Prefix Tuning</strong></p> <ul> <li>ê° attention layerì˜ key/valueì— í•™ìŠµ ê°€ëŠ¥í•œ prefix ë²¡í„° ì¶”ê°€.</li> </ul> </li> <li> <p><strong>Prompt Tuning</strong></p> <ul> <li>ëª¨ë¸ ì…ë ¥ ì„ë² ë”© ì•ì— í•™ìŠµ ê°€ëŠ¥í•œ ì—°ì†í˜• í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì¶”ê°€.</li> </ul> </li> <li> <p><strong>P-tuning</strong></p> <ul> <li>ë³„ë„ì˜ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµí•œ ì—°ì†í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ì— ì‚½ì….</li> </ul> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p>ì‚¬ìš©ëœ MIA ê¸°ë²•ê³¼ ì ìˆ˜ ê³„ì‚° ë°©ì‹:</p> <ol> <li> <p><strong>LOSS</strong> (Yeom et al., 2018)</p> <ul> <li> <p>Membership Score = ëª¨ë¸ì˜ ì†ì‹¤</p> <p>$ \text{Score} = L(x, M_t) $</p> <p>(ì†ì‹¤ì´ ë‚®ì„ìˆ˜ë¡ ë©¤ë²„ì¼ ê°€ëŠ¥ì„± â†‘)</p> </li> </ul> </li> <li> <p><strong>Reference-based (Ref)</strong> (Mireshghallah et al., 2022a)</p> <ul> <li> <p>ê¸°ì¤€ ëª¨ë¸ MrM_rMrì™€ ë¹„êµí•˜ì—¬ ì†ì‹¤ ì°¨ì´ ê³„ì‚°</p> <p>$ \text{Score} = L(x, M_t) - L(x, M_r) $</p> </li> </ul> </li> <li> <p><strong>Zlib Entropy (Zlib)</strong> (Carlini et al., 2021)</p> <ul> <li> <p>ì†ì‹¤ì„ zlib ì—”íŠ¸ë¡œí”¼ë¡œ ë‚˜ëˆˆ ë¹„ìœ¨</p> <p>$ \text{Score} = \frac{L(x, M_t)}{\text{zlib}(x)} $</p> </li> </ul> </li> <li> <p><strong>Min-K%</strong> (Shi et al., 2024)</p> <ul> <li> <p>í† í° í™•ë¥ ì´ ë‚®ì€ í•˜ìœ„ k% í† í°ë“¤ì˜ í‰ê·  ë¡œê·¸ likelihood</p> <table> <tbody> <tr> <td>$ \text{Score} = \frac{1}{E} \sum_{x_i \in \text{Min-}K\%(x)} \log p(x_i</td> <td>x_{&lt;i}) $</td> </tr> </tbody> </table> </li> </ul> </li> </ol> </li> </ul> <p><br/></p> <ul> <li> <p>ë°ì´í„°</p> <ul> <li> <p>Wikitext-2-raw-1</p> </li> <li> <p>WebNLG</p> <ul> <li>tripleë¡œ ì´ë£¨ì–´ì§ (Subject-Predicate-Object)</li> </ul> </li> <li> <p>Xsum: ìš”ì•½</p> <ul> <li>finetuningì— 5000ê°œë§Œ ì‚¬ìš©</li> </ul> </li> </ul> <p><br/></p> </li> <li> <p>í‰ê°€</p> <ul> <li>training and test setsì—ì„œ ìƒ˜í”Œë§</li> </ul> <p><br/></p> </li> <li> <p>ëª¨ë¸</p> <ul> <li> <p>LLaMA 2-7B (Touvron et al., 2023)</p> </li> <li> <p>GPT2-series (Radford et al., 2019)</p> </li> <li> <p>LLaMA 3-1B</p> </li> </ul> <p>â†’ 2025ì˜ ë…¼ë¬¸ì´ë¼ê³  ë¯¿ê¸°ì§€ ì•ŠëŠ”êµ°ì—¬!</p> <p><br/></p> </li> <li> <p>Evaluation Metrics</p> <ul> <li> <p>PERF: validation PPL as the primary metric</p> </li> <li> <p>MIA: AUC-ROC</p> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p>Implementation Details</p> <ul> <li> <p>15 epoch</p> </li> <li> <p>ëª¨ë“  ì„¸íŒ…ì€ ë…¼ë¬¸ì— ë‚˜ì˜¨ê±° ê·¸ëŒ€ë¡œ ë”°ë¼í•¨</p> </li> <li> <p>4090ì´ë‘ H100 í•œëŒ€ ì‚¬ìš©</p> </li> </ul> </li> </ul> <p><br/></p> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <p><br/></p> <p><br/></p> <blockquote> <p>Observation â™¯1: (ë‹¹ì—°)</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter-based fine-tuning demonstrates a higher tendency to explicitly memorize training data.
</code></pre></div></div> <p><br/></p> <p>ëª¨ë“  ë°©ë²•ë¡ ì€ validation PPLê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ ì¢‹ì•˜ìŒ.</p> <p>í•˜ì§€ë§Œ, prompt-based methods ëŠ” parameter-based ë³´ë‹¤ ì™¸ìš°ëŠ” ì„±ëŠ¥ ë–¨ì–´ì§ (ë‹¹ì—°)</p> <p><br/></p> <blockquote> <p>Observation â™¯2:</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Parameter-based fine-tuning exhibits increasing memorization over training epochs, while prompt-based fine-tuning maintains consistently low memorization throughout training.
</code></pre></div></div> <p><br/></p> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the modelâ€™s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <p><br/></p> <ul> <li> <p><strong>Prefix Tuning ìˆ˜ì‹ (Petrov et al., 2024)</strong></p> <p>â€…$ â€Št^{pt}<em>i = A^{pt}</em>{i0} W_V S_1 + (1 - A^{pt}_{i0})\; t_i $</p> <ul> <li> <p>soft-prefixê°€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ $ A^{pt} $ë¥¼ â€˜ì–´ë””ë¥¼ ë³¼ì§€â€™ë§Œ ì¬ì¡°ì •, <strong>ë³¸ë˜ í† í° ê°„ ìƒëŒ€ ë¶„í¬ëŠ” ê·¸ëŒ€ë¡œ</strong>.</p> </li> <li> <p>ì¦‰ <strong>ìƒˆë¡œìš´ attention íŒ¨í„´ì„ í•™ìŠµ</strong>í•˜ê¸°ë³´ë‹¤ëŠ” <strong>ê¸°ì¡´ ëŠ¥ë ¥ì„ ì¬í™œìš©</strong>.</p> </li> </ul> </li> <li> <p>ê²°ê³¼ì ìœ¼ë¡œ <strong>í‘œí˜„ ê³µê°„ì˜ ì´ë™(shift) &lt; ì ìŒ</strong> â†’ í•™ìŠµ, ë¹„í•™ìŠµ ìƒ˜í”Œ ë¶„í¬ ì°¨ì´ê°€ ì‘ì•„ MIAê°€ ì–´ë µë‹¤.</p> <ul> <li>Petrov et al. (2024) prove that the presence of a prefix does not alter the relative distribution of the input but only shifts the attention to different content.</li> </ul> </li> </ul> <p><br/></p> <p>ì´ ê°€ì„¤ì„ í™•ì¸í•˜ê¸° ìœ„í•´:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7Bë¥¼ ì„¸ ì„¸íŒ…ì—ì„œ ë¹„êµí•¨:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <p>LoRAëŠ” membership and non-membership samples ì‚¬ì´ ë¶„í¬ ì°¨ì´ê°€ í°ë°, prefix tuningì€ ë¯¸ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ</p> <p><br/></p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <p><br/></p> <p>ë‘ ë°©ë²•ë¡ ì´ ìµœì¢…ì ìœ¼ë¡œëŠ” ë¹„ìŠ·í•œ PPLì„ ê°€ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , Learning trajactoriesëŠ” ê½¤ë‚˜ ë‹¬ëìŒ</p> <p><br/></p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>ì´ëŠ” ì•„ê¹Œë„ ì´ì•¼ê¸° í–ˆë“¯ì´, í›„ìê°€ internal sample distribution of the modelì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— ìª¼ë” ë” ë‚˜ì€ biasë¥¼ ì¶”ê°€í•˜ëŠ” ì •ë„ì„ì„ ë‹¤ì‹œí•œë²ˆ ë³´ì¸ë‹¤</p> <p><br/></p> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ memorizationì— ì¤‘ìš”í•œ ì˜í–¥ë ¥ì„ ì¤„ ê²ƒì„.</p> <p>â†’ To what extent does model size influence memorization under different fine-tuning strategies?</p> <p><br/></p> <p><br/></p> <blockquote> <p>Observation â™¯3</p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model size significantly enhances memorization in parameter-based fine-tuning methods, while prompt-based methods show minimal sensitivity and maintain consistently low memorization.
</code></pre></div></div> <p><br/></p> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p><br/></p> <p>LLaMA2-7B vs LLaMA3-1B</p> <p><br/></p> <h2 id="-ìš”ì•½-íŒŒë¼ë¯¸í„°-ë°”ê¾¸ëŠ”-ì• ë“¤ì€-ëª¨ë¸-í¬ê¸°-ì»¤ì§ˆìˆ˜ë¡-ë”-ì˜-ì™¸ì› ëŠ”ë°-ë°˜ëŒ€ëŠ”-ë¯¸ë¯¸í•˜ë”ë¼-low-sensitivity-of-prompt-tuning-to-model-scale">ğŸ“ ìš”ì•½ íŒŒë¼ë¯¸í„° ë°”ê¾¸ëŠ” ì• ë“¤ì€ ëª¨ë¸ í¬ê¸° ì»¤ì§ˆìˆ˜ë¡ ë” ì˜ ì™¸ì› ëŠ”ë° ë°˜ëŒ€ëŠ” ë¯¸ë¯¸í•˜ë”ë¼ (low sensitivity of prompt tuning to model scale)</h2> <p>íŠ¹íˆ, gpt2ì˜ ê²½ìš°ë‚˜ 1B ìŠ¤ì¼€ì¼ì—ì„œ LoRAëŠ” ì‚¬ì‹¤ìƒ ê±°ì˜ ëª»ì™¸ì›€</p> <p><br/></p> <p><br/></p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation â™¯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p><br/></p> <p>ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì˜ ì¢…ë¥˜ì— ë”°ë¼ì„œë„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì´ë¥¼ ìœ„ LLaMA2-7Bë¥¼ ë‹¤ì–‘í•œ ë°©ë²•ì„ í†µí•´ í•™ìŠµì‹œí‚¤ê³  LOSS attack againstì— ëŒ€í•´ì„œ ê°ê°ì„ í‰ê°€í•´ë´„</p> <p><br/></p> <p><br/></p> <p>Prompt-based ë§Œ ë´¤ì„ ë•Œ, WebNLGê°€ ë‹¤ë¥¸ ê²ƒë“¤ì— ë¹„í•´ì„œ ì„±ëŠ¥ì´ ë†’ë‹¤</p> <p>ì•„ë§ˆë„ êµ¬ì¡°í™”ëœ patterní•™ìŠµì—ëŠ” ìœ ë¦¬í•œ ê²ƒ ê°™ë‹¤</p> <p><br/></p> <h2 id="impact-of-lora-placement-on-memorization">Impact of LoRA Placement on Memorization</h2> <p>AUCâ†‘ â‡’ ê¸°ì–µ(privacy risk)â†‘</p> <p><br/></p> <ol> <li> <p><strong>Projection &gt; Attention</strong></p> <ul> <li>LoRAë¥¼ <strong>projection layer</strong>ì—ë§Œ ì ìš©í•  ë•Œ, ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ë„¤ ê°€ì§€ MIA ì§€í‘œì—ì„œ <strong>AUCê°€ ì¼ê´€ë˜ê²Œ ìƒìŠ¹</strong> â†’ ê¸°ì–µì´ ë” ê°•í•´ì§.</li> </ul> </li> <li> <p><strong>Both layers = ê¸°ì–µ ì œì¼ ê°•í•¨</strong></p> <ul> <li>Attention + Projection ë™ì‹œ ì ìš© ì‹œ <strong>ê°€ì¥ ë†’ì€ AUC</strong> â†’ ìµœëŒ€ ìˆ˜ì¤€ì˜ memorization.</li> </ul> </li> <li> <p><strong>ë©”ì»¤ë‹ˆì¦˜ í•´ì„</strong></p> <ul> <li> <p>Projection layerëŠ” <strong>íŠ¹ì§• ë³€í™˜, ì •ë³´ ì••ì¶•</strong>ì„ ë‹´ë‹¹ â†’ í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì²´ì  íŒ¨í„´ì„ ë” ì˜ â€˜ë¶™ì¡ì•„ ë‘ëŠ”â€™ ìœ„ì¹˜.</p> </li> <li> <p>ê²°ê³¼ëŠ” Meng et al. (ROME)ì˜ Transformer ê¸°ì–µì€ ì£¼ë¡œ projection ì¸µì— ì§‘ì¤‘í•œë‹¤ëŠ” ê°€ì„¤ì„ ì¬í™•ì¸.</p> </li> </ul> <p><br/></p> <p><br/></p> </li> </ol> <p>Practicalí•œ ê´€ì ì—ì„œâ€¦</p> <ul> <li> <p>í”„ë¼ì´ë²„ì‹œì— ë¯¼ê°í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” LoRAë¥¼ attention ì¸µì—ë§Œ ì‚½ì…í•˜ê±°ë‚˜ rankë¥¼ ë‚®ì¶”ì–´ ìœ„í—˜ì„ ì™„í™”.</p> </li> <li> <p>ì„±ëŠ¥ê³¼ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ í•„ìš”í•  ë•Œ, ì‚½ì… ìœ„ì¹˜(attn vs proj)ì™€ ë²”ìœ„(ë‹¨ì¼ vs ë³µí•© ì¸µ)ë¥¼ ì£¼ìš” ì¡°ì ˆ ë³€ìˆ˜ë¡œ í™œìš©í•˜ë©´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆê² ë‹¤!</p> </li> </ul> <p><br/></p> <p><br/></p> <p>ë„ˆë¬´ ë§ì£ ..í•˜ì§€ë§Œ ì €ìê°€ ì´ì•¼ê¸°í•œ ê²ƒë§Œ ë§í•´ë³´ê² ìŠµë‹ˆë‹¤.</p> <ol> <li> <p>larger model</p> </li> <li> <p>MoE ê°™ì€ ë‹¤ë¥¸ êµ¬ì¡°</p> </li> <li> <p>ë°ì´í„° ì ìŒ</p> </li> </ol> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Impact of Fine-Tuning Methods on Memorization in Large Language Models]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in-reasoning-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: ì „ë¯¼ì§„</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>ìµœê·¼ LLMì´ ë³µì¡í•œ reasoning taskì—ì„œ ê´„ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆìœ¼ë‚˜, (ëª¨ë¸ì—ê²Œ) í¸í•œ reasoning patternì— ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆìŒ</p> <ul> <li>ì´ë¥¼ Reaosning rigidityë¡œ ì •ì˜</li> </ul> </li> <li> <p>ì‚¬ìš©ìì˜ ëª…ì‹œì ì¸ instructionì´ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ìŠµê´€ì ì¸ reasoning trajectoryë¥¼ ìƒì„±, ì˜¤ë‹µìœ¼ë¡œ ê·€ê²°</p> <ul> <li>íŠ¹íˆ ìˆ˜í•™ê³¼ í¼ì¦ ë¶„ì•¼ì—ì„œ ë‘ë“œëŸ¬ì§</li> </ul> </li> <li> <p>ì´ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´ reasoning trapì´ë¼ëŠ” ì§„ë‹¨ ë°ì´í„°ì…‹ì„ ë„ì…</p> <ul> <li> <p>deviationì„ ìš”êµ¬í•˜ë„ë¡ ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ìˆ˜ì •í•œ ìƒíƒœ</p> </li> <li> <p>ì˜ˆë¥¼ ë“¤ì–´, ëª¨ë“  í† ë¼ëŠ” ë¶ˆì„ì´ë‹¤. í† ë¼ê°€ 3ìŒì´ ìˆê³  í† ë¼ í•œ ìŒì´ ì•”ìˆ˜ ê° 1ë§ˆë¦¬ì”© ì´ 2ë§ˆë¦¬ë¥¼ ë‚³ëŠ”ë‹¤ê³  ê°€ì •í•˜ë©´, 2ì„¸ëŒ€ë¥¼ ê±°ì¹˜ë©´ ëª‡ë§ˆë¦¬ì˜ í† ë¼ê°€ ë˜ëŠ”ê°€? ë¼ëŠ” ì§ˆë¬¸.</p> </li> </ul> <p>â‡’ ì´ë¥¼ í†µí•´ì„œ ëª¨ë¸ì´ ìŠµê´€ì ìœ¼ë¡œ ì“°ëŠ” contaminationëœ patternì„ ì‹ë³„í•  ìˆ˜ ìˆìŒ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - ëª¨ë¸ì´ ì£¼ì–´ì§„ instruction ì„ ë¬´ì‹œí•˜ê±°ë‚˜ ì™œê³¡í•˜ë„ë¡ í•¨
</code></pre></div> </div> </li> <li> <p>reasoning trapì„ í†µí•´ì„œ ëª¨ë·ì´ ìŠµê´€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” reasoning patternì„ ë°œê²¬, ë¶„ë¥˜</p> <ul> <li> <p>interpretation overload</p> </li> <li> <p>input distrust</p> </li> <li> <p>partial instruction attnetion</p> </li> </ul> <p>â‡’ í•´ë‹¹ ë°ì´í„°ì…‹ì„ í†µí•´ LLMì— ìˆëŠ” reasoning rigidityë¥¼ í•´ì†Œí•˜ëŠ” ë¯¸ë˜ ì—°êµ¬ë¥¼ ìš©ì´í•˜ê²Œ í•¨</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLMì€ ìˆ˜í•™, ë³µì¡í•œ ì½”ë”© ë¬¸ì œ, í¼ì¦ í’€ì´ë¥¼ í¬í•¨í•œ ì—¬ëŸ¬ ì–´ë ¤ìš´ íƒœìŠ¤í¬ì—ì„œ ì£¼ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì„</p> <ul> <li>íŠ¹íˆ test-time scalingì„ í™œìš©í•´ í™•ì¥ëœ CoT promptingì„ í™œìš©í•˜ëŠ” reasoning modelë“¤ì´ í° ì£¼ëª©ì„ ë°›ê³  ìˆìŒ</li> </ul> </li> <li> <p>í•˜ì§€ë§Œ, ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì—ê²Œ ë¬¸ì œ í–‰ë™, reasoning rigidityê°€ ë°œê²¬ë¨</p> <ul> <li>íŠ¹íˆ ê¸´ CoT reasoningìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì—ê²Œ ë‚˜íƒ€ë‚¨</li> </ul> </li> <li> <p>reasoning rigidityëŠ” cognitive biasë¥¼ ë°˜ì˜, ì£¼ì–´ì§„ ì¡°ê±´ì„ ì´í•´í•´ë„ ìê¸° ë°©ì‹ëŒ€ë¡œ override, ë¬´ì‹œí•˜ê³  ë¬¸ì œë¥¼ í‘¸ëŠ” í˜„ìƒì„ ëœ»í•¨</p> <p>â‡’ ì´ëŠ” ê¸°ì¡´ì— ì–¸ê¸‰ë˜ì–´ì™”ë˜ hallucinataion, prompt brittlnessë“¤ì„ í•´ì†Œí•´ë„ ì¡´ì¬í•  ìˆ˜ ìˆìŒ</p> <ul> <li> <p>hallucination : í‹€ë¦° ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒ</p> </li> <li> <p>prompt brittlness : ë¯¸ë¬˜í•œ prompt ì°¨ì´ì— ë”°ë¼ ë‹µë³€ì´ ë°”ë€œ. ë‹µë³€ì´ unstableí•œ í˜„ìƒ</p> </li> <li> <p>reasoning rigidityëŠ” ì‚¬ìš©ìê°€ ì„œìˆ í•œ ì œì•½ì´ ì¤‘ìš”í•œ ë„ë©”ì¸ì—ì„œ í° ë¬¸ì œê°€ ë¨</p> </li> <li> <p>ì˜ˆë¥¼ ë“¤ì–´, ìˆ˜í•™ì´ë‚˜ í¼ì¦ í’€ì´ì˜ ê²½ìš°, ë’¤ì˜ ë¬¸ì œì™€ ê´€ê³„ ì—†ì´ ìœ ì €ê°€ ë°”ë¡œ ì •ë‹µìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆëŠ” ì¡°ê±´ì„ ì¤¬ì„ ê²½ìš°, ì´ë¥¼ ë¬´ì‹œí•˜ë©´ ì™„ì „íˆ ì˜¤ë‹µì´ ë  ìˆ˜ ë°–ì— ì—†ìŒ</p> </li> </ul> <p>â‡’ ì‚¬ìš©ìì˜ ì§€ì‹œë¥¼ ë¬´ì˜ì‹ ì¤‘ì— í¸ì§‘í•˜ê±°ë‚˜ ë¬´ì‹œ(reasoning rigidity), ëª¨ë¸ì˜ reasoning pathì „ì²´ê°€ ì˜¤ì—¼, ì˜¤ë‹µìœ¼ë¡œ ì´ì–´ì§</p> <ul> <li>ì´ëŸ¬í•œ í˜„ìƒì€ ì•„ì£¼ í¬ë¦¬í‹°ì»¬í•˜ë‚˜, ë³¸ ë…¼ë¬¸ì—ì„œ ì²˜ìŒìœ¼ë¡œ ë¬¸ì œë¥¼ ì •ì˜</li> </ul> </li> <li> <p>reasoning rigidityë¥¼ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡, ê¸°ì¡´ì˜ ìˆ˜í•™, í¼ì¦ ë°ì´í„°ì…‹ì„ í™œìš©í•´ reasoningtrapì´ë¼ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ì œì•ˆ</p> <ul> <li> <p>ì˜ ì•Œë ¤ì§„ cahllengesì™€ ë‹®ì•˜ìœ¼ë‚˜, ì¡°ê±´ì´ ìˆ˜ì •ë˜ì–´ì„œ ë‹µì´ ì™„ì „ ë°”ë€ŒëŠ” ë¬¸ì œë“¤ë¡œ êµ¬ì„±</p> </li> <li> <p>ëª¨ë¸ì´ ìŠµê´€ì ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ ê²½ìš° ì˜¤ë‹µìœ¼ë¡œ ì´ì–´ì§€ëŠ” êµ¬ì¡°ë¡œ ì„¤ê³„</p> </li> </ul> </li> <li> <p>ReasoningTrapìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ì„ í‰ê°€í•œ ê²°ê³¼, ì—¬ëŸ¬ ì¤‘ìš”í•œ í˜„ìƒë“¤ì„ ë°œê²¬</p> <ul> <li> <p>reasoning processì˜ ì¤‘ê°„ ë‹¨ê³„ì—ì„œ contaminationì´ ì‹œì‘</p> </li> <li> <p>ì´ëŸ¬í•œ contaminationì€ ëª…ë°±í•˜ê²Œ ì‹ë³„ ê°€ëŠ¥, ë°˜ë³µë˜ëŠ” íŒ¨í„´ì„ ê°€ì§</p> </li> </ul> </li> <li> <p>ë˜í•œ, ì´ëŸ¬í•œ contaminationì˜ íŒ¨í„´ì„ 3ê°€ì§€ë¡œ ë¶„ë¥˜</p> <ul> <li>interpretation overload, input distrust, partial insturction attention</li> </ul> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> <ul> <li> <p>LLMì˜ reasoning abilityë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ CoTë¥¼ ê¸¸ê²Œ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ë°©ë²•ë¡ ì´ ì œì•ˆ</p> </li> <li> <p>ë˜í•œ, Qwen3ì˜ ê²½ìš° reasoningê³¼ non-reasoning modeë¥¼ ë‘˜ë‹¤ ì§€ì›í•˜ëŠ” unified fusion architectureë¥¼ ê³µê°œ</p> <ul> <li>userê°€ ëª¨ë¸ì´ ê¸´ CoTë¥¼ ìƒì„±í•˜ë„ë¡ í• ì§€ ì—¬ë¶€ë¥¼ ê³ ë¥¼ ìˆ˜ ìˆìŒ</li> </ul> </li> </ul> </li> <li> <p>Instruction following of reasoning models</p> <ul> <li> <p>ì—¬ëŸ¬ in-context examples í˜¹ì€ ì¥í™©í•œ instructionì„ ë„£ìœ¼ë©´ reasoning modelë“¤ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ ì•Œë ¤ì§</p> <p>â‡’ ì¦‰, LRMì´ user-provided exampleì„ followingí•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±</p> </li> <li> <p>ë³¸ ì—°êµ¬ëŠ” ì´ì™€ ê²°ì€ ê°™ì§€ë§Œ, ëª¨ë¸ì´ ì¹œìˆ™í•œ reaosning patternì„ ê³ ì§‘í•œë‹¤ëŠ” ê²ƒì— ì´ˆì ì„ ë‘ </p> </li> </ul> </li> <li> <p>Rigidity in reasoning models</p> <ul> <li> <p>ëª‡ëª‡ ì—°êµ¬ë“¤ì´ LLMì´ reasoningí•  ë•Œ rigid patternì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì„ ì§€ì í•¨</p> <ul> <li> <p>medical domain, educational domain</p> </li> <li> <p>ìš°ë¦¬ì˜ ì—°êµ¬ëŠ” ë” í° ë„ë©”ì¸ì´ ìˆ˜í•™, í¼ì¦ì— ì´ˆì²¨</p> </li> </ul> </li> <li> <p>ë³¸ ì—°êµ¬ì™€ ìœ ì‚¬í•˜ê²Œ, ëª‡ëª‡ ë…¼ë¬¸ë“¤ì´ LLMì´ rigidityë¥¼ íƒêµ¬</p> <ul> <li>ì´ëŸ¬í•œ ì—°êµ¬ë“¤ì€ LLMì´ creative problem solvingì— ì ìš©ë  ë•Œ í˜¹ì€ matha word problemì˜ unseen variantì˜ ì¼ë°˜í™” ì— ì´ˆì ì„ ë‘ </li> </ul> </li> </ul> </li> <li> <p>Underlying reason for rigidity</p> <ul> <li> <p>ëª‡ëª‡ ì—°êµ¬ë“¤ì´ ì™œ LLMì´ ì´ëŸ¬í•œ rigidityë¥¼ ê°€ì§€ëŠ”ì§€ì— ëŒ€í•´ ë¶„ì„í–ˆê³ , training data í˜¹ì€ optimization ë°©ì‹ì— ì„ë² ë”©ëœ biasë¥¼ ì§€ì </p> </li> <li> <p>í•œ ì—°êµ¬ì—ì„œ RLë¡œ í•™ìŠµëœ ëª¨ë¸ë“¤ì´ exploitationì´ ë›°ì–´ë‚˜ê³ , ì´ë¡œ ì¸í•´ ë†’ì€ ì„±ëŠ¥ì€ ë‹¬ì„±í–ˆì§€ë§Œ ì—­ì„¤ì ì´ê²Œë„ non-reasoning modelì— ë¹„í•´ ì¢ì€ knowledge coverageë¥¼ ë³´ì¸ë‹¤ê³  ì£¼ì¥</p> </li> <li> <p>ë‹¤ë¥¸ ì—°êµ¬ì—ì„œëŠ” training dataì— ë‚´ì¬ëœ biasë•Œë¬¸ì´ë¼ê³  í•¨</p> </li> </ul> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>í¬ê²Œ 2ê°€ì§€ë¡œ ë„ë©”ì¸ìœ¼ë¡œ êµ¬ì„± : ìˆ˜í•™(ConditionedMath), í¼ì¦(PuzzleTrivial)</p> </li> <li> <p>ê° ë°ì´í„°ëŠ” ì›ë˜ Q-R-A tuple (q_orig, r_orig, a_orig)ê³¼ ìˆ˜ì •ëœ tuple (q_mod, r_mod, a_mod)ë¡œ êµ¬ì„±</p> </li> <li> <p>ì´ 164ê°œì˜ ë°ì´í„°ì…‹, 84ê°œëŠ” ìˆ˜í•™, 80ê°œëŠ” í¼ì¦</p> </li> <li> <p>ConditionedMathì— ìˆëŠ” ëª¨ë“  ì§ˆë¬¸ì€ ê°œë…ì ìœ¼ë¡œ ë‹¤ë¥´ê³ , ê²¹ì¹˜ì§€ ì•Šê³ , human annotatorì— ì˜í•´ ì—„ê²©í•˜ê²Œ ê²€ì¦ë¨</p> </li> <li> <p>PuzzleTrivalì€ 10ê°œì˜ puzzle conceptë¥¼ ê°€ì§</p> </li> </ul> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5ë¥¼ í™œìš©í•´ì„œ ì œì‘</p> </li> <li> <p>ì›ë˜ ì§ˆë¬¸ì„ ìˆ˜ì •í•˜ê³ , ìˆ˜ì •ëœ ì§ˆë¬¸ì´ ì•„ë˜ ì¡°ê±´ì— ë¶€í•©í•˜ëŠ”ì§€ë¥¼ í™•ì¸, í•„í„°ë§</p> <ul> <li> <p>validity : ê¸°ì¡´ conditionê³¼ ëª¨ìˆœë˜ëŠ”ì§€</p> </li> <li> <p>divergence : ê¸°ì¡´ ë‹µ, í’€ì´ì™€ ìƒì´í•œì§€</p> </li> <li> <p>existence : ë‹µì´ ìˆëŠ”ì§€</p> </li> </ul> <p>â‡’ ë¬¸ì œë¥¼ ìˆ˜ì •í•  ë•ŒëŠ” gpt-4o-minií™œìš©, í•„í„°ë§ í•  ë•ŒëŠ” o4-minië¥¼ ì‚¬ìš©</p> </li> <li> <p>220ê°œì˜ ì›ë³¸ ë°ì´í„°ë¥¼ 5ê°€ì§€ì˜ variantë¡œ modified, í•„í„°ë§ í›„ì— ìµœì¢… 84ê°œë§Œ ë‚¨ìŒ</p> </li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzleì€ ì¡°ê±´ì„ ìˆ˜ì •í•˜ë©´ ê¸‰ê²©í•˜ê²Œ ë‹¨ìˆœí•´ì§€ê±°ë‚˜ ë‹µì´ ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆìŒ</p> </li> <li> <p>ambiguityë¥¼ ì¤„ì´ê¸° ìœ„í•´, â€œvalid solutionì„ ìœ„í•´ ê°€ì¥ ê°„ë‹¨í•œ ë‹µì„ ì°¾ì•„ë¼â€ë¼ëŠ” ë¬¸êµ¬ë¥¼ instructionì— ì¶”ê°€</p> </li> <li> <p>ê³¼ì • ìì²´ëŠ” ìœ„ì™€ ë™ì¼</p> </li> </ul> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li> <p>ì‹œìŠ¤í…œì ìœ¼ë¡œ reasoning modelì˜ contaminationì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ, Contamination ratioë¥¼ ì œì•ˆ</p> <ul> <li> <p>ì¹œìˆ™í•œ íŒ¨í„´ì—ì„œ contaminated reasoningì´ ì–¼ë§ˆë‚˜ ì°¨ì§€í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„</p> </li> <li> <p>ì´ë¥¼ ìë™ì ìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ë°©ë²•ë„ ì œì•ˆ</p> </li> </ul> </li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>ëª¨ë¸ì´ ë¬¸ì œë¥¼ í’€ ë•Œ, ìˆ˜ì •ëœ ì¡°ê±´ì„ ì´í•´í•˜ê³  í’€ì—ˆëŠ”ì§€ ì´í•´í•˜ì§€ ì•Šê³  í’€ì—ˆëŠ”ì§€ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ metricì„ ë„ì…</p> </li> <li> <p>ìƒì„±ëœ reasoning pathë¥¼ ë‹¨ë½ë³„ë¡œ ìª¼ê°œê³ , ê° ë‹¨ë½ì„ textual representationìœ¼ë¡œ embedding</p> <ul> <li> <p>openAIì˜ text-embedding-small modelì„ ì‚¬ìš©</p> </li> <li> <p>ë‹¨ë½ì€ double line breakë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬</p> </li> </ul> </li> <li> <p>ê° ë‹¨ë½ê³¼ ì˜¤ë¦¬ì§€ë„ ë¬¸ì œì˜ reasoning path, ê° ë‹¨ë½ê³¼ modified reasoning pathì™€ì˜ cosine ìœ ì‚¬ë„ë¥¼ ê³„ì‚°, ë‘˜ì„ ë¹„êµí•´ original reasoning pathì™€ì˜ ìœ ì‚¬ë„ê°€ ë” ë†’ì„ ê²½ìš° 1ë¡œ ê³„ì‚°</p> <ul> <li>ì¦‰, ì¡°ê±´ì´ ìˆ˜ì •ë˜ì—ˆëŠ”ë°ë„ ë¬´ì‹œí•˜ê³  ìŠµê´€ì²˜ëŸ¼ reasoningì„ í–ˆë‹¤ëŠ” ëœ»</li> </ul> </li> </ul> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidityë¥¼ ì˜ ê´€ì°°í•˜ê¸° ìœ„í•´, ëª¨ë¸ì´ ìˆ˜ì •ëœ ì¡°ê±´ì„ ì´í•´í–ˆëŠ”ë°ë„ ìŠµê´€ì²˜ëŸ¼ í’€ì—ˆëŠ”ì§€ ì•„ë‹ˆë©´ ì¸ì§€ì¡°ì°¨ í•˜ì§€ ëª»í–ˆëŠ”ì§€ë¥¼ êµ¬ë¶„</p> <ul> <li>ëª¨ë¸ì´ ì¡°ê±´ì„ ì˜ëª» ì´í•´í•œ ê²½ìš° / ì¡°ê±´ì„ ì˜ ì´í•´í–ˆìœ¼ë‚˜ reasoningì„ ì˜ëª»í•œ ê²½ìš°</li> </ul> </li> <li> <p>ì´ë¥¼ ë°˜ì˜í•œ metricì„ p-passs@kë¼ê³  ì •ì˜, reasosning pathì—ì„œ constraintë¥¼ ì¸ì§€í•˜ê³  ìˆëŠ” ê²½ìš°ì—ë§Œ accuracyë¥¼ ì¸¡ì •</p> </li> <li> <p>constraintë¥¼ ì¸ì§€í–ˆëŠ”ì§€ëŠ” ëª¨ë¸ì´ ìƒì„±í•œ reasoning pathì¤‘ ì²« 15ê°œì˜ ë‹¨ë½ê³¼ ì •ë‹µ, ì§ˆë¬¸ì„ LLMì— ë„£ê³  íŒë‹¨í•˜ë„ë¡ í•¨(p_i)</p> </li> </ul> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>questionë§Œ ì£¼ì–´ì§€ëŠ” í˜„ì‹¤ì ì¸ ìƒí™©ì—ì„œ, generated reasosningì´ ì›ì¹˜ ì•Šì§€ë§Œ ì¹œìˆ™í•œ patternìœ¼ë¡œ contaminatedëëŠ”ì§€ ìë™ì ìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ê²ƒì„ ë¶ˆê°€ëŠ¥</p> </li> <li> <p>ê·¸ë˜ì„œ ê°„ë‹¨í•˜ê²Œ, contaminationì˜ ì¢…ë¥˜ë¥¼ ë¶„ë¥˜í•´ì„œ, ê° typeë³„ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ patternì„ ì‹ë³„</p> </li> <li> <p>Interpretation overload : ëª¨ë¸ì´ ì£¼ì–´ì§„ ë¬¸ì œ ì¡°ê±´ì„ ê±°ì ˆí•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘, ë¬¸ì œë¥¼ ë°”ë¡œ í•´ì„í•˜ëŠ” ê²ƒë³´ë‹¤ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ì¬í•´ì„. ë³´í†µ reasoning ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ë°œìƒ, inconsistent í˜¹ì€ contraditoryí•œ ê²°ë¡ ì„ ì•¼ê¸°</p> </li> <li> <p>Input Distrust : ëª¨ë¸ì´ ë²ˆì—­ ì˜¤ë¥˜, input error, typoì¡´ì¬ ë“±ì„ ê°€ì •í•¨. ì§ê´€ì ìœ¼ë¡œ ë°”ë¡œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆìŒì—ë„ ë¶€ì •í•˜ê³  ë§¤ìš° ë³µì¡í•˜ê²Œ í’€ê²Œ ë¨.</p> </li> <li> <p>Partial Instruction Attention : ëª¨ë¸ì´ ì œê³µëœ instructionì˜ ì¼ë¶€ë¶„ë§Œì„ ì„ íƒì ìœ¼ë¡œ ì§‘ì¤‘</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrapì„ ì—¬ëŸ¬ LLMì— inference</p> </li> <li> <p>ì‹¤í—˜ì€ CoT promptingì„ ì‚¬ìš©, â€˜Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}â€™ í¬ë§·ìœ¼ë¡œ ì§ˆë¬¸ì„ ì „ë‹¬</p> </li> <li> <p>table 2,3ì€ 16ë²ˆ sampling, ë‹¤ë¥¸ ì‹¤í—˜ì€ 4ë²ˆ sampling</p> </li> <li> <p>ìˆ˜í•™ ë¬¸ì œì˜ ê²½ìš°, exat matchingìœ¼ë¡œ correctness íŒë‹¨, puzzleì˜ ê²½ìš° free-from sentenceë¡œ ë‹µì´ êµ¬ì„±ë˜ë‹¤ ë³´ë‹ˆ, LLMì„ ì‚¬ìš©í•´ì„œ ì •ë‹µê³¼ ëª¨ë¸ ë‹µë³€ì„ í•¨ê»˜ ì œê³µí•´ correctnessë¥¼ íŒë‹¨</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼, ëŒ€ë¶€ë¶„ reasonëª¨ë“œì¼ ë•Œë³´ë‹¤ baseëª¨ë“œì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„</p> <ul> <li> <p>ì¦‰, ê¸¸ê²Œ reasoningì„ í•˜ë©´ì„œ ìŠµê´€ì ì¸ reasoning patternì„ ì‚¬ìš©, ì˜¤ë‹µìœ¼ë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°ê°€ ë§ë‹¤ëŠ” ê²ƒ</p> </li> <li> <p>Buget forcing : ë²„ì§“ ë§ˆì§€ë§‰ì— â€˜Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;â€™ë¥¼ ì¶”ê°€í•˜ì—¬ ë‹µì„ ë°”ë¡œ ë‚´ë„ë¡ í•¨</p> </li> <li> <p>MATH500 : low 2000, medium 4000, high 6000 í† í° ì‚¬ìš©</p> </li> <li> <p>AIME : low 2000, medium 6000, high 10000</p> </li> </ul> </li> <li> <p>prompt hinting : ë¬¸ì œì— ì˜¤íƒ€ ì—†ê³  ì§€ì‹œ ê·¸ëŒ€ë¡œ í•˜ë¼ëŠ” promptë¥¼ ì¶”ê°€</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼, budgetì´ ì»¤ì§ˆ ìˆ˜ë¡ ì„±ëŠ¥ì´ ì•…í™”ë¨</p> </li> <li> <p>promptë¡œ hintë¥¼ ì¤˜ë„ ì—¬ì „íˆ reasoning rigidityê°€ ì¡´ì¬</p> </li> <li> <p>ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì‹¤í—˜. baseëª¨ë¸ì´ ì„±ëŠ¥ì´ ì „ë°˜ì ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜¤ëŠ” í¸</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in-large-reasoning-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: ì „ë¯¼ì§„</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>ìµœê·¼ì˜ reasoning oriented model(LRM)ì€ ì—¬ëŸ¬ ìˆ˜í•™ ë°ì´í„°ì…‹ì—ì„œ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì„ ë³´ì´ë‚˜, natural instruction followingì— ëŒ€í•œ ì„±ëŠ¥ì€ ë¶„ì„ë˜ì§€ ì•ŠìŒ</p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ LRMë“¤ì˜ instruction following ëŠ¥ë ¥ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ MathIFë¼ëŠ” ë°ì´í„°ì…‹ì„ ì œì•ˆ, math ë„ë©”ì¸ì—ì„œì˜ instruction following ì„±ëŠ¥ì„ í‰ê°€</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼, reasoningì„ íš¨ê³¼ì ìœ¼ë¡œ í•˜ëŠ” ëª¨ë¸ì´ user directionì— ë”°ë¥´ëŠ” ê²ƒì„ ì–´ë ¤ì›Œ í•˜ëŠ” í˜„ìƒ ë°œê²¬</p> <ul> <li> <p>ê¸´ CoT datasetì— SFTí•˜ê±°ë‚˜ RLë¡œ í•™ìŠµí•œ ëª¨ë¸ì´ ë‹µë³€ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ instruction following ëŠ¥ë ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ ë°œê²¬</p> </li> <li> <p>ê°„ë‹¨í•œ ê°œì…(CoT ë§ˆì§€ë§‰ ë¶€ë¶„ì— instructionì„ ë‹¤ì‹œ ë¶™ì—¬ì„œ ë„£ì–´ì¤Œ)ìœ¼ë¡œ instruction following ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŒì„ ë³´ì„</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoningì„ scalingí•˜ëŠ” ê²ƒì€ reasoning abilityë¥¼ í–¥ìƒì‹œí‚´</p> <ul> <li>SFT or RLVR ì‚¬ìš©</li> </ul> </li> <li> <p>LRMì˜ ê²½ìš° ê°„ë‹¨í•œ instructionë„ followingí•˜ëŠ” ê²ƒì„ ì–´ë ¤ì›Œ í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬</p> </li> </ul> <p>â‡’ reasoning-oriented learningì„ í•˜ë©´ ëª¨ë¸ ìì²´ì˜ reasoning abilityëŠ” í–¥ìƒë¼ë„ controllabilityëŠ” ë–¨ì–´ì§€ëŠ”ê²Œ ì•„ë‹ê¹Œ?</p> <ul> <li>í•˜ì§€ë§Œ í˜„ì¬ëŠ” ë²”ìš© ëª©ì ì˜ instruction following(IF) ë²¤ì¹˜ë§ˆí¬ë§Œ ì¡´ì¬</li> </ul> <p>â‡’ ìˆ˜í•™ ë„ë©”ì¸ì—ì„œì˜ IF ë²¤ì¹˜ë§ˆí¬ë¥¼ ë§Œë“¤ê³  í‰ê°€í•´ë³´ì!</p> <ul> <li> <p>ì‹¤í—˜ ê²°ê³¼, instruction followingê³¼ reasoning capabilityì‚¬ì´ì˜ ì¼ì¢…ì˜ trade-offê°€ ì¡´ì¬</p> <ul> <li> <p>ì¦‰, SFT í˜¹ì€ RLë¡œ reasoning abilityë¥¼ í–¥ìƒì‹œí‚¨ ëª¨ë¸ì€ reasoning ì„±ëŠ¥ì€ ì˜¬ë¼ë„ IF ì„±ëŠ¥ì€ ë–¨ì–´ì§</p> </li> <li> <p>íŠ¹íˆ, CoT ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ IF ì„±ëŠ¥ì´ ì•…í™”ë¨</p> </li> </ul> </li> <li> <p>contribution</p> <ul> <li> <p>MathIF, ì²«ë²ˆì§¸ë¡œ ìˆ˜í•™ ë„ë©”ì¸ì—ì„œ instruction following ëŠ¥ë ¥ì„ ì‹œìŠ¤í…œì ìœ¼ë¡œ ì¸¡ì •í•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë„ì…</p> </li> <li> <p>23ê°œì˜ LRMë¥¼ í•´ë‹¹ ë²¤ì¹˜ë§ˆí¬ì— ëŒ€í•´ì„œ í‰ê°€</p> </li> <li> <p>reasoning performanceì™€ instruction-followingì‚¬ì´ì˜ trade-offê°€ ìˆìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì„</p> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> <ul> <li> <p>high-quality long CoT by distilling from more capable LRMs or combining primitive reasoning actions</p> <ul> <li> <p>s1 : ì ì€ ì–‘ì˜ CoT dataë¡œë„ reasoning abilityë¥¼ í–¥ìƒì‹œí‚´</p> </li> <li> <p>LIMO : ê´€ë ¨ ë„ë©”ì¸ì´ ì´ë¯¸ pre-trainingë•Œ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ìµœì†Œí•œì˜ cognitive processë¥¼ ë‹´ì€ demonstrationìœ¼ë¡œ reasoning capabilitiesë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ê³  ì„œìˆ </p> </li> </ul> </li> <li> <p>cold-RL</p> <ul> <li> <p>deepseek-R1-zeroë¡œ ì£¼ëª© ë°›ê²Œ ëœ ë°©ë²•ë¡ </p> </li> <li> <p>SFTì™€ ë‹¬ë¦¬, cold-RLì€ ê¸´ CoT datasetì— ì˜ì¡´í•˜ì§€ ì•Šê³ , final outcomeìœ¼ë¡œ rewardë¥¼ ë°›ì•„ì„œ í•™ìŠµ</p> </li> <li> <p>RLê³¼ì •ì„ ê°„ë‹¨, ê°€ì†í™” í•˜ê¸° ìœ„í•´ì„œ dynamic sampling, process-reward, off-policy guidance, CoT preference optimziation ë“±ì´ ì œì•ˆë¨</p> </li> </ul> </li> </ul> </li> <li> <p>Instruction-followiwng benchmark</p> <ul> <li> <p>ì´ì „ì˜ ë²¤ì¹˜ë§ˆí¬ëŠ” ë³´í†µ user queryì˜ completenessì— ì´ˆì , proprietary language modelì— ì˜ì¡´í•´ì„œ win rateë¥¼ ì¸¡ì •í•˜ëŠ” ì‹ìœ¼ë¡œ í‰ê°€</p> </li> <li> <p>format constraint, multi-turn instruction, refutation instruction, compositional instructionì„ ë”°ë¥´ëŠ”ì§€ë¥¼ í‰ê°€</p> </li> <li> <p>í•˜ì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ IF ë²¤ì¹˜ë§ˆí¬ëŠ” ì¼ë°˜ì ì¸ ë„ë©”ì¸ì— ì§‘ì¤‘, ìƒëŒ€ì ìœ¼ë¡œ ì§ê´€ì ì¸ queryë¥¼ ì‚¬ìš©</p> </li> </ul> <p>â†’ ì´ëŸ¬í•œ ë„ë©”ì¸ ì°¨ì´ì™€ long CoTì˜ ë¶€ì¬ëŠ” LRMì„ í‰ê°€í•˜ëŠ”ë°ì— ë°©í•´ê°€ ë¨</p> </li> </ul> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> <ul> <li> <p>toy experimentë¡œ IFEvalê³¼ FollowBenchì— ëŒ€í•œ LRMê³¼ Instruct ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - í™•ì‹¤íˆ LRMì˜ ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìœ¼ë‚˜, ë‚®ì€ ì›ì¸ì´ domain shiftë•Œë¬¸ì¸ì§€ IFì„±ëŠ¥ ë•Œë¬¸ì¸ì§€ëŠ” ë¶„ëª…í•˜ì§€ ì•ŠìŒ
</code></pre></div> </div> <p>â‡’ ìˆ˜í•™ ë„ë©”ì¸ì˜ IF benchmarkë¥¼ ë§Œë“¤ì!</p> <ul> <li> <p>pythonìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥í•œ constraintë¥¼ ê³ ë ¤, 2-3ê°œì˜ constraintë¥¼ í•©ì³ì„œ instructionìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ë°©ì•ˆì„ ê³ ë ¤</p> </li> <li> <p>contraintë¥¼ ì–¼ë§ˆë‚˜ ë§Œì¡±í–ˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ Hard accuracy(HAcc), Soft accruacy(SAcc)ë¡œ ì¸¡ì •</p> </li> </ul> </li> </ul> </li> <li> <p>Constraint type</p> <ul> <li> <p>length, lexical, format, affixë¡œ í¬ê²Œ 4ê°€ì§€ typeìœ¼ë¡œ ë¶„ë¥˜, ê·¸ ì•ˆì— sub-typeì„ ëª…ì‹œ</p> </li> <li> <p>proprietary language modelì— ì˜ì¡´í•˜ì§€ ì•Šê¸° ìœ„í•´ì„œ pythonìœ¼ë¡œ ì œì•½ì„ ë§Œì¡±í–ˆëŠ”ì§€ ê²€ì¦ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„</p> <ul> <li>Compositional Constraint</li> </ul> </li> <li> <p>2-3ê°œì˜ constraintë¥¼ ì¡°í•©í•´ì„œ compositional constraintë¥¼ êµ¬ì¶•</p> </li> <li> <p>ê°™ì´ ì¡´ì¬í•  ìˆ˜ ì—†ëŠ” constraintë‚˜ ê°™ì€ subtypeë¼ë¦¬ ìˆìœ¼ë©´ filtering, ê·¸ ì™¸ì˜ ì¡°í•©ì—ì„œ random samplingí•´ì„œ ë°ì´í„°ì…‹ì„ êµ¬ì¶•</p> </li> <li> <p>ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ 30ê°œì˜ dual-constraintì™€ 15ê°œì˜ triple-constraintë¥¼ êµ¬ì¶•</p> </li> </ul> </li> <li> <p>Math problem collection</p> <ul> <li> <p>GSM8K, MATH-500, Minerva, Olympiadì—ì„œ 90ê°œì”© sampling</p> <ul> <li>ì´ˆë“±í•™êµë¶€í„° ì˜¬ë¦¼í”¼ì•„ë“œ ìˆ˜ì¤€ì˜ ë¬¸ì œê¹Œì§€ ì•„ìš°ë¥´ë„ë¡ í•¨</li> </ul> </li> <li> <p>ê° ë°ì´í„°ì— ëŒ€í•´ì„œ single, dual, triple constraintë¥¼ ì ìš©</p> </li> <li> <p>sanity checkë¥¼ ìœ„í•´ ì‚¬ëŒì´ ì§ì ‘ ê²€ìˆ˜, math problemì— ì¶”ê°€ëœ constraintê°€ ëª¨ìˆœë˜ì§€ ì•ŠëŠ”ì§€ ë”ë¸”ì·</p> </li> <li> <p>Evaluation metric</p> </li> <li> <p>HAcc : constraintë‹¤ ë§Œì¡±í•´ì•¼ 1</p> </li> <li> <p>SAcc : contraint ê°œë‹¹ ë§Œì¡±í•˜ë©´ 1 ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ê³„ì‚°, í‰ê· </p> <ul> <li>êµ¬ì²´ì ì¸ ì–¸ê¸‰ ì—†ìœ¼ë©´ correctnessëŠ” contraintê°€ ìˆëŠ” ìƒíƒœì—ì„œ ë‚˜ì˜¨ ë‹µë³€ìœ¼ë¡œ ê³„ì‚°</li> </ul> </li> </ul> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li> <p>ëª¨ë“  LRMì€ nucleus sampling(T=1.0, p=0.95)ë¡œ ë””ì½”ë”©, ìµœëŒ€ ë‹µë³€ ê¸¸ì´ 16,384 í† í°, vLLM ì‚¬ìš©</p> </li> <li> <p>ëª¨ë“  LRMì€ IFì„±ëŠ¥ì´ í•˜ë½í•¨</p> <ul> <li> <p>ìµœê³  ì„±ëŠ¥ì„ ë‚¸ Qwen3-14Bë§ˆì €ë„ 50.71ë°–ì— ì•ˆë¨</p> </li> <li> <p>íŠ¹íˆ deepseek-R1-distill-llama-70Bë‚˜ open-reasonser-zero-32Bì˜ ê²½ìš° ëª¨ë¸ í¬ê¸°ì— ë¹„í•´ì„œ ë§¤ìš° ë‚®ì€ IF ì„±ëŠ¥ì„ ë³´ì„</p> </li> </ul> </li> <li> <p>Qwen3 ì‹œë¦¬ì¦ˆê°€ ê·¸ë‚˜ë§ˆ ë†’ì€ IF ì„±ëŠ¥ì„ ë³´ì„</p> </li> <li> <p>ëª¨ë¸ í¬ê¸°ê°€ IF ì„±ëŠ¥ì„ ê²°ì •í•˜ì§„ ì•ŠìŒ</p> <ul> <li>ê°™ì€ ê³„ì—´ì—ì„œëŠ” ì¢…ì¢… ê²½í–¥ì„±ì´ ìˆìœ¼ë‚˜, ë‹¤ë¥¸ ê³„ì—´ê¹Œì§€ í•œë²ˆì— ë´¤ì„ ë•ŒëŠ” í¬ê¸°ê°€ í¬ë‹¤ê³  IF ì„±ëŠ¥ì´ ë³´ì¥ë˜ì§„ ì•ŠìŒ</li> </ul> </li> <li> <p>ëª…ì‹œì ì¸ reasoning seperation (<think>,</think>)ê°€ ìˆëŠ” ëª¨ë¸ì´ ì „ë°˜ì ìœ¼ë¡œ IF ì„±ëŠ¥ì´ ë†’ìŒ</p> <ul> <li>Qwen2.5-Math-1.5B, 7B-Instruct, Qwen2.5-1.5B, 7B-SimpleRL-Zoo ì¹œêµ¬ë“¤ì´ ëª…ì‹œì ì¸ reasoning token ì—†ëŠ” ì• ë“¤ â‡’ ì„±ëŠ¥ì´ ì˜ ì²˜ì°¸</li> </ul> </li> <li> <p>instruction-followingê³¼ mathematical reasoningì‚¬ì´ì— trade-offê°€ ì¡´ì¬</p> <ul> <li>Diffë¥¼ ë³´ë©´ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì´ constraintê°€ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œì˜ correctnessì°¨ì´ê°€ í¼</li> </ul> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

			- LRMëª¨ë¸ì´ constraintë¥¼ ì˜ ë”°ë¥´ëŠ”ê²Œ ë¬¸ì œ ë‚œì´ë„ì™€ ì—°ê´€ì´ ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³´ê¸° ìœ„í•´ ë°ì´í„°ì…‹ ë³„ë¡œ IFì„±ëŠ¥ì„ í‘œí˜„

	- ë¬¸ì œê°€ ì–´ë ¤ìš¸ìˆ˜ë¡ constraintë¥¼ ì˜ ë§Œì¡±í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

[//]: # (column is not supported)

			- ì œì•½ì´ ë§ì•„ì§ˆìˆ˜ë¡ IF ì„±ëŠ¥ í•˜ë½, íŠ¹íˆ 2ê°œì´ìƒë¶€í„° í¬ê²Œ í•˜ë½..
</code></pre></div></div> <ul> <li> <p>ì œì•½ì¡°ê±´ì„ ë§Œì¡±í•˜ë©´ì„œ ë¬¸ì œë¥¼ ë§ì¶”ëŠ” ê²½ìš°ëŠ” í¬ì§€ ì•ŠìŒ</p> </li> <li> <p>ë³´í†µ ì œì•½ì¡°ê±´ í˜¹ì€ ë¬¸ì œ í•˜ë‚˜ë§Œì„ ë§Œì¡±í•¨ + ì¦‰, ì œì•½ì¡°ê±´ì„ ê±¸ë©´ ë¬¸ì œ í’€ì´ ì„±ëŠ¥ì´ í•˜ë½</p> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

			- constraintê°€ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´

	- íŠ¹íˆ, GSM8K, Minervaì—ì„œ ê·¹ì‹¬ â‡’ ë¬¸ì œ ë‚œì´ë„ì™€ ìƒê´€ ì—†ì´ contraintê°€ ìˆìœ¼ë©´ reasoning abilityê°€ í•˜ë½

[//]: # (column is not supported)

			- CoTê°€ ê¸¸ìˆ˜ë¡ IF ì„±ëŠ¥ í•˜ë½
</code></pre></div></div> <ul> <li> <p>IFê°€ ë‚®ì•˜ë˜ Qwen2.5ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜, ë°ì´í„°ëŠ” deepscalarë¥¼ ì‚¬ìš©, QwQë¡œ CoTìƒì„±, ì •ë‹µì„ ë§ì¶”ë©´ì„œ ë„ˆë¬´ ê¸¸ì§€ ì•Šì€ ì• ë“¤ë§Œ í•„í„°ë§í•´ì„œ í•™ìŠµì— ì‚¬ìš©</p> <ul> <li>format rewardëŠ” think tokenë¥¼ í¬í•¨í•˜ëŠ”ì§€ ì—¬ë¶€ë¡œ í¬í•¨í•˜ë©´ ë‹µì´ í‹€ë ¤ë„ 0.1ì  ì¤Œ</li> </ul> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼, reasoning-orienteed ë°©ë²•ë¡ ì´ reasoningì„±ëŠ¥ì€ í–¥ìƒì‹œí‚¤ì§€ë§Œ IFëŠ” í•˜ë½í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ</p> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

		[//]: # (column is not supported)

	Figure 7
</code></pre></div></div> <ul> <li> <p>ëª¨ë¸ì´ reasonign pathë¥¼ ì¢…ë£Œí•˜ë ¤ê³  í•  ë•Œë§ˆë‹¤ waitë¥¼ ê±¸ì–´ì„œ ê°•ì œë¡œ CoTê¸¸ì´ë¥¼ ëŠ˜ë¦¼</p> </li> <li> <p>CoTê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ constraint instructionê³¼ ë©€ì–´ì ¸ì„œ constraintì— ëŒ€í•œ accê°€ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ì¶”ë¡ </p> </li> </ul> <p>Table 5</p> <ul> <li> <p>cold-RLì—ì„œ roll-out ê¸¸ì´ë¥¼ ì¡°ì •í•˜ë©° í•™ìŠµ, ê¸¸ì–´ì§ˆìˆ˜ë¡ reasoningì€ í–¥ìƒë˜ë‚˜ IFëŠ” ë–¨ì–´ì§</p> </li> <li> <p>ê°„ë‹¨í•˜ê²Œ reasoningì´ ëë‚˜ê°ˆ ë•Œ ì¯¤ì— waitì„ ë„£ê³  constraint instructionì„ ë°˜ë³µí•´ì„œ ë„£ì–´ì¤€ ê²½ìš°ì˜ ì„±ëŠ¥ì„ ì¸¡ì •</p> </li> <li> <p>IFì„±ëŠ¥ì€ í–¥ìƒë˜ë‚˜ CorrectnessëŠ” í•˜ë½í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented modelë“¤ì´ ìƒê°ë³´ë‹¤ instruction following ì„±ëŠ¥ì´ ì•…í™”ë¨</p> </li> <li> <p>ëŒ€ë¶€ë¶„ ê°„ë‹¨í•œ í˜•ì‹ì— ëŒ€í•œ ì œì•½ì¸ë°ë„, ì œì•½ì´ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œì˜ ì„±ëŠ¥ ì°¨ì´ê°€ í°ê²Œ ì¶©ê²©ì </p> </li> <li> <p>LLMì´ ì •ë§ reasoningì„ í•˜ëŠ”ê±¸ê¹Œ? ê·¸ëƒ¥ ë‹µë³€ ê¸¸ì´ê°€ ê¸¸ì–´ì ¸ì„œ ë°œìƒí•˜ëŠ” attention sinkì¼ê¹Œ?</p> </li> </ul> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models]]></summary></entry><entry><title type="html">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title><link href="https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/" rel="alternate" type="text/html" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/search-r1-training-llms-to-reason-and-leverage-search-engines-with-reinforcement-learning/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: ê±´ìš° ê¹€</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <ul> <li> <p>Reasoningê³¼ text generationì´ ê°€ëŠ¥í•œ LLMì—ê²Œ external knowledgeì™€ ìµœì‹  informationì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚½ì…í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•¨</p> <p>â†’ í•˜ì§€ë§Œ ê¸°ì¡´ advanced reasoning abilityë¥¼ ê°€ì§„ LLMì—ê²Œ prompt ê¸°ë°˜ì˜ search engineì„ í™œìš©í•˜ë„ë¡ í•˜ëŠ” ê²ƒì€ suboptimalì„ (LLMì´ search engineê³¼ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì™„ì „íˆ ì´í•´ ëª»í•¨)</p> </li> <li> <p>ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ RLì„ í™œìš©í•œ reasoning frameworkì¸ Search-R1ì„ ì†Œê°œí•¨</p> <ul> <li>ë‹¨ê³„ë³„ reasoning stepì—ì„œ autonomouslyí•˜ê²Œ multiple search queriesë¥¼ ìƒì„±í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ë„ë¡ í•™ìŠµ</li> </ul> </li> </ul> <p>LLMì€ natural language understandingê³¼ generationì—ì„œ ë†’ì€ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤¬ì§€ë§Œ, ì—¬ì „íˆ external sourcesê°€ í•„ìš”í•œ taskì—ì„œ í•œê³„ì ì„ ë³´ì—¬ì¤Œ.</p> <p>â†’ ì¦‰, ìµœì‹  informationì„ ì˜ í™œìš©í•  ìˆ˜ ìˆë„ë¡ search engineê³¼ <span style="color:yellow_background"><strong>íš¨ê³¼ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ëŠ”</strong></span> ëŠ¥ë ¥ì´ í•„ìˆ˜ì ì„</p> <p><br/></p> <p>ìµœê·¼ê¹Œì§€ LLMê³¼ Search Engineì„ ê²°í•©í•˜ëŠ” ëŒ€í‘œì ì¸ ë°©ì‹ì€ ë‘ê°€ì§€</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engineì„ í•˜ë‚˜ì˜ toolë¡œ í™œìš©í•˜ëŠ” ë°©ì‹</p> </li> </ol> <p><br/></p> <p>ìœ„ ë°©ë²• ë•ë¶„ì— LLMì´ external knowledgeë¥¼ í™œìš©í•  ìˆ˜ ìˆê¸´ í•˜ì§€ë§Œ, ìµœê·¼ ì—°êµ¬ (multi-turn, multi-query retrieval) ì—­ì‹œ ë³¸ì§ˆì ìœ¼ë¡œ <span style="color:red">**LLMì´ search engineê³¼ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì„ ìµœì í™”í•˜ì§€ ëª»í•œ ì±„ promptì—ë§Œ ì˜ì¡´í•˜ëŠ” í•œê³„ì ì´ ì¡´ì¬í•¨. **</span></p> <p>ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ LLMì´ ì¶”ë¡  ê³¼ì •ì—ì„œ search engineì„ í¬í•¨í•œ ì—¬ëŸ¬ toolì„ ì‚¬ìš©í•˜ë„ë¡ promptingí•˜ê±°ë‚˜ trainingí•˜ëŠ” ë°©ë²•ë“¤ì´ ìˆì§€ë§Œ</p> <ul> <li> <p>prompting ë°©ë²• ì—­ì‹œ LLMì˜ pre-training ë‹¨ê³„ì—ì„œ ê²½í—˜í•˜ì§€ ëª»í•œ ì‘ì—…ì— generalizeê°€ ì˜ ì•ˆë˜ëŠ” ë¬¸ì œ</p> </li> <li> <p>training ê¸°ë°˜ ë°©ì‹ì€ ë” ë‚˜ì€ adaptabilityë¥¼ ë³´ì´ì§€ë§Œ ëŒ€ê·œëª¨ high quality annotated trajectoriesê°€ í•„ìš”í•˜ê³  search ì—°ì‚°ì´ ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— end-to-end gradient descentë¡œ ìµœì í™”í•˜ê¸° ì–´ë ¤ì›€</p> </li> </ul> <p><br/></p> <p>í•œí¸ìœ¼ë¡œ RLì€ LLMì˜ reasoning capabilityë¥¼ ë†’ì´ëŠ” robust ë°©ë²•ìœ¼ë¡œ ìµœê·¼ì— ì£¼ëª© ë°›ëŠ”ë°, ì´ê²ƒì„ **search-and-reasoning **scenariosì— ì ìš©í•˜ëŠ” ë°ëŠ” 3ê°€ì§€ ë¬¸ì œê°€ ìˆìŒ</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engineì„ ì–´ë–»ê²Œ RLì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í• ì§€, íŠ¹íˆ <span style="color:red">ê²€ìƒ‰ëœ contextë¥¼ í¬í•¨í•  ë•Œ ì•ˆì •ì ì¸ ìµœì í™”ë¥¼ ì–´ë–»ê²Œ ë³´ì¥í• ì§€ ëª…í™•í•˜ì§€ ì•ŠìŒ</span></p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: ì´ìƒì ìœ¼ë¡œëŠ” <span style="color:red">LLMì´ ë°˜ë³µì ìœ¼ë¡œ ì¶”ë¡ í•˜ê³  search engineì„ í˜¸ì¶œí•˜ë©° ë¬¸ì œì˜ ë‚œì´ë„ì— ë”°ë¼ ê²€ìƒ‰ ì „ëµì„ ë™ì ìœ¼ë¡œ ì¡°ì •í• </span> ìˆ˜ ìˆì–´ì•¼ í•¨</p> </li> <li> <p><strong>Reward Design</strong>: Searchì™€ Reasoning tasksì— ì˜ë¯¸ ìˆê³  ì¼ê´€ëœ ê²€ìƒ‰ í–‰ë™ì„ í•™ìŠµí•˜ê²Œë” ìœ ë„í•  ìˆ˜ ìˆëŠ” íš¨ê³¼ì ì¸ reward function ì„¤ê³„ê°€ í•„ìš”í•˜ì§€ë§Œ, <span style="color:red">ë‹¨ìˆœí•œ ê²°ê³¼ ê¸°ë°˜ ë³´ìƒì´ ì¶©ë¶„í•œì§€ëŠ” ì•„ì§ ë¶ˆí™•ì‹¤í•¨</span>.</p> <p>â†’ 3ë²ˆì€ ìê¸°ë“¤ë„ ëª¨ë¥´ë©´ì„œ ë­”ê°€ ì‹¶ë„¤ìš” ã…‹ã…‹</p> </li> </ol> <p><br/></p> <p>â†’ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ <strong><em>Search-R1</em></strong>ì„ ì†Œê°œí•¨. ì´ê²ƒì€ LLMì´ ìì²´ ì¶”ë¡  ê³¼ì •ê³¼ search engineì„ interleavedí•˜ê²Œ ì—°ê³„í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ê°€ ë¨.</p> <p>ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŒ</p> <ol> <li> <p>Search engineì„ environmentì˜ ì¼ë¶€ë¡œ modelingí•˜ì—¬, <strong>LLMì˜ token ìƒì„±ê³¼ ê²€ìƒ‰ ê²°ê³¼ í˜¸ì¶œì´ í˜¼í•©ëœ trajectoryë¥¼ ìƒ˜í”Œë§í• </strong> ìˆ˜ ìˆìŒ.</p> </li> <li> <p><strong>Multi-turn retrievalê³¼ reasoningì„ ì§€ì›í•¨</strong>. <search>ì™€ </search> tokenìœ¼ë¡œ ê²€ìƒ‰ í˜¸ì¶œì„ íŠ¸ë¦¬ê±°í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ëŠ” <information>ì™€ </information> í† í°ìœ¼ë¡œ, LLMì˜ ì¶”ë¡  ë‹¨ê³„ëŠ” <think>ì™€ </think> í† í°ìœ¼ë¡œ, ìµœì¢… ë‹µë³€ì€ <answer>ì™€ </answer> í† í°ìœ¼ë¡œ ê°ì‹¸ êµ¬ì¡°ì ì´ê³  ë°˜ë³µì ì¸ ì˜ì‚¬ê²°ì •ì´ ê°€ëŠ¥í•¨</p> </li> <li> <p>process-based rewards ëŒ€ì‹  ë‹¨ìˆœí•œ <strong>outcome-based reward functionì„ ì ìš©í•˜ì—¬</strong> ë³µì¡ì„±ì„ ì¤„ì„</p> </li> </ol> <p><br/></p> <p>2.1 Large Language Models and Retrieval</p> <p>(ìƒëµ)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(ìƒëµ)</p> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1ì€ search engine $ R $ì„ í™œìš©í•˜ëŠ” RLì˜ objective functionì„ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•¨</p> <ul> <li> <p>$ r_{\phi} $: output qualityë¥¼ í‰ê°€í•˜ëŠ” reward function</p> </li> <li> <p>$ \pi_\theta $: policy LLM</p> </li> <li> <p>$ \pi_{ref} $: reference LLM</p> </li> <li> <p>$ x $: dataset $ D $ì—ì„œ ì¶”ì¶œëœ input sample</p> </li> <li> <p>$ y $: search engine calling ê²°ê³¼ì™€ interleavedëœ generated outputs</p> </li> <li> <p>$ D_{KL} $: KL-divergence</p> </li> </ul> <p>ê¸°ì¡´ RLì€ ì›ë˜ $ \pi_\theta $ê°€ ìƒì„±í•œ sequenceë§Œ í•™ìŠµí•˜ì§€ë§Œ, Search-R1ì€ ê²€ìƒ‰ í˜¸ì¶œê³¼ ì¶”ë¡ ì´ êµì°¨ëœ (interleaved) í˜•íƒœë¥¼ í•™ìŠµì— explicití•˜ê²Œ í¬í•¨í•¨.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- retrieval interleaved reasoning via $ \pi_{\theta}(.|x;R) =\pi_{ref}(.|x)\bigotimes R $

	- $ \bigotimes $ denotes interleaved retrieval-and-reasoning
</code></pre></div></div> <p>ì¦‰, ì¶”ë¡  ì¤‘ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ëŠ” íë¦„ì„ í†µí•´ external informationê°€ í•„ìš”í•œ reasoning-intensive tasksì—ì„œë„ ë” íš¨ê³¼ì ì¸ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ í•´ì¤Œ</p> <ul> <li> <p><strong>Formulation of RL with a Search Engine</strong></p> <p>LLMì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì›ë˜ ê¸°ì¡´ RLì˜ objectiveëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ë¨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ê·¸ëŸ°ë°, ìœ„ formulationì€ entire output sequence $ y $ê°€ $ \pi_{\theta} $ë¡œë¶€í„° ìƒì„±ë˜ì—ˆë‹¤ëŠ” ê°€ì •ì´ ìˆìŒ. ì´ ê°€ì •ì€ model behaviorê°€ internal reasoningê³¼ external information retrievalì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” ìƒí™©ì—ì„œ ì ìš©í•  ìˆ˜ ì—†ìŒ.
</code></pre></div> </div> <p>ë”°ë¼ì„œ, RL objectiveë¥¼ serach engine $ R $ê³¼ í†µí•©ì‹œí‚¤ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•¨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ìœ„ ìˆ˜ì •ëœ objectiveì—ì„œëŠ” trajectory $ y  $ëŠ” interleaved reasoning stepsê³¼ retrieved contentë¥¼ í¬í•¨
</code></pre></div> </div> </li> </ul> <p><br/></p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPOì™€ GRPOì—ì„œëŠ” token-level lossë¥¼ ì „ì²´ rollout sequenceì— ëŒ€í•´ ê³„ì‚°í•¨. í•˜ì§€ë§Œ Search-R1ì˜ rollout sequenceëŠ” LLMì´ ì§ì ‘ ìƒì„±í•œ tokenê³¼ external knowledgeì—ì„œ ê°€ì ¸ì˜¨ tokenì´ í•¨ê»˜ í¬í•¨ë¨.</p> <p>LLMì´ ì§ì ‘ ìƒì„±í•œ tokenì— ëŒ€í•´ ì†ì‹¤ì„ ìµœì í™”í•˜ëŠ” ê²ƒì€ modelì´ search engineê³¼ íš¨ê³¼ì ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•˜ê³  ì¶”ë¡ í•˜ëŠ” ëŠ¥ë ¥ì„ ë†’ì´ëŠ”ë° ë„ì›€ë¨. ê·¸ëŸ¬ë‚˜, ë™ì¼í•œ ìµœì í™”ë¥¼ ê²€ìƒ‰ëœ tokenì—ê¹Œì§€ ì ìš©í•˜ë©´ ì›ì¹˜ ì•ŠëŠ” í•™ìŠµ íš¨ê³¼ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.</p> <p>ë”°ë¼ì„œ, Search-R1ì€ <strong>ê²€ìƒ‰ëœ tokenì— ëŒ€í•œ loss maskingì„ ì ìš©í•˜ì—¬</strong>, policy gradient objectiveì€ LLMì´ ìƒì„±í•œ tokenì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ê³ , <strong>ê²€ìƒ‰ëœ contentëŠ” ìµœì í™” ê³¼ì •ì—ì„œ ì œì™¸ë¨</strong>.</p> <p>â†’ ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±ì˜ ìœ ì—°ì„±ì€ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì„</p> <p><br/></p> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1ì—ì„œëŠ” ê²€ìƒ‰ í˜¸ì¶œì´ í¬í•¨ëœ ì‹œë‚˜ë¦¬ì˜¤ì— ë§ì¶° PPOë¥¼ ì ìš©í•¨</p> <ul> <li> <p>$ \pi_{\theta} $: current policy</p> </li> <li> <p>$ \pi_{old} $: previous policy</p> </li> <li> <p>$ I(y_t) $: token loss masking ì—°ì‚°ìœ¼ë¡œ, $ y_t $ê°€ LLMì´ ìƒì„±í•œ tokenì´ë©´ 1, ê²€ìƒ‰ëœ tokenì´ë©´ 0ìœ¼ë¡œ ì„¤ì •</p> </li> </ul> <p><br/></p> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO ì—­ì‹œ PPOì™€ ë§ˆì°¬ê°€ì§€ë¡œ Search Engineì„ ì ìš©í• ë•Œ, ê²€ìƒ‰ëœ tokenì€ masking ì ìš©í•¨</p> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1ì´ ì–´ë–»ê²Œ multi-turn searchì™€ text ìƒì„±ì„ interleavedí•˜ê²Œ ìˆ˜í–‰í•˜ëŠ”ì§€ rollout processë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŒ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-  $ y $ ~ $ \pi_{\theta}(.|x;R) =\pi_{ref}(.|x)\bigotimes R $

â†’ LLMì€ xë¥¼ ì…ë ¥ ë°›ì•„ Search Engine $ R $ê³¼ì˜ interleaved íë¦„ì„ í†µí•´ yë¥¼ ìƒì„±
</code></pre></div></div> <p>Search-R1ì˜ ìƒì„± ê³¼ì •ì€ iterativeí•œ êµ¬ì¡°ë¡œ ì§„í–‰ë¨</p> <ul> <li><strong>LLMì€ textë¥¼ ìƒì„±í•˜ë‹¤ê°€ í•„ìš”í•  ë•Œë§ˆë‹¤ external search engine queriesë¥¼ ë³´ë‚¸ ë’¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë°˜ì˜í•˜ì—¬ ë‹¤ìŒ generation stepì„ ìˆ˜í–‰í•˜ë©° ì´ì–´ê°€ëŠ” ë°©ì‹</strong></li> </ul> <p><br/></p> <ul> <li> <p>system instructionì€ LLMì—ê²Œ external retrievalì´ í•„ìš”í•  ë•Œ search queryë¥¼ <search>ì™€ &lt;\search&gt; tokenìœ¼ë¡œ ê°ì‹¸ë„ë¡ í•¨</search></p> </li> <li> <p>generated sequenceì— ì´ëŸ¬í•œ tokenì´ ê°ì§€ë˜ë©´, systemì€ queryë¥¼ ì¶”ì¶œí•´ search engineì— ì „ë‹¬í•˜ê³  ì ì ˆí•œ relevant resultsë¥¼ ê°€ì ¸ì˜´</p> </li> <li> <p>retrieved informationì€ <information>ê³¼ &lt;\information&gt; tokenìœ¼ë¡œ ê°ì‹¸ì ¸ í˜„ì¬ rollout ì‹œí€€ìŠ¤ì— ì¶”ê°€ë¨. ì´ë ‡ê²Œ ì¶”ê°€ëœ ì •ë³´ëŠ” next generation stepì— ì¶”ê°€ contextë¡œ í™œìš©</information></p> </li> </ul> <p>ìœ„ ê³¼ì •ì´ ë°˜ë³µì ìœ¼ë¡œ ì´ì–´ê°€ë‹¤ê°€ ì•„ë˜ ë‘ ê°€ì§€ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•˜ë©´ ì¢…ë£Œí•¨</p> <ol> <li> <p>ì‚¬ì „ì— ì •ì˜ëœ ìµœëŒ€ í–‰ë™ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œ</p> </li> <li> <p>ëª¨ë¸ì´ ìµœì¢… ì‘ë‹µì„ ìƒì„±í•˜ì—¬ ì´ë¥¼ <answer>ì™€ &lt;\answer&gt; tokenìœ¼ë¡œ ê°ìŒ€ë•Œ</answer></p> </li> </ol> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1ì„ í•™ìŠµì‹œí‚¬ë•Œ ì‚¬ìš©í•˜ëŠ” prompt template</p> <ul> <li> <p>ì•„ë˜ templateì€ ëª¨ë¸ì´ ì¶œë ¥í•  êµ¬ì¡°ë¥¼ think â†’ search â†’ answer ìˆœì„œë¡œ ëª…í™•íˆ ë‚˜ëˆ„ë„ë¡ ìœ ë„í•¨</p> </li> <li> <p>ë‹¤ë§Œ íŠ¹ì • í•´ê²° ë°©ì‹ì´ë‚˜ ë°˜ì˜ ìˆ˜ì¤€ì„ ê°•ì œí•˜ì§€ ì•Šì•„ ëª¨ë¸ì´ RL ê³¼ì •ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„í•¨ (êµ¬ì¡°ì  í˜•ì‹ë§Œ ë”°ë¥´ê²Œ ì œí•œí•¨)</p> </li> </ul> <p><br/></p> <p><strong>Case Study</strong></p> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1ì€ outcome-based rule-based reward functionì„ ì‚¬ìš©í•¨</p> <ul> <li> <p>ì˜ˆë¥¼ ë“¤ì–´, factual reasoning taskì—ì„œ ì •ë‹µê³¼ ëª¨ë¸ì˜ ì¶œë ¥ì´ ì¼ì¹˜í•˜ëŠ”ì§€ exact matchë¡œ í‰ê°€í•¨</p> </li> <li> <p>ë³„ë„ì˜ í˜•ì‹ ë³´ìƒì´ë‚˜ ë³µì¡í•œ ê³¼ì • ê¸°ë°˜ ë³´ìƒì€ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ì‹ ê²½ë§ ê¸°ë°˜ ë³´ìƒ ëª¨ë¸ë„ í•™ìŠµí•˜ì§€ ì•Šì•„ í•™ìŠµ ë³µì¡ì„±ì„ ì¤„ì„</p> </li> </ul> <p><br/></p> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> <ol> <li> <p>Natural Questions (NQ)</p> </li> <li> <p>TriviaQA</p> </li> <li> <p>PopQA</p> </li> </ol> </li> <li> <p>Multi-Hop QA</p> <ol> <li> <p>HotpotQA</p> </li> <li> <p>2WikiMultiHopQA</p> </li> <li> <p>Musique</p> </li> <li> <p>Bamboogle</p> </li> </ol> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> <ol> <li> <p>Inference w/o Retrieval</p> <ol> <li> <p>Direct Inference</p> </li> <li> <p>Chain-of-Thought</p> </li> </ol> </li> <li> <p>Inference w/ Retrieval</p> <ol> <li> <p>RAG</p> </li> <li> <p>IRCoT (Information Retrieval CoT)</p> </li> <li> <p>Search-o1 (using search engine tool)</p> </li> </ol> </li> <li> <p>fine-tuning methods</p> <ol> <li> <p>SFT</p> </li> <li> <p>R1: search engineì—†ì´ RL fine-tuning (Search-R1ê³¼ fairí•œ ë¹„êµë¥¼ ìœ„í•´ ë™ì¼ ë°ì´í„°ë¡œ RLì„ í•™ìŠµí•˜ë˜ ê²€ìƒ‰ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)</p> </li> </ol> </li> </ol> <h2 id="43-experimental-setup">4.3 Experimental Setup</h2> <ul> <li> <p>LLMs: Qwen-2.5-3B, Qwen-2.5-7B (Base / Instruct)</p> </li> <li> <p>Retrieval</p> <ul> <li> <p>Knowledge Source: 2018 Wikipedia dump (using E5 as retriever)</p> </li> <li> <p>number of retrieved documents: 3</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>training data: NQ + HotpotQA for Search-R1 and fine-tuning methods</p> </li> <li> <p>evaluation data: (in-domain, out-of-domain)</p> </li> </ul> </li> <li> <p>metric: EM</p> </li> <li> <p>Inference ì„¤ì •</p> <ul> <li>Inference-style baselineì€ Instruct ëª¨ë¸ ì‚¬ìš© (Base ëª¨ë¸ì€ instructionì„ ë”°ë¥´ì§€ ëª»í•¨)</li> </ul> </li> <li> <p>RL ì„¤ì •</p> <ul> <li>ë³„ë„ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´ PPO ì‚¬ìš©</li> </ul> </li> </ul> <h2 id="44-performance">4.4 Performance</h2> <ul> <li> <p>Search-R1ì€ baselines ëŒ€ë¹„ ìš°ìˆ˜í•œ ì„±ëŠ¥ ë³´ì—¬ì¤Œ</p> <ul> <li> <p>Qwen2.5-7B: í‰ê· ì ìœ¼ë¡œ 41% í–¥ìƒ</p> </li> <li> <p>Qwen2.5-3B: í‰ê· ì ìœ¼ë¡œ 20% í–¥ìƒ</p> </li> </ul> <p>â†’ in-domain (NQ, HotpotQA)ì™€ out-of-domain (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle) <strong>ëª¨ë‘ ì¼ê´€ë˜ê²Œ ë†’ìŒ</strong></p> </li> <li> <p>ê²€ìƒ‰ ì—†ì´ ì¶”ë¡ ë§Œí•˜ëŠ” R1ë³´ë‹¤ë„ Search-R1ì´ ìš°ìˆ˜í•¨</p> <p>â†’ <strong>Searchê°€ LLM ì¶”ë¡ ì— external knowledgeë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ë„ì›€ë˜ëŠ” ê²ƒì„ ë³´ì„</strong></p> </li> <li> <p>Baseì™€ Instruct model ëª¨ë‘ ì¼ê´€ë˜ê²Œ Search-R1 íš¨ê³¼ì ì„</p> <p>â†’ DeepSeek-R1-Zero styleì˜ ë‹¨ìˆœ outcome-based rewardê°€ ìˆœìˆ˜ Reasoning ë¿ë§Œ ì•„ë‹ˆë¼ <strong>searchë¥¼ í¬í•¨í•œ complex reasoning scenariosì—ì„œë„ íš¨ê³¼ì ì„ì„</strong> ë³´ì—¬ì¤Œ</p> </li> <li> <p>**Model sizeê°€ í´ ìˆ˜ë¡ ê²€ìƒ‰ í™œìš© íš¨ê³¼ê°€ ë” í¼ **</p> </li> </ul> <p><br/></p> <h2 id="51-different-rl-methods-ppo-vs-grpo">5.1 Different RL methods: PPO vs. GRPO</h2> <p>Search-R1ì—ì„œ RL ë°©ë²•ìœ¼ë¡œ PPOì™€ GRPO ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ì‹¤í—˜í•¨</p> <ol> <li> <p><strong>GRPOëŠ” PPOë³´ë‹¤ ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¦„</strong> â†’ Figure2 (a)</p> <ol> <li>PPOëŠ” critic modelì— ì˜ì¡´í•˜ê¸° ë•Œë¬¸ì— íš¨ê³¼ì ì¸ í•™ìŠµì´ ì‹œì‘ë˜ë ¤ë©´ ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì›Œë°ì—…ì´ í•„ìš”í•˜ì§€ë§Œ, GRPOëŠ” baselineì„ ì—¬ëŸ¬ ìƒ˜í”Œ í‰ê· ìœ¼ë¡œ ì¡ì•„ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•¨</li> </ol> </li> <li> <p><strong>PPOëŠ” í•™ìŠµ ì•ˆì •ì„±ì´ ë” ë†’ìŒ</strong> â†’ Figure2 (a)</p> <ol> <li><span style="color:red_background"><strong>GRPOëŠ” ì¼ì • ë‹¨ê³„ ì´í›„ reward collapse</strong></span>ê°€ ë‚˜íƒ€ë‚˜ì§€ë§Œ, <span style="color:yellow_background"><strong>PPOëŠ” í•™ìŠµì´ ë” ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ë¨</strong></span></li> </ol> </li> <li> <p><strong>ìµœì¢… train rewardëŠ” PPOì™€ GRPO ëª¨ë‘ ìœ ì‚¬í•¨</strong></p> <ol> <li>ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„±ì€ ë‹¤ë¥´ì§€ë§Œ ìµœì¢… ì„±ëŠ¥ê³¼ train rewardëŠ” í° ì°¨ì´ê°€ ì—†ìŒ. ê·¸ë˜ë„ GRPOëŠ” ë‚˜ì¤‘ì— ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆê¸°ì— ë” ì•ˆì •ì ì¸ PPOê°€ ëŠë¦¬ì§€ë§Œ ì í•©í•¨.</li> </ol> </li> </ol> <p><br/></p> <p><em>(ë‹¤ë¥¸ ì„¸íŒ…ì—ì„œë„ ë™ì¼í•œ í˜„ìƒì´ ê´€ì°°ë¨)</em></p> <h2 id="52-base-vs-instruct-llms">5.2 Base vs. Instruct LLMs</h2> <ul> <li> <p>Figure2 (b)ì—ì„œ Instruction-tuned modelì€ Base modelë³´ë‹¤ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ê³  ì´ˆê¸° ì„±ëŠ¥ë„ ë” ë†’ê²Œ ë‚˜ì˜¤ì§€ë§Œ, <strong>ìµœì¢… train rewardëŠ” ë‘ ëª¨ë¸ ëª¨ë‘ ê±°ì˜ ë™ì¼í•œ ìˆ˜ì¤€ìœ¼ë¡œ ìˆ˜ë ´í•¨</strong></p> <p>â†’ ì´ëŠ” ì‚¬ì „ instruction tuningì´ ì´ˆê¸° í•™ìŠµì„ ê°€ì†í™”í•˜ëŠ”ë° ë„ì›€ì´ ë˜ì§€ë§Œ, <strong>RLë§Œìœ¼ë¡œë„ Base modelì´ ì¶©ë¶„íˆ ë”°ë¼ì¡ì„ ìˆ˜ ìˆìŒì„ ë³´ì„</strong></p> <p><br/></p> </li> </ul> <p>(ë‹¤ë¥¸ ì„¸íŒ…ì—ì„œë„ ë™ì¼í•œ í˜„ìƒì´ ê´€ì°°ë¨)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## 5.3 Response Length and Valid Search Study
</code></pre></div></div> <p>Qwen2.5-7B-base ëª¨ë¸ë¡œ response lengthì™€ ê²€ìƒ‰ í˜¸ì¶œ íšŸìˆ˜ ë³€í™”ë¥¼ ë¶„ì„í•¨</p> <ul> <li> <p>Figure2 (c)ë¥¼ ë³´ë©´</p> <ul> <li> <p>ì´ˆê¸° ë‹¨ê³„ (100 steps ì „í›„)</p> <ul> <li> <p><strong>ì‘ë‹µ ê¸¸ì´ê°€ ê¸‰ê²©íˆ ì¤„ê³ , train rewardëŠ” ì†Œí­ ìƒìŠ¹í•¨</strong></p> </li> <li> <p>ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ êµ°ë”ë”ê¸° ë‹¨ì–´ë¥¼ ì¤„ì´ê³  taskì— ì ì‘í•˜ê¸° ì‹œì‘í•¨ì„ ë³´ì—¬ì¤Œ</p> </li> </ul> </li> <li> <p>í›„ê¸° ë‹¨ê³„ (100 steps ì´í›„)</p> <ul> <li> <p><strong>ì‘ë‹µ ê¸¸ì´ì™€ train reward ëª¨ë‘ ì¦ê°€í•¨</strong></p> </li> <li> <p>ëª¨ë¸ì´ ê²€ìƒ‰ í˜¸ì¶œì„ ë” ìì£¼ í•˜ë©´ì„œ (Search Engineì„ ìì£¼ í˜¸ì¶œí•˜ëŠ” ë²• í•™ìŠµ) ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ë˜ì–´ ì‘ë‹µì´ ê¸¸ì–´ì§</p> </li> <li> <p>ê²€ìƒ‰ ê²°ê³¼ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ë©° train rewardë„ í¬ê²Œ í–¥ìƒë¨</p> </li> </ul> </li> </ul> </li> <li> <p>Figure2 (d)ë¥¼ ë³´ë©´ <strong>í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ LLMì´ ê²€ìƒ‰ ì—”ì§„ í˜¸ì¶œì„ ë” ë§ì´ í•™ìŠµí•œë‹¤ëŠ” ì ì´ ë“œëŸ¬ë‚¨</strong></p> </li> </ul> <p><br/></p> <h2 id="54-study-of-retrieved-tokens-loss-masking">5.4 Study of Retrieved Tokens Loss Masking</h2> <p>Retrieved Token Loss Maskingì€ unintended optimizationì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë„ì…í•œ ê²ƒì„. Retrieved token loss maskingì˜ íš¨ê³¼ë¥¼ ì¶”ê°€ë¡œ ë¶„ì„í•´ë´„ (Qwen2.5-7B-base)</p> <ul> <li> <p>Figure 3ì— ë”°ë¥´ë©´, <strong>maskingì„ ì ìš©í•˜ë©´ ì›ì¹˜ ì•ŠëŠ” ìµœì í™” íš¨ê³¼ë¥¼ ì¤„ì´ê³  LLM ì„±ëŠ¥ í–¥ìƒì´ ë” ì»¤ì§</strong></p> <ul> <li> <p>w. maskì™€ w.o. maskë¥¼ ë¹„êµí•œ ê²°ê³¼ **maskingì„ ì ìš©í•œ ê²½ìš°ê°€ í•­ìƒ ë” ë†’ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•¨ **</p> </li> <li> <p><strong>Appendix</strong></p> </li> </ul> <p><strong>Number of Retrieved Passages Study in SEARCH-R1 Training</strong></p> <ul> <li> <p>ë³¸ ì‹¤í—˜ì—ì„œëŠ” top-kë¥¼ 3ìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ, 1,3,5 ë°”ê¿”ê°€ë©° ì´ê²ƒì˜ effectë¥¼ ë¶„ì„í•¨</p> <p>â†’ top-kê°€ 1,3,5 ì„¤ì •ì´ <strong>ëª¨ë‘ training patternì´ ë¹„ìŠ·í•¨</strong> (top-k=5ê°€ ì´ˆê¸° ìˆ˜ë ´ ì†ë„ê°€ ë¹ ë¥¸ ëŒ€ì‹ ì— ì´í›„ train rewardê°€ ê°ì†Œí•˜ë©° í•™ìŠµ ì•ˆì •ì„± ë–¨ì–´ì§ì„ ë³´ì„)</p> </li> </ul> <p><br/></p> <p><strong>Group Size Study in SEARCH-R1 (GRPO) Training</strong></p> <ul> <li> <p>ë³¸ ì‹¤í—˜ì—ì„œëŠ” Search-R1 (GRPO)ì˜ group sizeë¥¼ 5ë¡œ ì„¤ì •í–ˆì§€ë§Œ, group sizeê°€ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ê³ ì 1,3,5ë¡œ ë¶„ì„í•¨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      â†’ Figure7ì„ ë³´ë©´ &lt;span style='color:red_background'&gt;**group sizeê°€ ì¹¼ìˆ˜ë¡ ìˆ˜ë ´ ì†ë„ ë¹¨ë¼ì§€ëŠ” ë°˜ë©´ RLì˜ ë¶ˆì•ˆì •ì„± ë•Œë¬¸ì— training collapse ìœ„í—˜ë„ ì¦ê°€**&lt;/span&gt;

      â†’ Table8ì„ ë³´ë©´ group sizeê°€ í° ê²½ìš° ë¹ ë¥¸ ìˆ˜ë ´ê³¼ ë” ë†’ì€ train rewardê°€ ìˆì—ˆì§€ë§Œ, group size=1ì¼ ë•Œ í•™ìŠµì´ ë” ì•ˆì •ì ì´ê³  ì¼ë°˜í™” ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•¨ (out-of-domainì—ì„œ ë” ìš°ìˆ˜í•¨)
</code></pre></div> </div> <p><br/></p> </li> </ul> </li> <li> <p>ë³¸ ì—°êµ¬ì—ì„œëŠ” LLMì´ self-reasoningê³¼ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì—”ì§„ ìƒí˜¸ì‘ìš©ì„ êµì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” frameworkì¸ Search-R1 ì œì•ˆí•¨</p> </li> <li> <p>ê¸°ì¡´ì˜ multi-turn searchë¥¼ ìœ„í•´ ë§ì€ promptì— ì˜ì¡´í•˜ëŠ” RAGë‚˜ ëŒ€ê·œëª¨ train dataê°€ í•„ìš”í•œ tool ì‚¬ìš© ê¸°ë°˜ ì ‘ê·¼ë²•ê³¼ ë‹¬ë¦¬, <strong>Search-R1ì€ RLì„ í†µí•´ ëª¨ë¸ì´ ììœ¨ì ìœ¼ë¡œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê³  ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ì „ëµì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”í•¨</strong></p> </li> </ul> <p>Limitations</p> <ul> <li>Reward Designê°€ ë‹¨ìˆœ ê²°ê³¼ ê¸°ë°˜ ë³´ìƒì´ë¼ ë³´ë‹¤ ë””ë²¨ë¡­ì´ í•„ìš”í•¨</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning]]></summary></entry><entry><title type="html">Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title><link href="https://alshedivat.github.io/al-folio/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of-reasoning-length-and-correctness-in-llms/" rel="alternate" type="text/html" title="Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of-reasoning-length-and-correctness-in-llms</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of-reasoning-length-and-correctness-in-llms/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> <li><strong>Reviewer</strong>: ì¤€ì› ì¥</li> <li><strong>Property</strong>: Reasoning</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>Test-time scaling is trending, but <strong>longer reasoning is not always better.</strong></p> </li> <li> <p>Reasoningì™€ accuracyê°€ í•­ìƒ ìƒê´€ê´€ê³„ë¥¼ ì´ë£¨ì§€ ì•ŠëŠ”ë‹¤ëŠ” ìµœì‹ ì—°êµ¬ ì¡´ì¬ (Xie et al., 2025; Jin et al., 2024; Wu et al., 2025)</p> <p>(ì—°êµ¬ë“¤ ì•ˆë´¤ì§€ë§Œ (1) ëª¨ë¸ì´ ì‘ê±°ë‚˜ (2) 32K, 64Kê¹Œì§€ inferenceì•ˆí•´ë´ì„œ ê·¸ëŸ´ê±° ê°™ë‹¤ê³  ìƒê°.. o3ë„ ì•„ì˜ˆ ëª»í‘¸ëŠ” lcb pro ìˆ˜ì¤€ì´ë¼ë©´ ì˜ˆì™¸)</p> </li> <li> <p>ì—¬íŠ¼, ì´ëŸ¬í•œ íë¦„ì— ë”°ë¼ ìµœê·¼ì— ë‚˜ì˜¨ ìš©ì–´</p> <ul> <li>Overthinking phenomenon; simple problemsì—ë„ verbose outputsì„ ìƒì„±í•˜ëŠ” í˜„ìƒ</li> </ul> </li> <li> <p>ê·¸ë˜ì„œ ë…¼ë¬¸ì€ DeepSeek-1.5B-Distillê³¼ DeepScaler-1.5B-Previewë¥¼ ê°€ì§€ê³  reasoning lengthì™€ accuracyë¥¼ ê°€ì§€ê³  ì²´ê³„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê² ë‹¤!</p> </li> </ul> <p><br/></p> <h2 id="2-related-work">2. Related Work</h2> <p>â‡’ lengthy reasoning ë¬¸ì œë¥¼ ê´€ì¸¡í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•™ìŠµë°©ë²•ë¡ ë“¤</p> <ul> <li> <p><strong>Concise thinking</strong></p> <ul> <li>rewardë‚˜ pre-sampling (ë­”ì§„ ëª°ê² ìŒ..)ìœ¼ë¡œ RL/SFT-trainingë•Œ good accuracyë¥¼ ìœ ì§€í•˜ë©´ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì§§ì€ reasoning pathë¥¼ ìƒì„±í•˜ëŠ” ì—°êµ¬ ê³„ì—´</li> </ul> </li> <li> <p><strong>Adaptive thinking</strong></p> <ul> <li> <p>(prompting ìœ„ì£¼) ë¬¸ì œ ë‚œì´ë„Â·ëª¨ë¸ í™•ì‹ ë„ì— ë”°ë¼ í† í° ì˜ˆì‚°ì„ ë™ì ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ ì¡°ê¸° ì¢…ë£Œ</p> <p>â†’ lengthy reasoning pathê°€ high accuracyë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì‹¤í—˜ì ì¸ ê²°ê³¼ë¥¼ ë³´ì´ëŠ” ê²½ìš°ê°€ ë§ìŒ</p> </li> </ul> </li> <li> <p><strong>Optimal Thinking</strong></p> <ul> <li> <p>reasoning pathê°€ ê¸¸ì–´ì§€ë©´ ì„±ëŠ¥ì´ ì²˜ìŒì—” ì˜¤ë¥´ë‹¤ê°€ ë‹¤ì‹œ ë–¨ì–´ì§„ë‹¤ëŠ” ì ì„ ì´ë¡ Â·ì‹¤í—˜ìœ¼ë¡œ ì…ì¦</p> <p>(â€¦ì´ê²Œ ì´ë¡ ìœ¼ë¡œ ì…ì¦ì´.. ë˜ë‚˜..?)</p> </li> </ul> </li> </ul> <p><br/></p> <h2 id="3-experimental-setting">3. Experimental Setting</h2> <ul> <li> <p>Model</p> <ul> <li> <p>DeepSeek-1.5B-Distill (Denoted as R1-Distill)</p> </li> <li> <p>DeepScaler-1.5B-Preview (Denoted as R1-Preview)</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>GSM8K</p> </li> <li> <p>MATH</p> </li> </ul> </li> <li> <p>Params.</p> <ul> <li> <p>temperature T = 1.0 (most calibrated)</p> </li> <li> <p>top-p = 1</p> </li> </ul> </li> <li> <p>Notations</p> <ul> <li> <p>question: $ q $</p> </li> <li> <h1 id="of-completions--n-">of completions: $ N $</h1> </li> <li> <p>$ {(o(q)<em>i , l(q)_i , c(q)_i )}^{N âˆ’1}</em>{i=0} $</p> <ul> <li> <p>$ o() $: output</p> </li> <li> <p>$ l() $: length</p> </li> <li> <p>$ l() $: correctness ì—¬ë¶€ {0,1}</p> </li> </ul> </li> </ul> </li> </ul> <p><br/></p> <h2 id="4-sample-level-analysis">4. Sample-Level Analysis</h2> <p>â†’ qëŠ” ê³ ì •í•˜ê³  ê¸¸ì´ê°€ ë‹¤ë¥¸ 10ê°œ completionì„ ë¹„êµí•´ lengthì™€ accuracyì˜ ì§ì ‘ ìƒê´€ì„ ì¡°ì‚¬</p> <ul> <li>ë‚œì´ë„ì— ëŒ€í•œ ë³€ì¸ì„ ê³ ì •í•˜ê³  length â†” accuracy ê´€ê³„ë§Œ ë³¼ ìˆ˜ ìˆìŒ</li> </ul> <h3 id="non-linear-relationship-of-sample-length-and-correctness">Non-Linear Relationship of Sample Length and Correctness</h3> <ul> <li> <p>$ L_r, Acc_r $: rë²ˆì§¸ë¡œ ì§§ì€ reasoning pathì˜ í‰ê·  length/accuracy</p> <p>(ëª¨ë“  qì— ëŒ€í•´ 10ê°œì”© ë‹¤ ìƒì„±í•˜ê³  ê°€ì¥ ì§§ì€ completionì˜ ê¸¸ì´ í‰ê· : $ L_0 $)</p> </li> <li> <p>consistent non-monotonic trend ê´€ì°°</p> <ul> <li>Distill ê¸°ì¤€ ì–´ëŠì •ë„ëŠ” ê¸¸ì–´ì•¼ best acc, ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ decline</li> </ul> </li> </ul> <p><br/></p> <p>(ì¤€ì› ë‡Œí”¼ì…œ: ì¼ë‹¨ R1ì€ (1) MATH ê´€ë ¨ ë°ì´í„°ëŠ” ì™¸ì›Œì„œ í’€ê²ƒ ê°™ê¸° ë•Œë¬¸ì— temp=1.0, top_p=1ë¡œ ì¤˜ì„œ decoding path ê¸¸ì–´ì§€ë©´ degen ë°œìƒí–ˆì„ê²ƒìœ¼ë¡œ ì˜ˆìƒ (2) GSM8K ìœ ì‚¬ ë‚œì´ë„ëŠ” ê±°ì˜ ì™¸ì› ì„ê²ƒì´ê³  + ìƒëŒ€ì ìœ¼ë¡œ ì‰½ê¸° ë•Œë¬¸ì— 1~1.5K thinking budgetë‚´ë¡œëŠ” ê±°ì˜ ë¹„ìŠ·í• ê±° ê°™ìŒ..)</p> <p><br/></p> <ul> <li> <p>ì´ˆë¡: qì— ëŒ€í•œ ì •ë‹µ completionì¤‘ ê°€ì¥ ì§§ì€ ê±°</p> </li> <li> <p>íŒŒë‘: qì— ëŒ€í•œ ì •ë‹µ completionì¤‘ ê°€ì¥ ê¸´ê±°</p> </li> <li> <p>ë¹¨ê°•: qì— ëŒ€í•œ ì˜¤ë‹µ completionì¤‘ ê°€ì¥ ì§§ì€ ê±°</p> </li> <li> <p>ë…¸ë‘: qì— ëŒ€í•œ ì •ë‹µ completionì¤‘ ê°€ì¥ ê¸´ê±°</p> </li> </ul> <p><br/></p> <ul> <li> <p>R1-PreviewëŠ” MATH, GSM8K ëª¨ë‘ 80% ì´ìƒì˜ ì§ˆë¬¸ì—ì„œ ê°€ì¥ ì§§ì€ ìƒ˜í”Œë¡œ ì •ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì„</p> </li> <li> <p>most lengthí•œ completionì¤‘ì— correct responseë„ ìˆì§€ë§Œ incorrect responseë„ ì¡´ì¬ (ë…¼ë¬¸ í•´ì„ ì´ìƒ..)</p> </li> </ul> <p><br/></p> <h2 id="5-question-level-analysis">5. Question-Level Analysis</h2> <ul> <li>ë‹¨ìˆœí•˜ê²Œ ë¬¸ì œ ë‚œì´ë„ë¥¼ í‹€ë¦¼ ì—¬ë¶€ë¡œ ë³¼ë•Œ, incorrect responseê°€ ì–´ë–¤ ì¡°í•©ì—ì„œë“  response ê¸¸ì´ê°€ ë” ê¸¸ì—ˆìŒ</li> </ul> <p><br/></p> <p><br/></p> <ul> <li> <p>N-completionë³„ë¡œ difficultyë¥¼ ë¶„ë¥˜</p> <ul> <li> <p>Easy: modelì´ 10 completion ëª¨ë‘ ì •ë‹µë¥  100%</p> </li> <li> <p>Medium: modelì´ 10 completionì—ì„œ ì •ë‹µë¥  0% &lt; acc &lt; 100%</p> </li> <li> <p>Hard: modelì´ 10 completionì—ì„œ ì •ë‹µë¥  0%</p> </li> </ul> </li> </ul> <p>â‡’ ê·¸ëŸ¬ë‚˜ (1) ë¬¸ì œê°€ ì–´ë ¤ì›Œì„œ lengthyí•œì§€ (2) lengthí•´ì ¸ì„œ í‹€ë¦°ê±´ì§€ íŒë‹¨ì´ ì–´ë ¤ì›€</p> <p><br/></p> <p><br/></p> <ul> <li> <p>$ Q^{easy}<em>{\cap} = Q^{easy}</em>{i} \cap Q^{easy}_{j} $</p> </li> <li> <p>$ Q^{easy}<em>{i/j} = Q^{easy}</em>{i} / Q^{easy}_{j} $ &gt; $ M_i $ ì—ì„œë§Œ ì‰¬ìš´ ë¬¸ì œ</p> </li> <li> <p>$ Q^{easy}<em>{j/i} = Q^{easy}</em>{j} / Q^{easy}_{i} $ &gt; $ M_j $ì—ì„œë§Œ ì‰¬ìš´ ë¬¸ì œ</p> </li> </ul> <p><br/></p> <ul> <li> <p>ë³´í¸ì ìœ¼ë¡œ ì‰¬ìš´ ë¬¸ì œê°€ ì•„ë‹ˆë¼ another modelâ€™s advantage set (ë‹¤ë¥¸ëª¨ë¸ì—ì„œ ì‰¬ìš´ ë¬¸ì œ)ì—ì„œ ì˜¤íˆë ¤ lengthy generationì„ ë³´ì„</p> </li> <li> <p>signficantë¡œ ë³´ë©´ $ M_i $ â†’ $ M_j $-Adv Setì„ í’€ë•Œ ë³´ë‹¤ lengthyí•´ì§</p> </li> </ul> <p><br/></p> <p><br/></p> <ul> <li> <p>hard questionì—ì„œëŠ” $ Q^{hard}_{\cap} $ì—ì„œ ë³´ë‹¤ another modelâ€™s advantage setì—ì„œ lengthyí•´ì§ˆ ê²ƒì„ ê¸°ëŒ€í–ˆìœ¼ë‚˜ ê·¸ë ‡ì§„ ì•ŠìŒ</p> <ul> <li>hard questionì—ì„œ clear patternì€ ì—†ìŒ</li> </ul> <p>â†’ ë¬¸ì œê°€ ë„ˆë¬´ ì–´ë ¤ìš´ ê²½ìš° ëª¨ë¸ì´ ì–´ë ¤ìš´ ë¬¸ì œì˜ ë‚œì´ë„ ì¦ê°€ë¥¼ ì¸ì‹í•˜ê³  ì´ì— ëŒ€ì‘í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆìŒ (e.g., ë¬¸ì œ ë‚œì´ë„ë¥¼ ê³¼ì†Œí‰ê°€í•˜ì—¬ ì§§ê²Œ ìƒì„±)</p> <p><br/></p> </li> </ul> <p><br/></p> <ul> <li> <p>(ì–´ë–»ê²Œ ì‹¤í—˜í–ˆëŠ”ì§€ëŠ” ëª¨ë¥´ê² ëŠ”ë°..) token lengthê°€ ì§§ì•„ì§ˆìˆ˜ë¡ accuracyê°€ ì˜¬ë¼ê°„ë‹¤.</p> </li> <li> <p>ìœ„ì— ì‹¤í—˜ì„ ê¸°ë°˜ìœ¼ë¡œ token legnthê°€ ì§§ìœ¼ë‹ˆ í™•ë¥ ì ìœ¼ë¡œ ë‹¹ì—°íˆ accuracyê°€ ë†’ì€ ë‹µë³€ì¼ìˆ˜ë¡ PPLë„ ë‚®ì„ ê²ƒ</p> </li> </ul> <p><br/></p> <h2 id="6-effect-of-length-preference-optimization">6. Effect of Length Preference Optimization</h2> <ul> <li> <p>ì§€ê¸ˆê¹Œì§€ ì§€ì ëœ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ correct/length-balanced reward-based RLë“±ì´ ì†Œê°œë˜ì—ˆìŒ</p> <p>(NÂ» samples, ground truth labelì´ í•„ìš”)</p> </li> <li> <p>ì´ë¥¼ ìœ„í•´ ì´ì „ì— drive-outí•œ ì§ê´€ë“¤ì„ ê°€ì§€ê³  ê°„ë‹¨í•œ ì‹¤í—˜ì„ ì§„í–‰.</p> <ul> <li> <p>LMì„ ê°€ì§€ê³  2ê°œì˜ completionì„ ìƒì„±</p> </li> <li> <p>(ë°ì´í„°ì…‹ì´ ì‰¬ì› ìœ¼ë‹ˆ) ì •ë‹µ ìœ ë¬´ì— ìƒê´€ì—†ì´ ì§§ì€ responseê°€ ì •ë‹µì¼ í™•ë¥ ì´ ë†’ì„ê±°ë¼ëŠ” ê°€ì •í•˜ì— ì§§ì€ responseì— preferenceê°€ ê°€í•´ì§€ë„ë¡ SimPO</p> </li> <li> <p>MATH/GSM8K training set, 8K rollout</p> </li> </ul> </li> <li> <p>training stepì„ ë°˜ë³µí• ìˆ˜ë¡ accuracy ë³€ë™í­ì€ ì ìœ¼ë‚˜ average token length 30%ì—ì„œ 60% ê°ì†Œ</p> <p>(length decreaseê°€ ì •ë‹µì˜ ë‹¨ì¶• ë•Œë¬¸ì¸ì§€, ì˜¤ë‹µì˜ ë‹¨ì¶• ë•Œë¬¸ì¸ì§€, ì•„ë‹ˆë©´ ë‘˜ ë‹¤ ë•Œë¬¸ì¸ì§€â€¦?)</p> </li> </ul> <p><br/></p> <p><br/></p> <ul> <li> <p>SimPOê°€ ì§„í–‰ë¨ì— ë”°ë¼ incorrect responseì˜ ìƒì„±ì´ ì¤„ì–´ë“¤ì—ˆë‹¤.</p> <p>â†’ (ì¤€ì›) í•´ì„ì„ í•´ë³´ë©´,</p> <ul> <li> <p>ì–´ì§œí”¼ ë§ì„ ë¬¸ì œëŠ” 2 completion ë‹¤ ì§§ì„ê±°ì˜€ìœ¼ë‹ˆ ê·¸ ì¤‘ì—ì„œë„ ì§§ê²Œ ìƒì„±í•˜ë„ë¡ model í•™ìŠµ</p> </li> <li> <p>í‹€ë¦° ë¬¸ì œëŠ” 2 completion ë‹¤ ê¸¸ê²Œ ìƒì„±í–ˆì„ê²ƒì´ë‚˜ (ê·¸ ì¤‘ í•˜ë‚˜ëŠ” ì¡°ê¸ˆì´ë¼ë„ ì§§ê²Œ ìƒì„±í–ˆì„í…Œë‹ˆ) í•™ìŠµì´ ë¨ì— ë”°ë¼ ì¡°ê¸ˆì”© ì§§ê²Œ ìƒì„±í•˜ë„ë¡ í–ˆì„ ê²ƒ</p> </li> </ul> </li> </ul> <p><br/></p> <h2 id="7-conclusion---limitation">7. Conclusion &amp; Limitation</h2> <ul> <li> <p>generation lengthì™€ final answer correctnessì— ëŒ€í•´ì„œ ì‹¬ë„ ìˆëŠ” ë¶„ì„</p> <ul> <li>ë³€ì¸ í†µì œë„ ì‹ ê²½ì¼ê³ , takeawayë„ ë§ìŒ</li> </ul> </li> <li> <p>LMì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ê³ , benchmarkê°€ ë„ˆë¬´ ì‰¬ì›€â€¦</p> <ul> <li>í° LMë„ ì–´ë ¤ìš´ ë¬¸ì œì— ëŒ€í•´ì„œ lengthy generationì„ í•˜ë©´ì„œ ì˜ ëª»í‘¸ëŠ” ëª¨ìŠµ (reflexion x)ì„ ë§ì´ ê´€ì°°í–ˆëŠ”ë°, ê´€ë ¨ ë‚´ìš©ì´ ìˆì—ˆìœ¼ë©´ ì¢‹ì•˜ì„ë“¯..</li> </ul> </li> </ul> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs]]></summary></entry><entry><title type="html">Reasoning Models Can Be Effective Without Thinking</title><link href="https://alshedivat.github.io/al-folio/blog/2025/reasoning-models-can-be-effective-without-thinking/" rel="alternate" type="text/html" title="Reasoning Models Can Be Effective Without Thinking"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/reasoning-models-can-be-effective-without-thinking</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/reasoning-models-can-be-effective-without-thinking/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> </ul> <p><br/></p> <p>LLMì„ ì´ìš©í•´ ë³µì¡í•œ ë¬¸ì œë¥¼ í’€ ë•Œ, ë³´í†µ ìš°ë¦¬ëŠ” â€œê¸´ chains of thoughtsâ€ë¥¼ ìƒì„±í•˜ê³  ê·¸ê²ƒì„ ì´ìš©í•´ reflection, backtracking, self-validation ë“±ì„ ìˆ˜í–‰í•˜ê³¤ í•œë‹¤ (â€œThinkingâ€). ì´ëŸ¬í•œ reasoning pathëŠ” ì¼ë°˜ì ìœ¼ë¡œ rewardë¥¼ ì´ìš©í•œ ê°•í™”í•™ìŠµ, í˜¹ì€ distilled reasoning traceë¥¼ ì´ìš©í•œ finetuningì„ í†µí•´ì„œ íšë“ë˜ë©°, ì´ <strong>explicití•œ reasoning pathê°€ ì‹¤ì œë¡œ ì„±ëŠ¥ì— ë§ì€ ë„ì›€ì´ ëœë‹¤ê³  ë¯¿ì–´ì ¸ì™”ë‹¤</strong>. ì´ ë•Œë¬¸ì— inference-time compute scalingì´ ì£¼ëœ paradigmì´ê¸°ë„.</p> <p><br/></p> <p>í•˜ì§€ë§Œ ì €ìë“¤ì€ ì´ì— ëŒ€í•œ ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì„ ë˜ì§„ë‹¤:</p> <ul> <li>ì •ë§ë¡œ explicit Thinking processê°€ ìƒìœ„ reasoningì„ ìœ„í•´ í•„ìš”í•œê°€?</li> </ul> <p>ê·¸ë¦¬ê³  ì €ìë“¤ì€ ì‚¬ì‹¤ ì •êµí•œ reasoning pathì€ ê·¸ë‹¥ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ì„œ ì¦ëª…í•œë‹¤.</p> <p><br/></p> <p>ì €ìë“¤ì€ DeepSeek-R1-Distill-Qwenì„ ì´ìš©í•´ì„œ Thinkingê³¼ ì´ë“¤ì´ ì‚¬ìš©í•˜ëŠ” NoThinking â€” ì‘ë‹µì— ê°€ì§œ Thinking ë¸”ë¡ì„ ë¯¸ë¦¬ ì±„ì›Œ ë„£ê³ , ëª¨ë¸ì´ ê·¸ ì´í›„ë¶€í„° ì´ì–´ì„œ ë‹µë³€í•˜ë„ë¡ í•˜ëŠ” ë°©ë²• â€” ì„ ë¹„êµí•´ë³´ì•˜ì„ ë•Œ, ì˜¤íˆë ¤ NoThinkingì´ í›¨ì”¬ ë” ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ê²ƒì„ ë³´ì¸ë‹¤ (pass@k metrics).</p> <p>NoThinkingì€ 2.0~5.1ë°° ì ì€ í† í°ì„ ì‚¬ìš©í•˜ë©´ì„œë„, k=1ì„ ì œì™¸í•˜ê³ ëŠ” Thinkingê³¼ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.</p> <p>ë˜í•œ, ë‘ ì ‘ê·¼ë²•ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ë™ì¼í•˜ê²Œ í†µì œí–ˆì„ ë•Œ, NoThinkingì€ íŠ¹íˆ low-budget êµ¬ê°„ì—ì„œ pass@1ê°€ Thinkingë³´ë‹¤ ë†’ì•˜ìœ¼ë©°, kê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ ì°¨ì´ëŠ” ë” ì»¤ì¡Œë‹¤ (Figure 2).</p> <p>íš¨ìœ¨ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ pass@kì™€ í‰ê·  í† í° ì‚¬ìš©ëŸ‰ ê°„ì˜ Pareto frontierë¥¼ ë¶„ì„í•œ ê²°ê³¼, NoThinkingì€ í•­ìƒ Thinkingë³´ë‹¤ ìš°ìˆ˜í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. ì´ëŠ” ëª…ì‹œì ì¸ ì¶”ë¡  ê³¼ì •ì„ ê±°ì¹˜ì§€ ì•Šë”ë¼ë„ NoThinkingì´ ë” ì¢‹ì€ accuracy-cost tradeoffsë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.</p> <p><br/></p> <p>pass@kì—ì„œ NoThinkingì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤ëŠ” ê²ƒì€, Thinkingì„ ì‚¬ìš©í•œ ìˆœì°¨ì  ì¶”ë¡ ì´ ì•„ë‹Œ, parallel scalingì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì €ìë“¤ì€ ì—¬ëŸ¬ ì‘ë‹µì„ ë³‘ë ¬ë¡œ ìƒ˜í”Œë§í•˜ê³ , best-of-Nìœ¼ë¡œ ìµœì¢… ì‘ë‹µì„ ê³ ë¥´ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•œë‹¤.</p> <p>ì´ë“¤ì´ ê³ ë ¤í•œ taskëŠ” ë‘ ê°€ì§€ì´ë‹¤:</p> <ol> <li> <p>tasks with perfect verifiers (e.g., formal theorem proving): ìë™ìœ¼ë¡œ ì •ë‹µ ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ê²½ìš°</p> </li> <li> <p>tasks without verifiers (e.g., general problem solving): simple confidence-based selection strategiesë¥¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ê²½ìš°</p> </li> </ol> <p>verifiersê°€ ìˆëŠ” ê²½ìš°, NoThinkingì´ Thinkingì„ ê°€ë¿íˆ ëŠ¥ê°€í–ˆë‹¤ ( both with and without parallel scaling). íŠ¹íˆ, ì§€ì—° ì‹œê°„ì„ 7ë°° ë‹¨ì¶•í•˜ê³  ì´ í† í° ì‚¬ìš©ëŸ‰ì„ 4ë°°ë‚˜ ì¤„ì˜€ë‹¤ëŠ” ì ì´ ì´ì . verifiersê°€ ì—†ëŠ” ê²½ìš°ì—ë„ NoThinking ì¤€ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Thinkingì„ 9Ã— lower latency + improved accuracy on OlympiadBench (Math)ë¡œ ëŠ¥ê°€í•¨. (Figure 3)</p> <p><br/></p> <p>ìš”ì•½í•˜ìë©´, ì´ ì—°êµ¬ëŠ” í˜„ì¬ì˜ ì¶”ë¡  ëª¨ë¸ë“¤ì´ í•™ìŠµ ê³¼ì •ì—ì„œ êµ¬ì¡°í™”ëœ ì¶”ë¡  í˜•ì‹ì„ ë”°ë¥´ë„ë¡ í›ˆë ¨ë˜ì—ˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ë†’ì€ ì„±ëŠ¥ì„ ë‚´ê¸° ìœ„í•´ ë°˜ë“œì‹œ ëª…ì‹œì ì¸ thinking ê³¼ì •ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ ì²˜ìŒìœ¼ë¡œ ë³´ì—¬ì£¼ê³  ìˆë‹¤.</p> <p>ë˜í•œ, NoThinking ë°©ì‹ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë©´, ìˆœì°¨ì  ì¶”ë¡ ë³´ë‹¤ ë” ì¢‹ì€ latency-accuracy tradeoffsê°€ ê°€ëŠ¥í•¨ì„ ë³´ì¸ë‹¤.</p> <p>ì „ë°˜ì ìœ¼ë¡œ, ì´ ì—°êµ¬ëŠ” ê¸´ thinking ê³¼ì •ì´ ê³¼ì—° ì •ë§ë¡œ í•„ìš”í•œ ê²ƒì¸ê°€ì— ëŒ€í•œ ì˜ë¬¸ì— ëŒ€í•œ ë‹µì„ ì¼ë¶€ë¶„ ë³´ì—¬ì£¼ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤!</p> <p><br/></p> <h3 id="test-time-scaling-for-language-models">Test-Time Scaling for Language Models</h3> <ul> <li> <p><strong>Sequential approaches</strong></p> <ul> <li> <p>OpenAI o1, DeepSeek R1, Qwen QwQ.</p> </li> <li> <p>ê¸´ chain-of-thought(CoT) ì‘ë‹µì„ í•œ ë²ˆì˜ ìˆœë°©í–¥ íŒ¨ìŠ¤ì—ì„œ ìƒì„±í•˜ë©°, ë°±íŠ¸ë˜í‚¹ê³¼ ê²€ì¦ í¬í•¨.</p> </li> <li> <p><strong>í•œê³„</strong>: ê°•í™” í•™ìŠµ(RL)ì´ë‚˜ iterative self-improvement ë“± ë¹„ìš©ì´ í° í•™ìŠµ ê³¼ì • í•„ìš”.</p> </li> </ul> </li> <li> <p><strong>Parallel approaches</strong></p> <ul> <li> <p>ì—¬ëŸ¬ í›„ë³´ ì¶œë ¥ì„ ìƒì„±í•˜ê³  ì„ íƒì ìœ¼ë¡œ ì‘ë‹µ ì§‘ê³„.</p> </li> <li> <p>ex. Best-of-N ìƒ˜í”Œë§, search-guided ë””ì½”ë”© (ex. ëª¬í…Œì¹´ë¥¼ë¡œ íŠ¸ë¦¬ íƒìƒ‰(MCTS))</p> </li> </ul> </li> <li> <p><strong>NoThinkingì˜ ì°¨ë³„ì </strong></p> <ul> <li> <p>ê¸°ì¡´ ì—°êµ¬ëŠ” ëª…ì‹œì  thinkingì´ í•„ìˆ˜ë¼ê³  ê°€ì •í–ˆìœ¼ë‚˜, NoThinkingì€ thinkingì„ ìƒëµí•´ë„ ë¨</p> </li> <li> <p>ì¶”ê°€ í•™ìŠµ, ë³´ìƒ, ê°ë… ì—†ì´ ê²½ìŸë ¥ ìˆëŠ” ì„±ëŠ¥</p> </li> <li> <p>Best-of-Nì„ í™œìš©í–ˆì§€ë§Œ ìƒ˜í”Œë§ ê¸°ë²• í˜ì‹ ì´ ì•„ë‹ˆë¼ cost-effective baseline for low-budget settings ì œê³µì´ ëª©ì </p> </li> </ul> </li> </ul> <h3 id="efficient-reasoning">Efficient Reasoning</h3> <p>recent work has explored various strategies to make reasoning in LLMs more efficient.</p> <ul> <li> <p><strong>ì¶”ë¡  ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœì í™”</strong></p> <ul> <li>ìƒì„±ë˜ëŠ” reasoning ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ì¤„ì´ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ë‹¨ê³„ë¥¼ ì œê±°í•´ ê°„ê²°í•œ ì¶”ë¡ ì„ ìœ ë„.</li> </ul> </li> <li> <p><strong>ê°•í™” í•™ìŠµ ê¸°ë°˜ CoT ìµœì í™”</strong></p> <ul> <li> <p>ê°•í™” í•™ìŠµì„ í™œìš©í•´ CoT ê¸¸ì´ë¥¼ ìµœì í™”í•˜ê³  íš¨ìœ¨ì ì¸ reasoningì„ í•™ìŠµ.</p> </li> <li> <p>ex: ê¸¸ì´ì— ë”°ë¼ ë³´ìƒì„ ì„¤ê³„í•´ ëª¨ë¸ì´ ì ì ˆí•œ ê¸¸ì´ì˜ reasoningì„ ìƒì„±í•˜ë„ë¡ ìœ ë„ (Aggarwal &amp; Welleck, Luo, Shen, Arora, Qu ë“±).</p> </li> </ul> </li> <li> <p><strong>Best-of-N ìƒ˜í”Œë§ì„ í™œìš©í•œ íŒŒì¸íŠœë‹</strong></p> <ul> <li>Best-of-N ë°©ì‹ìœ¼ë¡œ ìƒì„±í•œ ë‹¤ì–‘í•œ ê¸¸ì´ì˜ reasoningì„ íŒŒì¸íŠœë‹ì— í™œìš©í•´ concise reasoningì„ í•™ìŠµ.</li> </ul> </li> <li> <p><strong>ì¶œë ¥ ë°©ì‹ ìˆ˜ì •ìœ¼ë¡œ reasoning ê°„ê²°í™”</strong></p> <ul> <li>LLMì´ reasoningì„ latent representations ê¸°ë°˜ìœ¼ë¡œ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•´ ë” ê°„ê²°í•œ reasoningì„ ìœ ë„</li> </ul> </li> <li> <p><strong>í•™ìŠµ ì—†ëŠ” ì „ëµì  ê¸°ì¤€ ì„¤ì •</strong></p> <ul> <li>ë³„ë„ì˜ í•™ìŠµ ì—†ì´, í”„ë¡¬í”„íŠ¸ë‚˜ ìƒ˜í”Œ ì„ íƒ criteriaë§Œìœ¼ë¡œ ì¶”ë¡  ì „ëµì„ ê°€ì´ë“œí•˜ëŠ” training-free ë°©ì‹</li> </ul> </li> <li> <p><strong>ì¶”ë¡  ë‹¨ê³„ ìˆ˜ ì œí•œ</strong></p> <ul> <li>í”„ë¡¬í”„íŠ¸ì— í† í° ì˜ˆì‚°ì„ ëª…ì‹œí•˜ê±°ë‚˜, reasoning ë‹¨ê³„ë¥¼ ì ê²Œ ìƒì„±í•˜ë„ë¡ ëª¨ë¸ì— ì§ì ‘ ì§€ì‹œí•´ ì¶”ë¡ ì„ ê°„ê²°í™”</li> </ul> </li> <li> <p><strong>ë™ì  ì…ë ¥ ë¼ìš°íŒ…ìœ¼ë¡œ reasoning ë³µì¡ì„± ì œì–´</strong></p> <ul> <li>ì…ë ¥ ë°ì´í„°ë¥¼ ì‘ì—… ë‚œì´ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë¼ìš°íŒ…í•´ ë³µì¡ì„±ì„ ì¡°ì ˆí•˜ê³ , ì‰¬ìš´ ë¬¸ì œëŠ” ê°„ë‹¨íˆ ì²˜ë¦¬í•˜ê³  ì–´ë ¤ìš´ ë¬¸ì œë§Œ ê¹Šì´ ì¶”ë¡ </li> </ul> </li> </ul> <p><br/></p> <p><br/></p> <p>Section 3.1: define Thinking and NoThinking</p> <p>Section 3.2: describe experimental setup</p> <p>Section 3.3: present experimental results</p> <p>Section 3.4: Discussions and Analyses</p> <h2 id="31-method">3.1 Method</h2> <p>ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ë“¤ì€ ë³´í†µ ë¹„ìŠ·í•œ êµ¬ì¡°ë¡œ generationì„ í•œë‹¤:</p> <ul> <li> <table> <tbody> <tr> <td>reasoning process within the thinking box, marked by &lt;</td> <td>beginning of thinking</td> <td>&gt; and &lt;</td> <td>end of thinking</td> <td>&gt;, followed by the final answer.</td> </tr> </tbody> </table> </li> </ul> <p>ì´ êµ¬ì¡°ì— ê¸°ë°˜í•´ì„œ Thinking and NoThinkingì„ ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“¦:</p> <ul> <li> <p>**Thinking: **the reasoning process within the thinking box, the final solution, and the final answer (Figure 1 (blue)).</p> </li> <li> <p>**NoThinking: ** explicit reasoning process ë¬´ì‹œí•˜ê³  ë°”ë¡œ final solution and answer ë§Œë“¤ê¸°. thinking boxë¥¼ decoding í•  ë•Œ ë¹ˆì¹¸ìœ¼ë¡œ í•˜ë„ë¡ ê°•ì œ (Figure 1 (orange)).</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&lt;|</span><span class="n">beginning</span> <span class="n">of</span> <span class="n">thinking</span><span class="o">|&gt;</span>
<span class="n">Okay</span><span class="p">,</span> <span class="n">I</span> <span class="n">think</span> <span class="n">I</span> <span class="n">have</span> <span class="n">finished</span> <span class="n">thinking</span><span class="p">.</span>
<span class="o">&lt;|</span><span class="n">end</span> <span class="n">of</span> <span class="n">thinking</span><span class="o">|&gt;</span>

</code></pre></div></div> <p><br/></p> <table> <tbody> <tr> <td>token usageë¥¼ ì œì–´í•˜ê¸° ìœ„í•´ budget forcing technique from Muennighoff et al. (2025)ì„ ì‚¬ìš© â€” ëª¨ë¸ì´ token budgetì— ë„ë‹¬í•˜ë©´, ê°•ì œë¡œ Final Answer ë§Œë“¤ë„ë¡ í•¨. ë§Œì•½ ì•„ì§ thinking box ì•ˆì— ìˆì—ˆë‹¤ë©´, &lt;</td> <td>end of thinking</td> <td>&gt; ì„ final answer tag ì´ì „ì— ë¶™ì—¬ì„œ ë§Œë“¬.</td> </tr> </tbody> </table> <p><br/></p> <h2 id="32-evaluation-setup">3.2 Evaluation Setup</h2> <ul> <li> <p><strong>Models</strong></p> <ul> <li> <p>DeepSeek-R1-Distill-Qwen-32B</p> </li> <li> <p>Qwen-32B-Instruct</p> </li> <li> <p>(Appendix) R1-series models at smaller scales (7B and 14B)</p> </li> </ul> </li> <li> <p><strong>Tasks and Benchmarks</strong></p> <ul> <li> <p>**Mathematical problem solving: **</p> <ul> <li> <p>For standard problem solving: AIME 2024, AIME 2025, and AMC 2023</p> </li> <li> <p>For more advanced reasoning: OlympiadBench</p> </li> </ul> </li> <li> <p><strong>Coding</strong>: LiveCodeBench</p> </li> <li> <p><strong>Formal theorem proving</strong>:</p> <ul> <li> <p>MiniF2F â€” for formal mathematical reasoning,</p> </li> <li> <p>ProofNet â€” for logic and theorem proving.</p> </li> </ul> </li> </ul> </li> <li> <p>**Metrics: **pass@k</p> <ul> <li> <p>k = {1, 2, 4, 8, 16, 32} for theorem proving datasets (MiniF2F and ProofNet)</p> </li> <li> <p>k = {1, 2, 4, 8, 16, 32, 64} for smaller datasets (AIME24, AIME25, AMC23)</p> </li> <li> <p>k = {1, 2, 4, 8, 16} for larger datasets (OlympiaddBench, LiveCodeBench).</p> </li> <li> <p>for formal theorem-proving benchmarks: pass@32 is the standard</p> </li> <li> <p>for math and coding: pass@1 (i.e., accuracy) is most commonly used.</p> </li> </ul> </li> </ul> <h2 id="33-results">3.3 Results</h2> <p><br/></p> <h3 id="thinking-vs-nothinking-vs-qwen-instruct-without-token-budget-controlled">Thinking vs. NoThinking vs. Qwen Instruct without token budget controlled</h3> <p>budget forcingì—†ì´ ì„¸ ê²½ìš°ë¥¼ ë¹„êµí•œ ê²°ê³¼:</p> <ul> <li> <p>MiniF2F and ProofNetì—ì„œ NoThinkingì€ ëª¨ë“  Kì— ëŒ€í•´ì„œ Thinkingê³¼ ë¹„ìŠ·í–ˆìœ¼ë©°, ë‘˜ì€ Qwen-Instructë³´ë‹¤ ì„±ëŠ¥ í›¨ì”¬ ì¢‹ì•˜ìŒ</p> <ul> <li>NoThinkingì´ 3.3â€“3.7x ë” ì ì€ í† í°ì„ ì‚¬ìš©í•˜ëŠ”ë°ë„!</li> </ul> </li> <li> <p>ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ì„œëŠ” k = 1ì¼ ë•ŒëŠ” NoThinkingì˜ ì„±ëŠ¥ì´ í›¨ì”¬ ë–¨ì–´ì§€ì§€ë§Œ, kê°€ ì»¤ì§ˆìˆ˜ë¡ ê°­ì´ ì‘ì•„ì§</p> </li> <li> <p>ê²°ê³¼ì ìœ¼ë¡œ, NoThinkingì€ ê°€ì¥ í° kì¼ë•Œ, 2.0â€“5.1x fewer tokensì„ ì‚¬ìš©í•˜ëŠ”ë°ë„, Thinkingì˜ ì„±ëŠ¥ì„ ë„˜ê±°ë‚˜ ê±°ì˜ ê·¼ì‚¬í•¨.</p> </li> <li> <p>Qwen-Instructì˜ ê´€ì ì—ì„œ:</p> <ul> <li> <p>For AIME24, AIME25, and LiveCodeBenchì—ì„œ Thinking and NoThinkingì´ í›¨ì”¬ ì„±ëŠ¥ ì¢‹ìŒ</p> </li> <li> <p>AMC23 and OlympiadBenchì—ì„œëŠ” Thinking and NoThinkingê³¼ ë¹„ìŠ·</p> </li> </ul> </li> </ul> <p><br/></p> <p><br/></p> <h3 id="thinking-vs-nothinking-with-token-budget-controlled">Thinking vs. NoThinking with token budget controlled</h3> <p><br/></p> <p>ìœ„ì—ì„œ í™•ì¸í–ˆë“¯,Thinkingì´ NoThinkingë³´ë‹¤ ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ì´ ë” ì¢‹ìŒ. í•˜ì§€ë§Œ, ê²°ê³¼ì ìœ¼ë¡œ Thinkingì´ ë” ë§ì€ í† í°ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ê°™ì€ í† í° ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•Œ ì–´ë–¤ ê²ƒì´ ë” ì„±ëŠ¥ì´ ì¢‹ì€ê°€ë¥¼ ë¹„êµí•¨</p> <p><br/></p> <p>ê²°ê³¼ì ìœ¼ë¡œ NoThinking generally outperforms Thinking.</p> <p>íŠ¹íˆ, low-budget setting (e.g., fewer than â‰ˆ 3, 000 tokens)ì—ì„œ NoThinkingì€ ëª¨ë“  kì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , kê°€ ì»¤ì§ˆìˆ˜ë¡ ì°¨ì´ëŠ” ì»¤ì¡ŒìŒ. ì¢€ ë” í† í° ì œí•œì„ ëŠ˜ë ¸ì„ ë•Œ (e.g., around 3, 500 tokens), Thinkingì´ pass@1ì—ì„œëŠ” ë” ì¢‹ì•˜ìœ¼ë‚˜, k = 2ë¶€í„°ëŠ” ë‹¤ì‹œ NoThinkingì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</p> <p>Figure 6ëŠ” í•´ë‹¹ ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©í•œ ê°€ì¥ í° kì™€ 1, ê·¸ë¦¬ê³  token usageë¥¼ plotí•˜ë©´ì„œ ìœ„ì˜ ê²°ê³¼ë¥¼ ë” ì˜ ë³´ì—¬ì¤Œ.</p> <ul> <li> <p>pass@k</p> <ul> <li>NoThinkingì´ í•­ìƒ ë” ì¢‹ì•˜ìŒ</li> </ul> </li> <li> <p>pass@1</p> <ul> <li> <p>NoThinkingì´ low-budget regimeì—ì„œëŠ” ë” ì¢‹ê³  high-budget regimeì—ì„œëŠ” ë” ë‚˜ë¹´ìŒ</p> </li> <li> <p>LiveCodeBenchì€ ì˜ˆì™¸. ì•„ë§ˆë„ thinking boxë¥¼ ì—†ì• ëŠ” ê²ƒì´ token usageë¥¼ ê·¸ë ‡ê²Œ ë§ì´ ì¤„ì´ì§€ ëª»í–ˆê¸° ë•Œë¬¸ì´ë¼ê³  ì˜ˆìƒë¨</p> </li> <li> <p>data contaminationì˜ ìœ„í—˜ì„ ì˜ˆìƒí•˜ê³ , ì ˆëŒ€ í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì•˜ì„ AIME 2025ë¥¼ ì¶”ê°€í•¨</p> </li> <li> <p>ëª¨ë“  new and established benchmarksì˜ ê²°ê³¼ê°€ ì´ íŠ¸ë Œë“œê°€ artifacts of memorizationì´ ì•„ë‹Œ, generalizable model behaviorì„ì„ ë³´ì—¬ì¤Œ</p> </li> </ul> </li> </ul> <p><br/></p> <p>[ìš”ì•½]</p> <ul> <li> <p>reasoning modelsì˜ í•µì‹¬ì¸ thinking boxë¥¼ ì—†ì• ë„, ì—¬ì „íˆ íš¨ê³¼ ì¢‹ìŒ</p> </li> <li> <p>3.3â€“3.7x ì ì€ í† í°ì„ ì‚¬ìš©í•˜ëŠ”ë°ë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ ë‚˜ì˜´</p> </li> <li> <p>ë¹„ìŠ·í•œ ìˆ˜ì˜ í† í°ì´ë¼ë©´ thinkingë³´ë‹¤ ì„±ëŠ¥ ì¢‹ìŒ</p> </li> </ul> <h2 id="34-discussions-and-analyses">3.4 Discussions and Analyses</h2> <h3 id="task-specific-differences-in-nothinking-performance">Task-Specific Differences in NoThinking Performance</h3> <p>Section 3.3ì—ì„œ ë‚˜ë¦„ ì¼ê´€ì ì¸ íŠ¸ë Œë“œê°€ ë³´ì´ê¸´ í•˜ì§€ë§Œ, ê° ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ìì„¸íˆ ì‚´í´ë³´ë©´ ì¡°ê¸ˆ ë™ì‘ì´ ë‹¤ë¦„</p> <p>In Figure 4,</p> <ul> <li> <p>AMC 2023ëŠ” ëª¨ë“  ì„¸íŒ…ì—ì„œ ê±°ì˜ performance gapì—†ì´ convergenceë¥¼ ë³´ì„. ì•„ë§ˆë„ saturationì´ ì˜ˆìƒë¨</p> </li> <li> <p>MiniF2F and ProofNet pass@1ì—ì„œ NoThinkingì€ Thinkingì— ë¹„í•´ í›¨ì”¬ ë” ì ì€ í† í°ì„ ì‚¬ìš©í•˜ë©´ì„œ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ëƒ„. í•˜ì§€ë§Œ, ì´ëŠ” ë‹¨ìˆœíˆ task simplicity ì´ìŠˆë¡œ í•´ì„ë˜ë©´ ì•ˆë¨! ê²€ì¦ ê²°ê³¼, OpenAIâ€™s o1ê³¼ ê°™ì€ ì—„ì²­ ê°•í•œ ëª¨ë¸ì€ MiniF2Fì—ì„œ 30% accuracy ë°–ì— ì•ˆëê³ , ProofNetì€ ëª¨ë“  ë°©ë²•ë¡ ì—ì„œ ì„±ëŠ¥ ë‚®ì•˜ìŒ. ì¦‰, ì™œ ì–´ë–¤ ë²¤ì¹˜ë§ˆí¬ì—ì„œëŠ” NoThinkingì´ ì˜ë˜ì—ˆëŠ”ê°€ëŠ” open question for future workì´ë¼ëŠ” ê²ƒ</p> </li> </ul> <h3 id="how-increasing-k-affects-nothinking-performance">How Increasing k Affects NoThinking Performance</h3> <p>ì™œ kê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ NoThinkingì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ì§€ ëŒ€ëµì ì¸ ì´ìœ ë¥¼ ì°¾ì•„ë³´ê¸° ìœ„í•´ ìƒì„±ëœ ë‹µë³€ì˜ diversityë¥¼ ì¸¡ì •í•¨ â€” by computing the entropy of the answer distribution for each question.</p> <p>ë†’ì€ mean entropyëŠ” ë‹¹ì—°íˆ ë” ë†’ì€ overall diversityë¥¼ ì˜ë¯¸í•˜ê³ , lower standard deviationì€ ë” ì¼ê´€ì ì¸ ê²ƒì„ ì˜ë¯¸. ì‹¤í—˜ì€ token budgetì´ ì œí•œëœ í™˜ê²½ì—ì„œ ì§„í–‰</p> <ul> <li> <p>ì—”íŠ¸ë¡œí”¼ì˜ ê´€ì ì—ì„œëŠ” íŠ¹ë³„í•œ ì°¨ì´ë¥¼ ì°¾ì§€ ëª»í•¨. ì–´ë–¨ ë• NoThinkingì´ ì–´ë–¨ ë• Thinkingì´ ë” ë†’ìŒ</p> </li> <li> <p>varianceì˜ ê´€ì ì—ì„œ NoThinkingì€ í•­ìƒ ë” ë‚®ì€ ê°’ì„ ë³´ì„ â€” ë” uniformí•˜ê²Œ ë‹µì„ ë‚´ê³  ìˆë‹¤ëŠ” ê²ƒ.</p> </li> </ul> <p><br/></p> <p>ì´ëŸ¬í•œ ë‹¤ì–‘ì„±ì˜ ì¼ê´€ì„± ì¦ê°€ê°€ kê°€ ì»¤ì§ˆìˆ˜ë¡ ë” ì¢‹ì€ pass@kë¥¼ ë³´ì´ëŠ” ì´ìœ ì™€ ì—°ê´€ì´ ë˜ì–´ìˆì„ ê²ƒì´ë¼ ì˜ˆìƒì€ í•œë‹¤ë§Œ, ì´ë¥¼ ì´ìš©í•´ ì„±ëŠ¥ ì°¨ì´ë¥¼ ì™„ì „íˆ ì„¤ëª…í•˜ê¸°ëŠ” ì–´ë µë‹¤ê³ ì–¸ê¸‰</p> <p><br/></p> <p>ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ NoThinkingì´ kê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ë” ì´ì ì´ ëŠ˜ì–´ë‚œë‹¤ê³  ì´ì•¼ê¸°í•¨. ì¦‰, NoThinkingì„ í™œìš©í•˜ë©´ parallel scaling methodë¥¼ ë” ì˜ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ!</p> <p>Section 4ì—ì„œëŠ” accuracy vs. latencyì˜ ê´€ì ì—ì„œ ì–´ë–»ê²Œ Thinkingë³´ë‹¤ ë” ì¢‹ì€ ì„¸íŒ…ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ ë…¼í•œë‹¤.</p> <h2 id="41-motivation-and-methods">4.1 Motivation and Methods</h2> <h3 id="parallel-scaling-v-sequential-scaling">Parallel Scaling v. Sequential Scaling</h3> <ul> <li> <p>Parallel scaling:</p> <ul> <li> <p>low latency: ì—¬ëŸ¬ ìƒ˜í”Œì„ ë™ì‹œì— ìƒì„±í•˜ë¯€ë¡œ ì§€ì—° ì‹œê°„ì´ ì¤„ì–´ë“¦ â€” ì´ëŠ” API í˜¸ì¶œì´ë“  ë¡œì»¬ ëª¨ë¸ ì„œë¹„ìŠ¤ë“  ë™ì¼í•¨.</p> </li> <li> <p>ì „ì²´ ì§€ì—° ì‹œê°„ì€ ê°€ì¥ ì˜¤ë˜ ê±¸ë¦° ê°œë³„ ìƒ˜í”Œì˜ ìƒì„± ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¸¡ì •í•¨.</p> </li> <li> <p>NoThinkingì€ low-budget êµ¬ê°„ì—ì„œ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê³ , kê°€ ì»¤ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ í–¥ìƒë˜ê¸° ë•Œë¬¸ì—, ë‹¨ìˆœí•œ best-of-N ë°©ì‹ì„ ì‚¬ìš©í–ˆì„ ë•Œë„ ì •í™•ë„ì™€ ì§€ì—° ì‹œê°„ ì¸¡ë©´ì—ì„œ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŒ.</p> </li> <li> <p>ì‹¤ì œë¡œ budget forcingê³¼ ë³‘ë ¬ ìƒ˜í”Œë§ì„ ì ìš©í•œ Thinking, ê·¸ë¦¬ê³  sequential scaling í•˜ì˜ full Thinking(Thinking without budget forcing)ê³¼ ë¹„êµí–ˆì„ ë•Œë„ NoThinkingì´ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŒ.</p> </li> </ul> </li> </ul> <h3 id="methods">Methods</h3> <ul> <li> <p><strong>Parallel sampling</strong></p> <ul> <li> <p>ë³‘ë ¬ ìƒ˜í”Œë§ì€ Nê°œì˜ ë…ë¦½ì ì¸ í•´ë‹µì„ ì§‘ê³„í•´ ë‹¨ì¼ ì˜ˆì¸¡ì„ ìƒì„±í•˜ëŠ” best-of-N ë°©ì‹ì„ í•„ìš”ë¡œ í•¨.</p> </li> <li> <p>Nê°œì˜ ì˜ˆì¸¡ P = {pâ‚, Â·Â·Â·, pâ‚™}ì´ ìˆì„ ë•Œ, best-of-Nì€ ìµœì¢… ì¶œë ¥ì„ P ì¤‘ í•˜ë‚˜ë¡œ ì„ íƒí•¨.</p> </li> </ul> </li> <li> <p><strong>ê²€ì¦ ê°€ëŠ¥í•œ ì‘ì—…(MiniF2F, ProofNet)</strong></p> <ul> <li>Lean ì»´íŒŒì¼ëŸ¬ì™€ ê°™ì€ perfect verifier fë¥¼ ì‚¬ìš©í•´ ê° ì˜ˆì¸¡ p âˆˆ Pì˜ ì •ë‹µ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ìµœì¢… ì •ë‹µì„ ì„ íƒí•¨.</li> </ul> </li> <li> <p><strong>verifierê°€ ì—†ëŠ” ì‘ì—…</strong></p> <ul> <li> <p>**Confidence-based **</p> <ul> <li> <p>Kang et al. (2025)ë¥¼ ë”°ë¼ self-certainty ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ê²½ëŸ‰ì˜ confidence ê¸°ë°˜ ì„ íƒ ë°©ì‹ì„ ì ìš©.</p> </li> <li> <p>self-certaintyëŠ” ì˜ˆì¸¡ëœ í† í° ë¶„í¬ì™€ ê· ë“± ë¶„í¬ ê°„ KL divergenceë¥¼ ê³„ì‚°í•´ ëª¨ë¸ì˜ í™•ì‹ ë„ë¥¼ ìˆ˜ì¹˜í™”í•¨.</p> </li> <li> <p>ì˜ˆì¸¡ ì§‘í•© Pì˜ self-certainty ì ìˆ˜ câ‚, â€¦, câ‚™ë¥¼ ê³„ì‚°í•œ ë’¤, ë™ì¼ ì—°êµ¬ì—ì„œ ì†Œê°œëœ Borda voting ë°©ì‹ì„ í†µí•´ ìµœì¢… ë‹µë³€ ì„ íƒ.</p> </li> <li> <p>equivalence checkingì´ ë¶ˆê°€ëŠ¥í•œ ë²¤ì¹˜ë§ˆí¬(LiveCodeBench)ì—ì„œëŠ” self-certaintyê°€ ê°€ì¥ ë†’ì€ ì‘ë‹µì„ ìµœì¢… ì„ íƒí•¨.</p> </li> </ul> </li> <li> <p>**Majority voting **</p> <ul> <li> <p>ì •í™•í•œ ì •ë‹µì´ ì¡´ì¬í•˜ëŠ” ê³¼ì œ(ìˆ˜í•™ ë¬¸ì œ í’€ì´, ê³¼í•™ ë¬¸ì œ)ì—ì„œëŠ” ì´ì „ ì—°êµ¬ë¥¼ ë”°ë¼ majority vote ê¸°ë°˜ì˜ ê²°ê³¼ë¥¼ ë³´ê³ í•¨.</p> </li> <li> <p>ì˜ˆì¸¡ ì§‘í•© Pì—ì„œ ì¶”ì¶œí•œ ë‹µë³€ ëª¨ìŒ {aáµ¢}ë¡œë¶€í„° cons@n = argmaxâ‚ âˆ‘â‚â¿ 1(aáµ¢ = a)ë¡œ majority vote.</p> </li> <li> <p>k &lt; Nì¸ ê²½ìš°, ì „ì²´ Nê°œ ì˜ˆì¸¡ì—ì„œ ë¬´ì‘ìœ„ë¡œ kê°œë¥¼ ìƒ˜í”Œë§í•´ ì»¨ì„¼ë¥¼ ê³„ì‚°í•˜ê³ , Monte Carlo simulationìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ ì •í™•ë„ë¥¼ í‰ê· í•˜ì—¬ cons@kë¥¼ ì¶”ì •í•¨.</p> </li> </ul> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p><strong>Metrics</strong></p> <ul> <li>ì§€ì—° ì‹œê°„(latency)ì€ ê° ë°ì´í„°ì…‹ê³¼ NíšŒ ë°˜ë³µ ì‹¤í—˜ì—ì„œ ìƒì„±ëœ í† í° ìˆ˜ì˜ ìµœëŒ“ê°’ì„ í‰ê· í•˜ì—¬ ì •ì˜í•¨.</li> </ul> </li> </ul> <p><br/></p> <h2 id="42-results">4.2 Results</h2> <p><br/></p> <ul> <li> <p><strong>Tasks without verifiers</strong></p> <ul> <li> <p>Figure 7ì—ì„œëŠ” confidence-based ë°©ì‹ì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ì‹œê°í™”í–ˆê³ , Table 2ì—ëŠ” ì„ íƒëœ ì‹¤í—˜ì˜ ablation ê²°ê³¼ë¥¼ ì œì‹œí•¨.</p> </li> <li> <p>Table 2ì—ì„œëŠ” Section 4.1ì—ì„œ ë…¼ì˜í•œ Best-of-N ë°©ë²•ì„ ë¹„êµí–ˆìœ¼ë©°, ì „ë°˜ì ìœ¼ë¡œ confidence-based ì„ íƒì´ majority votingë³´ë‹¤ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„.</p> </li> <li> <p>ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ì„ í™œìš©í•  ê²½ìš°, ìƒ˜í”Œ ì¤‘ ê°€ì¥ ì¢‹ì€ ì˜ˆì¸¡ì„ ì„ íƒí•´ pass@k ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ pass@k ì •í™•ë„ë¥¼ pass@1ì˜ ìƒí•œìœ¼ë¡œ ë³´ê³  Table 2ì— í¬í•¨.</p> </li> <li> <p><strong>Perfect Verifiers</strong></p> </li> <li> <p>ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ê³¼ ê²°í•©í•œ NoThinkingì€ ê¸°ì¡´ sequential ì ‘ê·¼ë²•ì— ë¹„í•´ í›¨ì”¬ ë‚®ì€ ì§€ì—° ì‹œê°„+í† í° ìˆ˜ë¡œ ìœ ì‚¬í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±.</p> </li> <li> <p>Figure 7ì˜ ì²« ë‘ í”Œë¡¯ì—ì„œ ë³´ë“¯ NoThinkingì€ Thinkingê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ë©´ì„œë„ ì§€ì—° ì‹œê°„ì´ í›¨ì”¬ ë‚®ìŒ.</p> </li> <li> <p>ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ ì—†ì´ë„ NoThinkingì€ Thinkingê³¼ ë¹„ìŠ·í•œ ì •í™•ë„ë¥¼ í›¨ì”¬ ì§§ì€ ì§€ì—° ì‹œê°„ìœ¼ë¡œ ë‹¬ì„±í•¨.</p> </li> <li> <p>ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ê³¼ ê²°í•©í•  ë•Œ, NoThinkingì€ budget forcingê³¼ ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ ì—†ì´ ìˆ˜í–‰í•œ Thinkingê³¼ ìœ ì‚¬í•œ ì •í™•ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ì§€ì—° ì‹œê°„ì„ 7ë°° ì¤„ì„.</p> </li> <li> <p>íŠ¹íˆ MiniF2Fì™€ ProofNet ë°ì´í„°ì…‹ì—ì„œ NoThinkingì€ ì¶œë ¥ í† í° ìˆ˜ë¥¼ 4ë°° ì¤„ì´ë©´ì„œë„ ê°™ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•¨.</p> </li> </ul> </li> <li> <p>**Simple Best-of-N Methods **</p> <ul> <li> <p>NoThinkingì€ ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ê³¼ confidence-based ì„ íƒì„ ê²°í•©í–ˆì„ ë•Œ, ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ Thinkingì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€í•¨.</p> </li> <li> <p>Figure 7ì˜ ë§ˆì§€ë§‰ ë‹¤ì„¯ í”Œë¡¯ì€ ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ í† í° ì‚¬ìš©ëŸ‰ì„ í†µì œí•œ ìƒí™©ì—ì„œ Thinkingê³¼ NoThinkingì˜ confidence-based ì„ íƒ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ.</p> </li> <li> <p>ì—°êµ¬ëŠ” ì£¼ë¡œ low-budget í™˜ê²½ì— ì´ˆì ì„ ë§ì¶”ì—ˆìŒ. ì´ìœ ëŠ”</p> <ol> <li> <p>íš¨ìœ¨ì ì¸ ì¶”ë¡ ì— ëŒ€í•œ ì£¼ëœ ê´€ì‹¬ì‚¬ì™€ ë¶€í•©í•˜ê³ ,</p> </li> <li> <p>ìµœëŒ€ í† í° ìˆ˜ê°€ ë„ˆë¬´ í¬ë©´ ì§€ë‚˜ì¹˜ê²Œ ê¸¸ê³  ë¹„ë…¼ë¦¬ì ì¸ ì¶œë ¥(â€œbabblingâ€)ì´ ë°œìƒí•´ ë¹„êµ ê°€ì¹˜ê°€ ë–¨ì–´ì§€ê³  ì§€ì—° ì‹œê°„ë§Œ ì¦ê°€í•˜ê¸° ë•Œë¬¸.</p> </li> </ol> </li> <li> <p>ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ ìì²´ëŠ” Thinkingê³¼ NoThinking ëª¨ë‘ì—ì„œ pass@1 ì„±ëŠ¥ì„ ê°œì„ í•˜ì§€ë§Œ, ëª¨ë“  ìˆ˜í•™ ë²¤ì¹˜ë§ˆí¬ì—ì„œ NoThinkingì€ Thinkingë³´ë‹¤ í•­ìƒ ë” ì¢‹ì€ accuracyâ€“budget tradeoffsë¥¼ ê°€ì§</p> </li> <li> <p>íŠ¹íˆ ì˜ˆì‚° ì œí•œì´ ì—†ëŠ” full Thinkingê³¼ ë¹„êµí•´ë„, NoThinkingì€ ë” ë†’ì€ pass@1 ì ìˆ˜(55.79 vs. 54.1)ë¥¼ ê¸°ë¡í•˜ë©´ì„œ ì§€ì—° ì‹œê°„ì„ 9ë°° ë‹¨ì¶•í•¨.</p> </li> </ul> </li> <li> <p>**LiveCodeBench **</p> <ul> <li> <p>NoThinkingì€ LiveCodeBenchì—ì„œ less effective: ì´ëŠ” confidence-based ì„ íƒì´ ì •í™•í•œ ì¼ì¹˜ ê¸°ì¤€ì´ í•„ìš”í•œ ì½”ë”© ì‘ì—…ì—ì„œëŠ” í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì„.</p> </li> <li> <p>ì´ ê²½ìš° ì •í™•í•œ ì¼ì¹˜ ê¸°ë°˜ íˆ¬í‘œê°€ ë¶ˆê°€ëŠ¥í•´ self-certaintyê°€ ê°€ì¥ ë†’ì€ ì‘ë‹µì„ ì„ íƒí–ˆëŠ”ë°, ì´ëŠ” ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì„±ëŠ¥ì´ ë–¨ì–´ì§.</p> </li> <li> <p>Table 2ì— ë”°ë¥´ë©´ ì´ëŸ¬í•œ ë°©ì‹ì€ íˆ¬í‘œ ê¸°ë°˜ ë°©ë²•ì´ ê°€ëŠ¥í•œ ë‹¤ë¥¸ ì‘ì—…ë“¤ê³¼ ë¹„êµí•´ ì¼ê´€ë˜ê²Œ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì„.</p> </li> </ul> </li> </ul> <p><br/></p> <p>[ìš”ì•½]</p> <ul> <li> <p>NoThinkingì˜ pass@k ì„±ëŠ¥ì€ kê°€ ì¦ê°€í• ìˆ˜ë¡ ë”ìš± ì¢‹ì•„ì§€ë©°, ë³‘ë ¬ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ pass@1 ì„±ëŠ¥ì„ ìœ ì‚¬í•˜ê±°ë‚˜ í›¨ì”¬ ë” ë‚®ì€ ì§€ì—° ì‹œê°„(ìµœëŒ€ 9ë°° ê°ì†Œ)ìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆìŒ.</p> </li> <li> <p>Verifiersê°€ ìˆëŠ” ì‘ì—…ì—ì„œëŠ” ì •í™•ë„ëŠ” ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ë†’ì´ë©´ì„œ ì´ í† í° ì‚¬ìš©ëŸ‰ì„ ìµœëŒ€ 4ë°°ê¹Œì§€ ì¤„ì¼ ìˆ˜ ìˆìŒ.</p> </li> </ul> <p><br/></p> <ul> <li> <p>ì´ ì—°êµ¬ëŠ” ë™ì¼í•œ ëª¨ë¸ì´ ê¸´ thinking chain ì—†ì´ë„, kê°€ ì¦ê°€í•¨ì— ë”°ë¼ pass@kì—ì„œ Thinking ë°©ì‹ê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ í›¨ì”¬ ì ì€ í† í°ìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ</p> </li> <li> <p>ë™ì¼í•œ í† í° ì˜ˆì‚° í•˜ì—ì„œë„, NoThinkingì€ ëŒ€ë¶€ë¶„ì˜ k ê°’ì—ì„œ ê¸°ì¡´ Thinking ê²°ê³¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ ëŠ¥ê°€í•¨.</p> </li> <li> <p>NoThinkingì„ Best-of-N ì„ íƒ ë°©ë²•ê³¼ ê²°í•©í•˜ë©´, ê¸°ì¡´ Thinking ë°©ì‹ìœ¼ë¡œëŠ” ë‹¬ì„±í•˜ê¸° ì–´ë ¤ìš´ accuracyâ€“budget tradeoffs ë‹¬ì„±!</p> </li> </ul> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Reasoning Models Can Be Effective Without Thinking]]></summary></entry><entry><title type="html">See What You Are Told: Visual Attention Sink in Large Multimodal Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/see-what-you-are-told-visual-attention-sink-in-large-multimodal-models/" rel="alternate" type="text/html" title="See What You Are Told: Visual Attention Sink in Large Multimodal Models"/><published>2025-06-24T00:00:00+00:00</published><updated>2025-06-24T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/see-what-you-are-told-visual-attention-sink-in-large-multimodal-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/see-what-you-are-told-visual-attention-sink-in-large-multimodal-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-06-24</li> <li><strong>Reviewer</strong>: ì¡°ì˜ì¬</li> <li><strong>Property</strong>: Multimodal</li> </ul> <p><br/></p> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLMì˜ ë°œì „ê³¼ í•¨ê»˜ Multimodal ëª¨ë¸ë“¤ë„ ë§ì´ ë“±ì¥í•˜ê³  ìˆìŒ (VQA, image captioning, visual reasoning, â€¦)</p> </li> <li> <p>LMMì—ì„œë„ LLM ì²˜ëŸ¼ ë˜‘ê°™ì´ attention ë§¤ì»¤ë‹ˆì¦˜ì„ ë”°ë¦„. ì˜ˆë¥¼ ë“¤ì–´ â€˜birdâ€™ë¥¼ ë§í•˜ê³ ì í• ë•Œ modelì€ í•´ë‹¹ ì´ë¯¸ì§€ì— ê´€ë ¨ìˆëŠ” visual tokenì— ëŒ€í•´ ì§‘ì¤‘í•¨. (ì§ê´€ì ìœ¼ë¡œ) textì™€ visual tokenì´ ë§¤ì¹­ë¨.</p> </li> <li> <p>ê·¼ë° ì‹¤ì œë¡œëŠ” textì™€ visual ê°„ì˜ ê´€ê³„ê°€ unrelated ë˜ëŠ” ê²½ìš°ë„ ê´€ì¸¡ë¨. attention mapì„ í†µí•´ attentionì„ ë” ë©´ë°€íˆ ì‚´í´ë³´ì•˜ì„ë•Œ, Example 1ì„ ë³´ë©´, ìœ„ì— birdë¥¼ ë§í•˜ëŠ”ë° â€˜ë¹¨ê°„ ë„¤ëª¨â€™ ì²˜ëŸ¼ birdì™€ ë¬´ê´€í•œ ê³³ì— ë†’ì€ attentionì´ ê´€ì¸¡. ë‹¤ë¥¸ ì˜ˆì œë“¤ë„ ë§ˆì°¬ê°€ì§€ë¡œ í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•œ ê³³ì— ë†’ì€ ì–´í…ì…˜ì´ ê´€ì¸¡ë¨. ì´ê²Œ ì™œ ë°œìƒí•˜ëŠ”ì§€ ê¶ê¸ˆí•´ì„œ í•´ë‹¹ ì—°êµ¬ê°€ ì‹œì‘ë¨</p> </li> <li> <p>í•´ë‹¹ ì—°êµ¬ì˜ ë°œê²¬</p> <ul> <li> <p>ì´ëŸ¬í•œ attention ë§µì˜ ì˜¤ë¥˜ëŠ” ëª‡ëª‡ tokens ë“¤ì´ massive activation of specific dimensions in the hidden statesì—ì„œ ì¼ì–´ë‚¨ì„ ì°¾ìŒ. ì´ê²ƒì€ LLMì—ì„œ íŠ¹ì • limited semantic meaning(e.g. â€œBOSâ€, â€œ.â€, â€œ\nâ€)ì— large attentionì´ ë¶€ì—¬ë˜ëŠ” â€œattention sinkâ€ì˜ ê°œë…ê³¼ ìœ ì‚¬í•´ë³´ì„.</p> </li> <li> <p>ì¶”ê°€ë¡œ ì‹¤í—˜ì„ í•´ë³´ë‹ˆ ì´ëŸ¬í•œ visual sink token ë“¤ì€ ì—†ì• ë„ ëª¨ë¸ ë‹µë³€ì˜ qualityì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ.</p> </li> </ul> </li> <li> <p>ìµœê·¼ì— vlmì—ì„œ attentionì´ textì— ë¹„í•´ ì´ë¯¸ì§€ì— ë¶€ì¡±í•˜ê²Œ í• ë‹¹ëœë‹¤ëŠ” ì‚¬ì „ ì—°êµ¬ë„ ìˆì—ˆìŒ. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” attention budgetì˜ ê°œë…ìœ¼ë¡œ visual sink tokenë“¤ì— ê°€ëŠ” attentionì„ ì•„ê»´ì„œ ë‹¤ë¥¸ visual tokenë“¤ì— redistributeë¥¼ í•˜ê³ ì í•¨(Visual Attention Redistribuion (VAR))</p> </li> </ul> <p><br/></p> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Visual attention in large multimodal models.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - LMMì´ íŠ¹ì • ëª‡ê°œì˜ í† í°ì— ê³¼ë„í•˜ê²Œ attentionì„ ë¶€ì—¬í•œë‹¤ëŠ” ì—°êµ¬ê°€ ìˆì—ˆê³ , ì´ë¥¼ í™œìš©í•´ contrastive decodingìœ¼ë¡œ í•´ê²°í•˜ë ¤ëŠ” ì‹œë„ê°€ ìˆì—ˆìŒ(ë¹ˆ ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ë„£ì—ˆì„ ë•Œ ëª¨ë¸ì˜ ë‹µë³€ logitì„ ë¹¼ì„œ biasë¥¼ ì—†ì• ëŠ” ë°©ì‹, ìœ„ ì´ë¯¸ì§€ ì°¸ê³  ([https://arxiv.org/pdf/2405.17820](https://arxiv.org/pdf/2405.17820))). í˜¹ì€ ê°•ì œì ìœ¼ë¡œ textì— ê°€ë˜ attentionì„ visualì— ê°€ê²Œë” ë§Œë“œëŠ” ì‹œë„.
</code></pre></div> </div> </li> <li> <p>Attention sink in language models</p> <ul> <li> <p>ê¸°ì¡´ LLMì—ì„œë„ attention sinkëŠ” 2024 ë…„ë„ ë¶€í„° ì œê¸°ë˜ë˜ ë¬¸ì œ. íŠ¹íˆ BOS ê°™ì€ í† í°ì€ AR íŠ¹ì„±ìƒ ë’¤ì— ëª¨ë“  tokenë“¤ì˜ attentionì´ ì ë¦¬ê²Œ ë˜ì–´ ì˜ë¯¸ëŠ” ì ì§€ë§Œ attentionì´ ë†’ìŒ (c.f. StreamingLLMì´ë€ ì—°êµ¬ì—ì„œëŠ” attention sinkê°€ ê±¸ë¦° í† í°ì˜ KVë¥¼ ê³ ì •ì‹œì¼œ efficiencyë¥¼ ê°–ê¸°ë„ í•¨)</p> </li> <li> <p>ì´ëŸ¬í•œ attention sinkê°€ íŠ¹íˆ íŠ¹ì • dimenstionì˜ hidden stateì—ì„œ ë°œìƒ! ì—¬ê¸°ì˜ attentionì„ ë‹¤ë¥¸ê³³ì— ì¬ë¶„ë°°í•´ ì •êµí•œ ë‹µë³€ì„ ì–»ìœ¼ë ¤ëŠ” llmì—°êµ¬ë„ ìˆì—ˆìŒ. This workëŠ” ì´ ê°œë…ì„ VLMì— ì ìš©í•œ ëŠë‚Œ</p> </li> </ul> <p><br/></p> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>íŠ¸ëœìŠ¤í¬ë¨¸ ê³µì‹</p> <p><br/></p> <h2 id="visual-attention-sink">Visual Attention Sink</h2> <p>Figure1ë³´ë©´ attentionì´ ìš°ë¦¬ì˜ ì§ê´€ëŒ€ë¡œ ì˜ ë”°ë¼ê°€ê¸´ í•˜ì§€ë§Œ <strong>ê³ ì •ëœ ì–´ë–¤ background spot</strong>ì— êµ³ì´ í•„ìš”ì—†ëŠ” limited semanic meaningì— ë†’ì€ attentionì´ ë°°ì •ë˜ì–´ ìˆìŒ</p> <h3 id="how-to-distinguish-irrelevant-visual-tokens">How to distinguish irrelevant visual tokens?</h3> <p>irrelevant visual tokenì—ì„œ ë‘ ê°€ì§€ íŠ¹ì„±ì´ ë‚˜íƒ€ë‚¨. (1) figure 1ì—ì„œ ë³´ë“¯ ì´ë¯¸ì§€ì— same irrelevant visual tokenì— ê³ ì •ì ìœ¼ë¡œ ë“±ì¥. (2) BOS í† í°ì´ë‘ ìœ ì‚¬í•˜ê²Œ ê°™ì€ dimensionì—ì„œ ë“±ì¥ (Fig 2)</p> <p><br/></p> <h3 id="irrelevant-visual-tokens-have-high-activation-in-specific-dimensions">Irrelevant visual tokens have high activation in specific dimensions</h3> <ul> <li> <p>Fig2ì˜ BOSë‘ ë¹¨ê°„<img/> ë¥¼ ë³´ë©´ ê°™ì€ dimensionì—ì„œ attention ê°’ì´ íŠ€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ. ì´ëŠ” LLM ê°ìê°€ ê°™ì€ ê³ ìœ í•œ íŠ¹ì„±ì´ë¼ê³  í•¨. ì˜ˆë¥¼ë“¤ì–´ LLaVA-1.5-7Bê°€ ì‚¬ìš©í•œ LLaMA2 ë°±ë³¸ì€ ëª¨ë‘ ê³ ì •ì ìœ¼ë¡œ {1415, 2533} ì˜ dimensionì—ì„œ ìœ„ì™€ê°™ì€ í˜•íƒœë¥¼ ë³´ì„. (pretrain ê³¼ì •ì—ì„œ ì ë¦¬ëŠ” ê±°ë¼ finetuningì„ í•´ë„ sink dimensionì€ ê³„ì† ê³ ì •ë˜ì–´ìˆë‹¤ê³  í•¨)</p> </li> <li> <p>íŠ¹ì • í† í°ì´ ê°–ëŠ” sink dimension value <strong>Î¦(x)</strong>ë¥¼ ì•„ë˜ì™€ ê°™ì´ ì •ì˜</p> </li> </ul> <p>ì‰½ê²Œ ë§í•´ì„œ 1415, 2333 ë“±ê³¼ ê°™ì€ sink dimensionì—ì„œ í† í°ì´ ê°–ê²Œ ë˜ëŠ” íŠ€ëŠ” ê°’ì„ ë‚˜íƒ€ëƒ„. (Fig 2 ì°¸ê³ )</p> <ul> <li> <p>visual sink tokenì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ 20ë³´ë‹¤ <strong>Î¦(x)</strong>ê°€ í° í† í°ë“¤ì€ ë‹¤ visual sink tokenìœ¼ë¡œ ë¶„ë¥˜. ì´ë¥¼ í†µí•´ irrelevant visual token(sink dimensionì—ì„œ attention ê°’ì´ íŠ€ëŠ” ì• ë“¤)ê³¼ relevant visual token(íŠ€ì§€ ì•ŠëŠ” ì• ë“¤)ì„ êµ¬ë¶„í•¨.** **</p> </li> <li> <p>Fig3 (a)ë¥¼ í†µí•´ ë³¸ì¸ë“¤ì´ ì •ì˜í•œ high sink dimension value ë“¤ì´ ë†’ì€ attention ê°’ì„ ê°€ì§€ëŠ” ì• ë“¤ì´ì˜€ìœ¼ë©°, (b) ì‹¤ì œë¡œ visual sink ë“¤ì„ maskí•˜ê³  í•˜ë‹ˆ ì•ˆí• ë•Œë³´ë‹¤ ì„±ëŠ¥ì´ ë†’ì•˜ìŒ. (c) attention contributionë„ ì¸¡ì •í–ˆì„ ë•Œ (ì‹¤ì œë¡œ text ë‹µë³€ ë§Œë“œëŠ” logitì— ê¸°ì—¬í•˜ëŠ” ì •ë„) ëŠ” ì‘ì•˜ìŒ. (d) ë¥¼ ë´ë„ w/o sinks ê°€ noiseë¥¼ ì¡ì•„ë‚´ë©° ëŒ€ë¶€ë¶„ì˜ sink ë“¤ì€ backgroundì— ì¡´ì¬</p> </li> </ul> <hr/> <p><br/></p> <h2 id="surplus-attentions-in-visual-attention-sink--can-we-recycle-them">Surplus attentions in visual attention sink : can we recycle them?</h2> <p>(1) image centric-head ë¥¼ ë¨¼ì € ë½‘ê³  (2) í•´ë‹¹ headì—ì„œ sink tokenë“¤ì— ê°€ë˜ attentionì„ ë³´ì•„ì„œ non sink token ì—ê²Œ ë¶„ë°°í•  ì˜ˆì •</p> <ul> <li> <p>Image centric-head</p> <ul> <li> <p>ë¨¼ì € visual tokenì— ëŒ€í•œ attention weightì˜ sum ì´ 0.2 ë³´ë‹¤ ì‘ì€ headëŠ” ë‹¤ ë²„ë¦¼</p> </li> <li> <p>visual non-sink ratio ì •ì˜ (ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ attention ë¶„ì˜ non visual sink token ì— ëŒ€í•œ attention). ì¦‰, ì´ë¯¸ì§€ë¥¼ í•´ì„í•˜ëŠ”ë° ì‹¤ì œë¡œ í•„ìš”í•œ ì• ë“¤ì˜ ë¹„ìœ¨</p> <ul> <li>Redistributing attention weights</li> </ul> </li> <li> <p>sink í† í°ë“¤ì— ëŒ€í•´ decrease ì‹œí‚¤ê³ </p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - ì´ê²ƒë“¤ì„ ëª¨ì•„ì„œ attention budgetì— ë„£ì–´ì¤Œ (ì˜¤ë©”ê°€)

  - ê·¸ë¦¬ê³  ì•„ë˜ ì‹ì„ í†µí•´ attention sinkì— attention ì„ ë¹¼ì•—ê²¼ë˜ ë¶€ë¶„ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ redistribution
</code></pre></div> </div> <p>### Experiments</p> </li> </ul> </li> </ul> <p>(1) VL-task</p> <p>(2) visual hallucination</p> <p>(3) vision centric (spatial relationship between objects)</p> <h3 id="ablation-studies">Ablation studies</h3> <ul> <li> <p>(Table 4) visual non sink ratio ë¥¼ ì •ì˜í•´ì„œ ë¡œ ë³´ë‹¤ í° ì• ë“¤ì˜ head ë§Œ ì‚´ë ¸ì—ˆëŠ”ë° ì´ ê³¼ì •ì´ í•„ìˆ˜ì ì´ì˜€ìŒ.</p> </li> <li> <p>(Table 5) visual token ë‚´ì—ì„œë§Œ attention redistributionì´ ì„±ëŠ¥ì´ ì œì¼ ë†’ìŒ.</p> </li> </ul> <p><br/></p> <p><br/></p> <h3 id="appendix">Appendix</h3> <p><br/></p> <h3 id="discussion">Discussion</h3> <ul> <li> <p>latencyê°€ ë³„ë¡œ ì—†ìœ¼ë©´ì„œ ì •ë§ ë§ì€ VL taskì—ì„œ ì„±ëŠ¥ì´ ë‹¤ ì˜¤ë¥¸ê±´ ì‹ ê¸°! ë‹¤ë§Œ í•´ë‹¹ ë°©ë²•ë¡  ì—­ì‹œ projectorì„ ì´ìš©í•œ vision language modelì—ì„œë§Œ ì ìš© ê°€ëŠ¥í•  ê²ƒìœ¼ë¡œ ë³´ì„. (one-to-one ë§¤ì¹­, instructVL, instructBLIPê°™ì€ resamplerëŠ” ì ìš© ì•ˆë¨)</p> </li> <li> <p>ë§ˆì§€ë§‰ table5ì—ì„œ budetì„ ì•„ê»´ì„œ textì— ì¤¬ì„ ë•Œ ì„±ëŠ¥ì´ ì•ˆì˜¤ë¥¸ê±´ ì˜ì™¸. ì§ê´€ì ìœ¼ë¡œ í•„ìš” ì—†ëŠ” ì‰ì—¬ë¬¼ì„ ì¤€ë‹¤ê³  í•´ì„œ ì˜¤ë¥´ëŠ”ê±´ ì•„ë‹Œê²ƒê°™ìŒ</p> </li> </ul> <p><br/></p> <p><br/></p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - See What You Are Told: Visual Attention Sink in Large Multimodal Models]]></summary></entry><entry><title type="html">Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in-diffusion-language-models/" rel="alternate" type="text/html" title="Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models"/><published>2025-06-17T00:00:00+00:00</published><updated>2025-06-17T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in-diffusion-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in-diffusion-language-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-06-17</li> <li><strong>Reviewer</strong>: ìƒì—½</li> <li><strong>Property</strong>: Text Generation, DiffusionLM</li> </ul> <p><br/></p> <h2 id="introduction">Introduction</h2> <p><strong>ë„ì… ë°°ê²½</strong></p> <ul> <li> <p>LLMì˜ ì—„ì²­ë‚œ ì„±ì¥ â†’ Chain-of-Thought (CoT)ì™€ ê°™ì€ Reasoningì´ í•µì‹¬ ê¸°ë²•ìœ¼ë¡œ ë¶€ìƒ</p> </li> <li> <p>CoTëŠ” ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ë¥¼** autoregressive ë°©ì‹<strong>ìœ¼ë¡œ ìƒì„±í•˜ì—¬ **LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒ</strong>ì‹œí‚´</p> </li> <li> <p>í•˜ì§€ë§Œ <strong>ê¸°ì¡´ CoTì˜ í•œê³„ì </strong>ë“¤ì´ ì¡´ì¬</p> <ul> <li> <p>ì¤‘ê°„ ë‹¨ê³„ì˜ ì˜¤ë¥˜ê°€ ìµœì¢… ë‹µë³€ì— ì˜í–¥ì„ ë¯¸ì¹¨</p> </li> <li> <p>ìê¸° êµì •(self-correction) ëŠ¥ë ¥ì˜ ë¶€ì¡±</p> </li> <li> <p>íš¨ìœ¨ì„±ì— ëŒ€í•œ ìš°ë ¤</p> </li> </ul> </li> </ul> <p><br/></p> <p><strong>Diffusion Modelì˜ ë“±ì¥</strong></p> <ul> <li> <p>Vision ì˜ì—­ì—ì„œì˜ ì„±ê³µì— ì´ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œë„ ì£¼ëª©ë°›ê¸° ì‹œì‘</p> </li> <li> <p><strong>Why?</strong> Autoregressive model ëŒ€ë¹„ <span style="color:yellow_background">ê³ ìœ í•œ ê°•ì </span>ì„ ë³´ìœ </p> <ul> <li> <p>global planning ability</p> </li> <li> <p>self correction</p> </li> <li> <p>íš¨ìœ¨ì„± ê°œì„  ê°€ëŠ¥ì„± (ì´ê±´ ì¡°ê¸ˆ í™•ì¸ì´ í•„ìš”í•¨. ì ì ˆí•œ refëŠ” ì•„ë‹Œ ê²ƒ ê°™ìŒ. )</p> </li> </ul> </li> <li> <p><span style="color:yellow_background"><strong>Pre-trained diffusion language model</strong></span> â†’ Plaid, SEDD ë“± (ìµœê·¼ì—ëŠ” Llama3-8B ì •ë„ ìˆ˜ì¤€ì˜ LlaDA ëª¨ë¸ ë“±ì¥)</p> <ul> <li> <p>GPT-2 ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë‹¬ì„± (DoTëŠ” Neurips 2024 ë…¼ë¬¸)</p> </li> <li> <p>Scaling lawì˜ ì ìš© ê°€ëŠ¥ì„± í™•ì¸</p> </li> </ul> </li> </ul> <p><strong>RQ</strong></p> <p>ğŸ’¡ <strong>Can diffusion language models also leverage the CoT-style technique to gain enhanced complex reasoning abilities?</strong></p> <p><br/></p> <p><strong>Diffusion of Thoughts (DoT) ì œì•ˆ</strong></p> <ul> <li> <p><strong>Diffusion modelì— íŠ¹í™”ëœ inherent chain-of-thought ë°©ë²• ì œì•ˆ</strong></p> <ul> <li>ì¼ë ¨ì˜ latent variablesë¥¼ ìŠ¤í…ë³„ë¡œ ì—…ë°ì´íŠ¸ â†’ ê° ì¶”ë¡  ë‹¨ê³„ë“¤ì´ ì‹œê°„ì— ë”°ë¼ ë³‘ë ¬ì ìœ¼ë¡œ diffuse</li> </ul> </li> <li> <p>í•µì‹¬ íŠ¹ì§•</p> <ul> <li> <p><strong>Multi-pass variant DoT</strong>: causal biasë¥¼ ë§‰ê¸° ìœ„í•´ í•œ ë²ˆì— í•˜ë‚˜ì˜ thoughtë¥¼ ìƒì„±í•˜ëŠ” ë° ì´ˆì </p> </li> <li> <p><strong>Classifier-free guidance ì‚¬ìš©</strong>: gradient-based classifier guidance ëŒ€ì‹  ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì œì–´ ì‹ í˜¸ ì œê³µ</p> </li> <li> <p><strong>Training-time sampling</strong>: Self-correcting ëŠ¥ë ¥ í–¥ìƒ</p> </li> <li> <p><strong>Conditional ODE Solver</strong>: continuous diffusion modelì˜ inference ê°€ì†</p> </li> </ul> </li> </ul> <p><br/></p> <h2 id="preliminaries">Preliminaries</h2> <p><strong>ê¸°ë³¸ ê°œë…</strong></p> <ul> <li> <p>Forward process</p> <ul> <li> <table> <tbody> <tr> <td>$ q(\mathbf{z}_t</td> <td>\mathbf{z}_{t-1}) $, t-1 ì‹œì ì˜ representationì— noiseë¥¼ ì£¼ì…</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>Reverse process</p> <ul> <li> <p>$ z_{t} $ë¥¼ denoisingí•˜ì—¬ $ z_0 $ë¥¼ ë³µêµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œ</p> </li> <li> <table> <tbody> <tr> <td>$ z_t: p_{\theta}(\mathbf{z}<em>{0:T}) := p(\mathbf{z}_T)\prod</em>{t=1}^T p_{\theta}(\mathbf{z}_{t-1}</td> <td>\mathbf{z}_t) $ ë¡œ ì›ë³¸ ë°ì´í„° ë³µì›</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>Text generationì„ ìœ„í•œ diffusion ëª¨ë¸ì˜ ì¢…ë¥˜</p> <ol> <li> <p><span style="color:yellow_background">Continuous diffusion models</span></p> <ul> <li> <p>mapping functionì„ í™œìš© (ì‹¤ìˆ˜ â†’ í† í°í™”)</p> </li> <li> <p>discrete text $ w $ â†’ continuous space using $ \text{EMB}(w) $ â†’ <strong>rounding</strong> (inverse operation)</p> </li> <li> <p>forward perturbations: $ q(\mathbf{z}<em>{t} \vert \mathbf{z}</em>{t-1}) = \mathcal{N}(\mathbf{z}<em>{t};\sqrt{1-\beta_t}\mathbf{z}</em>{t-1}, {\beta}_t \mathbf{I}) $, where $ \beta_t \in (0, 1) $</p> </li> </ul> </li> <li> <p><span style="color:yellow_background">Discrete diffusion models</span></p> <ul> <li> <p>ë¬¸ì œ ìì²´ë¥¼ integer programìœ¼ë¡œ í’€ê¸°</p> </li> <li> <p>$ z_t $ë¥¼ ont-hot vectors in $ {0, 1}^K $ë¡œ í‘œí˜„. KëŠ” vocab size</p> </li> <li> <table> <tbody> <tr> <td>$ q(\mathbf{z}_t</td> <td>\mathbf{z}_{t-1}) $ì„ transition matrixë¡œ í‘œí˜„ â†’ uniform ë¶„í¬ë‚˜ [mask]ë¡œ ì „ë¶€ ë³€ê²½í•˜ëŠ” ë‹¨ê³„</td> </tr> </tbody> </table> </li> </ul> </li> </ol> </li> </ul> <p><strong>Seq2Seq Generation (e.g. DiffuSeq)</strong></p> <ul> <li> <p>ì…ë ¥-ì¶œë ¥ sequenceë¥¼ single sequenceë¡œ ì²˜ë¦¬: $ \mathbf{w}^{z}=\mathbf{w}^{[x; y]} $</p> </li> <li> <p>Left-aligned mask $ [\mathbf{0};\mathbf{1}] $ë¡œ $ x, y $ë¥¼ êµ¬ë¶„</p> </li> <li> <p><strong>Partial noising</strong>: mask valueê°€ 1ì¸ ë¶€ë¶„ì—ë§Œ noise ì ìš©</p> </li> </ul> <p><br/></p> <h2 id="diffusion-of-thoughts"><span style="color:yellow_background">Diffusion-of-Thoughts</span></h2> <ul> <li> <p>Notation: $ s $ (problem statement), $ a $ (answer), $ p_{\theta}^{LM} $ (language model)</p> </li> <li> <table> <tbody> <tr> <td>Answer-only generation model: $ \mathbf{a}\sim p_\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s}) $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>CoT: $ \mathbf{a}\sim p_\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{r}_{1\dots n}) $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>implicit CoT: $ \mathbf{a}\sim p_\theta^{\textit{iCoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}_{1\dots n}) $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>DoT: $ \mathbf{a}\sim p_\theta^{\textit{DoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}_t) $</td> </tr> </tbody> </table> </li> </ul> <p><br/></p> <p><br/></p> <h3 id="dot-modeling"><span style="color:yellow_background">DoT Modeling</span></h3> <ul> <li> <p><strong>Gradient-based token guidanceì˜ í•œê³„</strong></p> <ul> <li> <p>ì •í™•í•œ conditioning ì‹¤íŒ¨. íŠ¹íˆ ìˆ˜í•™ì  ì¶”ë¡ ê³¼ ê°™ì´ ì •í™•í•œ ìˆ«ìì™€ í† í°ì´ í•„ìš”í•œ ê³³ì—ì„œ ì¹˜ëª…ì </p> </li> <li> <p>ì˜ˆì‹œ: â€œTwo trainsâ€ â†’ â€œ<strong>Three</strong> trainsâ€</p> </li> </ul> </li> </ul> <p><strong>â†’ DiffuSeq-style classifier-free conditioning ì±„íƒ</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- ëª¨ë“  rationaleë“¤ì´ backward diffusion processì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ìƒì„±

- ëª¨ë“  conditional token, ì—¬ê¸°ì„œëŠ” $ s $ëŠ” ê³ ì •, $ r_{1...n} $ì—ë§Œ noise ì¶”ê°€
</code></pre></div></div> <p><span style="color:yellow_background">â†’ continuous ë°©ì‹ì˜ DiffuSeq-styleì´ ê°€ì§„ ì¥ì ì´ ë¬´ì—‡ì¸ê°€?</span></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Gradient-based token guidanceëŠ” ë³„ë„ì˜ classifierë¥¼ í•™ìŠµ (ìµœê·¼ ëª¨ë¸ì—ì„œëŠ” LLM ë‚´ë¶€ì˜ ì—°ì‚°ì„ í™œìš©í•˜ê¸°ë„), ì—¬ê¸°ì„œ ì–»ì€ ì •ë³´ë¥¼ conditionìœ¼ë¡œ í•˜ì—¬ pì˜ ì‚¬í›„ í™•ë¥ ì„ ì¡°ì ˆí•˜ëŠ” ê°„ì ‘ì ì¸ ë°©ì‹

- DiffuSeq ë°©ì‹ì€ ëª¨ë¸ ìì²´ì—ì„œ condition (ì´ì „ z)ë¥¼ denoisingí•˜ëŠ” ê³¼ì •ì—ì„œ ìƒ˜í”Œ ë¶„í¬ ìì²´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ê²ƒìœ¼ë¡œ ë” í™•ì‹¤í•œ ë³€í™”ê°€ ê°€ëŠ¥
</code></pre></div></div> <p><br/></p> <p><strong>Multi-pass DoT (MP-DoT)</strong></p> <ul> <li> <p>Causal inductive bias ë„ì… thought-by-thought ë°©ì‹ìœ¼ë¡œ rationalesì„ ìƒì„±í•˜ëŠ” ë°©ë²• ì œì•ˆ</p> </li> <li> <p><strong>Process</strong>:</p> <ol> <li> <table> <tbody> <tr> <td>$ \mathbf{r}<em>1\sim p</em>{\theta}^{\textit{DoT}}(\mathbf{r}_1</td> <td>\mathbf{s}, \mathbf{z}^{r_1}_t) $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ \mathbf{r}<em>2\sim p</em>{\theta}^{\textit{DoT}}(\mathbf{r}_2</td> <td>[\mathbf{s};\mathbf{r}_1], \mathbf{z}^{r_2}_t) $</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$ \mathbf{a}\sim p_{\theta}^{\textit{DoT}}(\mathbf{a}</td> <td>[\mathbf{s};\mathbf{r}_1;â€¦;\mathbf{r}_n], \mathbf{z}^{r_n}_t) $</td> </tr> </tbody> </table> </li> </ol> </li> <li> <p>ì´í›„ rationaleì´ ì´ì „ rationaleë“¤ì„ ë” ê°•í•œ condition signalë¡œ ì´ìš©í•  ìˆ˜ ìˆìŒ.</p> </li> </ul> <p><br/></p> <h3 id="training"><span style="color:yellow_background">Training</span></h3> <p><strong>Scheduled Sampling</strong></p> <ul> <li> <p>Diffusion ëª¨ë¸ì´ denoisingì„ í•˜ëŠ” ê³¼ì •ì—ì„œ ì´ë¯¸ self-correcting ëŠ¥ë ¥ì´ ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŒ. â†’ Sampling ê³¼ì •ì„ í†µí•´ ì´ë¥¼ ë”ìš± ë°œì „</p> </li> <li> <p>Trainingê³¼ inference ê°„ì˜ <strong>exposure bias</strong>ê°€ errorë¥¼ ë°œìƒì‹œí‚¨ë‹¤ê³  ìƒê°</p> </li> <li> <p>Any timesteps: $ s, t, u $ that satisfy $ 1 &lt; s &lt; t &lt; u &lt; T $</p> <ul> <li> <table> <tbody> <tr> <td>Training stage: $ \mathbf{z}_t \sim q(\mathbf{z}_t</td> <td>\mathbf{z}_0) $ (oracle dataì—ì„œ diffuse)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>Inference stage: $ q(z_t</td> <td>z_{\theta}(z_u;u)) $</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p><strong>í•´ê²°ì±…</strong>: ì¶”ë¡  ë‹¨ê³„ë¥¼ ëª¨ë°©í•˜ê¸° ìœ„í•´ $ \epsilon_i $ í™•ë¥ ë¡œ ë‹¤ìŒê³¼ ê°™ì´ forward stepì—ì„œ ë§Œë“¤ì–´ì§„ $ z $ë¥¼ í™œìš©</p> <ul> <li> <table> <tbody> <tr> <td>$ u \in {t+1, â€¦, T} $, $ \hat{z_0} = z_{\theta}(z_u;u) $ â†’ $ q(z_t</td> <td>\hat{z_0}) $</td> </tr> </tbody> </table> </li> <li>$ \epsilon_i $ëŠ” 1ì—ì„œ $ \epsilon_{min} $ë¡œ ì„ í˜• ê°ì†Œ</li> </ul> </li> </ul> <p><br/></p> <p><strong>Coupled Sampling</strong></p> <ul> <li> <p>Multi-pass DoTì—ì„œ rationaleì— ìŒ“ì´ëŠ” error accumulation ë¬¸ì œ í•´ê²°</p> </li> <li> <p><strong>Training ì‹œ í˜„ì¬ thoughtë¿ë§Œ ì•„ë‹ˆë¼ ì´ì „ thoughtë“¤ì—ë„ í™•ë¥ ì ìœ¼ë¡œ noise ì¶”ê°€</strong></p> <ul> <li> <p>$ \mathbf{z}<em>0=\text{EMB}([\mathbf{s};\mathbf{r}</em>{1};\mathbf{r}_{2}]) $ ê³¼ì •ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ $ r_1 $ì—ë§Œ noise ì ìš©</p> </li> <li> <p>ì¼ì • í™•ë¥ ë¡œ $ [r_1;r_2] $ ëª¨ë‘ì— noise ì ìš©</p> </li> </ul> </li> </ul> <p><br/></p> <p><strong>Training Objective</strong></p> <p>DoT ëª¨ë¸ì— ëŒ€í•´ ë‘ ê°€ì§€ í•™ìŠµ ë°©ë²•ì„ ì‚¬ìš©</p> <ul> <li> <p>from scratch</p> </li> <li> <p>fine-tuning from pre-trained diffusion model</p> </li> </ul> <p><br/></p> <p><strong>ê³µí†µ Objective function:</strong> Negative variational lower bound ìµœì†Œí™”</p> <ul> <li>$ z_t $ë¥¼ denoising í•¨ìœ¼ë¡œì¨ $ z_0 $ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì„ ë°°ìš°ëŠ” ê²ƒ</li> </ul> \[\mathcal{L}_{\text{VLB}}(\mathbf{w}^z)=\mathbb{E}{q({\mathbf{z}_0}\mid \mathbf{w}^z)} \left[ \underbrace{\log\frac{ q(\mathbf{z}_T|\mathbf{w}^z)}{p_{\theta}(\mathbf{z}_T)}}_{\text{Prior loss}} + \underbrace{\mathcal{L}_{\text{VLB}}(\mathbf{z}_0)}_{\text{Diffusion loss}} \underbrace{-\log p_\theta(\mathbf{w}^z|\mathbf{z}_0)}_{\text{Rounding loss}}\right]\] \[\mathcal{L}_{\mathrm{VLB}}\left(\mathbf{z}_0\right)=\mathbb{E}_{q\left(\mathbf{z}_{1: T} \mid \mathbf{z}_0\right)}[\underbrace{\log \frac{q\left(\mathbf{z}_T \mid \mathbf{z}_0\right)}{p_\theta\left(\mathbf{z}_T\right)}}_{\mathcal{L}_T}+\underbrace{\sum_{t=2}^T \log \frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_0, \mathbf{z}_t\right)}{p_\theta\left(\mathbf{z}_{t-1} \mid \mathbf{z}_t\right)}}_{\mathcal{L}_{T-1}+\cdots+\mathcal{L}_1}-\underbrace{\log p_\theta\left(\mathbf{z}_0 \mid \mathbf{z}_1\right)}_{\mathcal{L}_0}]\] <ul> <li> <p><strong>Prior loss</strong></p> <ul> <li> <p>$ p_{\theta}(z_T) $: ìµœì¢… noiseì—ì„œ ëª¨ë¸ì˜ ë¶„í¬</p> </li> <li> <table> <tbody> <tr> <td>$ q(z_T</td> <td>w^z) $: ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì—ì„œ ë§Œë“¤ì–´ì§„ ìµœì¢… zì˜ ë¶„í¬</td> </tr> </tbody> </table> </li> </ul> <p>â†’ ì´ìƒì ìœ¼ë¡  ë‘˜ì´ ë™ì¼í•´ì ¸ì•¼ í•˜ë©° prior lossëŠ” 0ì´ ë˜ì–´ì•¼ í•¨.</p> <p>â†’ ë” ì§ê´€ì ìœ¼ë¡œ) ì¶©ë¶„íˆ ë§ì€ noiseë¥¼ ì£¼ì…í•˜ë©´ ìµœì¢… noise ë¶„í¬ $ \mathcal{N(0, I)} $ê°€ ë˜ì–´ì•¼ í•¨.</p> </li> <li> <p><strong>Diffusion loss</strong>: ê° ë‹¨ê³„ì—ì„œ ì–¼ë§ˆë‚˜ noiseë¥¼ ì˜ ì œê±°í•˜ëŠ”ê°€ì— ëŒ€í•œ íƒìƒ‰</p> <ul> <li> <table> <tbody> <tr> <td>ìš°ë¦¬ê°€ ê¶ê¸ˆí•œ ê²ƒ: **pë¥¼ í†µí•œ denoisingì´ ì˜ ëœ ê²ƒì´ ë§ì„ê¹Œ? == **$ p_{\theta}(z_{t-1}</td> <td>z_t) $** ë¶„í¬ë¥¼ ì˜ êµ¬í–ˆëŠ”ê°€?**</td> </tr> </tbody> </table> </li> <li> <p>ìš°ë¦¬ê°€ ì•„ëŠ” ê²ƒ, $ z_t $ (í˜„ì¬ ì£¼ì–´ì§„ ì •ë³´), $ z_0 $ (ì›ë³¸)</p> </li> <li>Posteriorë¥¼ í™œìš©, ë‹¤ìŒì˜ ë¶„í¬ë¥¼ ì´ìš©í•´ $ p_{\theta} $ë¥¼ ê²€ì •</li> </ul> \[q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_t, \mathbf{z}_0\right)=q\left(\mathbf{z}_t \mid \mathbf{z}_{t-1}, \mathbf{z}_0\right) \frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}_0\right)}{q\left(\mathbf{z}_t \mid \mathbf{z}_0\right)}\] <ul> <li>ë” ì§ê´€ì ìœ¼ë¡œ $ z_{t-1} $ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ noise, denoise ê³¼ì •ì—ì„œ ë™ì¼í•œê°€</li> </ul> </li> <li> <p><strong>Rounding loss</strong>: ë³µì›ë ¥ $ z_0 $ â†’ $ \text{w}^z $</p> </li> </ul> <p><br/></p> <h3 id="inference-strategy">Inference Strategy</h3> <ul> <li> <p>diffusion ëª¨ë¸ì˜ ì¶”ë¡  flexibilityëŠ” í° ì¥ì  â†’ ì–´ë ¤ìš´ ë¬¸ì œì¼ìˆ˜ë¡ ë” ë§ì€ reasoning timeì„ ê°€ì ¸ì•¼ í•¨. â†’ backward timestep Të¥¼ í¬ê²Œ ê°€ì ¸ê°€ì! (ì´ê±° ì•ˆë˜ëŠ” ê²Œ ìˆë‚˜? ë…¼ë¬¸ì—ì„œ autoregressive ë°©ë²•ì—ì„œ í† í° ìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì€ ë” ì–´ë µë‹¤ê³  ì£¼ì¥.)</p> </li> <li> <p><strong>ë¬¸ì œ</strong>: Continuous diffusionì˜ ë†’ì€ timestep ìš”êµ¬ì‚¬í•­ (ì˜ˆ: Plaid 4096 timesteps)</p> </li> </ul> <p>â†’ ODE solverë¥¼ conditional formì„ í™œìš©í•´ accelerate</p> \[\mathbf{y}{t_i} = \frac{\sigma_{t_i}}{\sigma_{t_{i-1}}}\mathbf{y}_{t_{i-1}} - \alpha_{t_i}(e^{-h_i} - 1)\tilde{\mathbf{z}}_\theta(\mathbf{z}_{t_{i-1}}, t_{i-1})\] <ul> <li><del>ì´ê²Œ ìµœì¢…ì‹ì¸ë° ë¯¸ë¶„ë°©ì •ì‹ ì–˜ê¸°ê°€ ë‚˜ì™€ì„œ ì•„ì§ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤â€¦.</del></li> </ul> <p><br/></p> <p><strong>Self-consistency Integration</strong></p> <ul> <li> <p>Multiple samplingì„ í†µí•œ ë‹¤ì–‘í•œ reasoning pathway ìƒì„±</p> </li> <li> <p>ë™ì¼ ë¬¸ì œ $ s $ì— ëŒ€í•´ ë‹¤ì–‘í•œ $ (r_{i;1â€¦n}, a_i) $ë¥¼ êµ¬í•¨. (<span style="color:yellow_background">Diffusion ëª¨ë¸ì˜ ê°•ì : noise seedë§Œ ë‹¤ë¥´ê²Œ í•´ë„ ë¨!</span>)</p> </li> <li> <p>Majority vote:</p> </li> </ul> <p><br/></p> <h2 id="evaluation">Evaluation</h2> <h3 id="experimental-setup">Experimental Setup</h3> <p><strong>ë°ì´í„°ì…‹ ë° ë©”íŠ¸ë¦­</strong></p> <ul> <li> <p><strong>Simple reasoning</strong>:</p> <ul> <li> <p>4Ã—4, 5Ã—5 digit multiplication (BIG-bench)</p> </li> <li> <p>Boolean logic reasoning (DyVal)</p> </li> </ul> </li> <li> <p><strong>Complex reasoning</strong>: GSM8K grade school math problems</p> </li> </ul> <p><br/></p> <p><strong>Base Model</strong></p> <ul> <li> <p><strong>From scratch</strong>: Following DiifuSeq (12-layer Transformer encoder, 124M)</p> </li> <li> <p><strong>Pre-trained model for fine-tuning</strong>:</p> <ul> <li> <p>Plaid (1.3B): OpenWebTextì—ì„œ í›ˆë ¨, GPT-2 ìˆ˜ì¤€ perplexity</p> </li> <li> <p>SEDD-small (170M), SEDD-medium (424M)</p> </li> </ul> </li> </ul> <p><br/></p> <p><strong>Baseline</strong></p> <ul> <li> <p>Answer-only, CoT, Implicit CoT</p> </li> <li> <p>GPT-2 (small 124M, medium 355M, large 774M)</p> </li> <li> <p>ChatGPT (gpt-3.5-turbo-1106) with 5-shot CoT</p> </li> </ul> <p><br/></p> <p><strong>êµ¬í˜„ ì„¸ë¶€ì‚¬í•­</strong></p> <ul> <li> <p>Tokenization: ëª¨ë“  digitì„ ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬</p> </li> <li> <p>MP-DoT: ë§ˆì§€ë§‰ thought ë’¤ì— <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> í† í° ì¶”ê°€ (ëª¨ë¸ì´ rationale ìˆ˜ ê²°ì •)</p> </li> <li> <p>8 * V100-32G</p> </li> <li> <p>Training:</p> <ul> <li> <p>scheduled sampling: $ \epsilon_{min}=0.95 $</p> </li> <li> <p>coupled sampling: $ \gamma $ (0.01, noise ì¶”ê°€í•  í™•ë¥ ), $ k $ (1, ì´ì „ step)</p> </li> <li> <p>self-consistency: $ m $ (20)</p> </li> </ul> </li> <li> <p>Inference:</p> <ul> <li>temperature 0.5, default timestep $ T $: 64</li> </ul> </li> </ul> <p><br/></p> <h2 id="results">Results</h2> <p><strong>Digit Multiplication &amp; Boolean Logic</strong></p> <ul> <li> <p>DoT: 100% ì •í™•ë„ ë‹¬ì„±, ì´ëŠ” CoTë¥¼ í™œìš©í•˜ë©´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€</p> </li> <li> <p>ì†ë„ì—ì„œ GPT-2 CoT ëŒ€ë¹„ ìµœëŒ€ 27ë°° ë¹ ë¦„.</p> </li> <li> <p>**Optimal sampling timestep: 1 for multiplication, 2 for boolean logic **(very EZ)</p> </li> <li> <p>ChatGPTì™€ Implicit CoTë„ 100% accuracy ë‹¬ì„± ì‹¤íŒ¨</p> </li> </ul> <p>â†’ ê°„ë‹¨í•œ ì‘ì—…ì—ì„œ DoTì˜ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„± ë™ì‹œ ë‹¬ì„±</p> <p><br/></p> <p><strong>Results on Complex Reasoning (GSM8K)</strong></p> <ul> <li> <p><strong>From-scratch training</strong>: ~5% accuracy (pre-trained capabilityì˜ ì¤‘ìš”ì„± í™•ì¸)</p> </li> <li> <p><strong>Fine-tuned DoT</strong>: ì—„ì²­ë‚œ ì„±ëŠ¥ í–¥ìƒ</p> <ul> <li> <p>SEDD-medium DoT &gt; similar-sized GPT2-medium CoT (10%ê¹Œì§€ ì°¨ì´)</p> </li> <li> <p>DoT-SEDD-medium (424M) &gt; GPT2-medium (355M) + CoT</p> </li> </ul> </li> <li> <p><strong>Multi-pass DoT</strong></p> <ul> <li> <p>Plaidì—ì„œ single-pass ëŒ€ë¹„ ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒ, íš¨ìœ¨ì„±ì€ single-passê°€ ìš°ìˆ˜</p> </li> <li> <p>(ì„±ëŠ¥ë„ ë‚®ê³  throughputë„ ë‚®ì•„ì„œ ì´ê±´ ì™œ í•œ ê²ƒì¸ê°€â€¦)</p> </li> </ul> </li> <li> <p><strong>Self-consistency</strong>: DoT ëª¨ë¸ì—ì„œ í° ì„±ëŠ¥ í–¥ìƒ</p> </li> </ul> <p><br/></p> <p><strong>Ablation Study</strong></p> <ul> <li> <p>Sampling ë°©ë²•ì€ íš¨ê³¼ì </p> </li> <li> <p>Continue pre-training (Gradient-based token guidance)ë¥¼ í™œìš©í•  ê²½ìš° 0.5%ê°€ ë¨. (pre-trained ëª¨ë¸ë³´ë‹¤ë„ ê°ì†Œ)</p> </li> <li> <p><strong>Gradient-based conditioning ì‹¤íŒ¨ ì‚¬ë¡€</strong>:</p> </li> <li> <p><strong>Conditional ODE solver</strong>: 4096 â†’ 8 timestep ì •ë„ì—ì„œ ê²°ê³¼ ìˆ˜ë ´</p> </li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Reasoning-Efficiency Trade-off</strong></span></p> <ul> <li> <p>left-to-right ë°©ì‹ì˜ reasoningì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ ì•„ì´ë””ì–´ (prompting, decoding ë“±)</p> </li> <li> <p>Diffusionë„ ì´ëŸ¬í•œ reasoning capabilitiesë¥¼ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ë°©ì•ˆ</p> </li> </ul> <p>â†’ ë” ë§ì€ timesteps â†’ ë” ë§ì€ reasoning â†’ íš¨ìœ¨ì„± ê°ì†Œ</p> <ul> <li> <p>Efficiency trade-offë¥¼ í™•ì¸</p> </li> <li> <p>DoTëŠ” timestep, ë‚˜ë¨¸ì§€ ë°©ë²•ì€ output tokenì˜ ìˆ˜ë¥¼ stepìœ¼ë¡œ ìƒê°</p> </li> <li> <p><strong>Simple task (5Ã—5)</strong>: 1 reasoning stepìœ¼ë¡œ 100% accuracy, ì¶”ê°€ computation ë¶ˆí•„ìš”</p> </li> <li> <p><strong>Complex task (GSM8K)</strong>: Timestep ì¦ê°€ì— ë”°ë¥¸ ì§€ì†ì ì¸ ì„±ëŠ¥ í–¥ìƒ</p> <ul> <li> <p>DoT-SEDD-mediumì€ outperform</p> </li> <li> <p>DoT-SEDD-smallì˜ ê²½ìš°, 32 timestepì—ì„œ GPT2-medium CoT ëŠ¥ê°€, T=64ì—ì„œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±</p> </li> </ul> </li> <li> <p>**Diffsuion ëª¨ë¸ì˜ ì¥ì : **ì‘ì—… ë‚œì´ë„ì— ë”°ë¼ timestepsì„ ììœ ë¡­ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.</p> </li> <li> <p><strong>Autoregressive í•œê³„</strong>: CoTì™€ Implicit CoTëŠ” token-by-token prediction íŠ¹ì„±ìƒ ëª…í™•í•œ ì¡°ì ˆì´ ì–´ë ¤ì›€.</p> </li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Self-consistency in DoT</strong></span></p> <ul> <li> <p>sampling ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ ì§€ì†ì  ê°œì„ </p> </li> <li> <p>Autoregressive modelê³¼ ë‹¬ë¦¬ decoding algorithm ì—†ì´ë„ ìì—°ìŠ¤ëŸ¬ìš´ diversity ì œê³µ (NoiseëŠ” ë‹¤ë¥´ë‹ˆê¹!)</p> </li> </ul> <p><br/></p> <p><span style="color:yellow_background"><strong>Self-correction Capability</strong></span></p> <ul> <li>Autoregressive CoTì™€ ëª…í™•íˆ ë‹¤ë¥¸ self-correction ëŠ¥ë ¥ì„ ë³´ì„</li> </ul> <p><strong>Case 1: Fast thinking</strong></p> <ul> <li> <p>ì‰¬ìš´ ë¬¸ì œì˜ ê²½ìš°, ëª¨ë“  ì˜¬ë°”ë¥¸ thoughtë¥¼ single reasoning stepìœ¼ë¡œ ë„ì¶œ</p> </li> <li> <p>ë‘ ë²ˆì§¸ Stepì—ì„œ ì •í™•í•œ ìµœì¢… ë‹µë³€ ìƒì„±</p> </li> </ul> <p><strong>Case 2: Slow thinking</strong></p> <ul> <li> <p>ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ ë¬¸ì œ ì´ˆê¸°ì—ëŠ” ì—ëŸ¬ê°€ ë°œìƒ</p> </li> <li> <p>í›„ì† ë‹¨ê³„ì—ì„œ ì ì§„ì  refinementë¥¼ í†µí•œ ì •í™•í•œ ë‹µë³€ ë„ì¶œ</p> </li> <li> <p>ì´ˆê¸°ì— ë¬¸ì œì˜ ëŒ€ëµì ì¸ outlineì„ ì¡ê³  refine &amp; improveí•˜ëŠ” ê²ƒì€ ì‚¬ëŒì˜ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰ ë°©ì‹ê³¼ ìœ ì‚¬</p> </li> </ul> <p><strong>Case 3: Non-sequential correction</strong></p> <ul> <li> <p>Step 4: ì˜ëª»ëœ ì¤‘ê°„ thought <code class="language-plaintext highlighter-rouge">&lt;2*3=4&gt;</code> ì¡´ì¬í•˜ì§€ë§Œ ì´í›„ thoughtì™€ ë‹µë³€ì€ ì •í™•</p> </li> <li> <p>Step 5: ì˜ëª»ëœ ì¤‘ê°„ thought êµì •</p> </li> <li> <p><strong>í•µì‹¬ íŠ¹ì§•</strong>: Left-to-right paradigmì„ ë²—ì–´ë‚˜ ì¢Œìš°ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •</p> </li> </ul> <p><br/></p> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Diffusion modelì— íŠ¹í™”ëœ ìµœì´ˆì˜ CoT reasoning ë°©ë²• DoT ì œì•ˆ</p> </li> <li> <p>Scheduled sampling, coupled sampling, conditional ODE solver ë“± ê³ ìœ  ê¸°ë²• ê°œë°œ</p> </li> <li> <p>Mathematical reasoning taskì—ì„œ comprehensive evaluation ìˆ˜í–‰</p> </li> </ul> <hr/> <ul> <li> <p><strong>Current limitations</strong>: Pre-trained diffusion modelì˜ scaleê³¼ generalization í•œê³„ (ì œí•œëœ ëª¨ë¸ í¬ê¸°)</p> </li> <li> <p><strong>Future promise</strong>: ë” ê°•ë ¥í•œ pre-trained modelê³¼ í•¨ê»˜ autoregressive LLMì— í•„ì í•˜ëŠ” ì„±ëŠ¥ ê¸°ëŒ€</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><category term="nlp"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models]]></summary></entry><entry><title type="html">DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</title><link href="https://alshedivat.github.io/al-folio/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for-r1-zero-like-training-of-large-language-models/" rel="alternate" type="text/html" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://alshedivat.github.io/al-folio/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for-r1-zero-like-training-of-large-language-models</id><content type="html" xml:base="https://alshedivat.github.io/al-folio/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for-r1-zero-like-training-of-large-language-models/"><![CDATA[<p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li><strong>Date</strong>: 2025-06-10</li> <li> <p><strong>Reviewer</strong>: ê±´ìš° ê¹€</p> </li> <li> <p>ìµœê·¼ì— post-trainingì„ ìœ„í•œ RLì—ì„œ <strong>GRPO</strong>ì™€ ê°™ì´ low-resource settingsì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ</p> <p><strong>â†’ GRPOëŠ” solution-levelì˜ scalar reward signalsì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„, samplingëœ ë¬¸ì¥ë“¤ê°„ì˜ semantic diversityë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•¨</strong></p> <p>â†’ ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ reasoning pathë¥¼ ê°–ëŠ” responseë“¤ì´ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ë™ì¼í•œ rewardë¥¼ ë°›ëŠ” (<strong>diversity-quality inconsistency</strong>) ë¬¸ì œê°€ ìˆìŒ</p> </li> <li> <p>ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reward computation ê³¼ì •ì—ì„œ semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ” ë°©ë²•ì¸ <strong>Diversity-aware Reward Adjustment (DRA)</strong>ë¥¼ ì œì•ˆí•¨</p> </li> <li> <p>DRAëŠ” Submodular Mutual Information (SMI)ë¥¼ í™œìš©í•˜ì—¬</p> <ol> <li> <p>ì¤‘ë³µëœ responseì˜ rewardëŠ” ê°ì†Œì‹œí‚´</p> </li> <li> <p>diverse responseì˜ rewardëŠ” ì¦í­ì‹œí‚´</p> </li> </ol> </li> <li> <p>5ê°œ Mathematical Reasoning benchmarkì—ì„œ recent methods ëŒ€ë¹„ outperform ì„±ëŠ¥ ë³´ì—¬ì¤Œ</p> <p>(ë‹¨ 7,000ê°œ sampleë¡œë§Œ fine-tuningì„ í•˜ê³ , $55 training costë¡œ í‰ê·  acc 58.2% sota ë‹¬ì„±)</p> </li> </ul> <p>DeepSeek-R1-Zero (Guo et al., 2025)ì—ì„œ ê¸°ì¡´ LLMì— SFTë¥¼ ì ìš©í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜, base LMì— ë°”ë¡œ RLì„ ì ìš©í•  ìˆ˜ ìˆëŠ” R1-Zero training pipelineì„ ì œì•ˆí•¨.</p> <p>â†’ Group Relative Policy Optimization (GRPO) ì•Œê³ ë¦¬ì¦˜ ë•ë¶„ì— ê°€ëŠ¥í•œ ë°©ë²•</p> <p>GRPOëŠ” PPOì™€ ë‹¤ë¥´ê²Œ critic model ì—†ì´ ì£¼ì–´ì§„ promptì— ëŒ€í•´ ì—¬ëŸ¬ samplingëœ completionsì˜ relative performanceì— ëŒ€í•œ advantageë¥¼ í‰ê°€í•¨.</p> <p><br/></p> <p>í•˜ì§€ë§Œ ìµœê·¼ì— ê³µê°œëœ GRPO ë° ê·¸ variants (e.g,. DR. GRPO)ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì •ë‹µ ì—¬ë¶€ì™€ ê°™ì€ <span style="color:red"><strong>solution-levelì˜ scalar reward signalsì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆì–´, ê°™ì€ ì •ë‹µì´ë¼ë„ diverse reasoning pathì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨</strong></span>.</p> <p>â†’ ì´ëŠ” semanticí•˜ê²Œ ë‹¤ë¥¸ completionsë“¤ì´ ì˜¬ë°”ë¥´ê±°ë‚˜ í‹€ë¦° ê²½ìš° ëª¨ë‘ ê±°ì˜ ë™ì¼í•œ rewardsë¥¼ ë°›ì•„, ì˜ë¯¸ ìˆëŠ” reasoning ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” <strong>indistinguishable advantage estimates</strong>ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆìŒ</p> <ul> <li> <p><strong>Example1 (Correct Reward)</strong></p> <p>GRPO training processì˜ examplesì„ ì¤€ë¹„í•¨. â†’ ì €ìë“¤ì˜ key motivation of research</p> <ul> <li> <p>LLMì€ diverse answerë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŸ° answersë“¤ì€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ reward scoreë¥¼ ë°›ìŒ</p> <p>â†’ ì¦‰, solution-level judgementsëŠ” different reasoning pathsë¥¼ êµ¬ë³„í•˜ì§€ ëª»í•¨</p> </li> </ul> <p>ì•„ë˜ëŠ” questionì— ë™ì¼í•œ ì •ë‹µì„ ìƒì„±í–ˆì§€ë§Œ reasoning pathê°€ ì™„ì „íˆ ë‹¤ë¥¸ ë‘ê°€ì§€ ì‘ë‹µì— ëŒ€í•œ ì¼€ì´ìŠ¤ (ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  rewardëŠ” ë¹„ìŠ·í•¨)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      - **Example2 (Incorrect Reward)**
</code></pre></div> </div> <p>ì´ë²ˆ ì˜ˆì‹œëŠ” Questionì— ëŒ€í•´ ë‘ê°€ì§€ ì‘ë‹µì´ ëª¨ë‘ Incorrectì¸ ë°˜ë©´, reasoning pathëŠ” ì„œë¡œ ë‹¤ë¦„ â†’ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  reward scoreëŠ” ë‘˜ ë‹¤ ë¹„ìŠ·í•˜ê²Œ ë‚®ìŒ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              â†’ ë˜í•œ, ì´ëŠ” resource-constrained settingsì—ì„œ ë” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ
</code></pre></div> </div> <ul> <li> <p>ê° promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” completionsì˜ ê°œìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì—, <span style="color:orange_background"><strong>rewardê°€ ë†’ì€ outputsì— ëŒ€í•œ exploitationë§Œ reinforceí•˜ë©°, alternativeí•˜ê³  potentially validí•œ reasoning pathsì— ëŒ€í•œ explorationì„ ìœ ë„í•˜ì§€ ëª»í•¨.</strong></span></p> </li> <li> <p>(ë¹„ìœ ) ì„ ìƒë‹˜ì´ ìˆ˜í•™ ë¬¸ì œë¥¼ ëª¨ë‘ ì •í™•í•˜ê²Œ í‘¼ í•™ìƒë“¤ì—ê²Œ 100ì ì„ ì£¼ëŠ” ì¼€ì´ìŠ¤. ì •ë‹µì´ ë§ë”ë¼ë„, í•™ìƒì˜ ì´í•´ë„ë‚˜ ì‚¬ê³  ë°©ì‹ì„ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¤ì–‘í•œ ë°©ì‹ì€ í‰ê°€ë˜ì§€ ì•Šê³ , ì˜¤ë‹µì¼ ê²½ìš°ì—ë„ ê·¸ ê³¼ì •ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ë‹¤ì–‘í•œ ì¶”ë¡  ì ‘ê·¼ì„ í‰ê°€í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ê°™ì€ ê°ì ì„ ë°›ìŒ.</p> </li> </ul> </li> </ul> <p><br/></p> <p>ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì €ìë“¤ì€ <span style="color:orange_background"><strong>Diversity-aware Reward Adjustment (DRA)</strong></span>ë¥¼ ì œì•ˆí•¨.</p> <p>ì´ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ samplingëœ completions ê°„ì˜ <em>semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ</em> ê·¸ë£¹ ë‚´ ë‹¤ë¥¸ <em>completionsê³¼ì˜ semantic similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° outputì˜ rewardë¥¼ reweight</em>í•¨.</p> <ul> <li><span style="color:orange_background"><strong>diverse completionsì—ëŠ” ë” ë†’ì€ weight, ì¤‘ë³µëœ completionì—ëŠ” ë” ë‚®ì€ weight ë¶€ì—¬</strong></span></li> </ul> <h3 id="preliminary">Preliminary</h3> <p>LMì˜ generationì€ token-level Markov Decision Processë¡œ ë³¼ ìˆ˜ ìˆìŒ. ê° generation step $ t $ì—ì„œ state $ s_t $ëŠ” input question $ q $ì™€ ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ partial output sequence $ o_{&lt;t} $ì˜ concatenationì´ê¸°ì—, satesëŠ” ë‹¤ìŒê³¼ ê°™ìŒ $ s_t=[q;o_{&lt;t}] $.</p> <table> <tbody> <tr> <td>policy $ \pi_{\theta}(.</td> <td>s_t) $ëŠ” vocab set $ A $ì—ì„œ next token $ o_t $ë¥¼ ì„ íƒí•˜ê³ , ì´ëŠ” deterministic transitionì„ ìœ ë„í•˜ì—¬ next state $ s_{t+1}=[s_t;o_t] $ë¡œ ì´ë™í•¨.</td> </tr> </tbody> </table> <p>GRPOëŠ” ê° question $ q $ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ responses $ C= ${$ o_1,â€¦o_G $}ë¥¼ samplingí•˜ê³ , ê° responseì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•¨ $ R= ${$ R(q,o_1), â€¦ , R(q,o_G) $}</p> <p>ê³„ì‚°ëœ reward $ R $ì„ ì´ìš©í•´ advantage $ A_{i,t} $ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•¨ (normalize)</p> <p>GRPOì˜ objective function $ J_{GRPO}(\pi_{\theta}) $ë¥¼ optimizeí•¨</p> <p><br/></p> <p>ì´í›„ ì—°êµ¬ì¸ DR.GRPO (Liu et al., 2025)ì—ì„œëŠ” token efficiencyë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ <strong>GRPOì˜ objective functionì—ì„œ â€˜response lengthâ€™ termê³¼ Advantageì—ì„œ stdë¡œ normalizeí•´ì£¼ëŠ” termì„ ì§€ì›€</strong></p> <ul> <li> <p>DR.GRPO (Zichen Liu, et al 2025)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  GRPOì˜ ë‘ ê°€ì§€ í¸í–¥
</code></pre></div> </div> <ol> <li> <p><strong>Response-level length bias</strong></p> <ol> <li> <table> <tbody> <tr> <td>ê° responseì— ëŒ€í•´ í‰ê· ì„ êµ¬í• ë•Œ, $ 1/</td> <td>o_i</td> <td>$ê°€ ê³±í•´ì§€ëŠ”ë°,</td> </tr> </tbody> </table> <ol> <li> <p>Correct responseì¸ ê²½ìš° advantageê°€ ì–‘ìˆ˜ì´ë©´ shorter responseì— ëŒ€í•´ì„œ greater gradient updatesë¥¼ ì•¼ê¸°í•¨ â†’ ì´ PolicyëŠ” correct answerì— ëŒ€í•´ brevity favorìˆìŒ</p> </li> <li> <table> <tbody> <tr> <td>Incorrect responseì¸ ê²½ìš° advantageê°€ ìŒìˆ˜ longer responseëŠ” $</td> <td>o_i</td> <td>$ê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— penalizedë¥¼ ë” ë°›ìŒ â†’ ì´ PolicyëŠ” Incorrect answerì— ëŒ€í•´ ê¸¸ê²Œ ë§í•˜ëŠ” favor ìˆìŒ</td> </tr> </tbody> </table> </li> </ol> <table> <tbody> <tr> <td>â‡’ ì‰½ê²Œ ë§í•˜ë©´ GRPOì˜ ê°œë³„ advantageë¥¼ $ A_{i,t}/</td> <td>o_i</td> <td>$ë¡œ ë³´ë©´</td> </tr> </tbody> </table> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> - positive advantage ì— ëŒ€í•´ì„œëŠ” ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ ì‘ì„ìˆ˜ë¡ updateê°€ ì»¤ì§€ê¸°ì— ì§§ì€ responseì— ê°•í•œ signalì„ ì£¼ê³ 

 - negative advantage ì— ëŒ€í•´ì„œ ì—­ì‹œ ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ í´ìˆ˜ë¡ updateê°€ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ê¸´ responseì— ê°•í•œ signalì„ ì¤Œ (=ê¸´ ì˜¤ë‹µì€ under-penalized)

 &lt;span style='color:orange_background'&gt;**â†’ ì¦‰, GRPOëŠ” ì •ë‹µì€ ì§§ê²Œ, ì˜¤ë‹µì€ ê¸¸ê²Œ ë§í•˜ê²Œë” biased policyë¥¼ ìœ ë„í•¨**&lt;/span&gt;
</code></pre></div> </div> </li> </ol> </li> <li> <p><strong>Question-level difficulty bias</strong></p> <ol> <li> <p>advantageë¥¼ íŠ¹ì • question ë‚´ì˜ group averageì™€ stdë¡œ normalizationì„ í•˜ê¸° ë•Œë¬¸ì—, stdê°€ ì‘ìœ¼ë©´ ìƒëŒ€ì ìœ¼ë¡œ í•´ë‹¹ questionì— ëŒ€í•œ training signal (weight update)ê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§</p> <p>â†’ ì¼ë°˜ì ì¸ RLì—ì„œëŠ” batch ë‹¨ìœ„ë¡œ normalizationë˜ì–´ biasë¥¼ ìƒì‡„ì‹œí‚¤ì§€ë§Œ, GRPOëŠ” question ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë˜ì–´ ê·¸ë ‡ì§€ ëª»í•¨</p> </li> </ol> <p>â‡’ ìœ„ì™€ ê°™ì€ ë¬¸ì œëŠ” LMì˜ responseê°€ ê¸¸ì–´ì§€ëŠ” ì´ìœ ê°€ reasoning capability ë•Œë¬¸ì¸ì§€ ì•„ë‹ˆë©´ bias ë•Œë¬¸ì¸ì§€ êµ¬ë¶„í•˜ê¸°ê°€ ì–´ë ¤ì›Œì§. ì´ì— ë”°ë¼ <span style="color:orange_background"><strong>unbiased optimization methodì¸ DR.GRPO ì†Œê°œí•¨</strong></span></p> </li> </ol> <p><strong>Question1</strong>: ìœ„ì—ì„œ ë¶„ëª… Correct responseì— ëŒ€í•´ì„œëŠ” ì§§ì•„ì§€ëŠ”ë° ì™œ responseê°€ ê¸¸ì–´ì§„ë‹¤ê³  í‘œí˜„í•˜ëŠ”ì§€?</p> <p>â‡’ (ê±´í”¼ì…œ) Correct response ê°œìˆ˜ë³´ë‹¤ Incorrect responseì˜ ìˆ˜ê°€ ë” ë§ê¸°ì—, ëŒ€ë¶€ë¶„ì˜ responseëŠ” RL trainingì—ì„œ ì˜¤ë‹µì´ë¼ ê¸¸ê²Œ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìƒê¹€</p> <p><strong>Question2</strong>: GRPOë¥¼ ë³´ë©´ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ reasoning accuracyê°€ ì˜¬ë¼ê°€ëŠ”ë°, ê·¸ëŸ¬ë©´ biasì— ë”°ë¼ response lengthê°€ ì§§ì•„ì ¸ì•¼ í•˜ëŠ”ê±° ì•„ë‹Œê°€?</p> <p>â‡’ (ê±´í”¼ì…œ) complex tasksì—ì„œëŠ” ì•„ë¬´ë¦¬ ì˜¤ë˜ í•™ìŠµì‹œì¼œë„ ë†’ì€ accì— ë„ë‹¬í•˜ëŠ” ëª¨ë¸ì´ ì—†ì–´ì„œ ê·¸ë ‡ì§€ ì•Šì„ê¹Œ..?</p> <p><br/></p> <p>DR.GRPOëŠ” GRPOì˜ optimization biasë¥¼ ì—†ì• ê¸° ìœ„í•´ ì•„ë˜ ë‘ê°€ì§€ termsì„ ì—†ì•°</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  R1-Zeroì™€ ë¹„ìŠ·í•œ minimal recipeë¡œ í•™ìŠµí•œ Oat-Zero-7B ì„±ëŠ¥
</code></pre></div> </div> <ul> <li> <p>minimal recipe</p> <ul> <li> <p>base model: Qwen2.5-Math-7B</p> </li> <li> <p>training dataset: MATH datasetì˜ level3~5</p> </li> <li> <p>GPU: 8xA100 27hrs</p> </li> </ul> <p>ì•„ë˜ (ìš°ì¸¡) ê·¸ë¦¼ì„ ë³´ë©´</p> </li> <li> <p>(solid lines) DR.GRPOëŠ” reasoning accuracyê°€ ì§€ì†ì ìœ¼ë¡œ ì˜¤ë¥´ëŠ” ë°˜ë©´ì—, GRPOëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ (ë¶ˆì•ˆì •í•¨)</p> </li> <li> <p>(dashed lines) DR.GRPOëŠ” response lengthê°€ ì§§ê³  ì•ˆì •ì ì¸ ë°˜ë©´ì—, GRPOëŠ” response lengthê°€ ê³„ì† ê¸¸ì–´ì§</p> </li> </ul> <p><br/></p> </li> </ul> <p><br/></p> <h3 id="diversity-quality-inconsistency">Diversity-Quality Inconsistency</h3> <p>GRPOì™€ DR.GRPOì˜ reward signalì€ <strong>solution-level correctness</strong>ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° completionì— ëŒ€í•´ <strong>sparse scalar judgement</strong>ë¥¼ ê³„ì‚°í•¨.</p> <p>â†’ ì´ëŸ¬í•œ scalar rewardëŠ” ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ëŠ” diverse reasoning-pathë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, Diversity-Quality Inconsistencyê°€ ë°œìƒí•¨.</p> <p>ìœ„ì— Example ë§ê³ , ë³´ë‹¤ ì‹¤ì¦ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ statement (â€<strong><em>reward alone fails to reflect the underlying variability in reasoning strategies</em></strong>â€) ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ embedding distancesë¡œ ì¸¡ì •ëœ completionsì˜ structural dissimilarityë¥¼ ê³„ì‚°í•¨.</p> <ul> <li> <p>Spearmanâ€™s rank correlationì„ ì‚¬ìš©í•˜ì—¬ sampled completions ì‚¬ì´ì—ì„œ reward differenceì™€ semantic distanceë¥¼ ì¸¡ì •í•¨ â†’semantic distanceê°€ ì»¤ì§ˆìˆ˜ë¡ reward ì°¨ì´ë„ ì»¤ì§€ëŠ”ê°€?</p> <ul> <li>3000ê°œ prompt ë½‘ì•„ì„œ p-value ì¸¡ì •</li> </ul> </li> <li> <p>Figure2ëŠ” Spearmanâ€™s rank correlationì˜ p-valuesì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ p-valueê°€ significance levelì¸ 0.05 ë³´ë‹¤ í° ê°’ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œë¡œ 80% ì´ìƒì˜ promptì— ëŒ€í•´ statistically significant correlationì´ ì—†ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ</p> <p>â†’ ì¦‰, rewardê°€ semantic diversityì™€ ìƒê´€ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ</p> <p><br/></p> <p>$ \tilde{a} \text{ì´ë ‡ê²Œ} $</p> </li> </ul> <h3 id="diversity-aware-reward-adjustment">Diversity-aware Reward Adjustment</h3> <p>Diversity-Quality Inconsistency ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° sampleì˜ relative diversity/redundancyì— ë”°ë¼ rewardë¥¼ reweightí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨.</p> <p><span style="color:orange_background"><strong>â†’ diverse completionsì€ ë” ë†’ì€ weight, ì¤‘ë³µëœ responseëŠ” ë‚®ì€ weight</strong></span></p> <p>ë¨¼ì € ê¸°ì¡´ì˜ reward $ R(q,o_i) $ë¥¼ diversity-aware adjusted reward $ \tilde{R}(q,o_i) $ <span style="color:red">(í‹¸ë‹¤ í‘œì‹œ ì–´ë–»ê²Œ í•˜ë‚˜ìš”â€¦) </span>ìœ¼ë¡œ ëŒ€ì²´í•¨</p> <ul> <li> <p>SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” completion $ o_i $ì™€ ë‚˜ë¨¸ì§€ group $ C $ \ $ o_i $ ê°„ì˜ Submodular Mutual Informationì„ ë‚˜íƒ€ëƒ„</p> </li> <li> <p>Submodular functionsì€ diminishing returns íŠ¹ì„±ì„ ê°–ìœ¼ë©°, diversityì™€ redundancyë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ</p> <ul> <li><strong><em>diminishing returns</em></strong><em>: ì–´ë–¤ ì§‘í•©ì— ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ë” ì¶”ê°€í•  ë•Œ, ì´ë¯¸ ë¹„ìŠ·í•œ ìš”ì†Œê°€ ë§ì„ ìˆ˜ë¡ ê·¸ ìš”ì†Œê°€ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê°€ì¹˜(ì •ë³´ ê¸°ì—¬ë„)ëŠ” ì¤„ì–´ë“œëŠ” ì„±ì§ˆ</em></li> </ul> </li> <li> <p>SMIëŠ” ë‘ ì§‘í•© ê°„ì˜ shared informationì„ ì •ëŸ‰í™”í•˜ë©° (Iyer et al., 2021a,b)ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•¨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - $ s(o_i,j)=s(j,o_i) $ë¼ê³  ê°€ì •í•˜ë©°, SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” $ o_i $ì™€ ë‚˜ë¨¸ì§€ elementsê°„ì˜ total semmetric simialrityë¥¼ ê³„ì‚°í•¨
</code></pre></div> </div> <ul> <li> <p>ì¦‰, $ o_i $ê°€ ë‚˜ë¨¸ì§€ completionsê³¼ ìœ ì‚¬í• ìˆ˜ë¡ SMIê°€ ì»¤ì ¸ reward ê°’ì´ ë‚®ê²Œ reweightë˜ê³ , $ o_i $ê°€ ë‹¤ë¥¼ìˆ˜ë¡ SMIê°€ ì‘ì•„ì ¸ reward ê°’ì´ í¬ê²Œ reweight ë¨.</p> </li> <li> <p>ê° completionì˜ embeddingì€ small LMìœ¼ë¡œ ì–»ê³ , $ s() $ëŠ” cosine similarityë¥¼ ì‚¬ìš©í•¨</p> </li> </ul> </li> </ul> <p><br/></p> <ul> <li> <p>SMIë¥¼ ì‰½ê²Œ ë§í•˜ë©´ â€œ<span style="color:orange_background"><strong>íŠ¹ì • completion í•˜ë‚˜ê°€ group ë‚´ ë‹¤ë¥¸ completionê³¼ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€</strong></span>â€ë¥¼ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê°’</p> <ul> <li> <p><strong>ì¤‘ë³µì´ ë§ìœ¼ë©´ (high redundancy) â†’ SMIê°€ í¼ â†’ rewardê°€ ì‘ì•„ì§</strong></p> </li> <li> <p><strong>ì¤‘ë³µì´ ì ìœ¼ë©´ (high diversity) â†’ SMIê°€ ì‘ìŒ â†’ rewardê°€ ì»¤ì§</strong></p> </li> </ul> </li> <li> <p>Submodular í•¨ìˆ˜ëŠ” ìˆ˜í•™ ê°œë…ìœ¼ë¡œ â€œìƒˆë¡œìš´ elementê°€ ê¸°ì¡´ì— ë¹„ìŠ·í•œê²Œ ë§ì„ìˆ˜ë¡ ê¸°ì—¬ë„ê°€ ì¤„ì–´ë“œëŠ” ì„±ì§ˆâ€ì„ ê°–ê³  ìˆìŒ</p> <p>ex) ìœ ì‚¬í•œ completionsì´ 9ê°œê°€ ìˆëŠ” ìƒí™©ì—ì„œ, 10ë²ˆì§¸ ë¹„ìŠ·í•œ completionì€ informationì„ ë³„ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ. ë°˜ë©´, ì™„ì „íˆ ë‹¤ë¥¸ completionì´ ë“±ì¥í•˜ë©´ information ê¸°ì—¬ë„ê°€ ì»¤ì§</p> </li> </ul> <p><br/></p> <p>â†’ ì´ë ‡ê²Œ ìƒˆë¡œìš´ rewardë¥¼ êµ¬í•˜ëŠ” ì—°ì‚°ì€ Pytorchì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- $ \Sigma{L_{ij}} $ termì´ simiarlity matrix $ L $ì˜ $ i $th rowì˜ í•©
</code></pre></div></div> <ul> <li> <p>Pytorch Code for DAR</p> <p>## 3. Experiment</p> </li> </ul> <h3 id="31-experimental-setup">3.1 Experimental Setup</h3> <p>**Training Dataset: **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- s1 dataset + DeepScaleR dataset with mixed problem difficulties
</code></pre></div></div> <p>**Evaluation Dataset: **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- five mathematical reasoning benchmarks (AIME24, MATH-500, AMC23, Minerva, OlympiadBench)
</code></pre></div></div> <p><strong>Baselines</strong>:</p> <ul> <li> <p>general purpose large model: Llama-3.1-70B-Instruct, o1-preivew</p> </li> <li> <p>Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL</p> </li> <li> <p>Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS</p> </li> </ul> <p><strong>Implementations:</strong></p> <ul> <li> <p>ë³¸ ì—°êµ¬ëŠ” DRAì˜ proof-of-conceptë§Œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ëª©ì ì´ê¸°ì— DeepSeek-R1-Distill-Qwen-1.5Bë¥¼ base modelë¡œ ë‘ì–´ í•™ìŠµì‹œí‚´</p> </li> <li> <p>4 x A100 (40GB) GPUs</p> </li> </ul> <h3 id="32-empirical-analysis">3.2 Empirical Analysis</h3> <p><strong>Main Results</strong></p> <ul> <li> <p>DRA-DR.GRPOëŠ” avg accê°€ 58.2%ë¡œ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜´ (DRA-GRPOì—­ì‹œ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜´)</p> <ul> <li>AMC23ì—ì„œ íŠ¹íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ (85.0%)</li> </ul> </li> <li> <p>DRA-GRPOì™€ DRA-DR.GRPOëŠ” fine-tuning samplesì„ 7,000ê°œ ë°–ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  40,000ê°œ ì‚¬ìš©í•œ DeepScaleR-1.5B-previewë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë³´ì—¬ì¤Œ</p> <p>â†’ Low-resource settingsì—ì„œë„ íš¨ê³¼ì ì„</p> <ul> <li> <p>DeepScaleR-1.5B-preview</p> <p>ë‚˜ë¦„ GOATê¸‰ì˜ ì„±ëŠ¥ ë³´ì—¬ì£¼ëŠ” ì˜ë‚˜ê°€ëŠ” ëª¨ë¸</p> <p>(<a href="https://www.notion.so/19681902c1468005bed8ca303013a4e2]">https://www.notion.so/19681902c1468005bed8ca303013a4e2</a></p> <p><a href="https://github.com/giterinhub/DeepScaleR-1.5B-Preview">https://github.com/giterinhub/DeepScaleR-1.5B-Preview</a></p> <p><strong>Ablation Study</strong></p> </li> </ul> </li> <li> <p>Base modelì¸ DeepSeek-R1-Distill-Qwen-1.5Bì™€ ë¹„êµí•˜ì—¬ DRA-GRPO, DRA-DR.GRPOëŠ” ê°ê° 7.8%, 9.3% ì„±ëŠ¥ í–¥ìƒë˜ê³  ë‹¨ìˆœ RL (GRPO, DR.GRPO) ëŒ€ë¹„ 1.9%, 2.2% í–¥ìƒ</p> <p>â†’ ì´ê²Œ ì™œ Ablation studyë¼ ë§í•˜ëŠ”ê±°ì§€ã…‹ã…‹</p> </li> </ul> <p><strong>Efficiency</strong></p> <p>DRAëŠ” completionsì„ encoding í•´ì•¼í•˜ê¸°ì— over-headê°€ ì¡´ì¬í•˜ì§€ë§Œ, ë³„ë¡œ í¬ì§€ ì•ŠìŒ.</p> <p>â†’ ì €ìë“¤ì´ ì‹¤í—˜ì— ì‚¬ìš©í•œ GPUìŠ¤í™ì¸ (A100-40GB)ì—ì„œëŠ” ì–´ì°¨í”¼ DRA ì—†ì´ë„ mini-batchë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•´ì„œ DRA ì ìš©í•˜ëŠ” ê²ƒì´ ë³„ ë¬¸ì œê°€ ë˜ì§€ ì•Šë‹¤ê³  í•˜ëŠ”ë°â€¦. â†’ ğŸ¶Â ğŸ”ŠÂ ë¼ê³  ìƒê°í•©ë‹ˆë‹¤</p> <p><strong>Training Cost</strong></p> <p>500 steps í•™ìŠµì‹œì¼œ 12.5hr ì†Œìš”ë¨ â‡’ $55 ë¹„ìš©</p> <p>â†’ ë‹¤ë¥¸ ë°©ë²•ëŒ€ë¹„ íš¨ìœ¨ì ì„</p> <h3 id="33-discussion">3.3 Discussion</h3> <p><strong>Exploration-exploitation Balance</strong></p> <p>DRAëŠ” Exploration-exploitation balanceë¥¼ policy gradient ì•ˆì— ì§ì ‘ í†µí•©í•˜ì—¬ ì ìš©í•¨</p> <ul> <li> <p>Base rewardëŠ” high scoreë¥¼ ë°›ëŠ” completionì„ reinforceí•¨</p> <p>â†’ <strong>Exploitation ìœ ë„</strong></p> </li> <li> <p>Diversity weightingì€ semantically novel completionì— learning signalì„ amplify</p> <p>â†’ <strong>Exploration ìœ ë„</strong></p> </li> </ul> <p>ì´ëŸ¬í•œ íƒìƒ‰ì€ low-resource settings (promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” ì‘ë‹µ ìˆ˜ê°€ ì œí•œ ì ì¸ ê²½ìš°)ì—ì„œ ì¤‘ìš”í•¨</p> <p>â†’ DRAëŠ” mode collapseë¥¼ ë°©ì§€í•˜ê³  ë” ë„“ì€ reasoning strategiesë¥¼ ìœ ë„í•¨</p> <p><strong>Ad-hoc vs Post-hoc Diversity</strong></p> <p>generated completionsê°„ì˜ diversityë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ Ad-hoc, Post-hoc ë°©ì‹ì´ ìˆìŒ</p> <ol> <li> <p><strong>Ad-hoc</strong></p> <ol> <li> <p>generation ë‹¨ê³„ì—ì„œ ë‹¤ì–‘ì„±ì„ ìœ ë„í•¨ (temperature ì¡°ì ˆ, decoding ì„¤ì • ë³€ê²½)</p> </li> <li> <p>ì´ë ‡ê²Œ í•˜ë©´ ê° completionì´ ë…ë¦½ì ìœ¼ë¡œ samplingë˜ì–´ â†’ ì‘ë‹µ ê°„ correlationì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ì—†ìŒ (completionì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ëª…ì‹œì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìŒ)</p> </li> </ol> </li> <li> <p><strong>Post-hoc (ë³¸ ì—°êµ¬ì—ì„œ ì±„íƒí•œ ë°©ë²•)</strong></p> <ol> <li> <p>diversityë¥¼ reward signalì— ë°”ë¡œ í†µí•©</p> </li> <li> <p>Semantic redundancyë¥¼ í‰ê°€í•˜ì—¬ policyê°€ íš¨ìœ¨ì ìœ¼ë¡œ learningì„ ì¡°ì •í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ</p> </li> </ol> </li> </ol> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li> <p>GRPO í˜•ì‹ì˜ RLì—ì„œ completions ê°„ì˜ semantic diversityë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” DRA ì•Œê³ ë¦¬ì¦˜ ì œì•ˆí•¨</p> <p>â†’ ê¸°ì¡´ scalar rewardì˜ ë¬¸ì œì¸ exploration-exploitation imbalance issueë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•¨</p> </li> <li> <p>ë‘ê°€ì§€ í•œê³„ì ì´ ìˆìŒ</p> <ol> <li> <p>small-scale models (1.5B) / small group sizes (e.g 6 completions per prompt)</p> </li> <li> <p>diversityë¥¼ ì¸¡ì •í•  ë•Œ ì‚¬ìš©ëœ sentence embeddingsì€ ì™¸ë¶€ modelì— ì˜ì¡´í•˜ëŠ” êµ¬ì¡°</p> </li> </ol> </li> <li> <p>ì´ëŸ° ìª½ë„ ì¬ë°Œë‹¤!ã…‹ã…‹</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[ë…¼ë¬¸ ë¦¬ë·° - DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models]]></summary></entry></feed>