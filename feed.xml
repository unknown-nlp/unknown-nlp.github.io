<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://unknownnlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://unknownnlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-10T13:06:31+00:00</updated><id>https://unknownnlp.github.io/feed.xml</id><title type="html">Unknown NLP Papers</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS</title><link href="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/" rel="alternate" type="text/html" title="BLOCK DIFFUSION: INTERPOLATING BETWEEN AUTOREGRESSIVE AND DIFFUSION LANGUAGE MODELS"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/block-diffusion-interpolating-between-autoregressive-and-diffusion-language/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: DiffusionLM, LLM</li> </ul> <h2 id="introduction">Introduction</h2> <p><strong>Diffusion 모델이 가진 한계점</strong></p> <ol> <li> <p>현재 대부분의 diffusion 모델의 경우 <strong>고정된 길이의 답변</strong>만을 생성.</p> </li> <li> <p>Bidirectional context를 사용하기 때문에 <strong>KV 캐시와 같이 AR 추론에서 효율적인 방법들을 사용할 수 없음</strong>.</p> </li> <li> <p>Standard metric (e.g. perplexity)에서 <strong>여전히 낮은 성능</strong>을 보임.</p> </li> </ol> <p>→ <strong>Block Discrete Denoising Diffusion Language Models (BD3-LMs)</strong></p> <p><strong>BD3-LMs</strong></p> <p>Interpolation between discrete diffusion and autoregressive model</p> <ul> <li> <p>Block diffusion model: semi-autoregressive model</p> </li> <li> <p><strong>Inter-Block</strong>: Block을 생성하는 과정은 AR 과정으로 모델링</p> </li> <li> <p><strong>Intra-Block</strong>: 이전 block이 주어질 경우, 현재 block 내부는 discrete diffusion 과정으로 모델링</p> </li> </ul> <p><strong>Block Diffusion 모델이 가진 Two challenges를 발견: 핵심!!</strong></p> <ul> <li> <p>Block diffusion을 학습하기 위해서 두 번의 forward pass가 필요함. → 계산량 증가</p> </li> <li> <p>높은 gradient variance로 인한 성능 저하</p> </li> </ul> <p>→ 지금은 이해가 어려우니 뒤에서 확인</p> <p><strong>Contribution</strong></p> <ul> <li> <p>Block discrete diffusion language model 제안. <strong>기존 diffusion 모델과 달리 variable-length generation과 KV caching을 지원</strong></p> </li> <li> <p>학습 시 토큰 배치를 효율적으로 활용할 수 있도록 block diffusion model을 위한 <strong>훈련 알고리즘 제안</strong> (Challenge 1)</p> </li> <li> <p>Gradient variance가 diffusion 모델 성능의 제한 요소임을 밝힘 + 데이터 기반 <strong>노이즈 스케줄</strong>로 해결 (Challenge 2)</p> </li> <li> <p><strong>성능 향상</strong>!</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="background-language-modeling-paradigms">BACKGROUND: LANGUAGE MODELING PARADIGMS</h2> <h3 id="notation"><strong>notation</strong></h3> <ul> <li> <p>scalar discrete random variables with V categories as ‘one-hot’ column</p> </li> <li> <p>\Delta^V: simplex 공간</p> </li> <li> <p>m \in \mathcal{V}: [MASK] token’s one-hot vector</p> </li> </ul> <h3 id="autoregressive-models"><strong>AUTOREGRESSIVE MODELS</strong></h3> <h3 id="discrete-denoising-diffusion-probabilistic-models-d3pm"><strong>DISCRETE DENOISING DIFFUSION PROBABILISTIC MODELS (D3PM)</strong></h3> <ul> <li> <p>p: denoising, q: noising</p> </li> <li> <p>s(j) = (j-1)/T, t(j) = j/T (이후에 j는 생략!)</p> </li> <li> <p>D3PM framework: q를 Markov forward process, 각각의 토큰에 대해 독립적으로 아래의 식을 진행</p> </li> <li> <p>이상적인 diffusion model p_{\theta}는 q의 역방향이므로 D3PM에서는 아래 수식으로 p_{\theta}를 정의</p> </li> <li> <p><strong>Negative ELBO (NELBO)를 이용해 학습</strong></p> </li> <li> <p>1, 2항: noise, denoise 과정에서의 샘플의 일치 정도</p> </li> <li> <p>3항 얼마나 noise를 잘 만들었는가</p> </li> </ul> <h2 id="block-diffusion-language-modeling">BLOCK DIFFUSION LANGUAGE MODELING</h2> <h3 id="block-diffusion-distributions-and-model-architectures">BLOCK DIFFUSION DISTRIBUTIONS AND MODEL ARCHITECTURES</h3> <p><strong>Block Definition</strong></p> <ul> <li> <p>길이 L’이 되게 B개의 block으로 만들기 (x^b: x^{(b-1)L’:bL’} \in {1,…,B})</p> </li> <li> <p>Likelihood over block</p> </li> </ul> <p>block 내에서 reverse diffusion 프로세스 적용</p> <ul> <li>block이 constraint인 것을 제외하면 preliminaries의 수식과 동일!</li> </ul> <p><strong>Learning Objective</strong></p> <p>NELBO를 적용해 위와 같이 학습 목적함수 정의, 이것도 Sum을 제외하곤 전부 같음!</p> <p><strong>Denoiser model</strong></p> <ul> <li> <table> <tbody> <tr> <td>Transformer x_\theta를 사용해 파라미터화: p_\theta(x^b</td> <td>x_t^b, x^{&lt;b})</td> </tr> </tbody> </table> </li> <li> <p>Block들에 대해 병렬적 학습을 가능하게 함 (block-causal attention mask)</p> </li> <li>x_\theta의 학습: block b 내에서 x_\theta^b(x_t^b, x^{&lt;b}) → L’ 길이의 결과 예측</li> </ul> <p>→ 아래 K, V 캐시 수식을 보시면 모델을 이해하기 쉬움!</p> <p><strong>K, V caching</strong></p> <ul> <li>recomputing을 막기 위한 block 단위 caching</li> </ul> <h3 id="efficient-training-and-sampling-algorithms">EFFICIENT TRAINING AND SAMPLING ALGORITHMS</h3> <p><strong>Training</strong></p> <ul> <li>모든 block은 x_\theta의 forward pass를 두 번 거쳐야 함 (x_t^b, x^b) → 계산의 효율화 필요</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ol> <li> <p>Block 별로 noise level sampling</p> </li> <li> <p>각 block에 대해 noisy input x_{t_b}^b 생성</p> </li> <li> <p>\left(\emptyset, \mathbf{K}^{1: B}, \mathbf{V}^{1: B}\right) \leftarrow \mathbf{x}_\theta(\mathbf{x}): 원본 x를 이용해 K, V cache 미리 다 계산하기</p> </li> <li> <p>모든 b에 대해 x^b_{\text{logit}} 계산</p> </li> </ol> <p><strong>Sampling</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Block 단위의 순차적 샘플링, K, V 캐싱 가능 ← AR의 장점</p> </li> <li> <p>arbitrary length 생성 가능 ← AR의 장점</p> </li> <li> <p>block 내부에선 Parallel하게 생성 가능 ← Diffusion의 장점</p> </li> </ul> <h2 id="understanding-likelihood-gaps-between-diffusion--ar-models">UNDERSTANDING LIKELIHOOD GAPS BETWEEN DIFFUSION &amp; AR MODELS</h2> <h3 id="masked-bd3-lms">MASKED BD3-LMS</h3> <ul> <li> <p>최근 가장 큰 효과를 보이고 있는 masking noise process를 적용</p> </li> <li> <p>Per-token noise process</p> </li> <li> <p>목적 함수 (Sahoo et al. (2024b)의 SUBS-parameterization denoising 모델 철학을 따름!!)</p> </li> </ul> <h3 id="case-study-single-token-generation">CASE STUDY: SINGLE TOKEN GENERATION</h3> <ul> <li> <p>L^\prime = 1인 경우, MASKED BD3-LMS의 목적함수는 autoregressive NLL과 동등함.</p> </li> <li> <p>학습 목표의 기대값이 같음에도 불구하고 perplexity gap (=높은 학습 variance)가 존재함을 확인</p> </li> <li> <table> <tbody> <tr> <td>왜 그럴까? \mathbb{E}_{t\sim\mathcal{U}[0,1]}q(x_t^\ell=m</td> <td>x^\ell) = 0.5 기본적으로 학습에 사용하는 token의 수가 절반으로 줄기 때문에 variance가 커지는 것</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-05-block-diffusion-interpolating-between-autoregressive-and-diffusion-language/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <table> <tbody> <tr> <td>tuned schedule: q(x_t^\ell = m</td> <td>x^\ell) = 1</td> </tr> </tbody> </table> </li> </ul> <h3 id="diffusion-gap-from-high-variance-training">DIFFUSION GAP FROM HIGH VARIANCE TRAINING</h3> <ul> <li> <p>Case study를 넘어 L^\ell \geq 1인 케이스로 확장하고 싶음!</p> </li> <li> <p>Batch size를 K라고 할 때, batch of sequence \text{X} = [x^{(1)},x^{(1)},…,x^{(K)}], with each \text{x}^{(k)} \overset{\text{iid}}{\sim} q(x)</p> </li> <li> <p><strong>NELBO estimator</strong></p> </li> <li> <p><strong>Variance of the gradient estimator</strong></p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="diffusion"/><category term="diffusionlm"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="transformer"/><summary type="html"><![CDATA[논문 리뷰 - DiffusionLM, LLM 관련 연구]]></summary></entry><entry><title type="html">Impact of Fine-Tuning Methods on Memorization in Large Language Models</title><link href="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/" rel="alternate" type="text/html" title="Impact of Fine-Tuning Methods on Memorization in Large Language Models"/><published>2025-08-05T00:00:00+00:00</published><updated>2025-08-05T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/impact-of-fine-tuning-methods-on-memorization-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-08-05</li> <li><strong>Reviewer</strong>: hyowon Cho</li> </ul> <p>많은 연구들이 LLM이 사전학습 단계에서 학습 데이터를 외우는 이슈에 대해서 보고하고 있는 한편, finetuning에 대해서 비슷한 연구는 놀라울 정도로 적다.</p> <p>하지만, finetuning도 당연히 모델 대부의 업데이트와 때때로는 구조적인 변화까지도 이루어지기 때문에, finetuning의 memorization level에 대한 연구도 필요하다.</p> <p>그렇다면, 존재하는 다양한 finetuning 방법에 따른 memorization of fineuning data의 영향력은 어떻게 되는가?</p> <p>해당 연구는 이를 시험하기 위해 우선 finetuning 방법을 크게 두 가지로 구분한다:</p> <ol> <li> <p>Parameter-based finetuning: 모델 파라 바꿈</p> </li> <li> <p>Prompt-based fine-tuning: 모델 파라 고정, soft token/prefix embedding…</p> </li> </ol> <p>결과적으로 두 카테고리를 고루 포함한 5가지 방법을 시험했고,</p> <p>평가는 다양한 MIAs(membership inference attacks )로 했고,</p> <p>데이터는 Wikitext, WebNLG, Xsum 세 가지로 했다 (좀 적긴하네요)</p> <p>간단하고 빠르게 다음으로 넘어갑시다</p> <h1 id="fine-tuning-methods">Fine-Tuning Methods</h1> <ul> <li> <p>Parameter-based fine-tuning</p> </li> <li> <p>Prompt-based fine-tuning: task-specific prompts only</p> </li> </ul> <h1 id="memorization-and-mias">Memorization and MIAs</h1> <ul> <li>사용된 MIA 기법과 점수 계산 방식:</li> </ul> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>데이터</p> </li> <li> <p>평가</p> </li> <li> <p>모델</p> </li> <li> <p>Evaluation Metrics</p> </li> <li> <p>Implementation Details</p> </li> </ul> <h1 id="results-and-observations">Results and Observations</h1> <h2 id="memorization-across-tuning-methods">Memorization across Tuning Methods</h2> <blockquote> <p>Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?</p> </blockquote> <blockquote> <p>Observation ♯1: (당연)</p> </blockquote> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>모든 방법론은 validation PPL기준으로 성능 좋았음.</p> <p>하지만, prompt-based methods 는 parameter-based 보다 외우는 성능 떨어짐 (당연)</p> <blockquote> <p>Observation ♯2:</p> </blockquote> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="why-prompt-based-fine-tuning-exhibits-low-memorization">Why Prompt-Based Fine-Tuning Exhibits Low Memorization</h2> <p>prompt-based fine-tuning introduces a bias into the model’s attention mechanism indirectly via the soft prompt or prefix, rather than altering the attention mechanism itself.</p> <ul> <li> <p><strong>Prefix Tuning 수식 (Petrov et al., 2024)</strong></p> </li> <li> <p>결과적으로 <strong>표현 공간의 이동(shift) &lt; 적음</strong> → 학습, 비학습 샘플 분포 차이가 작아 MIA가 어렵다.</p> </li> </ul> <p>이 가설을 확인하기 위해:</p> <p>distributions of non-membership and membership examples on the LLaMA2-7B를 세 세팅에서 비교함:</p> <ol> <li> <p>pre-trained model,</p> </li> <li> <p>fine-tuned with LoRA</p> </li> <li> <p>fine-tuned with prefix tuning</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>LoRA는 membership and non-membership samples 사이 분포 차이가 큰데, prefix tuning은 미미하다는 것을 알 수 있음</p> <h2 id="performance-in-different-tuning-paradigms">Performance in Different Tuning Paradigms</h2> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>두 방법론이 최종적으로는 비슷한 PPL을 가졌음에도 불구하고, Learning trajactories는 꽤나 달랐음</p> <p>parameterbased fine-tuning:</p> <ul> <li> <p>decreases over the first few epochs</p> </li> <li> <p>later increases due to overfitting, before eventually converging</p> </li> </ul> <p>prompt-based fine-tuning:</p> <ul> <li> <p>slightly decreasing validation PPL throughout training,</p> </li> <li> <p>converging without the overfitting-induced rise</p> </li> </ul> <p>이는 아까도 이야기 했듯이, 후자가 internal sample distribution of the model을 바꾸는 것이 아니라 단순히 다운스트림 태스크에 쪼끔 더 나은 bias를 추가하는 정도임을 다시한번 보인다</p> <h1 id="discussion">Discussion</h1> <h2 id="regarding-model-scale">Regarding Model Scale</h2> <p>모델 사이즈가 memorization에 중요한 영향력을 줄 것임.</p> <p>→ To what extent does model size influence memorization under different fine-tuning strategies?</p> <blockquote> <p>Observation ♯3</p> </blockquote> <p>four variants of the GPT-2 architecture:</p> <ul> <li> <p>GPT-2 (124M),</p> </li> <li> <p>GPT-2 Medium (345M),</p> </li> <li> <p>GPT2 Large (762M),</p> </li> <li> <p>GPT-2 XL (1.5B).</p> </li> </ul> <p>LLaMA2-7B vs LLaMA3-1B</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>요약: 파라미터 바꾸는 애들은 모델 크기 커질수록 더 잘 외웠는데 반대는 미미하더라 (low sensitivity of prompt tuning to model scale)</p> <p>특히, gpt2의 경우나 1B 스케일에서 LoRA는 사실상 거의 못외움</p> <h2 id="impact-of-downstream-tasks">Impact of Downstream Tasks</h2> <blockquote> <p>Observation ♯4 Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.</p> </blockquote> <p>다운스트림 태스크의 종류에 따라서도 다를 수 있음. 이를 위 LLaMA2-7B를 다양한 방법을 통해 학습시키고 LOSS attack against에 대해서 각각을 평가해봄</p> <figure> <picture> <img src="/assets/img/posts/2025-08-05-impact-of-fine-tuning-methods-on-memorization-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prompt-based 만 봤을 때, WebNLG가 다른 것들에 비해서 성능이 높다</p> <p>아마도 구조화된 pattern학습에는 유리한 것 같다</p>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="embedding"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/" rel="alternate" type="text/html" title="Reasoning Model is Stubborn: Diagnosing Instruction Overriding in Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근 LLM이 복잡한 reasoning task에서 괄목할만한 성능을 보이고 있으나, (모델에게) 편한 reasoning pattern에 의존하는 경향이 있음</p> </li> <li> <p>사용자의 명시적인 instruction이 있음에도 불구하고, 습관적인 reasoning trajectory를 생성, 오답으로 귀결</p> </li> <li> <p>이를 분석하기 위해 reasoning trap이라는 진단 데이터셋을 도입</p> </li> <li> <p>reasoning trap을 통해서 모덷이 습관적으로 사용하는 reasoning pattern을 발견, 분류</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM은 수학, 복잡한 코딩 문제, 퍼즐 풀이를 포함한 여러 어려운 태스크에서 주목할만한 성능을 보임</p> </li> <li> <p>하지만, 이러한 모델들에게 문제 행동, reasoning rigidity가 발견됨</p> </li> <li> <p>reasoning rigidity는 cognitive bias를 반영, 주어진 조건을 이해해도 자기 방식대로 override, 무시하고 문제를 푸는 현상을 뜻함</p> </li> <li> <p>reasoning rigidity는 사용자가 서술한 제약이 중요한 도메인에서 큰 문제가 됨</p> </li> <li> <p>reasoning rigidity를 식별할 수 있도록, 기존의 수학, 퍼즐 데이터셋을 활용해 reasoningtrap이라는 벤치마크를 제안</p> </li> <li> <p>ReasoningTrap으로 여러 모델을 평가한 결과, 여러 중요한 현상들을 발견</p> </li> <li> <p>또한, 이러한 contamination의 패턴을 3가지로 분류</p> </li> </ul> <h2 id="related-works">Related Works</h2> <ul> <li> <p>Large Reasoning Models</p> </li> <li> <p>Instruction following of reasoning models</p> </li> <li> <p>Rigidity in reasoning models</p> </li> <li> <p>Underlying reason for rigidity</p> </li> </ul> <h2 id="reasoningtrap-reasoning-rigidity-diagnostic-set">ReasoningTrap: Reasoning Rigidity Diagnostic Set</h2> <h3 id="data-structure">Data structure</h3> <ul> <li> <p>크게 2가지로 도메인으로 구성 : 수학(ConditionedMath), 퍼즐(PuzzleTrivial)</p> </li> <li> <p>각 데이터는 원래 Q-R-A tuple (q_orig, r_orig, a_orig)과 수정된 tuple (q_mod, r_mod, a_mod)로 구성</p> </li> <li> <p>총 164개의 데이터셋, 84개는 수학, 80개는 퍼즐</p> </li> <li> <p>ConditionedMath에 있는 모든 질문은 개념적으로 다르고, 겹치지 않고, human annotator에 의해 엄격하게 검증됨</p> </li> <li> <p>PuzzleTrival은 10개의 puzzle concept를 가짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>ConditionedMath: popular math benchmark with addtional conditions</strong></p> <ul> <li> <p>AIME 2022-24 , MATH500 level 5를 활용해서 제작</p> </li> <li> <p>원래 질문을 수정하고, 수정된 질문이 아래 조건에 부합하는지를 확인, 필터링</p> </li> <li> <p>220개의 원본 데이터를 5가지의 variant로 modified, 필터링 후에 최종 84개만 남음</p> </li> </ul> <p><strong>PuzzleTrivial: Puzzles with subtle Modifications to Trivial Solutions</strong></p> <ul> <li> <p>classic puzzle은 조건을 수정하면 급격하게 단순해지거나 답이 여러개일 수 있음</p> </li> <li> <p>ambiguity를 줄이기 위해, “valid solution을 위해 가장 간단한 답을 찾아라”라는 문구를 instruction에 추가</p> </li> <li> <p>과정 자체는 위와 동일</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="contamination-ratio-and-early-detection-algorithm">Contamination Ratio and Early Detection Algorithm</h2> <ul> <li>시스템적으로 reasoning model의 contamination을 측정하기 위해서, Contamination ratio를 제안</li> </ul> <p><strong>Contamination Ratio in Synthetic Dataset</strong></p> <ul> <li> <p>모델이 문제를 풀 때, 수정된 조건을 이해하고 풀었는지 이해하지 않고 풀었는지를 구분하기 위해 metric을 도입</p> </li> <li> <p>생성된 reasoning path를 단락별로 쪼개고, 각 단락을 textual representation으로 embedding</p> </li> <li> <p>각 단락과 오리지널 문제의 reasoning path, 각 단락과 modified reasoning path와의 cosine 유사도를 계산, 둘을 비교해 original reasoning path와의 유사도가 더 높을 경우 1로 계산</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Evaluation of Reasoning Rigidity</strong></p> <ul> <li> <p>reasoning rigidity를 잘 관찰하기 위해, 모델이 수정된 조건을 이해했는데도 습관처럼 풀었는지 아니면 인지조차 하지 못했는지를 구분</p> </li> <li> <p>이를 반영한 metric을 p-passs@k라고 정의, reasosning path에서 constraint를 인지하고 있는 경우에만 accuracy를 측정</p> </li> <li> <p>constraint를 인지했는지는 모델이 생성한 reasoning path중 첫 15개의 단락과 정답, 질문을 LLM에 넣고 판단하도록 함(p_i)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Signals for Contamination in Realistic Situation</p> <ul> <li> <p>question만 주어지는 현실적인 상황에서, generated reasosning이 원치 않지만 친숙한 pattern으로 contaminated됐는지 자동적으로 식별하는 것을 불가능</p> </li> <li> <p>그래서 간단하게, contamination의 종류를 분류해서, 각 type별 의심스러운 pattern을 식별</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Interpretation overload : 모델이 주어진 문제 조건을 거절하는 것으로 시작, 문제를 바로 해석하는 것보다 여러 방식으로 재해석. 보통 reasoning 중간 단계에서 발생, inconsistent 혹은 contraditory한 결론을 야기</p> </li> <li> <p>Input Distrust : 모델이 번역 오류, input error, typo존재 등을 가정함. 직관적으로 바로 문제를 풀 수 있음에도 부정하고 매우 복잡하게 풀게 됨.</p> </li> <li> <p>Partial Instruction Attention : 모델이 제공된 instruction의 일부분만을 선택적으로 집중</p> </li> </ul> <h2 id="experiments">Experiments</h2> <ul> <li> <p>ReasoningTrap을 여러 LLM에 inference</p> </li> <li> <p>실험은 CoT prompting을 사용, ‘Please reason step by step, and put your final answer within \boxed{}.\n\n{Question}’ 포맷으로 질문을 전달</p> </li> <li> <p>table 2,3은 16번 sampling, 다른 실험은 4번 sampling</p> </li> <li> <p>수학 문제의 경우, exat matching으로 correctness 판단, puzzle의 경우 free-from sentence로 답이 구성되다 보니, LLM을 사용해서 정답과 모델 답변을 함께 제공해 correctness를 판단</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>실험 결과, 대부분 reason모드일 때보다 base모드에서 더 높은 성능을 보임</p> </li> <li> <p>Buget forcing : 버짓 마지막에 ‘Considering the limited time by the user, I have to give the solution based on the thinking directly now.&lt;/think&gt;’를 추가하여 답을 바로 내도록 함</p> </li> <li> <p>prompt hinting : 문제에 오타 없고 지시 그대로 하라는 prompt를 추가</p> </li> <li> <p>실험 결과, budget이 커질 수록 성능이 악화됨</p> </li> <li> <p>prompt로 hint를 줘도 여전히 reasoning rigidity가 존재</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-reasoning-model-is-stubborn-diagnosing-instruction-overriding-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>모델 크기에 따른 실험. base모델이 성능이 전반적으로 높게 나오는 편</li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="detection"/><category term="embedding"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models</title><link href="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/" rel="alternate" type="text/html" title="Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/scaling-reasoning-losing-control-evaluating-instruction-following-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 전민진</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>최근의 reasoning oriented model(LRM)은 여러 수학 데이터셋에서 높은 성능 달성을 보이나, natural instruction following에 대한 성능은 분석되지 않음</p> </li> <li> <p>본 논문에서는 이러한 LRM들의 instruction following 능력을 분석하기 위해 MathIF라는 데이터셋을 제안, math 도메인에서의 instruction following 성능을 평가</p> </li> <li> <p>실험 결과, reasoning을 효과적으로 하는 모델이 user direction에 따르는 것을 어려워 하는 현상 발견</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>CoT reasoning을 scaling하는 것은 reasoning ability를 향상시킴</p> </li> <li> <p>LRM의 경우 간단한 instruction도 following하는 것을 어려워 한다는 것을 발견</p> </li> </ul> <p>⇒ reasoning-oriented learning을 하면 모델 자체의 reasoning ability는 향상돼도 controllability는 떨어지는게 아닐까?</p> <ul> <li>하지만 현재는 범용 목적의 instruction following(IF) 벤치마크만 존재</li> </ul> <p>⇒ 수학 도메인에서의 IF 벤치마크를 만들고 평가해보자!</p> <ul> <li> <p>실험 결과, instruction following과 reasoning capability사이의 일종의 trade-off가 존재</p> </li> <li> <p>contribution</p> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>LRM</p> </li> <li> <p>Instruction-followiwng benchmark</p> </li> </ul> <h2 id="mathif">MathIF</h2> <ul> <li> <p>Overview</p> </li> <li> <p>Constraint type</p> </li> <li> <p>Compositional Constraint</p> </li> <li> <p>Math problem collection</p> </li> <li> <p>Evaluation metric</p> </li> </ul> <h2 id="experiment">Experiment</h2> <ul> <li>모든 LRM은 nucleus sampling(T=1.0, p=0.95)로 디코딩, 최대 답변 길이 16,384 토큰, vLLM 사용</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>모든 LRM은 IF성능이 하락함</p> </li> <li> <p>Qwen3 시리즈가 그나마 높은 IF 성능을 보임</p> </li> <li> <p>모델 크기가 IF 성능을 결정하진 않음</p> </li> <li> <p>명시적인 reasoning seperation (<think>,</think>)가 있는 모델이 전반적으로 IF 성능이 높음</p> </li> <li> <p>instruction-following과 mathematical reasoning사이에 trade-off가 존재</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>제약조건을 만족하면서 문제를 맞추는 경우는 크지 않음</p> </li> <li> <p>보통 제약조건 혹은 문제 하나만을 만족함 + 즉, 제약조건을 걸면 문제 풀이 성능이 하락</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>IF가 낮았던 Qwen2.5를 대상으로 실험, 데이터는 deepscalar를 사용, QwQ로 CoT생성, 정답을 맞추면서 너무 길지 않은 애들만 필터링해서 학습에 사용</p> </li> <li> <p>실험 결과, reasoning-orienteed 방법론이 reasoning성능은 향상시키지만 IF는 하락하는 것을 볼 수 있음</p> </li> </ul> <p>Figure 7</p> <ul> <li> <p>모델이 reasonign path를 종료하려고 할 때마다 wait를 걸어서 강제로 CoT길이를 늘림</p> </li> <li> <p>CoT길이가 길어질수록 constraint instruction과 멀어져서 constraint에 대한 acc가 떨어지는 것으로 추론</p> </li> </ul> <p>Table 5</p> <ul> <li>cold-RL에서 roll-out 길이를 조정하며 학습, 길어질수록 reasoning은 향상되나 IF는 떨어짐</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-scaling-reasoning-losing-control-evaluating-instruction-following-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>간단하게 reasoning이 끝나갈 때 쯤에 wait을 넣고 constraint instruction을 반복해서 넣어준 경우의 성능을 측정</p> </li> <li> <p>IF성능은 향상되나 Correctness는 하락하는 것을 볼 수 있음</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>Reasoning-oriented model들이 생각보다 instruction following 성능이 악화됨</p> </li> <li> <p>대부분 간단한 형식에 대한 제약인데도, 제약이 있을 때와 없을 때의 성능 차이가 큰게 충격적</p> </li> <li> <p>LLM이 정말 reasoning을 하는걸까? 그냥 답변 길이가 길어져서 발생하는 attention sink일까?</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</title><link href="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/" rel="alternate" type="text/html" title="Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning"/><published>2025-07-15T00:00:00+00:00</published><updated>2025-07-15T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-15</li> <li><strong>Reviewer</strong>: 건우 김</li> <li><strong>Property</strong>: Reinforcement Learning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="abstract">Abstract</h1> <ul> <li> <p>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</p> </li> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <strong>효과적으로 상호작용하는</strong> 능력이 필수적임</p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 **LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, 단순한 결과 기반 보상이 충분한지는 아직 불확실함.</p> </li> </ol> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <h1 id="2-related-works">2. Related Works</h1> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <h1 id="3-search-r1">3. Search-R1</h1> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine R을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>r_{\phi}: output quality를 평가하는 reward function</p> </li> <li> <p>\pi_\theta: policy LLM</p> </li> <li> <p>\pi_{ref}: reference LLM</p> </li> <li> <p>x: dataset D에서 추출된 input sample</p> </li> <li> <p>y: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>D_{KL}: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 \pi_\theta가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>\pi_{\theta}: current policy</p> </li> <li> <p>\pi_{old}: previous policy</p> </li> <li> <p>I(y_t): token loss masking 연산으로, y_t가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li> <p><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></p> </li> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Case Study</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h1 id="4-main-results">4. Main Results</h1> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> </li> <li> <p>Multi-Hop QA</p> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2>]]></content><author><name></name></author><category term="paper-reviews"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="pre-training"/><category term="reasoning"/><category term="reinforcement learning"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[논문 리뷰 - Reinforcement Learning 관련 연구]]></summary></entry><entry><title type="html">Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs</title><link href="https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of/" rel="alternate" type="text/html" title="Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/between-underthinking-and-overthinking-an-empirical-study-of/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> <li><strong>Reviewer</strong>: 준원 장</li> <li><strong>Property</strong>: Reasoning</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>Test-time scaling is trending, but <strong>longer reasoning is not always better.</strong></p> </li> <li> <p>Reasoning와 accuracy가 항상 상관관계를 이루지 않는다는 최신연구 존재 (Xie et al., 2025; Jin et al., 2024; Wu et al., 2025)</p> </li> <li> <p>여튼, 이러한 흐름에 따라 최근에 나온 용어</p> </li> <li> <p>그래서 논문은 DeepSeek-1.5B-Distill과 DeepScaler-1.5B-Preview를 가지고 reasoning length와 accuracy를 가지고 체계적인 분석을 수행하겠다!</p> </li> </ul> <h2 id="2-related-work">2. Related Work</h2> <p>⇒ lengthy reasoning 문제를 관측하고, 이를 해결하기 위한 학습방법론들</p> <ul> <li> <p><strong>Concise thinking</strong></p> </li> <li> <p><strong>Adaptive thinking</strong></p> </li> <li> <p><strong>Optimal Thinking</strong></p> </li> </ul> <h2 id="3-experimental-setting">3. Experimental Setting</h2> <ul> <li> <p>Model</p> </li> <li> <p>Dataset</p> </li> <li> <p>Params.</p> </li> <li> <p>Notations</p> </li> </ul> <h2 id="4-sample-level-analysis">4. Sample-Level Analysis</h2> <p>→ q는 고정하고 길이가 다른 10개 completion을 비교해 length와 accuracy의 직접 상관을 조사</p> <ul> <li>난이도에 대한 변인을 고정하고 length ↔ accuracy 관계만 볼 수 있음</li> </ul> <h3 id="non-linear-relationship-of-sample-length-and-correctness">Non-Linear Relationship of Sample Length and Correctness</h3> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>L_r, Acc_r: r번째로 짧은 reasoning path의 평균 length/accuracy</p> </li> <li> <p>consistent non-monotonic trend 관찰</p> </li> </ul> <p>(준원 뇌피셜: 일단 R1은 (1) MATH 관련 데이터는 외워서 풀것 같기 때문에 temp=1.0, top_p=1로 줘서 decoding path 길어지면 degen 발생했을것으로 예상 (2) GSM8K 유사 난이도는 거의 외웠을것이고 + 상대적으로 쉽기 때문에 1~1.5K thinking budget내로는 거의 비슷할거 같음..)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>초록: q에 대한 정답 completion중 가장 짧은 거</p> </li> <li> <p>파랑: q에 대한 정답 completion중 가장 긴거</p> </li> <li> <p>빨강: q에 대한 오답 completion중 가장 짧은 거</p> </li> <li> <p>노랑: q에 대한 정답 completion중 가장 긴거</p> </li> <li> <p>R1-Preview는 MATH, GSM8K 모두 80% 이상의 질문에서 가장 짧은 샘플로 정답을 생성할 수 있음을 보임</p> </li> <li> <p>most length한 completion중에 correct response도 있지만 incorrect response도 존재 (논문 해석 이상..)</p> </li> </ul> <h2 id="5-question-level-analysis">5. Question-Level Analysis</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>단순하게 문제 난이도를 틀림 여부로 볼때, incorrect response가 어떤 조합에서든 response 길이가 더 길었음</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>N-completion별로 difficulty를 분류</li> </ul> <p>⇒ 그러나 (1) 문제가 어려워서 lengthy한지 (2) length해져서 틀린건지 판단이 어려움</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>Q^{easy}<em>{\cap} = Q^{easy}</em>{i} \cap Q^{easy}_{j}</p> </li> <li> <p>Q^{easy}<em>{i/j} = Q^{easy}</em>{i} / Q^{easy}_{j} &gt; M_i 에서만 쉬운 문제</p> </li> <li> <p>Q^{easy}<em>{j/i} = Q^{easy}</em>{j} / Q^{easy}_{i} &gt; M_j에서만 쉬운 문제</p> </li> <li> <p>보편적으로 쉬운 문제가 아니라 another model’s advantage set (다른모델에서 쉬운 문제)에서 오히려 lengthy generation을 보임</p> </li> <li> <p>signficant로 보면 M_i → M_j-Adv Set을 풀때 보다 lengthy해짐</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>hard question에서는 Q^{hard}_{\cap}에서 보다 another model’s advantage set에서 lengthy해질 것을 기대했으나 그렇진 않음</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>(어떻게 실험했는지는 모르겠는데..) token length가 짧아질수록 accuracy가 올라간다.</p> </li> <li> <p>위에 실험을 기반으로 token legnth가 짧으니 확률적으로 당연히 accuracy가 높은 답변일수록 PPL도 낮을 것</p> </li> </ul> <h2 id="6-effect-of-length-preference-optimization">6. Effect of Length Preference Optimization</h2> <ul> <li> <p>지금까지 지적된 문제들을 해결하기 위해 correct/length-balanced reward-based RL등이 소개되었음</p> </li> <li> <p>이를 위해 이전에 drive-out한 직관들을 가지고 간단한 실험을 진행.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>training step을 반복할수록 accuracy 변동폭은 적으나 average token length 30%에서 60% 감소</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-01-between-underthinking-and-overthinking-an-empirical-study-of/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>SimPO가 진행됨에 따라 incorrect response의 생성이 줄어들었다.</li> </ul> <h2 id="7-conclusion---limitation">7. Conclusion &amp; Limitation</h2> <ul> <li> <p>generation length와 final answer correctness에 대해서 심도 있는 분석</p> </li> <li> <p>LM의 크기가 너무 작고, benchmark가 너무 쉬움…</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰 - Reasoning 관련 연구]]></summary></entry><entry><title type="html">Reasoning Models Can Be Effective Without Thinking</title><link href="https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking/" rel="alternate" type="text/html" title="Reasoning Models Can Be Effective Without Thinking"/><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/reasoning-models-can-be-effective-without-thinking/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-07-01</li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM을 이용해 복잡한 문제를 풀 때, 보통 우리는 “긴 chains of thoughts”를 생성하고 그것을 이용해 reflection, backtracking, self-validation 등을 수행하곤 한다 (“Thinking”). 이러한 reasoning path는 일반적으로 reward를 이용한 강화학습, 혹은 distilled reasoning trace를 이용한 finetuning을 통해서 획득되며, 이 <strong>explicit한 reasoning path가 실제로 성능에 많은 도움이 된다고 믿어져왔다</strong>. 이 때문에 inference-time compute scaling이 주된 paradigm이기도.</p> <p>하지만 저자들은 이에 대한 근본적인 질문을 던진다:</p> <ul> <li>정말로 explicit Thinking process가 상위 reasoning을 위해 필요한가?</li> </ul> <p>그리고 저자들은 사실 정교한 reasoning path은 그닥 중요하지 않다는 사실을 다양한 실험을 통해서 증명한다.</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>저자들은 DeepSeek-R1-Distill-Qwen을 이용해서 Thinking과 이들이 사용하는 NoThinking — 응답에 가짜 Thinking 블록을 미리 채워 넣고, 모델이 그 이후부터 이어서 답변하도록 하는 방법 — 을 비교해보았을 때, 오히려 NoThinking이 훨씬 더 성능이 좋다는 것을 보인다 (pass@k metrics).</p> <p>NoThinking은 2.0~5.1배 적은 토큰을 사용하면서도, k=1을 제외하고는 Thinking과 비슷하거나 더 좋은 성능을 보인다.</p> <p>또한, 두 접근법의 토큰 사용량을 동일하게 통제했을 때, NoThinking은 특히 low-budget 구간에서 pass@1가 Thinking보다 높았으며, k가 커질수록 성능 차이는 더 커졌다 (Figure 2).</p> <p>효율성을 체계적으로 평가하기 위해 pass@k와 평균 토큰 사용량 간의 Pareto frontier를 분석한 결과, NoThinking은 항상 Thinking보다 우수한 결과를 보였다. 이는 명시적인 추론 과정을 거치지 않더라도 NoThinking이 더 좋은 accuracy-cost tradeoffs를 가진다는 것을 의미한다.</p> <p>pass@k에서 NoThinking이 좋은 성능을 보였다는 것은, Thinking을 사용한 순차적 추론이 아닌, parallel scaling이 가능하다는 것이다. 저자들은 여러 응답을 병렬로 샘플링하고, best-of-N으로 최종 응답을 고르는 방법론을 제안한다.</p> <p>이들이 고려한 task는 두 가지이다:</p> <ol> <li> <p>tasks with perfect verifiers (e.g., formal theorem proving): 자동으로 정답 여부를 확인할 수 있는 경우</p> </li> <li> <p>tasks without verifiers (e.g., general problem solving): simple confidence-based selection strategies를 사용해야하는 경우</p> </li> </ol> <p>verifiers가 있는 경우, NoThinking이 Thinking을 가뿐히 능가했다 ( both with and without parallel scaling). 특히, 지연 시간을 7배 단축하고 총 토큰 사용량을 4배나 줄였다는 점이 이점. verifiers가 없는 경우에도 NoThinking 준수한 성능을 보인다. 예를 들어, Thinking을 9× lower latency + improved accuracy on OlympiadBench (Math)로 능가함. (Figure 3)</p> <p>요약하자면, 이 연구는 현재의 추론 모델들이 학습 과정에서 구조화된 추론 형식을 따르도록 훈련되었음에도 불구하고, 높은 성능을 내기 위해 반드시 명시적인 thinking 과정이 필요하지 않다는 사실을 처음으로 보여주고 있다.</p> <p>또한, NoThinking 방식을 병렬로 처리하면, 순차적 추론보다 더 좋은 latency-accuracy tradeoffs가 가능함을 보인다.</p> <p>전반적으로, 이 연구는 긴 thinking 과정이 과연 정말로 필요한 것인가에 대한 의문에 대한 답을 일부분 보여주고 있다고 할 수 있다!</p> <h1 id="2-related-work-and-background">2. Related Work and Background</h1> <h3 id="test-time-scaling-for-language-models">Test-Time Scaling for Language Models</h3> <ul> <li> <p><strong>Sequential approaches</strong></p> </li> <li> <p><strong>Parallel approaches</strong></p> </li> <li> <p><strong>NoThinking의 차별점</strong></p> </li> </ul> <h3 id="efficient-reasoning">Efficient Reasoning</h3> <p>recent work has explored various strategies to make reasoning in LLMs more efficient.</p> <ul> <li> <p><strong>추론 시퀀스 길이 최적화</strong></p> </li> <li> <p><strong>강화 학습 기반 CoT 최적화</strong></p> </li> <li> <p><strong>Best-of-N 샘플링을 활용한 파인튜닝</strong></p> </li> <li> <p><strong>출력 방식 수정으로 reasoning 간결화</strong></p> </li> <li> <p><strong>학습 없는 전략적 기준 설정</strong></p> </li> <li> <p><strong>추론 단계 수 제한</strong></p> </li> <li> <p><strong>동적 입력 라우팅으로 reasoning 복잡성 제어</strong></p> </li> </ul> <h1 id="3-nothinking-provides-better-accuracy-budget-tradeoffs-than-thinking">3. NoThinking Provides Better Accuracy-budget Tradeoffs than Thinking</h1> <p>Section 3.1: define Thinking and NoThinking</p> <p>Section 3.2: describe experimental setup</p> <p>Section 3.3: present experimental results</p> <p>Section 3.4: Discussions and Analyses</p> <h2 id="31-method">3.1 Method</h2> <p>대부분의 모델들은 보통 비슷한 구조로 generation을 한다:</p> <ul> <li> <table> <tbody> <tr> <td>reasoning process within the thinking box, marked by &lt;</td> <td>beginning of thinking</td> <td>&gt; and &lt;</td> <td>end of thinking</td> <td>&gt;, followed by the final answer.</td> </tr> </tbody> </table> </li> </ul> <p>이 구조에 기반해서 Thinking and NoThinking을 다음과 같이 만듦:</p> <ul> <li> <p>**Thinking: **the reasoning process within the thinking box, the final solution, and the final answer (Figure 1 (blue)).</p> </li> <li> <p>**NoThinking: ** explicit reasoning process 무시하고 바로 final solution and answer 만들기. thinking box를 decoding 할 때 빈칸으로 하도록 강제 (Figure 1 (orange)).</p> </li> </ul> <table> <tbody> <tr> <td>token usage를 제어하기 위해 budget forcing technique from Muennighoff et al. (2025)을 사용 — 모델이 token budget에 도달하면, 강제로 Final Answer 만들도록 함. 만약 아직 thinking box 안에 있었다면, &lt;</td> <td>end of thinking</td> <td>&gt; 을 final answer tag 이전에 붙여서 만듬.</td> </tr> </tbody> </table> <h2 id="32-evaluation-setup">3.2 Evaluation Setup</h2> <ul> <li> <p><strong>Models</strong></p> </li> <li> <p><strong>Tasks and Benchmarks</strong></p> </li> <li> <p>**Metrics: **pass@k</p> </li> </ul> <h2 id="33-results">3.3 Results</h2> <h3 id="thinking-vs-nothinking-vs-qwen-instruct-without-token-budget-controlled">Thinking vs. NoThinking vs. Qwen Instruct without token budget controlled</h3> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>budget forcing없이 세 경우를 비교한 결과:</p> <ul> <li> <p>MiniF2F and ProofNet에서 NoThinking은 모든 K에 대해서 Thinking과 비슷했으며, 둘은 Qwen-Instruct보다 성능 훨씬 좋았음</p> </li> <li> <p>다른 데이터셋에서는 k = 1일 때는 NoThinking의 성능이 훨씬 떨어지지만, k가 커질수록 갭이 작아짐</p> </li> <li> <p>결과적으로, NoThinking은 가장 큰 k일때, 2.0–5.1x fewer tokens을 사용하는데도, Thinking의 성능을 넘거나 거의 근사함.</p> </li> <li> <p>Qwen-Instruct의 관점에서:</p> </li> </ul> <h3 id="thinking-vs-nothinking-with-token-budget-controlled">Thinking vs. NoThinking with token budget controlled</h3> <p>위에서 확인했듯,Thinking이 NoThinking보다 대부분의 데이터셋에서 성능이 더 좋음. 하지만, 결과적으로 Thinking이 더 많은 토큰을 사용하기 때문에 같은 토큰 수를 사용할 때 어떤 것이 더 성능이 좋은가를 비교함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>결과적으로 NoThinking generally outperforms Thinking.</p> <p>특히, low-budget setting (e.g., fewer than ≈ 3, 000 tokens)에서 NoThinking은 모든 k에서 더 좋은 성능을 보였고, k가 커질수록 차이는 커졌음. 좀 더 토큰 제한을 늘렸을 때 (e.g., around 3, 500 tokens), Thinking이 pass@1에서는 더 좋았으나, k = 2부터는 다시 NoThinking이 더 좋은 성능을 보임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 6는 해당 데이터셋에서 사용한 가장 큰 k와 1, 그리고 token usage를 plot하면서 위의 결과를 더 잘 보여줌.</p> <ul> <li> <p>pass@k</p> </li> <li> <p>pass@1</p> </li> </ul> <p>[요약]</p> <ul> <li> <p>reasoning models의 핵심인 thinking box를 없애도, 여전히 효과 좋음</p> </li> <li> <p>3.3–3.7x 적은 토큰을 사용하는데도 비슷한 성능 나옴</p> </li> <li> <p>비슷한 수의 토큰이라면 thinking보다 성능 좋음</p> </li> </ul> <h2 id="34-discussions-and-analyses">3.4 Discussions and Analyses</h2> <h3 id="task-specific-differences-in-nothinking-performance">Task-Specific Differences in NoThinking Performance</h3> <p>Section 3.3에서 나름 일관적인 트렌드가 보이긴 하지만, 각 벤치마크 결과를 자세히 살펴보면 조금 동작이 다름</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>In Figure 4,</p> <ul> <li> <p>AMC 2023는 모든 세팅에서 거의 performance gap없이 convergence를 보임. 아마도 saturation이 예상됨</p> </li> <li> <p>MiniF2F and ProofNet pass@1에서 NoThinking은 Thinking에 비해 훨씬 더 적은 토큰을 사용하면서 비슷한 성능을 냄. 하지만, 이는 단순히 task simplicity 이슈로 해석되면 안됨! 검증 결과, OpenAI’s o1과 같은 엄청 강한 모델은 MiniF2F에서 30% accuracy 밖에 안됐고, ProofNet은 모든 방법론에서 성능 낮았음. 즉, 왜 어떤 벤치마크에서는 NoThinking이 잘되었는가는 open question for future work이라는 것</p> </li> </ul> <h3 id="how-increasing-k-affects-nothinking-performance">How Increasing k Affects NoThinking Performance</h3> <p>왜 k가 늘어날수록 NoThinking이 더 좋은 성능을 보이는지 대략적인 이유를 찾아보기 위해 생성된 답변의 diversity를 측정함 — by computing the entropy of the answer distribution for each question.</p> <p>높은 mean entropy는 당연히 더 높은 overall diversity를 의미하고, lower standard deviation은 더 일관적인 것을 의미. 실험은 token budget이 제한된 환경에서 진행</p> <figure> <picture> <img src="/assets/img/posts/2025-07-01-reasoning-models-can-be-effective-without-thinking/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>엔트로피의 관점에서는 특별한 차이를 찾지 못함. 어떨 땐 NoThinking이 어떨 땐 Thinking이 더 높음</p> </li> <li> <p>variance의 관점에서 NoThinking은 항상 더 낮은 값을 보임 — 더 uniform하게 답을 내고 있다는 것.</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry><entry><title type="html">See What You Are Told: Visual Attention Sink in Large Multimodal Models</title><link href="https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink/" rel="alternate" type="text/html" title="See What You Are Told: Visual Attention Sink in Large Multimodal Models"/><published>2025-06-24T00:00:00+00:00</published><updated>2025-06-24T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/see-what-you-are-told-visual-attention-sink/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-24</li> <li><strong>Reviewer</strong>: 조영재</li> <li><strong>Property</strong>: Multimodal</li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>LLM의 발전과 함께 Multimodal 모델들도 많이 등장하고 있음 (VQA, image captioning, visual reasoning, …)</p> </li> <li> <p>LMM에서도 LLM 처럼 똑같이 attention 매커니즘을 따름. 예를 들어 ‘bird’를 말하고자 할때 model은 해당 이미지에 관련있는 visual token에 대해 집중함. (직관적으로) text와 visual token이 매칭됨.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>근데 실제로는 text와 visual 간의 관계가 unrelated 되는 경우도 관측됨. attention map을 통해 attention을 더 면밀히 살펴보았을때, Example 1을 보면, 위에 bird를 말하는데 ‘빨간 네모’ 처럼 bird와 무관한 곳에 높은 attention이 관측. 다른 예제들도 마찬가지로 텍스트와 무관한 곳에 높은 어텐션이 관측됨. 이게 왜 발생하는지 궁금해서 해당 연구가 시작됨</p> </li> <li> <p>해당 연구의 발견</p> </li> <li> <p>최근에 vlm에서 attention이 text에 비해 이미지에 부족하게 할당된다는 사전 연구도 있었음. 그래서 우리는 attention budget의 개념으로 visual sink token들에 가는 attention을 아껴서 다른 visual token들에 redistribute를 하고자 함(Visual Attention Redistribuion (VAR))</p> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Visual attention in large multimodal models.</p> </li> <li> <p>Attention sink in language models</p> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p>트랜스포머 공식</p> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="visual-attention-sink">Visual Attention Sink</h2> <p>Figure1보면 attention이 우리의 직관대로 잘 따라가긴 하지만 <strong>고정된 어떤 background spot</strong>에 굳이 필요없는 limited semanic meaning에 높은 attention이 배정되어 있음</p> <h3 id="how-to-distinguish-irrelevant-visual-tokens">How to distinguish irrelevant visual tokens?</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>irrelevant visual token에서 두 가지 특성이 나타남. (1) figure 1에서 보듯 이미지에 same irrelevant visual token에 고정적으로 등장. (2) BOS 토큰이랑 유사하게 같은 dimension에서 등장 (Fig 2)</p> <h3 id="irrelevant-visual-tokens-have-high-activation-in-specific-dimensions">Irrelevant visual tokens have high activation in specific dimensions</h3> <ul> <li> <p>Fig2의 BOS랑 빨간<img/> 를 보면 같은 dimension에서 attention 값이 튀는 것을 볼 수 있음. 이는 LLM 각자가 같은 고유한 특성이라고 함. 예를들어 LLaVA-1.5-7B가 사용한 LLaMA2 백본은 모두 고정적으로 {1415, 2533} 의 dimension에서 위와같은 형태를 보임. (pretrain 과정에서 쏠리는 거라 finetuning을 해도 sink dimension은 계속 고정되어있다고 함)</p> </li> <li> <p>특정 토큰이 갖는 sink dimension value <strong>Φ(x)</strong>를 아래와 같이 정의</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>쉽게 말해서 1415, 2333 등과 같은 sink dimension에서 토큰이 갖게 되는 튀는 값을 나타냄. (Fig 2 참고)</p> <ul> <li>visual sink token을 구분하기 위해 20보다 <strong>Φ(x)</strong>가 큰 토큰들은 다 visual sink token으로 분류. 이를 통해 irrelevant visual token(sink dimension에서 attention 값이 튀는 애들)과 relevant visual token(튀지 않는 애들)을 구분함.** **</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Fig3 (a)를 통해 본인들이 정의한 high sink dimension value 들이 높은 attention 값을 가지는 애들이였으며, (b) 실제로 visual sink 들을 mask하고 하니 안할때보다 성능이 높았음. (c) attention contribution도 측정했을 때 (실제로 text 답변 만드는 logit에 기여하는 정도) 는 작았음. (d) 를 봐도 w/o sinks 가 noise를 잡아내며 대부분의 sink 들은 background에 존재</li> </ul> <h2 id="surplus-attentions-in-visual-attention-sink--can-we-recycle-them">Surplus attentions in visual attention sink : can we recycle them?</h2> <p>(1) image centric-head 를 먼저 뽑고 (2) 해당 head에서 sink token들에 가던 attention을 보아서 non sink token 에게 분배할 예정</p> <ul> <li>Image centric-head</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Redistributing attention weights</li> </ul> <h3 id="experiments">Experiments</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>(1) VL-task</p> <p>(2) visual hallucination</p> <p>(3) vision centric (spatial relationship between objects)</p> <h3 id="ablation-studies">Ablation studies</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>(Table 4) visual non sink ratio 를 정의해서 로 보다 큰 애들의 head 만 살렸었는데 이 과정이 필수적이였음.</p> </li> <li> <p>(Table 5) visual token 내에서만 attention redistribution이 성능이 제일 높음.</p> </li> </ul> <h3 id="appendix">Appendix</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-24-see-what-you-are-told-visual-attention-sink/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="discussion">Discussion</h3> <ul> <li> <p>latency가 별로 없으면서 정말 많은 VL task에서 성능이 다 오른건 신기! 다만 해당 방법론 역시 projector을 이용한 vision language model에서만 적용 가능할 것으로 보임. (one-to-one 매칭, instructVL, instructBLIP같은 resampler는 적용 안됨)</p> </li> <li> <p>마지막 table5에서 budet을 아껴서 text에 줬을 때 성능이 안오른건 의외. 직관적으로 필요 없는 잉여물을 준다고 해서 오르는건 아닌것같음</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="attention"/><category term="language-model"/><category term="llm"/><category term="multimodal"/><category term="paper-review"/><category term="reasoning"/><category term="vision"/><summary type="html"><![CDATA[논문 리뷰 - Multimodal 관련 연구]]></summary></entry><entry><title type="html">Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models</title><link href="https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in/" rel="alternate" type="text/html" title="Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models"/><published>2025-06-17T00:00:00+00:00</published><updated>2025-06-17T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/diffusion-of-thought-chain-of-thought-reasoning-in/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-17</li> <li><strong>Reviewer</strong>: 상엽</li> <li><strong>Property</strong>: Text Generation, DiffusionLM</li> </ul> <p>https://huggingface.co/blog/ProCreations/diffusion-language-model?utm_source=chatgpt.com</p> <h2 id="introduction">Introduction</h2> <p><strong>도입 배경</strong></p> <ul> <li> <p>LLM의 엄청난 성장 → Chain-of-Thought (CoT)와 같은 Reasoning이 핵심 기법으로 부상</p> </li> <li> <p>CoT는 중간 추론 단계를** autoregressive 방식<strong>으로 생성하여 **LLM의 추론 능력을 향상</strong>시킴</p> </li> <li> <p>하지만 <strong>기존 CoT의 한계점</strong>들이 존재</p> </li> </ul> <p><strong>Diffusion Model의 등장</strong></p> <ul> <li> <p>Vision 영역에서의 성공에 이어 텍스트 처리 분야에서도 주목받기 시작</p> </li> <li> <p><strong>Why?</strong> Autoregressive model 대비 고유한 강점을 보유</p> </li> <li> <p><strong>Pre-trained diffusion language model</strong> → Plaid, SEDD 등 (최근에는 Llama3-8B 정도 수준의 LlaDA 모델 등장)</p> </li> </ul> <p><strong>RQ</strong></p> <p><strong>Diffusion of Thoughts (DoT) 제안</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p><strong>Diffusion model에 특화된 inherent chain-of-thought 방법 제안</strong></p> </li> <li> <p>핵심 특징</p> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <p><strong>기본 개념</strong></p> <ul> <li> <p>Forward process</p> </li> <li> <p>Reverse process</p> </li> <li> <p>Text generation을 위한 diffusion 모델의 종류</p> </li> </ul> <p><strong>Seq2Seq Generation (e.g. DiffuSeq)</strong></p> <ul> <li> <p>입력-출력 sequence를 single sequence로 처리: \mathbf{w}^{z}=\mathbf{w}^{[x; y]}</p> </li> <li> <p>Left-aligned mask [\mathbf{0};\mathbf{1}]로 x, y를 구분</p> </li> <li> <p><strong>Partial noising</strong>: mask value가 1인 부분에만 noise 적용</p> </li> </ul> <h2 id="diffusion-of-thoughts">Diffusion-of-Thoughts</h2> <ul> <li> <p>Notation: s (problem statement), a (answer), p_{\theta}^{LM} (language model)</p> </li> <li> <table> <tbody> <tr> <td>Answer-only generation model: \mathbf{a}\sim p_\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>CoT: \mathbf{a}\sim p_\theta^{\textit{LM}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{r}_{1\dots n})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>implicit CoT: \mathbf{a}\sim p_\theta^{\textit{iCoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}_{1\dots n})</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>DoT: \mathbf{a}\sim p_\theta^{\textit{DoT}}(\mathbf{a}</td> <td>\mathbf{s}, \mathbf{z}_t)</td> </tr> </tbody> </table> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="dot-modeling">DoT Modeling</h3> <ul> <li><strong>Gradient-based token guidance의 한계</strong></li> </ul> <p><strong>→ DiffuSeq-style classifier-free conditioning 채택</strong></p> <p>→ continuous 방식의 DiffuSeq-style이 가진 장점이 무엇인가?</p> <p><strong>Multi-pass DoT (MP-DoT)</strong></p> <ul> <li> <p>Causal inductive bias 도입 thought-by-thought 방식으로 rationales을 생성하는 방법 제안</p> </li> <li> <p><strong>Process</strong>:</p> </li> <li> <p>이후 rationale이 이전 rationale들을 더 강한 condition signal로 이용할 수 있음.</p> </li> </ul> <h3 id="training">Training</h3> <p><strong>Scheduled Sampling</strong></p> <ul> <li> <p>Diffusion 모델이 denoising을 하는 과정에서 이미 self-correcting 능력이 있다고 할 수 있음. → Sampling 과정을 통해 이를 더욱 발전</p> </li> <li> <p>Training과 inference 간의 <strong>exposure bias</strong>가 error를 발생시킨다고 생각</p> </li> <li> <p>Any timesteps: s, t, u that satisfy 1 &lt; s &lt; t &lt; u &lt; T</p> </li> <li> <p><strong>해결책</strong>: 추론 단계를 모방하기 위해 \epsilon_i 확률로 다음과 같이 forward step에서 만들어진 z를 활용</p> </li> </ul> <p><strong>Coupled Sampling</strong></p> <ul> <li> <p>Multi-pass DoT에서 rationale에 쌓이는 error accumulation 문제 해결</p> </li> <li> <p><strong>Training 시 현재 thought뿐만 아니라 이전 thought들에도 확률적으로 noise 추가</strong></p> </li> </ul> <p><strong>Training Objective</strong></p> <p>DoT 모델에 대해 두 가지 학습 방법을 사용</p> <ul> <li> <p>from scratch</p> </li> <li> <p>fine-tuning from pre-trained diffusion model</p> </li> </ul> <p><strong>공통 Objective function:</strong> Negative variational lower bound 최소화</p> <ul> <li> <p>z_t를 denoising 함으로써 z_0를 복원하는 것을 배우는 것</p> </li> <li> <p><strong>Prior loss</strong></p> </li> <li> <p><strong>Diffusion loss</strong>: 각 단계에서 얼마나 noise를 잘 제거하는가에 대한 탐색</p> </li> <li> <p><strong>Rounding loss</strong>: 복원력 z_0 → \text{w}^z</p> </li> </ul> <h3 id="inference-strategy">Inference Strategy</h3> <ul> <li> <p>diffusion 모델의 추론 flexibility는 큰 장점 → 어려운 문제일수록 더 많은 reasoning time을 가져야 함. → backward timestep T를 크게 가져가자! (이거 안되는 게 있나? 논문에서 autoregressive 방법에서 토큰 수를 조절하는 것은 더 어렵다고 주장.)</p> </li> <li> <p><strong>문제</strong>: Continuous diffusion의 높은 timestep 요구사항 (예: Plaid 4096 timesteps)</p> </li> </ul> <p>→ ODE solver를 conditional form을 활용해 accelerate</p> <ul> <li>이게 최종식인데 미분방정식 얘기가 나와서 아직은 모르겠습니다….</li> </ul> <p><strong>Self-consistency Integration</strong></p> <ul> <li> <p>Multiple sampling을 통한 다양한 reasoning pathway 생성</p> </li> <li> <p>동일 문제 s에 대해 다양한 (r_{i;1…n}, a_i)를 구함. (Diffusion 모델의 강점: noise seed만 다르게 해도 됨!)</p> </li> <li> <p>Majority vote:</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-17-diffusion-of-thought-chain-of-thought-reasoning-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="evaluation">Evaluation</h2> <h3 id="experimental-setup">Experimental Setup</h3> <p><strong>데이터셋 및 메트릭</strong></p> <ul> <li> <p><strong>Simple reasoning</strong>:</p> </li> <li> <p><strong>Complex reasoning</strong>: GSM8K grade school math problems</p> </li> </ul> <p><strong>Base Model</strong></p> <ul> <li> <p><strong>From scratch</strong>: Following DiifuSeq (12-layer Transformer encoder, 124M)</p> </li> <li> <p><strong>Pre-trained model for fine-tuning</strong>:</p> </li> </ul> <p><strong>Baseline</strong></p> <ul> <li> <p>Answer-only, CoT, Implicit CoT</p> </li> <li> <p>GPT-2 (small 124M, medium 355M, large 774M)</p> </li> <li> <p>ChatGPT (gpt-3.5-turbo-1106) with 5-shot CoT</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="diffusion"/><category term="diffusionlm"/><category term="fine-tuning"/><category term="gpt"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><category term="text generation"/><category term="transformer"/><category term="vision"/><summary type="html"><![CDATA[논문 리뷰 - Text Generation, DiffusionLM 관련 연구]]></summary></entry><entry><title type="html">DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</title><link href="https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for/" rel="alternate" type="text/html" title="DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for</id><content type="html" xml:base="https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for/"><![CDATA[<p><strong>논문 정보</strong></p> <ul> <li><strong>Date</strong>: 2025-06-10</li> <li><strong>Reviewer</strong>: 건우 김</li> </ul> <h1 id="abstract">Abstract</h1> <ul> <li> <p>최근에 post-training을 위한 RL에서 <strong>GRPO</strong>와 같이 low-resource settings에서 가능성을 보여줌</p> </li> <li> <p>위 문제를 해결하기 위해 reward computation 과정에서 semantic diversity를 직접적으로 반영하는 방법인 <strong>Diversity-aware Reward Adjustment (DRA)</strong>를 제안함</p> </li> <li> <p>DRA는 Submodular Mutual Information (SMI)를 활용하여</p> </li> <li> <p>5개 Mathematical Reasoning benchmark에서 recent methods 대비 outperform 성능 보여줌</p> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>DeepSeek-R1-Zero (Guo et al., 2025)에서 기존 LLM에 SFT를 적용하는 것에서 벗어나, base LM에 바로 RL을 적용할 수 있는 R1-Zero training pipeline을 제안함.</p> <p>→ Group Relative Policy Optimization (GRPO) 알고리즘 덕분에 가능한 방법</p> <p>GRPO는 PPO와 다르게 critic model 없이 주어진 prompt에 대해 여러 sampling된 completions의 relative performance에 대한 advantage를 평가함.</p> <p>하지만 최근에 공개된 GRPO 및 그 variants (e.g,. DR. GRPO)들은 일반적으로 정답 여부와 같은 <strong>solution-level의 scalar reward signals에만 의존하는 경향이 있어, 같은 정답이라도 diverse reasoning path의 차이를 반영하지 못함</strong>.</p> <p>→ 이는 semantic하게 다른 completions들이 올바르거나 틀린 경우 모두 거의 동일한 rewards를 받아, 의미 있는 reasoning 차이를 반영하지 못하는 <strong>indistinguishable advantage estimates</strong>를 생성하는 문제가 있음</p> <p>→ 또한, 이는 resource-constrained settings에서 더 문제가 될 수 있음</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>이 문제를 해결하기 위해서 저자들은 <strong>Diversity-aware Reward Adjustment (DRA)</strong>를 제안함.</p> <p>이는 학습 과정에서 sampling된 completions 간의 <em>semantic diversity를 직접적으로 모델링하는 방식으로</em> 그룹 내 다른 <em>completions과의 semantic similarity를 기반으로 각 output의 reward를 reweight</em>함.</p> <ul> <li><strong>diverse completions에는 더 높은 weight, 중복된 completion에는 더 낮은 weight 부여</strong></li> </ul> <h1 id="2-method">2. Method</h1> <h3 id="preliminary">Preliminary</h3> <p>LM의 generation은 token-level Markov Decision Process로 볼 수 있음. 각 generation step t에서 state s_t는 input question q와 지금까지 생성된 partial output sequence o_{&lt;t}의 concatenation이기에, sates는 다음과 같음 s_t=[q;o_{&lt;t}].</p> <table> <tbody> <tr> <td>policy \pi_{\theta}(.</td> <td>s_t)는 vocab set A에서 next token o_t를 선택하고, 이는 deterministic transition을 유도하여 next state s_{t+1}=[s_t;o_t]로 이동함.</td> </tr> </tbody> </table> <p>GRPO는 각 question q에 대해 여러 개의 responses C={o_1,…o_G}를 sampling하고, 각 response에 대해 reward를 계산함 R={R(q,o_1), … , R(q,o_G)}</p> <p>계산된 reward R을 이용해 advantage A_{i,t}를 아래와 같이 계산함 (normalize)</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>GRPO의 objective function J_{GRPO}(\pi_{\theta})를 optimize함</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>이후 연구인 DR.GRPO (Liu et al., 2025)에서는 token efficiency를 개선하기 위해 <strong>GRPO의 objective function에서 ‘response length’ term과 Advantage에서 std로 normalize해주는 term을 지움</strong></p> <h3 id="diversity-quality-inconsistency">Diversity-Quality Inconsistency</h3> <p>GRPO와 DR.GRPO의 reward signal은 <strong>solution-level correctness</strong>만 사용하기 때문에, 각 completion에 대해 <strong>sparse scalar judgement</strong>를 계산함.</p> <p>→ 이러한 scalar reward는 동일하거나 유사한 결과를 산출하는 diverse reasoning-path를 고려하지 않기 때문에, Diversity-Quality Inconsistency가 발생함.</p> <p>위에 Example 말고, 보다 실증적인 방식으로 다음 statement (”<strong><em>reward alone fails to reflect the underlying variability in reasoning strategies</em></strong>”) 를 검증하기 위해 embedding distances로 측정된 completions의 structural dissimilarity를 계산함.</p> <ul> <li>Spearman’s rank correlation을 사용하여 sampled completions 사이에서 reward difference와 semantic distance를 측정함 →semantic distance가 커질수록 reward 차이도 커지는가?</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li>Figure2는 Spearman’s rank correlation의 p-values의 분포를 보여주는데, 대부분의 p-value가 significance level인 0.05 보다 큰 값을 보여주며, 실제로 80% 이상의 prompt에 대해 statistically significant correlation이 없음을 확인할 수 있음</li> </ul> <h3 id="diversity-aware-reward-adjustment">Diversity-aware Reward Adjustment</h3> <p>Diversity-Quality Inconsistency 문제를 해결하기 위해, 각 sample의 relative diversity/redundancy에 따라 reward를 reweight하는 방법을 제안함.</p> <p><strong>→ diverse completions은 더 높은 weight, 중복된 response는 낮은 weight</strong></p> <p>먼저 기존의 reward R(q,o_i)를 diversity-aware adjusted reward \tilde{R}(q,o_i) (틸다 표시 어떻게 하나요…) 으로 대체함</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <ul> <li> <p>SMI({o_i},C \ {o_i})는 completion o_i와 나머지 group C \ o_i 간의 Submodular Mutual Information을 나타냄</p> </li> <li> <p>Submodular functions은 diminishing returns 특성을 갖으며, diversity와 redundancy를 모델링할 수 있음</p> </li> <li> <p>SMI는 두 집합 간의 shared information을 정량화하며 (Iyer et al., 2021a,b)에서는 아래와 같이 정의함</p> </li> <li> <p>SMI를 쉽게 말하면 “<strong>특정 completion 하나가 group 내 다른 completion과 얼마나 겹치는가</strong>”를 수치로 나타내는 값</p> </li> <li> <p>Submodular 함수는 수학 개념으로 “새로운 element가 기존에 비슷한게 많을수록 기여도가 줄어드는 성질”을 갖고 있음</p> </li> </ul> <p>→ 이렇게 새로운 reward를 구하는 연산은 Pytorch에서 효과적으로 처리될 수 있음</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="3-experiment">3. Experiment</h2> <h3 id="31-experimental-setup">3.1 Experimental Setup</h3> <p>**Training Dataset: **</p> <p>**Evaluation Dataset: **</p> <p><strong>Baselines</strong>:</p> <ul> <li> <p>general purpose large model: Llama-3.1-70B-Instruct, o1-preivew</p> </li> <li> <p>Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL</p> </li> <li> <p>Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS</p> </li> </ul> <p><strong>Implementations:</strong></p> <ul> <li> <p>본 연구는 DRA의 proof-of-concept만 검증하는 것이 목적이기에 DeepSeek-R1-Distill-Qwen-1.5B를 base model로 두어 학습시킴</p> </li> <li> <p>4 x A100 (40GB) GPUs</p> </li> </ul> <h3 id="32-empirical-analysis">3.2 Empirical Analysis</h3> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Main Results</strong></p> <ul> <li> <p>DRA-DR.GRPO는 avg acc가 58.2%로 가장 높게 나옴 (DRA-GRPO역시 비슷한 수준으로 높게 나옴)</p> </li> <li> <p>DRA-GRPO와 DRA-DR.GRPO는 fine-tuning samples을 7,000개 밖에 사용하지 않았음에도 불구하고 40,000개 사용한 DeepScaleR-1.5B-preview보다 높은 성능 보여줌</p> </li> </ul> <p><strong>Ablation Study</strong></p> <ul> <li>Base model인 DeepSeek-R1-Distill-Qwen-1.5B와 비교하여 DRA-GRPO, DRA-DR.GRPO는 각각 7.8%, 9.3% 성능 향상되고 단순 RL (GRPO, DR.GRPO) 대비 1.9%, 2.2% 향상</li> </ul> <p><strong>Efficiency</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>DRA는 completions을 encoding 해야하기에 over-head가 존재하지만, 별로 크지 않음.</p> <p>→ 저자들이 실험에 사용한 GPU스펙인 (A100-40GB)에서는 어차피 DRA 없이도 mini-batch를 늘리는 것이 불가능해서 DRA 적용하는 것이 별 문제가 되지 않다고 하는데…. → 🐶 🔊 라고 생각합니다</p> <p><strong>Training Cost</strong></p> <p>500 steps 학습시켜 12.5hr 소요됨 ⇒ $55 비용</p> <p>→ 다른 방법대비 효율적임</p> <figure> <picture> <img src="/assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h3 id="33-discussion">3.3 Discussion</h3> <p><strong>Exploration-exploitation Balance</strong></p> <p>DRA는 Exploration-exploitation balance를 policy gradient 안에 직접 통합하여 적용함</p> <ul> <li> <p>Base reward는 high score를 받는 completion을 reinforce함</p> </li> <li> <p>Diversity weighting은 semantically novel completion에 learning signal을 amplify</p> </li> </ul> <p>이러한 탐색은 low-resource settings (prompt당 sampling할 수 있는 응답 수가 제한 적인 경우)에서 중요함</p> <p>→ DRA는 mode collapse를 방지하고 더 넓은 reasoning strategies를 유도함</p> <p><strong>Ad-hoc vs Post-hoc Diversity</strong></p> <p>generated completions간의 diversity를 모델링하는 방법은 크게 Ad-hoc, Post-hoc 방식이 있음</p> <ol> <li> <p><strong>Ad-hoc</strong></p> </li> <li> <p><strong>Post-hoc (본 연구에서 채택한 방법)</strong></p> </li> </ol> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li> <p>GRPO 형식의 RL에서 completions 간의 semantic diversity를 모델링할 수 있는 DRA 알고리즘 제안함</p> </li> <li> <p>두가지 한계점이 있음</p> </li> <li> <p>이런 쪽도 재밌다!ㅋㅋ</p> </li> </ul>]]></content><author><name></name></author><category term="paper-reviews"/><category term="embedding"/><category term="fine-tuning"/><category term="language-model"/><category term="llm"/><category term="paper-review"/><category term="reasoning"/><summary type="html"><![CDATA[논문 리뷰]]></summary></entry></feed>