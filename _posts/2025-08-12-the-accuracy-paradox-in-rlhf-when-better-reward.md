---
categories:
  - paper-reviews
date: "2025-08-12 00:00:00"
description: ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, Reward Model ê´€ë ¨ ì—°êµ¬
giscus_comments: true
layout: post
related_posts: false
tags:
  - alignment
  - language-model
  - paper-review
  - reinforcement learning
  - reward model
  - rlhf
thumbnail: assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/thumbnail.jpg
title: "The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better
  Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2025-08-12
- **Reviewer**: ì¤€ì› ì¥
- **Property**: Reinforcement Learning, Reward Model

---

---

# The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models

## 1. Introduction

- ìš°ë¦¬ëŠ” ì™œ RLì„ í†µí•´ alignmentë¥¼ í• ê¹Œ?

- SFT suffers from exposure bias

- SFT lacks the ability to optimize for sequence-level rewards

- RQ & Our Common Myth

- ë” Accurateì„ ì¤„ ìˆ˜ ìˆëŠ” RMì´ ë” effectivenessí•œ RMì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

- ë…¼ë¬¸ì€ ì—¬ëŸ¬ ì‹¤í—˜ì„ í†µí•´ ì´ë¥¼ ë°˜ë°•í•˜ê³ ì í•¨.

â‡’ moderateí•œ accuracyë¥¼ ê°€ì§„ RMì´ the most accuracyë¥¼ ê°€ì§„ RMë³´ë‹¤ LM performanceë¥¼ ë” í–¥ìƒì‹œí‚¨ë‹¤.

â‡’ RM accuracyë‘ LM final performanceëŠ” correlationì´ ì—†ë‹¤.

## 2. Motivation and Problem Setting & Recap RLHF Formula

### Motivation

- LM performanceë¥¼ maximizeí•  ìˆ˜ ìˆëŠ” reward modelì˜ optimal accuracy rangeê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •

### RLHF Formula

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_000.png" class="img-fluid rounded z-depth-1" %}

triplet: `(x, y_w, y_l)`

- accepted response score: `s_w = r_Î¸(x, y_w)`

- rejected response score: `s_l = r_Î¸(x, y_l)`

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_001.png" class="img-fluid rounded z-depth-1" %}

### Problem Setting

- RM strengthê°€ LM performanceì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íƒìƒ‰

  - metric

- factuality

- relevance

- completeness

- P*{LM} = f (S*{RM}, Ï„ )

  - P\_{LM}: LM acc on task

  - S\_{LM}: RM acc on RM binary task

  - Ï„: RL training time

## 3. Experiment and Results

### Experimental Setting

- Models

  - LM: T5 (small, base, large)

  - RM: Longformer-base-4096

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_002.png" class="img-fluid rounded z-depth-1" %}

- Datasets

  - QA-FEEDBACK (3,853/500/948)

(Q, Gold, non-fact, â€¦)ê°€ ì¡´ì¬

- Training

  - PPO

  - RM list

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_003.png" class="img-fluid rounded z-depth-1" %}

- Critic LM: T5-base

- Reward hacking ë°©ì§€ë¥¼ ìœ„í•´ KLD(â†” ref LM)ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ training interrupt (ì•½ê°„ì˜ íœ´ë¦¬ìŠ¤í‹±)

### Results

### Are High-Accuracy and Deeply Trained Reward Models Always the Best?

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_004.png" class="img-fluid rounded z-depth-1" %}

- moderate accuracyì™€ appropriate number of trained stepsë¡œ í•™ìŠµëœ RMì´ ë†’ì€ LM performanceë¡œ ì´ì–´ì§„ë‹¤.

  - relevance: mitigating the risk of overfitting

  - factuality: prevent overfitting and ensure reliable outcomes

consistent across the T5-base and T5-large models

### How Do Best and Most Accurate Reward Models Differ?

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_005.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_006.png" class="img-fluid rounded z-depth-1" %}

- Relevance

â†’ `high-score`, `high-variance`

- Factuality

â†’ `high-score`, `less-variance`

- Completeness

â†’ `low-score`, `high-variance`

â‡’ ê³µí†µì ìœ¼ë¡œ ëª¨ë“  taskì— ëŒ€í•´ì„œ best-performance RMì€ varianceê°€ ë†’ìŒ.

ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´, ì´ ë§ì€ ê³§ RMì´ broader range of responsesì— ëŒ€í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨ := exploration â‡’ improving the quality of the generated text

(ê°œì¸ì ìœ¼ë¡œ, varianceê°€ í´ìˆ˜ ë°–ì— ì—†ëŠ” verifiable rewardê°€ ì´ë˜ì„œ LM performanceê°€ ì¢‹ì•˜ë‚˜..ë¼ëŠ” ìƒê°ì´ ë“¤ìŒ)

### How Do Best and Most Accurate Rewards Impact Models? (i.e., Role of KLD)

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_007.png" class="img-fluid rounded z-depth-1" %}

- Relevance

â†’ `low-score`, `low-variance`

relevanceì¸¡ë©´ì—ì„œ stable alignment

- Factuality

â†’ `high-score`, `low-variance`

factuality ì¸¡ë©´ì—ì„œ consistent yet varied alignment (refê°€ í‹€ë¦° ì‚¬ì‹¤ì„ ë§í•˜ê³  ìˆë‹¤ë©´)

- Completeness

â†’ `high-score`,`high-variance`

flexible approach suitable for evaluating complex texts (ì´ê±´ í•´ì„ì´ ì¢€..)

## 4. Conclusion

- RMì„ í‰ê°€í• ë•Œ accuracy ìì²´ë¡œë§Œ í‰ê°€í•˜ëŠ” ê²ƒì˜ í•œê³„ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ëª…í™•íˆ ë³´ì—¬ì¤€ ë…¼ë¬¸

# What Makes a Reward Model a Good Teacher? An Optimization Perspective

## 1. Introduction

- ì´ì „ ë…¼ë¬¸ì—ì„œ RL ê´€ì ì—ì„œ RMì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ accuracyë§Œìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒì˜ í•œê³„ë¥¼ ì§€ì í•˜ë©°, varianceì´ ë†’ì€ RMì´ ì˜¤íˆë ¤ ë” ë‚˜ì€ policy model performanceìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.

- ì´ ë…¼ë¬¸ì€ ì´ ë…¼ì˜ë¥¼ í™•ì¥í•´ ë‹¤ìŒì˜ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•˜ê³ ì í•¨.

**_â€œwhat makes a reward model a good teacher for RLHF?â€_**

(ìˆ˜í•™ì ìœ¼ë¡œ ë§ì€ ì¦ëª…ë“¤ì´ ìˆì§€ë§Œ, ì°¨ì¹˜í•˜ê³  ë…¼ë¬¸ì—ì„œ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ë°”ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.)

- \pi\_{\theta} (policy)ì—ì„œ ì¶©ë¶„íˆ ë†’ì€ í™•ë¥ ë¡œ rolloutí•œ outputì— ëŒ€í•´ì„œ ì–¼ë§Œí¼ ì˜ êµ¬ë¶„í•˜ëŠ”ê°€ = reward variance

- r_G (ground truth reward: ìš°ë¦¬ê°€ ì˜¬ë ¤ì•¼í•˜ëŠ” reward)

- r\_{rm} (proxy reward: policy modelì— ì˜í•´ í•™ìŠµë˜ëŠ” reward)

â‡’ low reward varianceëŠ” policy gradientë¡œ í•™ìŠµì‹œ r\_{rm}ë¿ë§Œ ì•„ë‹ˆë¼ r_Gë„ êµ‰ì¥íˆ ëŠë¦¬ê²Œ updateí•˜ê²Œ ë§Œë“ ë‹¤.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_008.png" class="img-fluid rounded z-depth-1" %}

## 2. Preliminaries

**Reward model training or selection**

- ìš°ë¦¬ì˜ ëª©í‘œ: E*{yâˆ¼Ï€*Î¸ (Â·|x)}[r_G(x, y)]

maximize the exp. ground truth reward

- Proxy ëª©í‘œ: r\_{RM} : X Ã— Y â†’ [âˆ’1, 1]

**Reward maximization via policy gradient**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_009.png" class="img-fluid rounded z-depth-1" %}

**Accuracy in RM**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_010.png" class="img-fluid rounded z-depth-1" %}

- RMì—ì„œì˜ accuracyë€ r*Gë‘ r*{rm} ì´ ê°™ì€ rankingìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆëƒ?ì´ë‹¤.

- ì¦‰,

- r\_{G}: 0.9 (x1) > 0.5 (x2) > 0.2 (x3)

- r\_{rm}: 0.54 (x1) > 0.51 (x2) > 0.49 (x3)

ì´ë©´ r\_{rm}ì˜ accëŠ” 1.0ì´ë‹¤.

- ì¼ë°˜ì ìœ¼ë¡œ RMì˜ accëŠ” off-policy bencmark (e.g., HH test set)ì—ì„œ ì´ë£¨ì–´ì§. ê·¸ëŸ¬ë‚˜ RMì´ ì ìš©ë˜ëŠ” ì‹œì ì€ on-policy ì‹œì  â‡’ ë…¼ë¬¸ì€ ì´ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì„œ ë¶„ì„ì„ ì§„í–‰.

**Reward Variance in RM**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_011.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_012.png" class="img-fluid rounded z-depth-1" %}

- \pi\_{\theta} (policy)í•˜ì—ì„œ ë°œìƒí•˜ëŠ” rolloutì„ RMì´ ì–¼ë§Œí¼ ì˜ êµ¬ë¶„í•˜ëŠ”ê°€?

## 3. Theory: Optimization Perspective on What Makes a Good Reward Model

### Technical Setting

- ë…¼ë¬¸ì—ì„œ ì¦ëª…ì„ ìœ„í•´ ì •ì˜í•œ policyì˜ generation ì‹

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_013.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_014.png" class="img-fluid rounded z-depth-1" %}

### Low Reward Variance Implies Slow Reward Maximization

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_015.png" class="img-fluid rounded z-depth-1" %}

- ì´ˆê¸° reward ëŒ€ë¹„ \gammaë§Œí¼ ê¸°ëŒ€ë³´ìƒì„ ì˜¬ë¦¬ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ tëŠ” reward varianceì˜ -1/3 ì œê³±ì— ë¹„ë¡€í•œë‹¤.

(ë…¼ë¬¸ì— ì¦ëª…ìˆìŠµë‹ˆë‹¤!)

- reward variance ğŸ”½Â â†’ RLHF loss ê³ ì°¨ë¯¸ë¶„ ğŸ”½Â = hessian ğŸ”½Â â†’ gradient normì´ ì»¤ì§€ëŠ”ê±° ë°©ì§€ â†’ í•™ìŠµ ë°©í•´

### More Accurate Reward Models Are Not Necessarily Better Teachers

- ì–´ë–¤ ì´ˆê¸° policy \pi\_{\theta(0)}ì— ëŒ€í•´ì„œë„,

  - acc =1ì¸ ì™„ë²½í•œ ë³´ìƒ ëª¨ë¸ r\_{\mathrm{RM}} ì¡´ì¬

  - acc â‰¤2/âˆ£Yâˆ£ ì¸ ë§¤ìš° ë¶€ì •í™•í•œ ë³´ìƒ ëª¨ë¸ r'\_{\mathrm{RM}} ì¡´ì¬

- ê·¸ëŸ°ë° r*{\mathrm{RM}}ì„ ì“°ë©´ t*\gammaê°€ **ë¬´í•œíˆ ì»¤ì§ˆ ìˆ˜ ìˆìŒ** (í•™ìŠµì´ ê·¹ë„ë¡œ ëŠë¦¼)

- ë°˜ë©´ r'_{\mathrm{RM}}ì„ ì“°ë©´ t_\gamma = O(\pi\_{\theta(0)}(y^\gamma|x)^{-1})ë¡œ í›¨ì”¬ ì§§ì„ ìˆ˜ ìˆìŒ

(ì „ì¬ ì¡°ê±´ì€ ê·¸ë˜ë„ r'_{\mathrm{RM}}(x,y^\gamma) > r'_{\mathrm{RM}}(x,y))

## 4. Experiments

- Ground truth reward.

- ArmoRMë¼ëŠ” ëª¨ë¸ì´ ì£¼ëŠ” rewardê°€ gt rewardë¼ê³  ê°€ì •

- Data.

- UltraFeedback (80: RM tr / 20: policy gradient)

- Ref.

- Pythia2.8B â†’ AlpacaFarm SFT

- Reward model

- On-Policy Data: 100%, 75%, 50%, 25%, 0% on-policy data samplingí•´ì„œ ArmoRMë¡œ labeling.

- Off-Policy Data: UltraFeedback

- Policy Gradient

- RLOO

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_016.png" class="img-fluid rounded z-depth-1" %}

- Reward varianceê°€ ë†’ì„ìˆ˜ë¡ proxy rewardëŠ” ë¹ ë¥´ê²Œ ì¦ê°€í•¨.

í•˜ì§€ë§Œ, reward varianceê°€ ë†’ì€ RMì´ë¼ê³  í•˜ë”ë¼ë„ í•´ë‹¹ RMì˜ ë³¸ì§ˆì ì¸ ë¶ˆì•ˆì •ì„± (GTì˜ accë¥¼ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ì§€ëŠ” ëª»í•¨ = Reward Hacking)ì´ ìˆê¸°ì— Ground Truth rewardë§Œí¼ì„ True Rewardë¥¼ ëª»ì˜¬ë¦¼

â‡’ Epoch3ê°€ Reward Hacking ì§€ì 

- Accuracyê°€ ë†’ê³  Reward varianceê°€ ë‚®ì€ RMì€ í•™ìŠµë„ ëŠë¦¬ë©° ì‹¤ì œ Ground Truth rewardë„ ê·¸ë ‡ê²Œ ë§ì´ ì˜¬ë¦¬ì§€ëŠ” ëª»í•¨

- ìš°ë¦¬ê°€ â€˜Ground Truth rewardâ€™ë¥¼ 100%ë¡œ ë°˜ì˜í•˜ëŠ” RMì„ ë°˜ì˜í•  ìˆ˜ ì—†ìœ¼ë‹ˆ ì‹¤ì œë¡œëŠ” reward varianceê°€ ë†’ì€ RMìœ¼ë¡œ on-policy trainingí•˜ê³  ëŠì–´ì£¼ëŠ”ê±´ ì¢‹ì€ RL optimizationì´ë‹¤.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_017.png" class="img-fluid rounded z-depth-1" %}

- Corrì„ ë³´ë©´, reward varianceëŠ” í•™ìŠµí•˜ê³ ì í•˜ëŠ” proxyë§ê³  ground truthë¥¼ targetí•˜ê¸°ì—ë„ ì¢‹ì€ feature

- ìš°ë¦¬ê°€ ìì£¼ ë´ì™”ë˜ off-policy acc (e.g., HH-test set)ì€ RLì‹œ RMì´ ë„ì›€ë˜ëŠ”ê°€?ì— ëŒ€í•œ ëŒ€ë‹µì´ ë˜ì§€ ëª»í•¨

- ë§ˆì§€ë§‰ ì§€í‘œëŠ” â€˜initial policyì— ëŒ€í•œ accuracyì™€ off-policy datasetì— ëŒ€í•œ accuracyë¥¼ reward varianceë¡œ í‰ê· ë‚¸ê±°â€™ë¼ëŠ”ë° í•´ì„ì€ ì˜ ëª»í–ˆìŠµë‹ˆë‹¤â€¦

{% include figure.liquid loading="eager" path="assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_018.png" class="img-fluid rounded z-depth-1" %}

- initial policyì˜ outputì— ëŒ€í•œ reward varianceë¥¼ ì¶©ë¶„íˆ í¬ê²Œ í•˜ëŠ” RMì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.

## 5. Conclusion

- on-policy training ìƒí™©ì—ì„œ off-policy accuracyë§Œìœ¼ë¡œ RMì˜ effectivenessë¥¼ í‰ê°€í•˜ë©´ optimizationì— ì•…ì˜í–¥ì„ ë¼ì¹¨ì„ ë³´ì„

- reward varianceëŠ” RMì˜ optimizationë¥¼ ë¯¸ë¦¬ ê°€ëŠ í•´ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì§€í‘œ

â†’ ground truth rewardë¥¼ ë³´ì¥í•´ì£¼ì§€ëŠ” ì•ŠìŒ.
