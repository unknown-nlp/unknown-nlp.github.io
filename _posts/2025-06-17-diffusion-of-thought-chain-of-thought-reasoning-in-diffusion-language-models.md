---
categories: paper-reviews
date: "2025-06-17 00:00:00"
description: " ë…¼ë¬¸ ë¦¬ë·° - Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion
  Language Models"
giscus_comments: true
layout: post
related_posts: false
tags: llm paper-review nlp
title: "Diffusion of Thought: Chain-of-Thought Reasoning in Diffusion Language Models"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2025-06-17
- **Reviewer**: ìƒì—½
- **Property**: Text Generation, DiffusionLM

<br/>

## Introduction

**ë„ì… ë°°ê²½**

- LLMì˜ ì—„ì²­ë‚œ ì„±ì¥ â†’ Chain-of-Thought (CoT)ì™€ ê°™ì€ Reasoningì´ í•µì‹¬ ê¸°ë²•ìœ¼ë¡œ ë¶€ìƒ

- CoTëŠ” ì¤‘ê°„ ì¶”ë¡  ë‹¨ê³„ë¥¼** autoregressive ë°©ì‹**ìœ¼ë¡œ ìƒì„±í•˜ì—¬ **LLMì˜ ì¶”ë¡  ëŠ¥ë ¥ì„ í–¥ìƒ**ì‹œí‚´

- í•˜ì§€ë§Œ **ê¸°ì¡´ CoTì˜ í•œê³„ì **ë“¤ì´ ì¡´ì¬

  - ì¤‘ê°„ ë‹¨ê³„ì˜ ì˜¤ë¥˜ê°€ ìµœì¢… ë‹µë³€ì— ì˜í–¥ì„ ë¯¸ì¹¨

  - ìê¸° êµì •(self-correction) ëŠ¥ë ¥ì˜ ë¶€ì¡±

  - íš¨ìœ¨ì„±ì— ëŒ€í•œ ìš°ë ¤

<br/>

**Diffusion Modelì˜ ë“±ì¥**

- Vision ì˜ì—­ì—ì„œì˜ ì„±ê³µì— ì´ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œë„ ì£¼ëª©ë°›ê¸° ì‹œì‘

- **Why?** Autoregressive model ëŒ€ë¹„ <span style='color:yellow_background'>ê³ ìœ í•œ ê°•ì </span>ì„ ë³´ìœ 

  - global planning ability

  - self correction

  - íš¨ìœ¨ì„± ê°œì„  ê°€ëŠ¥ì„± (ì´ê±´ ì¡°ê¸ˆ í™•ì¸ì´ í•„ìš”í•¨. ì ì ˆí•œ refëŠ” ì•„ë‹Œ ê²ƒ ê°™ìŒ. )

- <span style='color:yellow_background'>**Pre-trained diffusion language model**</span> â†’ Plaid, SEDD ë“± (ìµœê·¼ì—ëŠ” Llama3-8B ì •ë„ ìˆ˜ì¤€ì˜ LlaDA ëª¨ë¸ ë“±ì¥)

  - GPT-2 ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë‹¬ì„± (DoTëŠ” Neurips 2024 ë…¼ë¬¸)

  - Scaling lawì˜ ì ìš© ê°€ëŠ¥ì„± í™•ì¸

**RQ**

ğŸ’¡ **Can diffusion language models also leverage the CoT-style technique to gain
enhanced complex reasoning abilities?**

<br/>

**Diffusion of Thoughts (DoT) ì œì•ˆ**

- **Diffusion modelì— íŠ¹í™”ëœ inherent chain-of-thought ë°©ë²• ì œì•ˆ**

  - ì¼ë ¨ì˜ latent variablesë¥¼ ìŠ¤í…ë³„ë¡œ ì—…ë°ì´íŠ¸ â†’ ê° ì¶”ë¡  ë‹¨ê³„ë“¤ì´ ì‹œê°„ì— ë”°ë¼ ë³‘ë ¬ì ìœ¼ë¡œ diffuse

- í•µì‹¬ íŠ¹ì§•

  - **Multi-pass variant DoT**: causal biasë¥¼ ë§‰ê¸° ìœ„í•´ í•œ ë²ˆì— í•˜ë‚˜ì˜ thoughtë¥¼ ìƒì„±í•˜ëŠ” ë° ì´ˆì 

  - **Classifier-free guidance ì‚¬ìš©**: gradient-based classifier guidance ëŒ€ì‹  ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì œì–´ ì‹ í˜¸ ì œê³µ

  - **Training-time sampling**: Self-correcting ëŠ¥ë ¥ í–¥ìƒ

  - **Conditional ODE Solver**: continuous diffusion modelì˜ inference ê°€ì†

<br/>

## Preliminaries

**ê¸°ë³¸ ê°œë…**

- Forward process

  - $ q(\mathbf{z}_t|\mathbf{z}_{t-1}) $, t-1 ì‹œì ì˜ representationì— noiseë¥¼ ì£¼ì…

- Reverse process

  - $ z\_{t} $ë¥¼ denoisingí•˜ì—¬ $ z_0 $ë¥¼ ë³µêµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œ

  - $ z*t: p*{\theta}(\mathbf{z}_{0:T}) := p(\mathbf{z}\_T)\prod_{t=1}^T p*{\theta}(\mathbf{z}*{t-1}|\mathbf{z}\_t) $ ë¡œ ì›ë³¸ ë°ì´í„° ë³µì›

- Text generationì„ ìœ„í•œ diffusion ëª¨ë¸ì˜ ì¢…ë¥˜

  1.  <span style='color:yellow_background'>Continuous diffusion models</span>

      - mapping functionì„ í™œìš© (ì‹¤ìˆ˜ â†’ í† í°í™”)

      - discrete text $ w $ â†’ continuous space using $ \text{EMB}(w) $ â†’ **rounding** (inverse operation)

      - forward perturbations: $ q(\mathbf{z}_{t} \vert \mathbf{z}_{t-1}) = \mathcal{N}(\mathbf{z}_{t};\sqrt{1-\beta_t}\mathbf{z}_{t-1}, {\beta}\_t \mathbf{I}) $, where $ \beta_t \in (0, 1) $

  1.  <span style='color:yellow_background'>Discrete diffusion models</span>

      - ë¬¸ì œ ìì²´ë¥¼ integer programìœ¼ë¡œ í’€ê¸°

      - $ z_t $ë¥¼ ont-hot vectors in $ \{0, 1\}^K $ë¡œ í‘œí˜„. KëŠ” vocab size

      - $ q(\mathbf{z}_t|\mathbf{z}_{t-1}) $ì„ transition matrixë¡œ í‘œí˜„ â†’ uniform ë¶„í¬ë‚˜ [mask]ë¡œ ì „ë¶€ ë³€ê²½í•˜ëŠ” ë‹¨ê³„

**Seq2Seq Generation (e.g. DiffuSeq)**

- ì…ë ¥-ì¶œë ¥ sequenceë¥¼ single sequenceë¡œ ì²˜ë¦¬: $ \mathbf{w}^{z}=\mathbf{w}^{[x; y]} $

- Left-aligned mask $ [\mathbf{0};\mathbf{1}] $ë¡œ $ x, y $ë¥¼ êµ¬ë¶„

- **Partial noising**: mask valueê°€ 1ì¸ ë¶€ë¶„ì—ë§Œ noise ì ìš©

<br/>

## <span style='color:yellow_background'>Diffusion-of-Thoughts</span>

- Notation: $ s $ (problem statement), $ a $ (answer), $ p\_{\theta}^{LM} $ (language model)

- Answer-only generation model: $ \mathbf{a}\sim p\_\theta^{\textit{LM}}(\mathbf{a}|\mathbf{s}) $

- CoT: $ \mathbf{a}\sim p*\theta^{\textit{LM}}(\mathbf{a}|\mathbf{s}, \mathbf{r}*{1\dots n}) $

- implicit CoT: $ \mathbf{a}\sim p*\theta^{\textit{iCoT}}(\mathbf{a}|\mathbf{s}, \mathbf{z}*{1\dots n}) $

- DoT: $ \mathbf{a}\sim p\_\theta^{\textit{DoT}}(\mathbf{a}|\mathbf{s}, \mathbf{z}\_t) $

<br/>

<br/>

### <span style='color:yellow_background'>DoT Modeling</span>

- **Gradient-based token guidanceì˜ í•œê³„**

  - ì •í™•í•œ conditioning ì‹¤íŒ¨. íŠ¹íˆ ìˆ˜í•™ì  ì¶”ë¡ ê³¼ ê°™ì´ ì •í™•í•œ ìˆ«ìì™€ í† í°ì´ í•„ìš”í•œ ê³³ì—ì„œ ì¹˜ëª…ì 

  - ì˜ˆì‹œ: "Two trains" â†’ "**Three** trains"

**â†’ DiffuSeq-style classifier-free conditioning ì±„íƒ**

    - ëª¨ë“  rationaleë“¤ì´ backward diffusion processì—ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ìƒì„±

    - ëª¨ë“  conditional token, ì—¬ê¸°ì„œëŠ” $ s $ëŠ” ê³ ì •, $ r_{1...n} $ì—ë§Œ noise ì¶”ê°€

<span style='color:yellow_background'>â†’ continuous ë°©ì‹ì˜ DiffuSeq-styleì´ ê°€ì§„ ì¥ì ì´ ë¬´ì—‡ì¸ê°€?</span>

    - Gradient-based token guidanceëŠ” ë³„ë„ì˜ classifierë¥¼ í•™ìŠµ (ìµœê·¼ ëª¨ë¸ì—ì„œëŠ” LLM ë‚´ë¶€ì˜ ì—°ì‚°ì„ í™œìš©í•˜ê¸°ë„), ì—¬ê¸°ì„œ ì–»ì€ ì •ë³´ë¥¼ conditionìœ¼ë¡œ í•˜ì—¬ pì˜ ì‚¬í›„ í™•ë¥ ì„ ì¡°ì ˆí•˜ëŠ” ê°„ì ‘ì ì¸ ë°©ì‹

    - DiffuSeq ë°©ì‹ì€ ëª¨ë¸ ìì²´ì—ì„œ condition (ì´ì „ z)ë¥¼ denoisingí•˜ëŠ” ê³¼ì •ì—ì„œ ìƒ˜í”Œ ë¶„í¬ ìì²´ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ê²ƒìœ¼ë¡œ ë” í™•ì‹¤í•œ ë³€í™”ê°€ ê°€ëŠ¥

<br/>

**Multi-pass DoT (MP-DoT)**

- Causal inductive bias ë„ì… thought-by-thought ë°©ì‹ìœ¼ë¡œ rationalesì„ ìƒì„±í•˜ëŠ” ë°©ë²• ì œì•ˆ

- **Process**:

  1.  $ \mathbf{r}_1\sim p_{\theta}^{\textit{DoT}}(\mathbf{r}\_1|\mathbf{s}, \mathbf{z}^{r_1}\_t) $

  1.  $ \mathbf{r}_2\sim p_{\theta}^{\textit{DoT}}(\mathbf{r}\_2|[\mathbf{s};\mathbf{r}_1], \mathbf{z}^{r_2}\_t) $

  1.  $ \mathbf{a}\sim p\_{\theta}^{\textit{DoT}}(\mathbf{a}|[\mathbf{s};\mathbf{r}_1;...;\mathbf{r}_n], \mathbf{z}^{r_n}\_t) $

- ì´í›„ rationaleì´ ì´ì „ rationaleë“¤ì„ ë” ê°•í•œ condition signalë¡œ ì´ìš©í•  ìˆ˜ ìˆìŒ.

<br/>

### <span style='color:yellow_background'>Training</span>

**Scheduled Sampling**

- Diffusion ëª¨ë¸ì´ denoisingì„ í•˜ëŠ” ê³¼ì •ì—ì„œ ì´ë¯¸ self-correcting ëŠ¥ë ¥ì´ ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŒ. â†’ Sampling ê³¼ì •ì„ í†µí•´ ì´ë¥¼ ë”ìš± ë°œì „

- Trainingê³¼ inference ê°„ì˜ **exposure bias**ê°€ errorë¥¼ ë°œìƒì‹œí‚¨ë‹¤ê³  ìƒê°

- Any timesteps: $ s, t, u $ that satisfy $ 1 < s < t < u < T $

  - Training stage: $ \mathbf{z}\_t \sim q(\mathbf{z}\_t|\mathbf{z}\_0) $ (oracle dataì—ì„œ diffuse)

  - Inference stage: $ q(z*t|z*{\theta}(z_u;u)) $

- **í•´ê²°ì±…**: ì¶”ë¡  ë‹¨ê³„ë¥¼ ëª¨ë°©í•˜ê¸° ìœ„í•´ $ \epsilon_i $ í™•ë¥ ë¡œ ë‹¤ìŒê³¼ ê°™ì´ forward stepì—ì„œ ë§Œë“¤ì–´ì§„ $ z $ë¥¼ í™œìš©

  - $ u \in \{t+1, ..., T\} $, $ \hat{z*0} = z*{\theta}(z_u;u) $ â†’ $ q(z_t|\hat{z_0}) $

  - $ \epsilon*i $ëŠ” 1ì—ì„œ $ \epsilon*{min} $ë¡œ ì„ í˜• ê°ì†Œ

<br/>

**Coupled Sampling**

- Multi-pass DoTì—ì„œ rationaleì— ìŒ“ì´ëŠ” error accumulation ë¬¸ì œ í•´ê²°

- **Training ì‹œ í˜„ì¬ thoughtë¿ë§Œ ì•„ë‹ˆë¼ ì´ì „ thoughtë“¤ì—ë„ í™•ë¥ ì ìœ¼ë¡œ noise ì¶”ê°€**

  - $ \mathbf{z}_0=\text{EMB}([\mathbf{s};\mathbf{r}_{1};\mathbf{r}\_{2}]) $ ê³¼ì •ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ $ r_1 $ì—ë§Œ noise ì ìš©

  - ì¼ì • í™•ë¥ ë¡œ $ [r_1;r_2] $ ëª¨ë‘ì— noise ì ìš©

<br/>

**Training Objective**

DoT ëª¨ë¸ì— ëŒ€í•´ ë‘ ê°€ì§€ í•™ìŠµ ë°©ë²•ì„ ì‚¬ìš©

- from scratch

- fine-tuning from pre-trained diffusion model

<br/>

**ê³µí†µ Objective function:** Negative variational lower bound ìµœì†Œí™”

- $ z_t $ë¥¼ denoising í•¨ìœ¼ë¡œì¨ $ z_0 $ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì„ ë°°ìš°ëŠ” ê²ƒ

$$ \mathcal{L}_{\text{VLB}}(\mathbf{w}^z)=\mathbb{E}{q({\mathbf{z}\_0}\mid \mathbf{w}^z)} \left[ \underbrace{\log\frac{ q(\mathbf{z}\_T|\mathbf{w}^z)}{p_{\theta}(\mathbf{z}_T)}}_{\text{Prior loss}} + \underbrace{\mathcal{L}_{\text{VLB}}(\mathbf{z}\_0)}_{\text{Diffusion loss}} \underbrace{-\log p*\theta(\mathbf{w}^z|\mathbf{z}\_0)}*{\text{Rounding loss}}\right] $$

$$ \mathcal{L}_{\mathrm{VLB}}\left(\mathbf{z}\_0\right)=\mathbb{E}_{q\left(\mathbf{z}_{1: T} \mid \mathbf{z}\_0\right)}[\underbrace{\log \frac{q\left(\mathbf{z}\_T \mid \mathbf{z}\_0\right)}{p_\theta\left(\mathbf{z}_T\right)}}_{\mathcal{L}_T}+\underbrace{\sum_{t=2}^T \log \frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}\_0, \mathbf{z}\_t\right)}{p_\theta\left(\mathbf{z}_{t-1} \mid \mathbf{z}\_t\right)}}_{\mathcal{L}_{T-1}+\cdots+\mathcal{L}\_1}-\underbrace{\log p_\theta\left(\mathbf{z}_0 \mid \mathbf{z}\_1\right)}_{\mathcal{L}\_0}] $$

- **Prior loss**

  - $ p\_{\theta}(z_T) $: ìµœì¢… noiseì—ì„œ ëª¨ë¸ì˜ ë¶„í¬

  - $ q(z_T|w^z) $: ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì—ì„œ ë§Œë“¤ì–´ì§„ ìµœì¢… zì˜ ë¶„í¬

  â†’ ì´ìƒì ìœ¼ë¡  ë‘˜ì´ ë™ì¼í•´ì ¸ì•¼ í•˜ë©° prior lossëŠ” 0ì´ ë˜ì–´ì•¼ í•¨.

  â†’ ë” ì§ê´€ì ìœ¼ë¡œ) ì¶©ë¶„íˆ ë§ì€ noiseë¥¼ ì£¼ì…í•˜ë©´ ìµœì¢… noise ë¶„í¬ $ \mathcal{N(0, I)} $ê°€ ë˜ì–´ì•¼ í•¨.

- **Diffusion loss**: ê° ë‹¨ê³„ì—ì„œ ì–¼ë§ˆë‚˜ noiseë¥¼ ì˜ ì œê±°í•˜ëŠ”ê°€ì— ëŒ€í•œ íƒìƒ‰

  - ìš°ë¦¬ê°€ ê¶ê¸ˆí•œ ê²ƒ: **pë¥¼ í†µí•œ denoisingì´ ì˜ ëœ ê²ƒì´ ë§ì„ê¹Œ? == **$ p*{\theta}(z*{t-1}|z_t) $** ë¶„í¬ë¥¼ ì˜ êµ¬í–ˆëŠ”ê°€?**

  - ìš°ë¦¬ê°€ ì•„ëŠ” ê²ƒ, $ z_t $ (í˜„ì¬ ì£¼ì–´ì§„ ì •ë³´), $ z_0 $ (ì›ë³¸)

  - Posteriorë¥¼ í™œìš©, ë‹¤ìŒì˜ ë¶„í¬ë¥¼ ì´ìš©í•´ $ p\_{\theta} $ë¥¼ ê²€ì •

  $$ q\left(\mathbf{z}_{t-1} \mid \mathbf{z}\_t, \mathbf{z}\_0\right)=q\left(\mathbf{z}\_t \mid \mathbf{z}_{t-1}, \mathbf{z}_0\right) \frac{q\left(\mathbf{z}_{t-1} \mid \mathbf{z}\_0\right)}{q\left(\mathbf{z}\_t \mid \mathbf{z}\_0\right)} $$

  - ë” ì§ê´€ì ìœ¼ë¡œ $ z\_{t-1} $ì˜ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ noise, denoise ê³¼ì •ì—ì„œ ë™ì¼í•œê°€

- **Rounding loss**: ë³µì›ë ¥ $ z_0 $ â†’ $ \text{w}^z $

<br/>

### Inference Strategy

- diffusion ëª¨ë¸ì˜ ì¶”ë¡  flexibilityëŠ” í° ì¥ì  â†’ ì–´ë ¤ìš´ ë¬¸ì œì¼ìˆ˜ë¡ ë” ë§ì€ reasoning timeì„ ê°€ì ¸ì•¼ í•¨. â†’ backward timestep Të¥¼ í¬ê²Œ ê°€ì ¸ê°€ì! (ì´ê±° ì•ˆë˜ëŠ” ê²Œ ìˆë‚˜? ë…¼ë¬¸ì—ì„œ autoregressive ë°©ë²•ì—ì„œ í† í° ìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê²ƒì€ ë” ì–´ë µë‹¤ê³  ì£¼ì¥.)

- **ë¬¸ì œ**: Continuous diffusionì˜ ë†’ì€ timestep ìš”êµ¬ì‚¬í•­ (ì˜ˆ: Plaid 4096 timesteps)

â†’ ODE solverë¥¼ conditional formì„ í™œìš©í•´ accelerate

$$ \mathbf{y}{t*i} = \frac{\sigma*{t*i}}{\sigma*{t*{i-1}}}\mathbf{y}*{t*{i-1}} - \alpha*{t*i}(e^{-h_i} - 1)\tilde{\mathbf{z}}*\theta(\mathbf{z}_{t_{i-1}}, t\_{i-1}) $$

- ~~ì´ê²Œ ìµœì¢…ì‹ì¸ë° ë¯¸ë¶„ë°©ì •ì‹ ì–˜ê¸°ê°€ ë‚˜ì™€ì„œ ì•„ì§ì€ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤â€¦.~~

<br/>

**Self-consistency Integration**

- Multiple samplingì„ í†µí•œ ë‹¤ì–‘í•œ reasoning pathway ìƒì„±

- ë™ì¼ ë¬¸ì œ $ s $ì— ëŒ€í•´ ë‹¤ì–‘í•œ $ (r\_{i;1...n}, a_i) $ë¥¼ êµ¬í•¨. (<span style='color:yellow_background'>Diffusion ëª¨ë¸ì˜ ê°•ì : noise seedë§Œ ë‹¤ë¥´ê²Œ í•´ë„ ë¨!</span>)

- Majority vote:

<br/>

## Evaluation

### Experimental Setup

**ë°ì´í„°ì…‹ ë° ë©”íŠ¸ë¦­**

- **Simple reasoning**:

  - 4Ã—4, 5Ã—5 digit multiplication (BIG-bench)

  - Boolean logic reasoning (DyVal)

- **Complex reasoning**: GSM8K grade school math problems

<br/>

**Base Model**

- **From scratch**: Following DiifuSeq (12-layer Transformer encoder, 124M)

- **Pre-trained model for fine-tuning**:

  - Plaid (1.3B): OpenWebTextì—ì„œ í›ˆë ¨, GPT-2 ìˆ˜ì¤€ perplexity

  - SEDD-small (170M), SEDD-medium (424M)

<br/>

**Baseline**

- Answer-only, CoT, Implicit CoT

- GPT-2 (small 124M, medium 355M, large 774M)

- ChatGPT (gpt-3.5-turbo-1106) with 5-shot CoT

<br/>

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**

- Tokenization: ëª¨ë“  digitì„ ê°œë³„ í† í°ìœ¼ë¡œ ì²˜ë¦¬

- MP-DoT: ë§ˆì§€ë§‰ thought ë’¤ì— `<EOS>` í† í° ì¶”ê°€ (ëª¨ë¸ì´ rationale ìˆ˜ ê²°ì •)

- 8 \* V100-32G

- Training:

  - scheduled sampling: $ \epsilon\_{min}=0.95 $

  - coupled sampling: $ \gamma $ (0.01, noise ì¶”ê°€í•  í™•ë¥ ), $ k $ (1, ì´ì „ step)

  - self-consistency: $ m $ (20)

- Inference:

  - temperature 0.5, default timestep $ T $: 64

<br/>

## Results

**Digit Multiplication & Boolean Logic**

- DoT: 100% ì •í™•ë„ ë‹¬ì„±, ì´ëŠ” CoTë¥¼ í™œìš©í•˜ë©´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€

- ì†ë„ì—ì„œ GPT-2 CoT ëŒ€ë¹„ ìµœëŒ€ 27ë°° ë¹ ë¦„.

- **Optimal sampling timestep: 1 for multiplication, 2 for boolean logic **(very EZ)

- ChatGPTì™€ Implicit CoTë„ 100% accuracy ë‹¬ì„± ì‹¤íŒ¨

â†’ ê°„ë‹¨í•œ ì‘ì—…ì—ì„œ DoTì˜ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„± ë™ì‹œ ë‹¬ì„±

<br/>

**Results on Complex Reasoning (GSM8K)**

- **From-scratch training**: ~5% accuracy (pre-trained capabilityì˜ ì¤‘ìš”ì„± í™•ì¸)

- **Fine-tuned DoT**: ì—„ì²­ë‚œ ì„±ëŠ¥ í–¥ìƒ

  - SEDD-medium DoT > similar-sized GPT2-medium CoT (10%ê¹Œì§€ ì°¨ì´)

  - DoT-SEDD-medium (424M) > GPT2-medium (355M) + CoT

- **Multi-pass DoT**

  - Plaidì—ì„œ single-pass ëŒ€ë¹„ ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒ, íš¨ìœ¨ì„±ì€ single-passê°€ ìš°ìˆ˜

  - (ì„±ëŠ¥ë„ ë‚®ê³  throughputë„ ë‚®ì•„ì„œ ì´ê±´ ì™œ í•œ ê²ƒì¸ê°€â€¦)

- **Self-consistency**: DoT ëª¨ë¸ì—ì„œ í° ì„±ëŠ¥ í–¥ìƒ

<br/>

**Ablation Study**

- Sampling ë°©ë²•ì€ íš¨ê³¼ì 

- Continue pre-training (Gradient-based token guidance)ë¥¼ í™œìš©í•  ê²½ìš° 0.5%ê°€ ë¨. (pre-trained ëª¨ë¸ë³´ë‹¤ë„ ê°ì†Œ)

- **Gradient-based conditioning ì‹¤íŒ¨ ì‚¬ë¡€**:

- **Conditional ODE solver**: 4096 â†’ 8 timestep ì •ë„ì—ì„œ ê²°ê³¼ ìˆ˜ë ´

<br/>

<span style='color:yellow_background'>**Reasoning-Efficiency Trade-off**</span>

- left-to-right ë°©ì‹ì˜ reasoningì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ ì•„ì´ë””ì–´ (prompting, decoding ë“±)

- Diffusionë„ ì´ëŸ¬í•œ reasoning capabilitiesë¥¼ ì¦ê°€ì‹œí‚¤ê¸° ìœ„í•œ í•˜ë‚˜ì˜ ë°©ì•ˆ

â†’ ë” ë§ì€ timesteps â†’ ë” ë§ì€ reasoning â†’ íš¨ìœ¨ì„± ê°ì†Œ

- Efficiency trade-offë¥¼ í™•ì¸

- DoTëŠ” timestep, ë‚˜ë¨¸ì§€ ë°©ë²•ì€ output tokenì˜ ìˆ˜ë¥¼ stepìœ¼ë¡œ ìƒê°

- **Simple task (5Ã—5)**: 1 reasoning stepìœ¼ë¡œ 100% accuracy, ì¶”ê°€ computation ë¶ˆí•„ìš”

- **Complex task (GSM8K)**: Timestep ì¦ê°€ì— ë”°ë¥¸ ì§€ì†ì ì¸ ì„±ëŠ¥ í–¥ìƒ

  - DoT-SEDD-mediumì€ outperform

  - DoT-SEDD-smallì˜ ê²½ìš°, 32 timestepì—ì„œ GPT2-medium CoT ëŠ¥ê°€, T=64ì—ì„œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±

- **Diffsuion ëª¨ë¸ì˜ ì¥ì : **ì‘ì—… ë‚œì´ë„ì— ë”°ë¼ timestepsì„ ììœ ë¡­ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.

- **Autoregressive í•œê³„**: CoTì™€ Implicit CoTëŠ” token-by-token prediction íŠ¹ì„±ìƒ ëª…í™•í•œ ì¡°ì ˆì´ ì–´ë ¤ì›€.

<br/>

<span style='color:yellow_background'>**Self-consistency in DoT**</span>

- sampling ìˆ˜ ì¦ê°€ì— ë”°ë¥¸ ì§€ì†ì  ê°œì„ 

- Autoregressive modelê³¼ ë‹¬ë¦¬ decoding algorithm ì—†ì´ë„ ìì—°ìŠ¤ëŸ¬ìš´ diversity ì œê³µ (NoiseëŠ” ë‹¤ë¥´ë‹ˆê¹!)

<br/>

<span style='color:yellow_background'>**Self-correction Capability**</span>

- Autoregressive CoTì™€ ëª…í™•íˆ ë‹¤ë¥¸ self-correction ëŠ¥ë ¥ì„ ë³´ì„

**Case 1: Fast thinking**

- ì‰¬ìš´ ë¬¸ì œì˜ ê²½ìš°, ëª¨ë“  ì˜¬ë°”ë¥¸ thoughtë¥¼ single reasoning stepìœ¼ë¡œ ë„ì¶œ

- ë‘ ë²ˆì§¸ Stepì—ì„œ ì •í™•í•œ ìµœì¢… ë‹µë³€ ìƒì„±

**Case 2: Slow thinking**

- ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ ë¬¸ì œ ì´ˆê¸°ì—ëŠ” ì—ëŸ¬ê°€ ë°œìƒ

- í›„ì† ë‹¨ê³„ì—ì„œ ì ì§„ì  refinementë¥¼ í†µí•œ ì •í™•í•œ ë‹µë³€ ë„ì¶œ

- ì´ˆê¸°ì— ë¬¸ì œì˜ ëŒ€ëµì ì¸ outlineì„ ì¡ê³  refine & improveí•˜ëŠ” ê²ƒì€ ì‚¬ëŒì˜ ë³µì¡í•œ ì‘ì—… ìˆ˜í–‰ ë°©ì‹ê³¼ ìœ ì‚¬

**Case 3: Non-sequential correction**

- Step 4: ì˜ëª»ëœ ì¤‘ê°„ thought `<2*3=4>` ì¡´ì¬í•˜ì§€ë§Œ ì´í›„ thoughtì™€ ë‹µë³€ì€ ì •í™•

- Step 5: ì˜ëª»ëœ ì¤‘ê°„ thought êµì •

- **í•µì‹¬ íŠ¹ì§•**: Left-to-right paradigmì„ ë²—ì–´ë‚˜ ì¢Œìš°ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìˆ˜ì •

<br/>

## Conclusion

- Diffusion modelì— íŠ¹í™”ëœ ìµœì´ˆì˜ CoT reasoning ë°©ë²• DoT ì œì•ˆ

- Scheduled sampling, coupled sampling, conditional ODE solver ë“± ê³ ìœ  ê¸°ë²• ê°œë°œ

- Mathematical reasoning taskì—ì„œ comprehensive evaluation ìˆ˜í–‰

---

- **Current limitations**: Pre-trained diffusion modelì˜ scaleê³¼ generalization í•œê³„ (ì œí•œëœ ëª¨ë¸ í¬ê¸°)

- **Future promise**: ë” ê°•ë ¥í•œ pre-trained modelê³¼ í•¨ê»˜ autoregressive LLMì— í•„ì í•˜ëŠ” ì„±ëŠ¥ ê¸°ëŒ€
