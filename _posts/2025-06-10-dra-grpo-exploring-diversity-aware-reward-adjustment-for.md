---
categories:
  - paper-reviews
date: "2025-06-10 00:00:00"
description: ë…¼ë¬¸ ë¦¬ë·°
giscus_comments: true
layout: post
related_posts: false
tags:
  - embedding
  - fine-tuning
  - language-model
  - llm
  - paper-review
  - reasoning
thumbnail: assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/thumbnail.jpg
title: "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training
  of Large Language Models"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2025-06-10
- **Reviewer**: ê±´ìš° ê¹€

# Abstract

- ìµœê·¼ì— post-trainingì„ ìœ„í•œ RLì—ì„œ **GRPO**ì™€ ê°™ì´ low-resource settingsì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ

- ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reward computation ê³¼ì •ì—ì„œ semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ” ë°©ë²•ì¸ **Diversity-aware Reward Adjustment (DRA)**ë¥¼ ì œì•ˆí•¨

- DRAëŠ” Submodular Mutual Information (SMI)ë¥¼ í™œìš©í•˜ì—¬

- 5ê°œ Mathematical Reasoning benchmarkì—ì„œ recent methods ëŒ€ë¹„ outperform ì„±ëŠ¥ ë³´ì—¬ì¤Œ

# 1. Introduction

DeepSeek-R1-Zero (Guo et al., 2025)ì—ì„œ ê¸°ì¡´ LLMì— SFTë¥¼ ì ìš©í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜, base LMì— ë°”ë¡œ RLì„ ì ìš©í•  ìˆ˜ ìˆëŠ” R1-Zero training pipelineì„ ì œì•ˆí•¨.

â†’ Group Relative Policy Optimization (GRPO) ì•Œê³ ë¦¬ì¦˜ ë•ë¶„ì— ê°€ëŠ¥í•œ ë°©ë²•

GRPOëŠ” PPOì™€ ë‹¤ë¥´ê²Œ critic model ì—†ì´ ì£¼ì–´ì§„ promptì— ëŒ€í•´ ì—¬ëŸ¬ samplingëœ completionsì˜ relative performanceì— ëŒ€í•œ advantageë¥¼ í‰ê°€í•¨.

í•˜ì§€ë§Œ ìµœê·¼ì— ê³µê°œëœ GRPO ë° ê·¸ variants (e.g,. DR. GRPO)ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì •ë‹µ ì—¬ë¶€ì™€ ê°™ì€ **solution-levelì˜ scalar reward signalsì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆì–´, ê°™ì€ ì •ë‹µì´ë¼ë„ diverse reasoning pathì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨**.

â†’ ì´ëŠ” semanticí•˜ê²Œ ë‹¤ë¥¸ completionsë“¤ì´ ì˜¬ë°”ë¥´ê±°ë‚˜ í‹€ë¦° ê²½ìš° ëª¨ë‘ ê±°ì˜ ë™ì¼í•œ rewardsë¥¼ ë°›ì•„, ì˜ë¯¸ ìˆëŠ” reasoning ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” **indistinguishable advantage estimates**ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆìŒ

â†’ ë˜í•œ, ì´ëŠ” resource-constrained settingsì—ì„œ ë” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_000.png" class="img-fluid rounded z-depth-1" %}

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì €ìë“¤ì€ **Diversity-aware Reward Adjustment (DRA)**ë¥¼ ì œì•ˆí•¨.

ì´ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ samplingëœ completions ê°„ì˜ _semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ_ ê·¸ë£¹ ë‚´ ë‹¤ë¥¸ *completionsê³¼ì˜ semantic similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° outputì˜ rewardë¥¼ reweight*í•¨.

- **diverse completionsì—ëŠ” ë” ë†’ì€ weight, ì¤‘ë³µëœ completionì—ëŠ” ë” ë‚®ì€ weight ë¶€ì—¬**

# 2. Method

### Preliminary

LMì˜ generationì€ token-level Markov Decision Processë¡œ ë³¼ ìˆ˜ ìˆìŒ. ê° generation step tì—ì„œ state s*tëŠ” input question qì™€ ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ partial output sequence o*{<t}ì˜ concatenationì´ê¸°ì—, satesëŠ” ë‹¤ìŒê³¼ ê°™ìŒ s*t=[q;o*{<t}].

policy \pi*{\theta}(.|s_t)ëŠ” vocab set Aì—ì„œ next token o_të¥¼ ì„ íƒí•˜ê³ , ì´ëŠ” deterministic transitionì„ ìœ ë„í•˜ì—¬ next state s*{t+1}=[s_t;o_t]ë¡œ ì´ë™í•¨.

GRPOëŠ” ê° question qì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ responses C={o_1,...o_G}ë¥¼ samplingí•˜ê³ , ê° responseì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•¨ R={R(q,o_1), ... , R(q,o_G)}

ê³„ì‚°ëœ reward Rì„ ì´ìš©í•´ advantage A\_{i,t}ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•¨ (normalize)

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_001.png" class="img-fluid rounded z-depth-1" %}

GRPOì˜ objective function J*{GRPO}(\pi*{\theta})ë¥¼ optimizeí•¨

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_002.png" class="img-fluid rounded z-depth-1" %}

ì´í›„ ì—°êµ¬ì¸ DR.GRPO (Liu et al., 2025)ì—ì„œëŠ” token efficiencyë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ **GRPOì˜ objective functionì—ì„œ â€˜response lengthâ€™ termê³¼ Advantageì—ì„œ stdë¡œ normalizeí•´ì£¼ëŠ” termì„ ì§€ì›€**

### Diversity-Quality Inconsistency

GRPOì™€ DR.GRPOì˜ reward signalì€ **solution-level correctness**ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° completionì— ëŒ€í•´ **sparse scalar judgement**ë¥¼ ê³„ì‚°í•¨.

â†’ ì´ëŸ¬í•œ scalar rewardëŠ” ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ëŠ” diverse reasoning-pathë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, Diversity-Quality Inconsistencyê°€ ë°œìƒí•¨.

ìœ„ì— Example ë§ê³ , ë³´ë‹¤ ì‹¤ì¦ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ statement (â€**_reward alone fails to reflect the underlying variability in reasoning strategies_**â€) ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ embedding distancesë¡œ ì¸¡ì •ëœ completionsì˜ structural dissimilarityë¥¼ ê³„ì‚°í•¨.

- Spearmanâ€™s rank correlationì„ ì‚¬ìš©í•˜ì—¬ sampled completions ì‚¬ì´ì—ì„œ reward differenceì™€ semantic distanceë¥¼ ì¸¡ì •í•¨ â†’semantic distanceê°€ ì»¤ì§ˆìˆ˜ë¡ reward ì°¨ì´ë„ ì»¤ì§€ëŠ”ê°€?

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_003.png" class="img-fluid rounded z-depth-1" %}

- Figure2ëŠ” Spearmanâ€™s rank correlationì˜ p-valuesì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ p-valueê°€ significance levelì¸ 0.05 ë³´ë‹¤ í° ê°’ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œë¡œ 80% ì´ìƒì˜ promptì— ëŒ€í•´ statistically significant correlationì´ ì—†ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

### Diversity-aware Reward Adjustment

Diversity-Quality Inconsistency ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° sampleì˜ relative diversity/redundancyì— ë”°ë¼ rewardë¥¼ reweightí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨.

**â†’ diverse completionsì€ ë” ë†’ì€ weight, ì¤‘ë³µëœ responseëŠ” ë‚®ì€ weight**

ë¨¼ì € ê¸°ì¡´ì˜ reward R(q,o_i)ë¥¼ diversity-aware adjusted reward \tilde{R}(q,o_i) (í‹¸ë‹¤ í‘œì‹œ ì–´ë–»ê²Œ í•˜ë‚˜ìš”â€¦) ìœ¼ë¡œ ëŒ€ì²´í•¨

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_004.png" class="img-fluid rounded z-depth-1" %}

- SMI({o_i},C \ {o_i})ëŠ” completion o_iì™€ ë‚˜ë¨¸ì§€ group C \ o_i ê°„ì˜ Submodular Mutual Informationì„ ë‚˜íƒ€ëƒ„

- Submodular functionsì€ diminishing returns íŠ¹ì„±ì„ ê°–ìœ¼ë©°, diversityì™€ redundancyë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ

- SMIëŠ” ë‘ ì§‘í•© ê°„ì˜ shared informationì„ ì •ëŸ‰í™”í•˜ë©° (Iyer et al., 2021a,b)ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•¨

- SMIë¥¼ ì‰½ê²Œ ë§í•˜ë©´ â€œ**íŠ¹ì • completion í•˜ë‚˜ê°€ group ë‚´ ë‹¤ë¥¸ completionê³¼ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€**â€ë¥¼ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê°’

- Submodular í•¨ìˆ˜ëŠ” ìˆ˜í•™ ê°œë…ìœ¼ë¡œ â€œìƒˆë¡œìš´ elementê°€ ê¸°ì¡´ì— ë¹„ìŠ·í•œê²Œ ë§ì„ìˆ˜ë¡ ê¸°ì—¬ë„ê°€ ì¤„ì–´ë“œëŠ” ì„±ì§ˆâ€ì„ ê°–ê³  ìˆìŒ

â†’ ì´ë ‡ê²Œ ìƒˆë¡œìš´ rewardë¥¼ êµ¬í•˜ëŠ” ì—°ì‚°ì€ Pytorchì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_005.png" class="img-fluid rounded z-depth-1" %}

## 3. Experiment

### 3.1 Experimental Setup

**Training Dataset: **

**Evaluation Dataset: **

**Baselines**:

- general purpose large model: Llama-3.1-70B-Instruct, o1-preivew

- Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL

- Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS

**Implementations:**

- ë³¸ ì—°êµ¬ëŠ” DRAì˜ proof-of-conceptë§Œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ëª©ì ì´ê¸°ì— DeepSeek-R1-Distill-Qwen-1.5Bë¥¼ base modelë¡œ ë‘ì–´ í•™ìŠµì‹œí‚´

- 4 x A100 (40GB) GPUs

### 3.2 Empirical Analysis

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_006.png" class="img-fluid rounded z-depth-1" %}

**Main Results**

- DRA-DR.GRPOëŠ” avg accê°€ 58.2%ë¡œ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜´ (DRA-GRPOì—­ì‹œ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜´)

- DRA-GRPOì™€ DRA-DR.GRPOëŠ” fine-tuning samplesì„ 7,000ê°œ ë°–ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  40,000ê°œ ì‚¬ìš©í•œ DeepScaleR-1.5B-previewë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë³´ì—¬ì¤Œ

**Ablation Study**

- Base modelì¸ DeepSeek-R1-Distill-Qwen-1.5Bì™€ ë¹„êµí•˜ì—¬ DRA-GRPO, DRA-DR.GRPOëŠ” ê°ê° 7.8%, 9.3% ì„±ëŠ¥ í–¥ìƒë˜ê³  ë‹¨ìˆœ RL (GRPO, DR.GRPO) ëŒ€ë¹„ 1.9%, 2.2% í–¥ìƒ

**Efficiency**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_007.png" class="img-fluid rounded z-depth-1" %}

DRAëŠ” completionsì„ encoding í•´ì•¼í•˜ê¸°ì— over-headê°€ ì¡´ì¬í•˜ì§€ë§Œ, ë³„ë¡œ í¬ì§€ ì•ŠìŒ.

â†’ ì €ìë“¤ì´ ì‹¤í—˜ì— ì‚¬ìš©í•œ GPUìŠ¤í™ì¸ (A100-40GB)ì—ì„œëŠ” ì–´ì°¨í”¼ DRA ì—†ì´ë„ mini-batchë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•´ì„œ DRA ì ìš©í•˜ëŠ” ê²ƒì´ ë³„ ë¬¸ì œê°€ ë˜ì§€ ì•Šë‹¤ê³  í•˜ëŠ”ë°â€¦. â†’ ğŸ¶Â ğŸ”ŠÂ ë¼ê³  ìƒê°í•©ë‹ˆë‹¤

**Training Cost**

500 steps í•™ìŠµì‹œì¼œ 12.5hr ì†Œìš”ë¨ â‡’ $55 ë¹„ìš©

â†’ ë‹¤ë¥¸ ë°©ë²•ëŒ€ë¹„ íš¨ìœ¨ì ì„

{% include figure.liquid loading="eager" path="assets/img/posts/2025-06-10-dra-grpo-exploring-diversity-aware-reward-adjustment-for/image_008.png" class="img-fluid rounded z-depth-1" %}

### 3.3 Discussion

**Exploration-exploitation Balance**

DRAëŠ” Exploration-exploitation balanceë¥¼ policy gradient ì•ˆì— ì§ì ‘ í†µí•©í•˜ì—¬ ì ìš©í•¨

- Base rewardëŠ” high scoreë¥¼ ë°›ëŠ” completionì„ reinforceí•¨

- Diversity weightingì€ semantically novel completionì— learning signalì„ amplify

ì´ëŸ¬í•œ íƒìƒ‰ì€ low-resource settings (promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” ì‘ë‹µ ìˆ˜ê°€ ì œí•œ ì ì¸ ê²½ìš°)ì—ì„œ ì¤‘ìš”í•¨

â†’ DRAëŠ” mode collapseë¥¼ ë°©ì§€í•˜ê³  ë” ë„“ì€ reasoning strategiesë¥¼ ìœ ë„í•¨

**Ad-hoc vs Post-hoc Diversity**

generated completionsê°„ì˜ diversityë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ Ad-hoc, Post-hoc ë°©ì‹ì´ ìˆìŒ

1. **Ad-hoc**

1. **Post-hoc (ë³¸ ì—°êµ¬ì—ì„œ ì±„íƒí•œ ë°©ë²•)**

## 4. Conclusion

- GRPO í˜•ì‹ì˜ RLì—ì„œ completions ê°„ì˜ semantic diversityë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” DRA ì•Œê³ ë¦¬ì¦˜ ì œì•ˆí•¨

- ë‘ê°€ì§€ í•œê³„ì ì´ ìˆìŒ

- ì´ëŸ° ìª½ë„ ì¬ë°Œë‹¤!ã…‹ã…‹
