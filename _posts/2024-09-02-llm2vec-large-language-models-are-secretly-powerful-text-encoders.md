---
categories: paper-reviews
date: "2024-09-02 00:00:00"
description: " ë…¼ë¬¸ ë¦¬ë·° - LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
giscus_comments: true
layout: post
related_posts: false
tags: llm paper-review nlp
title: "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2024-09-02
- **Reviewer**: ê¹€ì¬í¬
- **Property**: Embeddings, LLM

---

[//]: # "table_of_contents is not supported"

---

## 1. Intro

ğŸ’¡ Scalingì„ í†µí•´ ì¢‹ì€ ëŠ¥ë ¥ì„ ë³´ìœ í•œ Decoder ëª¨ë¸ì„ Encoderë¡œ <span style='color:red'>**ê°„ë‹¨íˆ**</span> Adaptation í•  ìˆ˜ ìˆë‹¤!

- BERTì™€ GPT ì´í›„ ì´ì–´ì§„ Encoder vs Decoder ëŒ€ì „

  - BERTëŠ” íŒ¨ë°°í•˜ì˜€ë‹¤â€¦ â†’ Encoder modelì˜ ê²½ìš° scalingì„ í†µí•œ ì„±ëŠ¥ ê°œì„  ë° í™œìš©ì²˜ê°€ ë¶„ëª…í•˜ì§€ ì•ŠìŒ

  - Decoder-only: <span style='color:blue'>scaling</span>ì„ í†µí•œ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ í™•ì‹¤

    â‡’ ì„±ëŠ¥ ê°œì„ ì„ í†µí•´ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¡œ í™•ì¥ ë° ë„ë©”ì¸ í™•ì¥ì´ ê°€ëŠ¥

- Embedding: ì—¬ì „íˆ ê´‘ë²”ìœ„í•œ íƒœìŠ¤í¬ì—ì„œ í™œìš©

  - Information Retrieval, Sentence Classification, Clustering ë“±ë“±

  â‡’ Encoder ëª¨ë¸ì´ ì—¬ì „íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆëŠ” ë¶„ì•¼

  â‡’ PLMì˜ ì„±ëŠ¥ ê°œì„ ì´ ì–´ë ¤ì›Œ ë¹„ì•½ì  ì„±ëŠ¥ê°œì„ ì´ ë§¤ìš° ì–´ë ¤ì›€

      â†’ Scalingì„ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ ì‹œí‚¤ëŠ” Decoder ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ?

      MNTP + UCLì„ í†µí•´ Decoderë¥¼ Encoderë¡œ í™œìš© ê°€ëŠ¥

## 2<span style='color:green_background'>. Method</span>

**3ë‹¨ê³„ ê³¼ì •ìœ¼ë¡œ êµ¬ì„±**

### 2-1. Bidirectional Attention

**ê¸°ì¡´ Decoder only ëª¨ë¸ì˜ Attention layerë¥¼ bi-Directional Attentionìœ¼ë¡œ ë³€ê²½ **

    â†’ ê¸°ì¡´ pretrained weightëŠ” ê·¸ëŒ€ë¡œ í™œìš©

<br/>

### 2-2. MNTP(Masked Next Token Prediction)

**Decoderê°€ ì ì ˆíˆ Embedding taskë¥¼ í•™ìŠµí•˜ë„ë¡ ìœ ë„**

1. ë¬¸ì¥ ë‚´ ì„ì˜ì˜ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³ , ì´ì „ ì‹œì ì˜ representationìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡

1. BERTì˜ MTP(Masked Token Prediction)ê³¼ GPTì˜ NTP(Next Token Prediction)ì˜ ì¤‘ê°„ ìˆ˜ì¤€

   - MTP: ë¬¸ì¥ ì¤‘ê°„ í† í°ì„ Maskingí•˜ê³  ì˜ˆì¸¡

     - bi-directional ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ contextë¥¼ ë°˜ì˜í•œ Representation ìƒì„± í•™ìŠµ

       â†’ Encoderì— ì í•©í•œ í•™ìŠµ ë°©ì‹

   - NTP: ë§¤ í† í°ì˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡

     - Scalingì— ìš©ì´í•˜ê³  ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì´ ê°€ëŠ¥

       â†’ Pretrain ì‹œ í•™ìŠµí•œ íƒœìŠ¤í¬ë¥¼ ìœ ì§€

### 2-3. Unsupervised Contrastive Learning

**Sentence Levelì˜ Representationì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ**

1. SimCSE í•™ìŠµ ë°©ë²•ë¡  ì´ìš©

   1. query: ì„ì˜ì˜ ë¬¸ì¥

   1. positive: query ë¬¸ì¥ì— ëŒ€í•´ dropoutì„ ë‹¤ë¥´ê²Œ ì ìš©í•œ Representation

   1. negative: in-batch negatives

## 3. Experimental Setup

### Masking Token: â€œ\_â€

- Decoder modelì€ masking tokenì´ ì—†ìŒ

- ë‚˜ëŠ” ë°”ë³´ê°€ ì•„ë‹ˆë‹¤. â†’ ë‚˜ëŠ” \_ ì•„ë‹ˆë‹¤.

  - ë‚˜ëŠ” <Mask> ì•„ë‹ˆë‹¤.

### training step (1 A100)

- MNTP: 1,000 step, 100 minutes

- UCL: 1,000 step, 3 hours

### Training Method: LoRA

### Training Dataset

Wikipedia ë°ì´í„° ì´ìš©: ëª¨ë“  LLMì˜ ì‚¬ì „í•™ìŠµì— í¬í•¨

    **â†’ ëª¨ë¸ì—ê²Œ ìƒˆë¡œìš´ ì§€ì‹ ì£¼ì… X, Encoderë¡œì„œì˜ íƒœìŠ¤í¬ í•™ìŠµ**

- MNTP: Wikitext-103

- UCL: Wikipedia sentences

## 4. Experiments

### 4-1. Word Level Tasks

- token ìˆ˜ì¤€ì˜ representationì„ í•„ìš”ë¡œ í•˜ëŠ” Task

- ì‹¤í—˜ ëª¨ë¸

  - Uni: Decoder only LLM (ì§ì„ )

  - DeBERTa-v3-large: Pretrained Encoder (ì ì„ )

  - Bi: LLMì˜ attnì„ bi-directional attnìœ¼ë¡œ ë³€ê²½, í•™ìŠµ X

  - Bi + MNTP : MNTP í•™ìŠµëœ ëª¨ë¸

  - Bi+ MNTP + SimCSE: MNTPë¡œ í•™ìŠµëœ ëª¨ë¸ + SimCSE í›ˆë ¨

- ì‹¤í—˜ í•´ì„

  - Bi: ì‹¬ê°í•œ í•™ìŠµ ì €í•˜ ì–‘ìƒì„ ë³´ì„ â†’ LLMì€ attnë§Œ ë°”ê¾¸ì–´ì„œëŠ” ì ì ˆí•œ representation ìƒì„± X

  - MNTP: ì„±ëŠ¥ ê°œì„  ê´€ì°° â†’ 1,000 step í•™ìŠµìœ¼ë¡œ ì¢‹ì€ Representation í˜•ì„± ê°€ëŠ¥

  - SimCSE: word-level taskì—ì„œë„ MNTP ëŒ€ë¹„ ì„±ëŠ¥ì €í•˜ ì ìŒ

### 4-2. Sentence Level task

MTEB ë²¤ì¹˜ë§ˆí¬ ë‚´ 15ê°œ íƒœìŠ¤í¬ì— ëŒ€í•œ í‰ê· ê°’

- Pooling ë°©ë²•ë¡ ê³¼ ê´€ê³„ì—†ì´ SimCSEê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

- Mistial: Bidirectional Attnë§Œ ì ìš©í•˜ë”ë¼ë„ ì„±ëŠ¥ ê°œì„ ì´ ê´€ì°°ë˜ëŠ” ëª¨ìŠµì„ ë³´ì„

  - ë‹¤ë¥¸ LLMê³¼ ë‹¤ë¥¸ ê²½í–¥ì„±

    â‡’ í•™ìŠµ ë°ì´í„° or pretrain methodì— ìˆì–´ ì°¨ì´ì ì´ ì¡´ì¬í•œë‹¤ê³  ì¶”ì¸¡ But ê³µê°œ X

    (ì¬í¬) Mistral ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œ token level taskê°€ ì ìš©â€¦?

ğŸ’¡

    1. Bi Directional Attnë§Œìœ¼ë¡œëŠ” Decoder ëª¨ë¸ì˜ Encoder ì „í™˜ ë¶ˆê°€

    1. MNTP íƒœìŠ¤í¬ë¡œ ì ì€ stepë§Œ í•™ìŠµí•˜ë”ë¼ë„ ëª¨ë¸ì€ ì‰½ê²Œ Encoderë¡œ ì „í™˜ë¨

    1. Unsupervised Contrastive Learning ì ìš© ì‹œ sentence levelì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

<br/>

### 4-3. MTEB Benchmark

- MTEB: Retrieval, Rerank, Clustering, Sentence Pair Classification, STS, Extractive Summarization ë“± Encoderìš© íƒœìŠ¤í¬ì— ëŒ€í•œ ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹

- Echo Embedding: ë™ì¼ ë¬¸ì¥ì„ ë‘ë²ˆ ë°˜ë³µí•˜ì—¬ ì…ë ¥í•˜ê³ , ë’· ë¬¸ì¥ì—ì„œ Representationì„ íšë“í•˜ëŠ” ë°©ë²•

  â†’ ë’·ë¬¸ì¥ì˜ ëª¨ë“  í† í°ì€ ì•ë¬¸ì¥ì„ í†µí•´ ëª¨ë“  í† í°ì— ëŒ€í•´ attní•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ bi-directional attnì˜ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ

  - Uni + Mean: Decoder ëª¨ë¸ì˜ Representationì— ëŒ€í•´ mean pooling ì ìš©

  - BERT + SimCSEì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ ê°€ëŠ¥

  - Decoder Modelì´ Scaling upë˜ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ì˜ í•œê³„ë¥¼ ë„˜ì„ ìˆ˜ ìˆìŒ

    (ì¬í¬): Decoder ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ Encoderë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ë°©í–¥ì„±ì˜ ê·¼ê±°

- Bi + Mean: Decoder ëª¨ë¸ì— ëŒ€í•´ Bi Attnë§Œ ì ìš©í•˜ê³  ëª¨ë“  tokenì— ëŒ€í•œ Mean pooling ì´ìš©

  - Mistralì„ ì œì™¸í•œ ëª¨ë¸ì˜ ê²½ìš° ë§¤ìš° ì•ˆì¢‹ì€ ì„±ëŠ¥ ê¸°ë¡

- LLM2Vec(w/o SimCSE): ê±°ì˜ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ Uni + Mean ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ ë„ì¶œ

  - Sentence Level íƒœìŠ¤í¬ë¼ëŠ” ì ì„ ê³ ë ¤í•˜ë©´ LLM2Vecì´ ë‹¨ìˆœí•˜ë©´ì„œ ê°•ë ¥í•œ ë°©ë²•ë¡ ì„ì„ ë³´ì—¬ì¤Œ

- LLM2Vec: ë§¤ìš° ë†’ì€ ì„±ëŠ¥ ë„ì¶œ

  - ì ì€ í•™ìŠµìœ¼ë¡œë„ Decoder ëª¨ë¸ì´ Sentence ë‹¨ìœ„ë¡œ representationì„ ë§¤ìš° ëŠ¥ìˆ™í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŒ

    â‡’ ìµœê·¼ MTEB ìƒìœ„ ë°©ë²•ë¡ ë“¤ì´ ëª¨ë‘ ë™ì¼í•œ ê²½í–¥ì„±ì„ ë³´ì„ (LLM + Bi Attn + Contr. Learning)

    <br/>

### 4-4. Analysis 1(MNTPì˜ íš¨ê³¼)

**LLM2Vecì„ í†µí•´ í•™ìŠµë˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?**

$$ q_i = (A_i, B_i), s^+\_i = (A_i, C_i) ,s^-\_i = (A_i, D_i) $$

- B, C: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë¬¸ì¥

- B, D: ë‹¤ë¥¸ ì˜ë¯¸ì˜ ë¬¸ì¥

- A ë¬¸ì¥ì˜ í† í°ì—ì„œ poolingí•˜ì—¬ representationì„ ì‚°ì¶œ

  - Bidirectionalì´ ì˜ëœë‹¤ë©´: $ q_i, s^+\_i $ëŠ” ë¹„ìŠ·í•œ Representationì´ì–´ì•¼ í•¨. â†’ ë’·ë¬¸ì¥ì˜ ì •ë³´ê°€ A ë¬¸ì¥ìœ¼ë¡œ í˜ë €ì–´ì•¼ í•¨ìœ¼ë¡œ

- ëª¨ë¸ í¬ê¸°ì— ê´€ê³„ì—†ì´ MNTP ì´í›„ posì™€ Neg ê°„ ê±°ë¦¬ê°€ ë²Œì–´ì§

  - MNTP: ìœ„ì¹˜ì— ê´€ê³„ì—†ì´ Tokenë“¤ì´ ì„œë¡œ attní•˜ë„ë¡ í•™ìŠµ

### 4-5. Analysis 2(why Mistral works)

- ë™ì¼ ë°ì´í„°ì— ëŒ€í•œ original attention(llm)ê³¼ bi directional attention ì‹œì˜ token/layer ë‹¨ìœ„ Representation ë¹„êµ

  - sheared LLaMA, LLaMA 2 ëª¨ë‘ ìƒìœ„ ë ˆì´ì–´ë¡œ ê°ˆìˆ˜ë¡ ë‘ attn ê°„ ë‹¤ë¥¸ Representationì´ ìƒì„±ë¨

  - Mistral: bi directional attn ì ìš© ì‹œì—ë„ ëª¨ë“  ë ˆì´ì–´ì—ì„œ ê¸°ì¡´ attnê³¼ ë¹„ìŠ·í•œ representationì´ í˜•ì„± ë¨

    â†’ Mistralì´ ë­”ì§“ì„ í•œ ê²ƒ ê°™ì€ë°â€¦ ë­”ì§€ëŠ” ëª°ë¼ìœ â€¦

## 5. Conclusion

- Decoder ëª¨ë¸ì„ Encoderë¡œ ì „í™˜í•˜ëŠ” ë°©ë²•ë¡  ì œì•ˆ

  - MNTP: NTPì™€ MTP ì‚¬ì´ì˜ task

- ë‹¨ìˆœ: 1,000 step, 128 batch size, 1 a100 80GBë©´ ì¶©ë¶„íˆ í•™ìŠµì´ ë¨

- ë²”ìš©: MNTP + SimCSEë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë²”ìš©ì ì¸ encoderë¡œì„œ ì‚¬ìš© ê°€ëŠ¥

  - ë‹¤ë¥¸ encoder í•™ìŠµ ë°©ë²•ë¡  ì—­ì‹œ ì¶”ê°€ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥ â‡’ MTEB ìƒìœ„ ëª¨ë¸ ëŒ€ë¶€ë¶„ì˜ baseline

- Encoder Pretraian ì‹œ scaling íš¨ê³¼ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ Encoder ë¶„ì•¼ í–¥í›„ ë°œì „ ë°©í–¥

  - DecoderëŠ” Scaling íš¨ê³¼ê°€ ìˆìŒ

  - Scalingí•˜ì—¬ Decoder í›ˆë ¨ â†’ LLM2Vec ì ìš© â‡’ Scaling íš¨ê³¼ê°€ ì ìš©ëœ Encoder ê°œë°œ ê°€ëŠ¥

<br/>
