---
categories: paper-reviews
date: '2025-04-22 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - Fine-tuning Vision-Language-Action Models: Optimizing Speed
  and Success'
giscus_comments: true
layout: post
related_posts: false
tags: paper-review
title: 'Fine-tuning Vision-Language-Action Models: Optimizing Speed and Success'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2025-04-22
- **Reviewer**: ì „ë¯¼ì§„

ğŸ’¡ ê¸°ì¡´ openVLAì˜ í•œê³„(1. low frequency 2. bi-manual ë¶ˆê°€ 3. action discrete representation)ì„ í•´ì†Œí•˜ëŠ” FT ë°©ë²•ë¡  ì†Œê°œ

## Motivation

- ê¸°ì¡´ openVLAëŠ” generalizationì—ëŠ” ì„±ê³µì (language following ability, semantic generalization)ì´ì—ˆì§€ë§Œ, ëª…ë°±í•œ í•œê³„ê°€ ì¡´ì¬

	- auto-regressive generation â‡’ low frequency

	- FTí•´ë„ bi-manual ì„±ëŠ¥ì´ êµ¬ë¦¼

- ìµœê·¼ì— ìƒì„± ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ê°€ ì—¬ëŸ¬ê°€ì§€ ì†Œê°œë˜ì—ˆìœ¼ë‚˜, bi-manualì— ì·¨ì•½í•¨ â‡’ ì†ë„ë„ ë¹ ë¥´ë©´ì„œ ë§Œì¡±í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë°©ë²•ì€ ì—†ìŒ

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” OpenVLAë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ íŒŒì¸íŠœë‹ ë ˆì‹œí”¼ Optimized Fine-Tuning(OFT) recipeë¥¼ íƒêµ¬

	- different action decoding schemes, action representations, learning objectivesì— ëŒ€í•´ ë¶„ì„

- ì—¬ëŸ¬ ë””ìì¸ ì´ˆì´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenVLA-OFTë¥¼ ì œì•ˆ, LIBERO simulation benchmarkì—ì„œ OpenVLAì˜ ì„±ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ë†’ì´ë©´ì„œë„ 26x generation throughputë¥¼ ë‹¬ì„±

	- real-world evaluationì—ì„œë„ OpenVLAë¥¼ bimanual ALOHA robot settingì— ë§ì¶°ì„œ í•™ìŠµ, ë‹¤ë¥¸ VLAë³´ë‹¤ í›¨ì”¬ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„

## Introduction

- ìµœê·¼ VLA ëª¨ë¸ì€ low-level robotic controlì„ ìœ„í•´ ëŒ€ëŸ‰ì˜ robot datasetì— pretrained VLMì„ í•™ìŠµí•´ êµ¬ì¶•, ë‹¤ì–‘í•œ ë¡œë´‡ê³¼ íƒœìŠ¤í¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ê³¼ semantic generalization, language following abilityë¥¼ ë³´ì„

- fine-tuningì€ ìƒˆë¡œìš´ ë¡œë´‡ê³¼ íƒœìŠ¤í¬ì— ëŒ€í•´ VLAì˜ ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•´ í•„ìˆ˜ì ì´ì§€ë§Œ, ì•„ì§ adapationì„ ìœ„í•´ ì–´ë–¤ ë””ìì¸ì´ ê°€ì¥ íš¨ê³¼ì ì¸ì§€ì— ëŒ€í•œ íƒêµ¬ëŠ” ë¶€ì¡±

	- action headë¡œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•  ê²ƒì¸ì§€, next token prediction lossê°€ ìµœì ì¸ì§€ ë“±ë“±

- ì´ì „ OpenVLA ë…¼ë¬¸ì—ì„œ LoRAë¥¼ í™œìš©í•œ FT adaptation strategyë¥¼ ì†Œê°œí•˜ì˜€ìœ¼ë‚˜, ëª‡ê°€ì§€ í•œê³„ê°€ ì¡´ì¬

	- autoregressive action generationì€ high-frequency control(25-50+Hz)ë¥¼ í•˜ê¸°ì—” ë„ˆë¬´ ëŠë¦¼(3-5Hz)

		- hzëŠ” 1ì´ˆë‹¹ action íšŸìˆ˜

	- LoRAì™€ full FT ëª¨ë‘ bi-manual manipulation taskì—ì„œ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ì„ ëƒ„

- ìµœê·¼ì— ë” ì¢‹ì€ action tokenization schemesë¥¼ í†µí•´ì„œ efficiencyë¥¼ ë†’ì´ëŠ”(ì†ë„ë¥¼ 2ë°°ì—ì„œ 13ë°° ì •ë„ ë†’ì„) ì—°êµ¬ë“¤ì´ ì œì•ˆë˜ì—ˆìœ¼ë‚˜, action chunkì‚¬ì´ì— ìƒë‹¹í•œ latencyê°€ ìˆì–´(ìµœê·¼ ê°€ì¥ ë¹ ë¥¸ ë°©ë²• ê¸°ì¤€ 750ms) ì•„ì§ high-frequency bimanual robotì— ì ìš©í•˜ê¸°ì—” í•œê³„ê°€ ì¡´ì¬

	â‡’ ì†ë„ë„ ë¹ ë¥´ê³  ë§Œì¡±í•  ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë°©ë²•ë¡ ì€ ì•„ì§ ë¶€ì¬

- ë³¸ ë…¼ë¬¸ì—ì„  ì´ëŸ¬í•œ ì¸ì‚¬ì´íŠ¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ OFT recipeì˜ ì´ˆì•ˆ, OpenVLA-OFTë¥¼ ì œì•ˆ

	- parallel decodingê³¼ action chunkingì„ í†µí•©

	- continuous action representation

	- L1 regression objective

- LIBERO simulation benchmarkì™€ real bimanul ALOHA robotìœ¼ë¡œ ì‹¤í—˜

	- LIBEROì—ì„œëŠ” 8 action chunkì‚¬ìš©, 26xë¹ ë¥¸ ì†ë„ë¡œ í‰ê·  97.1%ì˜ ì„±ëŠ¥ ë‹¬ì„±

	- ALOHA taskì—ì„œëŠ” enchanced language groudingì„ ìœ„í•´ FiLMì„ ë ˆì‹œí”¼ì— ì¶”ê°€, í•´ë‹¹ ëª¨ë¸ì„ OFT+ë¼ ëª…ì‹œ

		- ì˜·ì ‘ê¸°, íƒ€ê²Ÿ ìŒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¡°ì‘í•˜ëŠ” íƒœìŠ¤í¬ ë“±ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„

		- 25-timestep action chunckë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenVLAë³´ë‹¤ 43ë°° ë†’ì€ throughputë¥¼ ë³´ì„

## Related Work

- ì´ì „ì—” languageì™€ vision foundationëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ robotic capabilitiesë¥¼ ë†’ì´ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰

	- robotic policy learningì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ pretrained visual representationì„ ì‚¬ìš©

		- robotic taskì—ì„œ object localizationì´ë‚˜ high-level planningê³¼ reasoning ë“±

	- ë” ìµœê·¼ì—ëŠ” VLMì´ ë°”ë¡œ low-level robotic control actionì„ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” VLA ëª¨ë¸ì„ ë§Œë“œëŠ” ì—°êµ¬ë“¤ì´ ì œì•ˆë¨

		- íš¨ê³¼ì ì¸ OOD test conditionê³¼ unseen semantic conceptì— ëŒ€í•œ generalizationì„ ë³´ì„

		- ë³´í†µ ëª¨ë¸ ê°œë°œì— ì´ˆì ì„ ë‘ê³  ì—°êµ¬

		â‡’ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì„ FTí•  ë•Œì˜ ê°œë°œ ë ˆì‹œí”¼ì— ì´ˆì ì„ ì¤Œ

- VLA ëª¨ë¸ ê°œë°œì‹œ FTì˜ ì—­í• ì´ ì¤‘ìš”í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , íš¨ê³¼ì ì¸ ë ˆì‹œí”¼ íƒêµ¬ëŠ” ë¶€ì¡±í–ˆìŒ

	- ì´ì „ì—ëŠ” full FTì™€ FT with LoRA ì •ë„

	- í•˜ì§€ë§Œ ì´ì¡°ì°¨ë„ single-armì— í•œì •ì , low control frequenciesë¡œ bimanual robotì— í™•ì¥ì€ ë¶ˆê°€ëŠ¥

- ìµœê·¼ì˜ ì—°êµ¬ì—ì„œëŠ” VLA íš¨ìœ¨ì„±ì„ ìœ„í•´ ìƒˆë¡œìš´ action tokenization ë°©ë²•ì´ ê³ ì•ˆë¨

	- vector quantization ì‚¬ìš©

	- discrete cosine transform-based compression to represent action chunk with fewer tokens than simple per-dimension binning(RT-2ì™€ OpenVLAì—ì„œ ì‚¬ìš©ë˜ë˜ê±°)

	- ì´ëŸ¬í•œ ë°©ë²•ë¡ ì€ ê¸°ì¡´ autoregressive VLAì— ëŒ€í•´ 2ë°°ì—ì„œ 13ë°° ì •ë„ì˜ ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë‚˜, iterative generation íŠ¹ì„± ë•Œë§¤ í•œì •ì 

	- ë³¸ ë…¼ë¬¸ì˜ parallel decodingì€ action chunkì™€ í•¨ê»˜ ì‚¬ìš©í•  ê²½ìš° 26ë°°ì—ì„œ 43ë°°ì˜ throughputê³¼ í›¨ì”¬ ë‚®ì€ latency(0.07ms for single-arm task with one input image & 0.321ms for bimanual tasks with three input images)ë¥¼ ë³´ì—¬ì¤Œ

- ë‹¤ë¥¸ ì—°êµ¬ ë¼ì¸ì—ì„œëŠ” high-frequency, bimanual manipulationì„ ìœ„í•œ íš¨ê³¼ì ì¸ VLA FTë¥¼ ìœ„í•´ì„œ diffusion ì´ë‚˜ flow matchingì„ ì‚¬ìš©

	- diffusion ê¸°ë°˜ VLAëŠ” multi-step action chunksê°€ ë™ì‹œì— ê°€ëŠ¥í•´ throughputì€ ë†’ì§€ë§Œ í•™ìŠµ ì†ë„ëŠ” ëŠë¦¼

		- ê°ì ëª¨ë¸ ë””ìì¸ì´ ìƒì´í•´ì„œ ì–´ë–¤ ìš”ì†Œê°€ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ëŠ” í™•ì¸í•˜ê¸° ì–´ë ¤ì›€

	- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í†µì œëœ ì„¸íŒ…ì—ì„œ ì‹¤í—˜, ì–´ë–¤ ìš”ì†Œê°€ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ë„ ë¶„ì„

## Preliminaries

### Original OpenVLA formulation

- 7B manipulation policy, Prismatic VLMì„ OXEì˜ 1M episodeì— FTí•´ì„œ ë§Œë“  ëª¨ë¸

- autoregressive predictionì„ í™œìš©, ê° timestepë§ˆë‹¤ 7 discrete robot action tokenì„ ìƒì„±

	- 3ê°œëŠ” position control, 3ê°œëŠ” orientation control, 1ê°œëŠ” gripper control

	- cross-entropy lossë¡œ Next-token predictionë°©ì‹ìœ¼ë¡œ í•™ìŠµ (ì–¸ì–´ëª¨ë¸ í•˜ë˜ê±° ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)

### Action chunking

- ì´ì „ ì—°êµ¬ëŠ” action chunking(ì¤‘ê°„ replanningì—†ì´ future action sequenceë¥¼ ì˜ˆì¸¡, ì‹¤í–‰)ì´ ì—¬ëŸ¬ manipulation taskì—ì„œ policy ì„±ê³µë¥ ì„ ë†’ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì„

- ê·¸ëŸ¬ë‚˜, OpenVLAì˜ autoregressive generation schemeì—ì„œëŠ” action chunkingì´ ì–´ë ¤ì›€

	- single timestep actionë¥¼ ìƒì„±í•˜ëŠ”ë° 0.33ì´ˆê°€ ê±¸ë¦¼(A100ê¸°ì¤€)

- chunk sizeë¥¼ K, action dimensionalityë¥¼ Dë¼ê³  í•˜ë©´,  OpenVLAëŠ” KDë²ˆ forwardë¥¼ í•´ì•¼í•¨

## Proposed Method

### Studying key VLA Fine-Tuning Design Decisions

- ê¸°ì¡´ OpenVLAì˜ í•œê³„ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ì„œ ë‹¤ìŒ 3ê°€ì§€ ë””ìì¸ì— ëŒ€í•´ ì¡°ì‚¬

	- Action generation strategy

		- ê¸°ì¡´ì—” action dimensionì´ 7ì´ë¼ê³  í•˜ë©´, í•˜ë‚˜ì˜ ì•¡ì…˜ì„ ìƒì„±í•˜ê¸° ìœ„í•´ 7ë²ˆ forward â‡’ low frequency

	- Action representation

		- ê¸°ì¡´ì—” ì•¡ì…˜ì„ í† í°ìœ¼ë¡œ ìƒì„±, ì•¡ì…˜ì˜ ë²”ìœ„ë¥¼ 256ê°œë¡œ binning & normalization, ê°ê°ì„ í† í°ìœ¼ë¡œ ì·¨ê¸‰

		- ì¦‰, íŠ¹ì • ì•¡ì…˜ í† í°ì„ ìƒì„±í•˜ë©´ ê·¸ê±¸ ë‹¤ì‹œ ì •í•´ì§„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ continous valueë¡œ mapping â‡’ precisionì´ ë–¨ì–´ì§ˆ ìˆ˜ë°–ì— ì—†ìŒ

	- Learning objective

		- next-token predictionìœ¼ë¡œ íŒŒì¸íŠœë‹

	â‡’ ë³¸ ì—°êµ¬ì—ì„œëŠ” OpenVLAë¥¼ base modelë¡œ ì‹¤í—˜, OpenVLAë¥¼ 500ê°œì˜ demonstrationì— ëŒ€í•´ LoRA FTë¡œ adaptation

### Implementing Alternative Design Components

- Parallel decoding with action chunking

	- ì—¬ëŸ¬ action sequenceë¥¼ í•œë²ˆì˜ forwardë¡œ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ, ê¸°ì¡´ì˜ auto-regressiveë°©ì‹ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©

	- ëª¨ë¸ì´ empty action embeddingì„ input tokenìœ¼ë¡œ ë°›ë„ë¡ ìˆ˜ì •, caual attention maskë¥¼ bi-directional attentionìœ¼ë¡œ ë³€ê²½, decoderê°€ ëª¨ë“  actionì„ ë™ì‹œì— predictioní•˜ë„ë¡ í•¨

		â‡’ ì´ ë°©ì‹ìœ¼ë¡œ D(action dimension) sequential passë¥¼ í•œë²ˆì˜ single passë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ

	- Parallel decodingì—ì„œ action chunkingìœ¼ë¡œ í™•ì¥í•˜ëŠ”ê±°ëŠ” ì‰¬ì›€

		- empty action embeddingì„ decoderì˜ inputì— ì¶”ê°€, ì´ëŠ” future actionì˜ chunkë¡œ mappingë¨

		- chunk size Kì— ëŒ€í•´ì„œ, ëª¨ë¸ì€ ì´ì œ KD actionsì„ í•œë²ˆì˜ forwardë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ , ìµœì†Œí•œì˜ latencyë¡œ thoughput K-foldë¥¼ ì¦ê°€

			- thoughput K-fold: Kì˜ rolloutì„ ë³‘ë ¬ë¡œ ìƒì„±í–ˆì„ ë•Œì˜ ì²˜ë¦¬ëŸ‰

				- kë²ˆì— ê±¸ì³ action sequenceë¥¼ ìƒì„±, í‰ê°€í•´ ê°€ì¥ ì¢‹ì€ê±¸ ê³ ë¥´ëŠ” ë°©ë²•ë¡ (?)

		- parallel decodingì€ autoregressive approachì— ë¹„í•´ì„  ì´ë¡ ì ìœ¼ë¡œ ëœ expressiveí•˜ì§€ë§Œ ì—¬ëŸ¬ íƒœìŠ¤í¬ì— ì‹¤í—˜ ê²°ê³¼, ì„±ëŠ¥ í•˜ë½ì„ ë³´ì´ì§„ ì•ŠìŒ

- Continuous action space

	- OpenVLAëŠ” ì›ë˜ ê° action dimensionì´ [-1,+1]ë¡œ normalizedë˜ê³ , 256 binìœ¼ë¡œ uniformí•˜ê²Œ discretizedëœ discrete action tokenì„ ì‚¬ìš©

		- ì´ ë°©ë²•ë¡ ì€ ê¸°ì¡´ì˜ VLMì„ ìˆ˜ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œëŠ” í¸ë¦¬í•˜ì§€ë§Œ, fine-grained action detailì„ ë–¨ì–´ëœ¨ë¦¼

	- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ 2ê°€ì§€ ë°©ì‹ìœ¼ë¡œ continuous action spaceë¥¼ êµ¬í˜„

		- 1ì•ˆ) L1 regression êµ¬í˜„

			: LLMì˜ ëë‹¨ì— ìˆë˜ output embedding layerë¥¼ ë¹¼ê³ , ìƒˆë¡œ 4-layer MLP action headë¥¼ ì¶”ê°€, action tokenì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì •í™•í•œ action ê°’ ìì²´ë¥¼ ìƒì„±í•˜ë„ë¡ í•¨

		- 2ì•ˆ) conditional denoising diffusion modeling

			: ëª¨ë¸ì´ forward diffusionì—ì„œ action sampleì— ì¶”ê°€ëœ noiseë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ, inferenceì‹œ noisy action sampleì„ ì ì°¨ denoisingí•˜ë©´ì„œ real actionì„ ì˜ˆì¸¡

			- ì´ ë°©ì‹ì€ ë” expressive action modelingì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, inferenceì‹œ ì—¬ëŸ¬ë²ˆì˜ forward passê°€ í•„ìš”(50 diffusion steps in our implementation)

				- Parallel decodingì„ ì‚¬ìš©í•´ì„œ latencyê°€ ì¢€ ë” ì»¤ì§

- Additional model inputs and outputs

	- orignal OpenVLAì—ì„œëŠ” single camera viewë§Œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í–ˆìœ¼ë‚˜, ëª‡ëª‡ ë¡œë´‡ ì„¸íŒ…ì—ì„œëŠ” ì—¬ëŸ¬ viewpointì™€ ì¶”ê°€ì ì¸ robot state informationì´ í¬í•¨ë¨

	- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” flexible input processing pipelineì„ êµ¬í˜„

		- camera imageëŠ” OpenVLAì™€ ê°™ì€ dual vision encoderë¥¼ í†µí•´ viewë§ˆë‹¤ 256ê°œì˜ patch embeddingìœ¼ë¡œ ì¶”ì¶œ, language embedding spaceìœ¼ë¡œ projection

		- low-dimensional robot state inputì— ëŒ€í•´ì„œëŠ” separate projection networkë¥¼ ì‚¬ìš©í•´ì„œ ê°™ì€ embedding spaceë¡œ mapping, í•˜ë‚˜ì˜ input embeddingìœ¼ë¡œ ë³€í™˜í•´ ì‚¬ìš©

	- ëª¨ë“  input embedding(visual features, robot state, language token)ì„ concatí•´ì„œ decoderì— ë“¤ì–´ê°

		â‡’ ì´ëŸ¬í•œ unified latent representationì€ ëª¨ë¸ì´ actionì„ ìƒì„±í•  ë•Œ, ëª¨ë“  ì´ìš©ê°€ëŠ¥í•œ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•¨

### Augmenting OpenVLA-OFT with FiLM for Enchanced Language Grounding

- bimanualì—ì„œëŠ” input imageê°€ 3ê°œ(3rd person view, wrist-mounted cameras)

	â‡’ visual inputê°„ì˜ spurious correlationë•Œë¬¸ì— language followingì— ì–´ë ¤ì›€ì„ ê²ªìŒ

- ê·¸ë˜ì„œ bi-manualì—ì„œëŠ” language following abilityí–¥ìƒì„ ìœ„í•´ì„œ ViTì— FiLMì„ ì¶”ê°€

	- ê° image patchë§ˆë‹¤ ë‹¤ë¥¸ scaling, shift factorë¥¼ ì ìš©í–ˆë”ë‹ˆ langauge followingì´ ì•ˆì¢‹ì•„ì„œ ê°™ì€ scaling, shift factorë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•¨

		â‡’ ëª¨ë“  patchì— ê°™ì€ $ \gamma,\beta $ê°€ ì ìš©ë˜ì§€ë§Œ ê° ì°¨ì›ë³„ë¡œëŠ” ë‹¤ë¥¸ scalingê³¼ shiftê°€ ì ìš©

		- scaling, shift factorì˜ í¬ê¸°ëŠ” D_ViT

			- D_ViTëŠ” visual patch embeddingì—ì„œ hidden dimensionì˜ í¬ê¸°

	- FiLMì€ ê° ViT blockì—ì„œ self-attention layerì™€ FFNì‚¬ì´ì— ì ìš©, ê° ë¸”ë¡ë§ˆë‹¤ ë‹¤ë¥¸ proejctorì‚¬ìš©

	- FiLMì€ ALOHA experimentì—ì„œë§Œ ì‚¬ìš©!

			## Experiment

### Research Question

RQ1. How does each design decision affect the fine-tuned policyâ€™s success rate on downstream tasks?

RQ2. How does each design decisions affect model inference efficiency (action generation throughput and latency)?

RQ3. How do alternative fine-tuning formulations affect flexibility in model input-output specifications?

### LIBERO

- Experiment setup

	- Franka Emika Panda armê¸°ë°˜ì˜ simulator, camera images, robot state, task annotation, and delta end-effector pose actionsì´ í¬í•¨ë˜ì–´ ìˆìŒ

	- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 4ê°€ì§€ taskë¥¼ ì‚¬ìš© - LIBERO-Spatial, Object, Goal, Long

		- policy generalizationì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê°ê° 500ê°œì˜ expert demonstrationì„ ì œê³µ

			- OpenVLAì—ì„œëŠ” unsucessful demonstrationì„ í•„í„°ë§í•˜ê³  ê° taskë§ˆë‹¤ ë”°ë¡œ LoRAë¡œ FT

	- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” non-diffulsion methodëŠ” 50-150K gradient stepí•™ìŠµ, diffusion methodëŠ” 100-250K gradient stepì„ í•™ìŠµ

		- batch size 64-128, A100/H100 8ëŒ€ ì‚¬ìš©

	- íŠ¹ë³„í•œ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´, policyëŠ” í•˜ë‚˜ì˜ third-person imageì™€ language instructionì„ inputìœ¼ë¡œ ë°›ìŒ

	- action chunk size K=8ë¡œ ì„¤ì •, replanningì „ì— full chunk ì‹¤í–‰

- Task performance comparisons

	- ìš°ì„  LIBERO benchmakrì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” design decisionì— ëŒ€í•´ í‰ê°€

	- ì‹¤í—˜ ê²°ê³¼ PD(parallel decoding)ê³¼ AC(action chunking)ë¥¼ ë™ì‹œì— ì¨ì•¼ high-frequency controlì´ ê°€ëŠ¥í–ˆìŒ.

	- ì‹¤í—˜ ê²°ê³¼, PD&ACëŠ” ì“°ëŠ” ê²ƒì´ ë” ì¢‹ì•˜ê³ , Cont-L1ê³¼ diffusionì„ ë¹„êµí•˜ë©´, ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ë‚˜ latencyì¸¡ë©´ì—ì„œ Cont-L1ì´ ë” ì¢‹ìŒ(ì•„ë˜ ë‚˜ì˜´)

		- íŠ¹íˆ PD&ACì˜ íš¨ê³¼ëŠ” LIBERO-Longì—ì„œ ê°€ì¥ ë›°ì–´ë‚¨

	- Inference Efficiency comparisons

	- í•˜ë‚˜ì˜ robot action í˜¹ì€ action chunkë¥¼ ìƒì„±í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„ì„ ì¸¡ì •

	- Cont-L1ì˜ ê²½ìš° ê·¸ëƒ¥ PD&ACì™€ ê±°ì˜ ì°¨ì´ê°€ ì•ˆë‚¨(MLP action headëŠ” ìµœì†Œí•œì˜ computational costê°€ ì¶”ê°€ë¨)

	- diffusionì˜ ê²½ìš° denoising processê°€ ìˆì–´ ì‹œê°„ì´ ì¢€ ë” ê±¸ë¦¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸°ì¡´ OpenVLAë³´ë‹¤ëŠ” 2-3ë°° ëŠë¦¼

	- Model input-output flexibility

	- ìœ„ì˜ í‘œ(table2)ë¥¼ ë³´ë©´ additional inputì„ ì¶”ê°€í•´ë„ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¨ í›¨ì”¬ ë¹ ë¦„

	- table 1ì„ ë³´ë©´ additional inputì„ ë„£ì—ˆì„ ë•Œ, ëª¨ë“  ëª¨ë¸ë“¤ì´ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë‚˜, ê° ëª¨ë¸ì˜ ë³µì¡í•œ í•™ìŠµ êµ¬ì¡°($ \pi_0  $ì€ MDT(multi-modal diffusion transformer)ì™€ flow-matching ì‚¬ìš©)ë¥¼ ê³ ë ¤í•´ë´¤ì„ ë•Œ, openvla-oftê°€ êµ‰ì¥íˆ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„

- Optimized Fine-tuning recipe

	- ìœ„ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë´¤ì„ ë•Œ, ìµœì ì˜ design choiceëŠ” ë‹¤ìŒê³¼ ê°™ìŒ

	1. parallel decoding with action chunking

	1. continuous action representation

	1. L1 regression objective

	â‡’ ì´ì œ ì´ êµ¬ì¡°ì˜ ëª¨ë¸ì„ OpenVLA-OFTë¼ ëª…ëª…

- Additional experiments

	- FT formulationì„ ë°”ê¾¸ë©´ì„œ, base VLA pretrainê³¼ finetuningì‚¬ì´ì— í° distribution shiftê°€ ì¼ì–´ë‚¬ì„ ìˆ˜ë°–ì— ì—†ìŒ

		- ì •ë§ base VLAì˜ ì§€ì‹ì´ ë„ì›€ì´ ëì„ê¹Œ?

					- í¬ì§„ ì•Šì§€ë§Œ ë„ì›€ì´ ë¨!

### Real-World ALOHA Robot

: ì´ì „ê¹Œì§€ëŠ” simulationì—ì„œì˜ ì„±ëŠ¥ì„ ë´¤ë‹¤ë©´, ì´ë²ˆ íŒŒíŠ¸ëŠ” ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•´ë´¤ì„ ë•Œì˜ ì„±ëŠ¥

: OpenVLAì˜ ê²½ìš° pretrainingë•Œ bimanual dataë¥¼ ë³¸ ì ì´ ì—†ìŒ

: ì—¬ê¸°ì„œëŠ” OpenVLA-OFTì— FiLMì„ ì¶”ê°€í•œ OpenVLA-OFT+ë¡œ ì„±ëŠ¥ í‰ê°€

- Setup

	- ALOHA platformì€ 2ëŒ€ì˜ ViperX 300 S arms, 3ëŒ€ì˜ ì¹´ë©”ë¼ viewpoint(í•˜ë‚˜ëŠ” top-down, ë‘ëŒ€ëŠ” wrist-mounted), robot state input(14 dimensional joint angles)

	- 25Hzë¡œ ì‘ë™, ê° actionì€ target absolute joint angleë¥¼ í‘œí˜„

	â‡’ ì´ ì„¸íŒ…ì€ OpenVLA pretrainingë•Œì™€ ë§¤ìš° ë‹¤ë¦„

	- í¬ê²Œ 4ê°œì˜ taskë¥¼ ì„ íƒ

		- fold shorts : í° ë°˜ë°”ì§€ ì ‘ê¸°, 20ê°œì˜ demonìœ¼ë¡œ í•™ìŠµ, í‰ê°€ëŠ” 10ë²ˆ

		- fold shirt : í° í‹° ì ‘ê¸°, 30ê°œì˜ demonìœ¼ë¡œ í•™ìŠµ, í‰ê°€ëŠ” 10ë²ˆ

		- scoop X into bowl : ì™¼ì†ìœ¼ë¡œ tableì˜ ì¤‘ì•™ìœ¼ë¡œ ê·¸ë¦‡ì„ ì›€ì§ì´ê³ , ì˜¤ë¥¸ì†ìœ¼ë¡œ íŠ¹ì • ì¬ë£Œë¥¼ í¼ì„œ ë‹´ìŒ, 45ê°œ demon(ì¬ë£Œë‹¹ 15ê°œ)ìœ¼ë¡œ í•™ìŠµ, 12ë²ˆ(ì¬ë£Œë‹¹ 4ë²ˆ) í‰ê°€

		- put X into pot : ì™¼ì†ìœ¼ë¡œ ëƒ„ë¹„ ëšœê»‘ ì—´ì–´ì„œ ì˜¤ë¥¸ì†ìœ¼ë¡œ íŠ¹ì • ì•„ì´í…œì„ ë„£ìŒ, 300 demonìœ¼ë¡œ í•™ìŠµ(ì•„ì´í…œë‹¹ 100ë²ˆ), 24ë²ˆ í‰ê°€(12ë²ˆëŠ” í•™ìŠµ ì•„ì´í…œ, 12ë²ˆì€ ood)

	- OpenVLAë¥¼ ê° taskì— ë”°ë¡œë”°ë¡œ 50-150K gradient stepì •ë„ FT

		- action chunk size K=25, inferenceì‹œì— ë°”ë¡œ í’€ action chunkì‹¤í–‰

- Method in Comparison

	- ALOHAëŠ” OpenVLAê°€ adaptationí•˜ê¸°ê°€ ì–´ë ¤ì›€, ê·¸ë˜ì„œ ë” ìµœê·¼ VLA-RDT-1B, $ \pi_0 $ê³¼ ë¹„êµ

		- ë‘˜ì€ bimanual manipulation dataë¡œ pretrainë¨

	- ê° ëª¨ë¸ì€ ê° ì €ìì˜ ë ˆì‹œí”¼ë¡œ FTí•˜ê³  í‰ê°€í•¨

	- computational efficiencyë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì˜ imitation learning baseline, ACTì™€ Diffusion policyë¥¼ scratchë¡œ ê° taskì— í•™ìŠµí•´ ì‚¬ìš©

	- ê° baseline methodì˜ language followingì„ ìœ„í•´ì„œ, language-conditioned implementationì„ ì‚¬ìš©

		- ACTëŠ” EfficientNet-B0ë¥¼ ìˆ˜ì •, Diffusion PolicyëŠ” DROID dataset êµ¬í˜„ì„ ì‚¬ìš©(DistillBERT language embeddingì— ê¸°ë°˜í•´ action denoisingì§„í–‰)

- ALOHA Task Performance Results

	- ì‹¤í—˜ ê²°ê³¼, pretrainë•Œ ë³¸ ë°ì´í„°ì™€ ì „í˜€ ë‹¤ë¦„ì—ë„ ë§¤ìš° ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ

		- scratchë¶€í„° í•™ìŠµí•œ ACT, Diffusion policyì˜ ê²½ìš° language understandingì´ í•„ìš”í•œ Scoop X, put X taskì—ì„œ ì €ì¡°í•œ ì„±ëŠ¥ì„ ë³´ì„

		- finetuend-VLAëŠ” ë¹„êµì  ì˜í•˜ë‚˜, visual feedbackì— ëŒ€í•´ ê³¼í•˜ê²Œ ì˜ì§€í•˜ëŠ” ê²½í–¥ì„ ë³´ì„

				- FiLMì´ language followingì„ ìœ„í•´ ë§¤ìš° ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤Œ

		- ALOHA Inference Efficiency Comparison

	- ë‹¤ë¥¸ finetuned VLAê³¼ ë¹„êµí•´ì„œ í›¨ì”¬ ë†’ì€ efficiencyë¥¼ ë³´ì„

	## Limitations

- Handling multimodal demonstrations

	- L1 lossë¡œëŠ” robot dataíŠ¹ìœ ì˜ multi-modalë¥¼ í¬ì°©í•  ìˆ˜ ì—†ìŒ

- Pretraining versus fine-tuning

	- ë³¸ ë…¼ë¬¸ì€ FTì—ë§Œ ì§‘ì¤‘, pretrainingì˜ íš¨ê³¼ì— ëŒ€í•œ ë¶„ì„ì€ ë¶€ì¡±

- Inconsistent language grounding

	- LIBEROì—ì„œëŠ” ë¬¸ì œê°€ ì—†ì—ˆìœ¼ë‚˜, ALOHAì—ì„œëŠ” FiLMì—†ì´ëŠ” ì„±ëŠ¥ì´ í•˜ë½

		- ì¼ê´€ì„±ì´ ë–¨ì–´ì§

<br/>