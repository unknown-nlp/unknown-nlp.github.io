---
categories: paper-reviews
date: '2024-03-11 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - BitNet: Scaling 1-bit Transformers for Large Language Models'
giscus_comments: true
layout: post
related_posts: false
tags: transformer llm paper-review nlp
title: 'BitNet: Scaling 1-bit Transformers for Large Language Models'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2024-03-11
- **Reviewer**: ê¹€ì¬í¬
- **Property**: LLM, Quantization

The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits

---

[//]: # (table_of_contents is not supported)

---

## 1. Intro

### BitNet

- 1-Bit(1 or -1) parameterë¡œ scratchë¶€í„° í•™ìŠµ

- ê¸°ì¡´ LLM ëŒ€ë¹„ ì ì€ Inference/Train Costë¥¼ ê°€ì§

- ê¸°ì¡´ Post Quantization ë°©ë²•ë¡  ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ ê¸°ë¡

### 1-Bit

- 1.58Bit(1,0,-1) parameterë¡œ scratchë¶€í„° í•™ìŠµ

- ë™ì¼ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ëŠ” LLaMA êµ¬ì¡° ëŒ€ë¹„ ë†’ê±°ë‚˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ ê¸°ë¡

- (1,0,-1)ì˜ ìƒíƒœë¥¼ ê°€ì§€ëŠ” bit êµ¬ì¡°ë¥¼ ì´ìš©í•œ í•˜ë“œì›¨ì–´ ì„¤ê³„ë¥¼ í†µí•´ ëª¨ë¸ í•™ìŠµ/ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ìµœì í™” ë°©í–¥ì„± ì œì•ˆ

<br/>

### ê²°ë¡ 

- ì •ë§ ì œëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ ì˜ ëª¨ë¥´ê² ìŒ

- ìµœê·¼ LLMê³¼ ì—„ë°€í•œ ë¹„êµ ìˆ˜í–‰ X

- 1-BitëŠ” ê²°êµ­ ë”ì´ìƒì˜ Quantization ë¶ˆê°€

	â†’ FP32/FP16/BF16ì˜ ëª¨ë¸ë“¤ê³¼ ì •í™•í•œ ì„±ëŠ¥ ë¹„êµê°€ í•„ìš”

ğŸ’¡ ì¬ë°ŒëŠ” ì•„ì´ë””ì–´ì´ì§€ë§Œ ì´ ë°©ë²•ë¡ ì´ ë¯¸ë˜ì¸ì§€ëŠ” ë”ìš± ê²€ì¦ì´ í•„ìš”
â‡’ 70Bì˜ 1-bitê°€ knowledgeë¥¼ ì œëŒ€ë¡œ ë‹´ì„ ìˆ˜ ìˆì„ê¹Œ?
â‡’ Instruction Tuningê³¼ ê°™ì´ ë³µì¡í•œ íƒœìŠ¤í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì„ê¹Œ?

## 2<span style='color:green_background'>. BitNet</span>

### Architecture

- Transformerì˜ ì¼ë¶€ ë ˆì´ì–´ë¥¼ 1-bitë¡œ quantizationí•˜ì—¬ ì‚¬ìš©(original weightê°€ ì¡´ì¬)

	- Linear ë ˆì´ì–´: 1-Bit(1,-1) quantization

	- ì´ì™¸ ë ˆì´ì–´: 8-bit quantizationìœ¼ë¡œ ì—°ì‚° ì§„í–‰(attention, â€¦)

	- Input/Output Embedding: high precisionìœ¼ë¡œ ì§„í–‰(16 or 32 bit)

		â†’ Samplingì„ ìœ„í•´ì„œëŠ” high-precisionì´ í•„ìš”í•˜ê¸° ë•Œë¬¸

- BitLinear: ê¸°ì¡´ Transformer êµ¬ì¡°ì—ì„œ ì—°ì‚°ëŸ‰ì´ ë§‰ëŒ€í•œ Linear Layerë¥¼ ëŒ€ì²´

	- Input: 8 bit quantization

	- Input Quantization: AbsMax Quantization ì‚¬ìš©($ Q_b $:  quantizeí•  ë°ì´í„° ë²”ìœ„)

		â‡’ ë²¡í„°ë¥¼ maxë¡œ normalizing í›„ ë¶€í˜¸ë§Œ ë‚¨ê¹€

			- Input for Non-Linear Function Quantization

		- Activation Function(GELU)ì˜ ì…ë ¥ì˜ ê²½ìš° ë²”ìœ„ë¥¼ [0, $ Q_b $]ë¡œ ì œí•œ

			- Linear: 1 bit quantization

		- weightì˜ í‰ê·  ëŒ€ë¹„ í¬ê¸° ë¹„êµë¥¼ í†µí•´ Quantization ì‹¤í–‰

				- Matrix Multiplication: Quantized Linearì™€ Quantized Inputì€ ë‹¨ìˆœ ì—°ì‚°ì„ í†µí•´ ê³„ì‚° ê°€ëŠ¥

		$$ y=\tilde{W}\tilde{x} $$

		- í•˜ì§€ë§Œ ì´ëŒ€ë¡œ ìˆ˜í–‰í•œë‹¤ë©´ ê¸°ì¡´ LLMì˜ Layer Normì´ ì‚¬ë¼ì§

			â†’ Layer Norm: í•™ìŠµ ì•ˆì •í™” ë° ë°œì‚° ë°©ì§€

		- Input Quantization ì´ì „ì— Layer Normì„ ì ìš©

				- ì—°ì‚°ì´ ì™„ë£Œëœ ë²¡í„°ëŠ” ë‹¤ì‹œ Quantization ì‹œ ê³„ì‚° ëœ ìˆ˜ì¹˜ë¥¼ ì´ìš©í•˜ì—¬ Dequantization ì§„í–‰ â†’ Precision ë³µì›

- Pretrain ê³¼ì •ì—ì„œ Linear Layerì˜ ì—°ì‚°ëŸ‰ì„ ê°ì†Œ ë° ì†ë„ ê°œì„  ê°€ëŠ¥

- Distributed Training:

	- ê¸°ì¡´ Pretrainê³¼ ë‹¬ë¦¬ Input ë³„ë¡œ Quantization ìˆ˜ì¹˜ë¥¼ ê³„ì‚°í•´ì•¼ í•¨

		â‡’ ë¶„ì‚° í•™ìŠµ ì‹œ Machine ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚° â†’ Machine ê°„ í†µì‹  ë¹„ìš© ê°ì†Œ

- Mixed Precision Training

	- Forwarding ê³¼ì •

		- Linear Layer(FP16) ë° Sub-Module ë³„ Inputì— ëŒ€í•œ Quantization ì§„í–‰

			â†’ Low Precision(1Bit)ìœ¼ë¡œ Fowarding

	- Backwarding ê³¼ì •

		- Gradientì™€ Optimizer ë‚´ stateì€ ëª¨ë‘ high precision ì‚¬ìš©

		â†’ FP16 Linear Layer weight ì—…ë°ì´íŠ¸

- High Learning Rate

	- 1 bitë¡œ Quantized í•˜ë‹¤ë³´ë‹ˆ Learning Rateê°€ ë‚®ì„ ê²½ìš° ì‹¤ì œ weightì—ì„œ ì‘ì€ ë³€í™”ê°€ ë°œìƒ

		- 1.24214 â†’ 1.24232

		- 1 â†’ 1

		- í•™ìŠµ íš¨ê³¼ê°€ forwarding ê³¼ì •ì—ì„œ ë°˜ì˜ X

	- Learning Rateë¥¼ ëŒ€í­ ë†’í˜€ í•™ìŠµ ì§„í–‰

		- 2.4e-3 ~ 4e-4

### Experiments

- FP16 Trasnformerì™€ ë¹„êµ

	- 125m ~ 6.7bê¹Œì§€ Transformerì™€ BitNetì„ scratchë¶€í„° í•™ìŠµí•˜ì—¬ ë¹„êµ ì§„í–‰

- Quantization Methodì™€ ë¹„êµ

	- ê¸°ì¡´ Post Quantization ë°©ë²•ë¡ ë“¤ê³¼ ë¹„êµ ì§„í–‰(w:weight precision a: input precision)

					- fp16ì— ë¹„í•´ì„œëŠ” ë‚®ì§€ë§Œ quantization ë°©ë²•ë¡  ëŒ€ë¹„ ë§¤ìš° ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

- Energe Consumption ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ (zero/few shot)

		- ë™ì¼ ì—ë„ˆì§€ ì‚¬ìš© ì‹œ ë” ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

	- ë™ì¼ ì—ë„ˆì§€ ì‚¬ìš©=fp16 ëŒ€ë¹„ ë” í° ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥

## 3. 1.58bit

### Architecture

- 1.58 bitâ€¦?

	- bit= ì •ë³´ëŸ‰ì„ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì´ì§„ë¶„ë¥˜ í‘œê¸° ì²´ê³„ ë‹¨ìœ„

	- 1bit : (-1, 1)

		- 0: -1

		- 1: 1

	- 2bit: (0,1,2,3)

		- 00: 0

		- 01: 1

		- 10: 2

		- 11: 3

	- ë§Œì•½ BitNetì—ì„œ 0ë§Œ ì¶”ê°€í•œë‹¤ë©´?

		- weightë¥¼ í†µí•´ inputì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ ë§Œë“¤ ìˆ˜ ìˆìŒ

		- $ log_23 \approx 1.58 $

		- í˜„ì¬ í•˜ë“œì›¨ì–´ ìƒ êµ¬í˜„: 2 bit í•„ìš”

			- 00: 0

			- 11: 1

			- 01: -1

- 1.58bit: fp16ê³¼ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚´ë©´ì„œ inference costë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•

	- Modification: BitLinear êµ¬ì¡° ê±°ì˜ ê·¸ëŒ€ë¡œ í™œìš©

	- AbsMean Quantization ì‚¬ìš© (-1, 0, 1ë¡œ quantization)

			- non-linear function ì…ë ¥ì— ëŒ€í•œ scaling

		- BitNet: $ [0,Q_b] $

		- 1.58B: $ [-Q_b, Q_b] $

- ëª¨ë¸ êµ¬ì¡°: LLaMA configuration ì‚¬ìš©

### Experiments

- LLaMA Configurationì„ ì´ìš©í•˜ì—¬ FP16 Transformer/1.58B scratchë¶€í„° í•™ìŠµ

- StableLM-3Bì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ì‚¬ìš©(data recipe)

	- 2T token í•™ìŠµ

- ë©”ëª¨ë¦¬ ë° latencyì™€ PPL ê°„ ë¹„êµ

		- ë™ì¼ ëª¨ë¸ í¬ê¸° ì‹œ: ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš© ë° Latency

		â‡’ Quantized Weightë¥¼ ì´ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸

		- ë¹„ìŠ·í•œ PPL ê¸°ë¡

	- ë¹„ìŠ·í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ ì‹œ(LLaMA-700m vs BitNet b1.58 3B)

		- ë” ë†’ì€ ì„±ëŠ¥ ê¸°ë¡

- ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ Memory ë° Latency ê²½í–¥

		- ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ FP16ë³´ë‹¤ ë” ë¹ ë¥´ê³ , ë” ì ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

	- BitLinearê°€ ê°œì„ ì‹œí‚¤ëŠ” ë¶€ë¶„ì€ ëª¨ë¸ ë‚´ Linear ë ˆì´ì–´ ê´€ë ¨

		â†’ ëª¨ë¸ í¬ê¸°ê°€ ì»¤ì§ˆìˆ˜ë¡ í•´ë‹¹ íŒŒíŠ¸ì˜ ë¹„ì¤‘ì´ ì»¤ì§

- OpenSource LLMê³¼ ë¹„êµ

		- StableLM-3B ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ

		- ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ ì„±ëŠ¥ì´ ë” ì¢‹ì€ ëª¨ìŠµì„ ë³´ì„

		- ì†ë„ê°€ ë¹ ë¥¸ ê±´ ì´í•´ê°€ ë˜ëŠ”ë°, ì„±ëŠ¥ì´ ì¢‹ì€ ì´ìœ ì— ëŒ€í•œ ì–¸ê¸‰ì´ ì—†ìŒ

## 4. Conclusion

- BitNet

	- ì–˜ë„¤ Transformer í•™ìŠµí•  ë•Œ Drop-out ì•ˆì“°ëŠ”ë°ìš”â€¦

		â‡’ Transformer ì œëŒ€ë¡œ í•™ìŠµëœ ê²Œ ë§ëŠ”ì§€ ëª¨ë¥´ê²ŸìŒâ€¦

	- Quantizationì„ ìœ„í•´ Pretrainë¶€í„° Quantizationëœ í•™ìŠµì´ í•„ìš”í•˜ë‹¤ê³  ì£¼ì¥

		- 175Bì—ì„œë„ ìœ ì˜ë¯¸í• ì§€ëŠ” ìƒê°í•´ë´ì•¼ í•¨

		- í•™ìŠµì†ë„ê°€ ë¹ ë¥¸ì§€ë„ ì¤‘ìš”í•œ ìš”ì†Œ

			â†’ Fowarding ì†ë„ ê°œì„ , Not Backward

			â†’ í•™ìŠµ ì†ë„ ì¸¡ë©´ì—ì„œëŠ” ê°œì„ ì´ ì•ˆë˜ì—ˆì„ ìˆ˜ ìˆìŒ

			â‡’ ìì„¸í•œ ì–¸ê¸‰ X

- 1.58B

	- ì§€ë‚˜ì¹˜ê²Œ ë§ˆì¼€íŒ…ëœ ë…¼ë¬¸

		- 1.58Bì´ ì•„ë‹ˆë¼ ì‚¬ì‹¤ìƒ 2bit quantization

		- post-quantizationë„ ì•„ë‹ˆê³  pretrain quantization

		- ë…¼ë¬¸ì˜ ê°€ì¥ ê°•í•œ ì–¸ê¸‰: (1,0,-1)ì´ 1 bitì—ì„œ ê°€ëŠ¥í•œ í•˜ë“œì›¨ì–´ê°€ í•„ìš”í•˜ë‹¤

			â†’ ì •ë§ë¡œâ€¦?

		- FP16 ëª¨ë¸(StableLM)ë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì´ ê°€ëŠ¥í•œ ì´ìœ ì— ëŒ€í•œ ì–¸ê¸‰ X

		- Original Weight ì‚¬ìš© ì‹œ ì„±ëŠ¥ì€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ë„ ë¦¬í¬íŒ… X

		<br/>