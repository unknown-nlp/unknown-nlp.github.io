---
categories:
  - paper-reviews
date: "2024-09-09 00:00:00"
description: ë…¼ë¬¸ ë¦¬ë·° - Knowledge Distillation, LLM, Limited Budget ê´€ë ¨ ì—°êµ¬
giscus_comments: true
layout: post
related_posts: false
tags:
  - knowledge distillation
  - language-model
  - limited budget
  - llm
  - paper-review
  - reasoning
thumbnail: assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/thumbnail.jpg
title: "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2024-09-09
- **Reviewer**: hyowon Cho
- **Property**: Knowledge Distillation, LLM, Limited Budget

> Google DeepMind (2024-08-30)

# Introduction

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_000.png" class="img-fluid rounded z-depth-1" %}

ë§Žì€ ì—°êµ¬ë“¤ì´ ì´ë¯¸ ì–¸ì–´ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë° synthetic ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê³  ìžˆë‹¤. ì´ë“¤ ì¤‘, reasoning taskì—ì„œ ê°€ìž¥ ì¼ë°˜ì ì¸ ë°©ë²•ì€ í•˜ë‚˜ì˜ ì§ˆë¬¸ì— ëŒ€í•´ì„œ ì—¬ëŸ¬ ê°€ì§€ í›„ë³´ ë‹µë³€ë“¤ì„ ìƒì„±í•˜ê²Œ í•˜ê³ , ì´ë¥¼ gold answerì™€ ë¹„êµí•´, ë§žëŠ” ì •ë‹µì„ ê°€ì§„ ê²ƒë“¤ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¬ëŠ” ê²ƒì´ë‹¤.

í•˜ì§€ë§Œ, ì´ë ‡ê²Œ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ strong LMsë¡œ ë¶€í„° ìƒì„±í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¹„ì‹¸ê³ , resource-intensiveí•˜ë‹¤. ë˜í•œ í˜„ì‹¤ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ì˜ˆì‚°ì€ ì •í•´ì ¸ìžˆê¸° ë•Œë¬¸ì— ë§Œë“¤ ìˆ˜ ìžˆëŠ” solutionë„ ê·¸ë¦¬ ë§Žì§€ëŠ” ëª»í•˜ë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” fixed compute budget ìƒí™©ì—ì„œ, weaker but cheaper (WC) modelì´ ìš°ë¦¬ê°€ ì¼ë°˜ì ìœ¼ë¡œ ìƒê°í•˜ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ stronger but more expensive (SE) modelì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ ë‚«ë‹¤ê³  ì£¼ìž¥í•œë‹¤.

ì´ë¥¼ ì¦ëª…í•˜ê¸° ìœ„í•´ ì €ìžë“¤ì€ í¬ê²Œ 3ê°€ì§€ ì¶•ì—ì„œ ë°ì´í„°ë“¤ì— ëŒ€í•œ ë¹„êµë¥¼ ì§„í–‰í•œë‹¤.

1. coverage, the number of unique problems that are solved,

1. diversity, the average number of unique solutions we obtain per problem,

1. false positive rate (FPR), the percentage of problems that arrive at the correct final answer but with a wrong reasoning.

ë‹¹ì—°ížˆ ê³ ì •ëœ ì˜ˆì‚° í•˜ì—ì„œ, WC modelì´ SE modelë³´ë‹¤ ë” ë§Žì€ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìžˆë‹¤. í•˜ì§€ë§Œ SEê°€ ë‹¹ì—°ížˆ í€„ë¦¬í‹°ëŠ” ë†’ì„ ê²ƒ. ê·¸ë ‡ê¸° ë•Œë¬¸ì—, WCê°€ ë” ë†’ì€ coverage and diversity ê·¸ë¦¬ê³  ë™ì‹œì— higher FPRë¥¼ ê°€ì§ˆ ê²ƒì´ë¼ê³  ì´ì•¼ê¸°í•œë‹¤.

ì´í›„, ì €ìžë“¤ì€ ì´ ì¶”ì¸¡ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ì„œ SE and WCë¡œ ë§Œë“  ë°ì´í„°ë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ë“¤ì„ finetuningí•œë‹¤. ë‹¨ìˆœ í•˜ë‚˜ì˜ ë°©ë²•ì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°€ì§€ë¡œ.

1. knowledge distillation, where a student LM learns from a teacher LM (Hinton et al., 2015);

1. self-improvement, where an LM learns from self-generated data (Huang et al., 2022); and

1. a new paradigm we introduce called Weak-to-Strong Improvement, where a strong student LM improves using synthetic data from a weaker teacher LM.

ì €ìžë“¤ì€ ì—¬ëŸ¬ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì•„ì£¼ ì¼ê´€ì  SE-generated data ë³´ë‹¤ WC-generated dataë¡œ í•™ìŠµí•œ ê²°ê³¼ê°€ í›¨ì”¬ ì¢‹ìŒì„ ë³´ì¸ë‹¤ (ì¼ë°˜ì ì¸ ë¯¿ìŒê³¼ ë‹¬ë¦¬) ì¦‰, WC-generated dataì—ì„œ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ í›¨ì”¬ compute optimalí•˜ë‹¤ëŠ” ê²ƒ.

ì–´ëŠ ì •ë„ í° ìŠ¤ì¼€ì¼ì—ì„œ ë” ìž‘ì€ ëª¨ë¸ê³¼ í° ëª¨ë¸ì˜ ì„±ëŠ¥ ê°­ì´ ì ì°¨ ì¤„ì–´ë“¤ê³  ìžˆëŠ” ìš”ì¦˜, LM reasonersë¥¼ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•™ìŠµí• ì§€ ìƒê°ì„ í•˜ê²Œ í•œë‹¤.

# Preliminaries

- D = {ð‘žð‘–, ð‘Žð‘–} = training

- reasoning questions = ð‘žð‘–

- final answers = ð‘Žð‘–

ë‹¤ ì•„ì‹œê² ì§€ë§Œ, ì´ë“¤ì„ ê°€ì§€ê³  synthetic dataë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤:

1. sample multiple solutions for each ð‘žð‘– at a non-zero temperature

1. create the synthetic data with reasoning chain & generated answer

1. filter the incorrect solutions by comparing ð‘ŽË†ð‘– ð‘— to ð‘Žð‘– and removing the solutions whose final answer do not match that of the gold answer

### Metric

- ð‘ð‘œð‘£ð‘’ð‘Ÿð‘Žð‘”ð‘’@ð‘˜ (aka ð‘ð‘Žð‘ ð‘ @ð‘˜)

  - kê°œì˜ ì†”ë£¨ì…˜ì„ ìƒì„±í–ˆì„ ë•Œ, ìµœì†Œ í•˜ë‚˜ ì´ìƒì´ ì •ë‹µì„ ë§žì¶¤

- ð‘‘ð‘–ð‘£ð‘’ð‘Ÿð‘ ð‘–ð‘¡ð‘¦@ð‘˜

  - kê°œì˜ ë‹µë³€ì„ ìƒì„±í–ˆì„ ë•Œ, kê°œì¤‘ ì •ë‹µì„ ë§žì¶˜ solution ê°œìˆ˜ì˜ í‰ê· 

- false positive rate

  - ë‹µì€ ë§žì•˜ëŠ”ë°, reasoningì´ ìž˜ëª»ëœ ë¹„ìœ¨

# Compute-Matched Sampling and Training

ë‹¹ì—°ížˆ ê³ ì •ëœ ì˜ˆì‚°, ê³ ì •ëœ sampling budget (FLOPs) ìƒì—ì„œ, ì‚¬ëžŒë“¤ì€ ë” weaker but cheaper (WC) modelì„ í†µí•´ì„œ ë” ë§Žì€ ìƒ˜í”Œë“¤ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ë„ ìžˆê³  í˜¹ì€ stronger but more expensive (SE) modelì„ í†µí•´ ë” ì ì§€ë§Œ ì–‘ì§ˆì˜ ë°ì´í„°ë¥¼ ë§Œë“¤ ìˆ˜ ìžˆë‹¤.

WC modelê°€ ð‘ƒ*ð‘Šð¶ parametersë¥¼ ê°€ì§€ê³ , SEê°€ ð‘ƒ*ð‘†ð¸ parametersë¥¼ ê°€ì§„ë‹¤ê³  í•˜ê³ , ë‘ ëª¨ë¸ì„ ì´ìš©í•´ì„œ ë¬´ì¡°ê±´ ê°™ì€ ì˜ˆì‚°ë§Œì„ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤ê³  í•˜ë©´, ë§Œë“¤ ìˆ˜ ìžˆëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì°¨ì´ê°€ ë‚œë‹¤.

- Following (Kaplan et al., 2020), FLOPs per inference token = 2ð‘ƒ

- FLOPs for ð‘‡ inference tokens = 2ð‘ƒð‘‡

- assume that generating each solution requires an average of ð‘Š inference tokens

- ð‘†*ð‘Šð¶ and ð‘†*ð‘†ð¸ = number of samples we generate per question

- total cost

  - ð¶ð‘œð‘ ð‘¡ð‘Šð¶ = ð‘›Ã—ð‘†*ð‘Šð¶ Ã—ð‘Š Ã— (2ð‘ƒ*ð‘Šð¶)

  - ð¶ð‘œð‘ ð‘¡ð‘†ð¸ = ð‘›Ã—ð‘†*ð‘†ð¸ Ã—ð‘Š Ã— (2ð‘ƒ*ð‘†ð¸)

  - ð‘†*ð‘Šð¶ =(ð‘ƒ*ð‘†ð¸/ð‘ƒ*ð‘Šð¶)\* ð‘†*ð‘†ð¸

ì¦‰, ê³ ì • ì˜ˆì‚°ì—ì„œ ð‘ƒ_ð‘†ð¸/ð‘ƒ_WC ë§Œí¼ WCì—ì„œ ë” ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìžˆë‹¤ëŠ” ë§. ì´ë ‡ê²Œ ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë°ì´í„°ë¥¼ ë§Œë“  ì´í›„, ê³ ì •ëœ ìŠ¤í…ìœ¼ë¡œ ëª¨ë¸ë“¤ì„ í•™ìŠµì‹œì¼œë³´ê³  ë¹„êµí•˜ì—¬ ë°ì´í„°ë“¤ì˜ ìœ ìš©ì„±ì„ íŒë‹¨í•  ìˆ˜ ìžˆë‹¤.

ì–¸ê¸‰í–ˆë“¯ í•™ìŠµ ë°©ë²•ì€ 3ê°€ì§€:

1. knowledge distillation(Student-LM finetuning)

- ì¼ë°˜ì ìœ¼ë¡œ, student ëª¨ë¸ì˜ í•™ìŠµìš©ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ” ë°ì´í„°ëŠ” ë” ë˜‘ë˜‘í•˜ê³  ê°•í•œ ëª¨ë¸ì—ì„œ ë§Œë“¤ì–´ì§€ëŠ” ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ë†’ì€ í€„ë¦¬í‹°ë¥¼ ë³´ìž¥í•˜ê¸° ìœ„í•´ì„œ.

1. self-improvement

- Prior work (Singh et al., 2023)ëŠ” finetuning a WC model through self-generated dataëŠ” 1ì˜ ë°©ì‹ë³´ë‹¤ í›¨ì”¬ ë³„ë¡œë¼ê³  ì¦ëª…í•´ëƒˆë‹¤. í•˜ì§€ë§Œ, í•´ë‹¹ ì—°êµ¬ì˜ ë¹„êµ ë°©ì‹ì€ ê°™ì€ ì˜ˆì‚°ì„ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ì •ë‹¹í•˜ì§€ ì•Šë‹¤ê³  ì–¸ê¸‰í•œë‹¤. ë”°ë¼ì„œ, ë‹¤ì‹œ ì •ë‹¹í•˜ê²Œ ë™ì¼í•œ ì„¸íŒ…ìœ¼ë¡œ ë‹¤ì‹œ í•˜ì—¬ ì‹¤í—˜ì„ ìž¬ê°œí•œë‹¤.

1. ê·¸ë¦¬ê³  ì´ë“¤ì´ ì œì•ˆí•˜ëŠ” novel weak-to-strong improvement paradigm

- weak-to-strong improvement (W2S-I)ì€ ì¼ë°˜ì ì¸ ë°©ë²•ê³¼ ë‹¬ë¦¬, ê°•í•œ ëª¨ë¸ì€ ì•½í•œ ëª¨ë¸ì˜ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤.

- ì¦‰, ì•½í•œ ëª¨ë¸ë„ ê°•í•œ ëª¨ë¸ì„ ë°œì „ì‹œí‚¬ ìˆ˜ ìžˆë‹¤

# Experimental Setup

- Datasets

  - MATH

  - GSM-8K

- Data Generation

  - Gemma2 models for synthetic data generation

  - Gemma2-9B : Gemma2-27B = WC : SE models

  - MATH using a 4-shot prompt

  - GSM-8K using an 8-shot prompt

  - 9B modelê°€ 27Bì™€ 3ë°° ì •ë„ í¬ê¸° ì°¨ì´ê°€ ë‚˜ë¯€ë¡œ, ë°ì´í„°ë¥¼ 3ë°° ì •ë„ ë” ë§Œë“¤ ìˆ˜ ìžˆë‹¤.

  - ì‹¤í—˜ì—ì„œëŠ” ë‘˜ ë‚˜ ë‚®ì€ ì˜ˆì‚°ì˜ ê²½ìš°: a low budget, where we generate 1 and 3 candidate solutions per problem from Gemma2-27B and Gemma2-9B

  - ë†’ì€ ì˜ˆì‚°ì˜ ê²½ìš°: high budget, where we generate 10 and 30 candidate solutions per problem

- Synthetic Data Evaluation

  - ê°™ì€ ë¹„ìš©ì„ ê°€ì§€ëŠ” ê°œìˆ˜ë¼ë¦¬ coverge/diversity@k ê³„ì‚°

  - FRPëŠ” 50ê°œ for each modelì— ëŒ€í•œ human eval & 500ê°œì— ëŒ€í•œ LLM eval

- Evaluating Finetuned Models:

  - pass@1 accuracy

# Experiments and Results

- (Â§5.1) we analyze the data along various quality metrics .

- (Â§5.2) Subsequently, we present the supervised finetuning results for the different setups .

- (Â§5.3) Finally, we perform ablation studies to study the impact of dataset size, sampling
  strategy, and the role of quality dimensions in the model performance.

## 1. Synthetic Data Analysis

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_001.png" class="img-fluid rounded z-depth-1" %}

### 1) Coverage

ê²°ë¡ : Gemma2-9B (WC)ì´ Gemma2-27B (SE)ë³´ë‹¤ í›¨ì”¬ ì¢‹ì•˜ë‹¤.

- MATH:

- 11% and 6% at the low and high sampling budgets,

- 8% and 1% for GSM-8K.

ì¦‰, ë” ë§Žì´ ë§Œë“œëŠ” ê²ƒì´ í€„ë¦¬í‹°ê°€ ë” ë‚®ë”ë¼ë„ ë” ë§Žì´ ë¬¸ì œë¥¼ í‘¸ëŠ”ë° ë„ì›€ì´ ë˜ì—ˆë‹¤. converge trendëŠ” ë‹¤ì–‘.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_002.png" class="img-fluid rounded z-depth-1" %}

ì¶”ê°€ì ìœ¼ë¡œ, ë” ë§Žì´ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ê²ƒì€ ë¹„ë‹¨ ë‚®ì€ ë‚œì´ë„ì˜ ë¬¸ì œ ë¿ë§Œ ì•„ë‹ˆë¼ ë†’ì€ ë‚œì´ë„ì—ì„œë„ ê³µí†µì ì´ì—ˆë‹¤.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_003.png" class="img-fluid rounded z-depth-1" %}

ì˜¤ížˆë ¤ ë°˜ëŒ€ë¡œ, ë” í° ëª¨ë¸ë¡œ 10ê°œ ë§Œë“¤ ë•ŒëŠ” í’€ì§€ ëª»í–ˆë˜ ì–´ë ¤ìš´ ë¬¸ì œë„ ë” ì•½í•œ ëª¨ë¸ë¡œ 30ë²ˆ ë§Œë“œëŠ” ê²½ìš°ì— ìžì£¼ í’€ë ¸ë‹¤ê³ .

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_004.png" class="img-fluid rounded z-depth-1" %}

### 2) Diversity

Gemma2-9Bì—ì„œ ìƒì„±ëœ ë°ì´í„°ì˜ ë‹¤ì–‘ì„±ì€ Gemma2-27Bë³´ë‹¤ MATH ë°ì´í„°ì…‹ì˜ ì €ì˜ˆì‚°ì—ì„œ 86%, ê³ ì˜ˆì‚°ì—ì„œ 125% ë” ë†’ì•˜ìœ¼ë©°, GSM-8K ë°ì´í„°ì…‹ì—ì„œëŠ” ê°ê° 134%ì™€ 158% ë” ë†’ìŒ.

### 3) FPR

human í‰ê°€ì— ë”°ë¥´ë©´ WC ëª¨ë¸ì´ ìƒì„±í•œ í•´ê²°ì±…ì˜ FPRì´ MATHì—ì„œëŠ” SE ëª¨ë¸ë³´ë‹¤ 7% ë†’ì•˜ê³ , GSM-8Kì—ì„œëŠ” 2% ë†’ì•˜ìŒ.
-> ìƒê°ë³´ë‹¤ ì°¨ì´ëŠ” ì•ˆë‚˜ì§€ë§Œ, ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì¢‹ì§€ëŠ” ì•ŠìŒ.

## 2. Compute-Optimality Results for Training

ê·¸ë ‡ë‹¤ë©´ ì´ë“¤ì€ ë‹¤ì–‘í•˜ê²Œ í•™ìŠµí•´ë´¤ì„ ë•ŒëŠ” ì–´ë–¨ê¹Œ.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_005.png" class="img-fluid rounded z-depth-1" %}

1. Student-LM Finetuning
   Gemma-7Bë¥¼ WCì™€ SCë¡œ í•™ìŠµí•œ ê²°ê³¼.
   WC consistently outperforms the one finetuned on data from SC. ì¼ë°˜ì ì¸ ë¯¿ìŒê³¼ ë‹¤ë¥´ë‹¤.

1. WC-LM Finetuning
   Gemma2-9Bë¥¼ Gemma2-9Bê°€ ë§Œë“  WCì™€ 27Bê°€ ë§Œë“  SCë¡œ í•™ìŠµí•œ ê²°ê³¼. ëŒ€ë¶€ë¶„ WCê°€ ë” ë‚˜ì•˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ë„ ì¡´ìž¬. ì €ìžë“¤ì€ ì´ê²ƒì´ ë°ì´í„°ê°€ ë„ˆë¬´ ì‰¬ì›Œì„œë¼ê³  ì£¼ìž¥.

1. SE-LM finetuning
   Gemma2-27Bë¥¼ ì–‘ ë°ì´í„°ë¡œ í•™ìŠµ. ë” ìž‘ì€ ëª¨ë¸ë¡œ ë§Œë“  ë°ì´í„°ê°€ ë†€ëžê²Œë„ ë” ë„ì›€ì´ ë˜ì—ˆë‹¤.

> Takeaway: Overall, our findings challenge the conventional wisdom that advocates training on samples from the SE model, by showing that training on samples from the WC model may be more compute-optimal across various tasks and setups.

## 3. Ablation Studies

### Impact of Dataset Size

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_006.png" class="img-fluid rounded z-depth-1" %}

### Default vs Compute-Optimal Sampling from Cheap LMs

ê·¸ë ‡ë‹¤ë©´, ê¸°ì¡´ì— SEë¥¼ ì‚¬ìš©í•˜ë˜ ì¼ë°˜ì ì¸ ë°©ë²•ì²˜ëŸ¼ ë°ì´í„° ê°œìˆ˜ë¥¼ í†µì¼í•˜ë©´?

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_007.png" class="img-fluid rounded z-depth-1" %}

ë³¼ ìˆ˜ ìžˆëŠ” ê²ƒì²˜ëŸ¼, ë” í€„ë¦¬í‹° ì¢‹ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ë” ì¢‹ì€ ê²ƒì€ í™•ì‹¤í•˜ë‹¤.

### Coverage and Diversity:

ë°ì´í„°ì˜ ì •ë‹µë¥ ê³¼ ë‹¤ì–‘ì„±ì˜ ì¸¡ë©´ì—ì„œ ì–¼ë§ˆë‚˜ ì˜í–¥ë ¥ì´ í°ê°€ë¥¼ small scaleë¡œ ì‹¤í—˜í•œ ê²°ê³¼ë¬¼. 30ê°œì˜ ì§ˆë¬¸ìœ¼ë¡œ êµ¬ì„±.

1. high coverage, high diversity

1. high coverage, low diversity

- í•˜ë‚˜ì˜ ì§ˆë¬¸ ë‹¹ ë§žëŠ”ê±° í•˜ë‚˜ë§Œ

- reduces the diversity of the original WC dataset from 11 to 1, while maintaining the coverage.

1. low coverage, low diversity

- one solution per problem from the WC model + ë‚®ì€ ì •ë‹µë¥  ê°€ì§€ë„ë¡ ì¼ë¶€ëŸ¬ í•„í„°

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_008.png" class="img-fluid rounded z-depth-1" %}

# Scaling to state-of-the-art language models

ìœ„ì˜ ë¶€ë¶„ì—ì„œëŠ” open LMë“¤ì— ëŒ€í•´ì„œ ì‹¤í—˜í•œ ê²°ê³¼. ì´ ì„¹ì…˜ì—ì„œëŠ” Gemini-1.5-Pro and Gemini-1.5-Flashë¥¼ ì‚¬ìš©í•´ë³¸ë‹¤.

ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ ê³µì‹ì ìœ¼ë¡œ ê³µê°œë˜ì§€ ì•Šì•˜ê¸°ì—, compute-matched samplingì—ëŠ” pricing per output tokenì„ Proxyë¡œ ì‚¬ìš©í•œë‹¤ê³ ..

- synthetic data from the Pro (SE) - $10.5

- Flash (WC) models. - $0.3

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_009.png" class="img-fluid rounded z-depth-1" %}

> Takeaway: We demonstrate that price-matched sampling from weaker SoTA LMs produces
> superior reasoners compared to finetuning with data from stronger SoTA models.

# Conclusion

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_010.png" class="img-fluid rounded z-depth-1" %}
