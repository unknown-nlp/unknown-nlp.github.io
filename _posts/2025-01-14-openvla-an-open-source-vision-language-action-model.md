---
categories: paper-reviews
date: '2025-01-14 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - OpenVLA: An Open-Source Vision-Language-Action Model'
giscus_comments: true
layout: post
related_posts: false
tags: paper-review
title: 'OpenVLA: An Open-Source Vision-Language-Action Model'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2025-01-14
- **Reviewer**: ì „ë¯¼ì§„

## Summary

- ê¸°ì¡´ì˜ VLAëª¨ë¸ë“¤ì€ 1) closed, 2) ìƒˆë¡œìš´ taskì— íš¨ìœ¨ì ìœ¼ë¡œ FTí•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬

- í˜„ì¬ ê³µê°œëœ public modelì¤‘ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” vision-language-action model

	- llama-2-7Bëª¨ë¸ì— visual encoder 2ê°œ(SigLIP, DINOv2)ë¥¼ ì‚¬ìš©í•´ì„œ 970K real-world robot demonstration(Open-X-embodiment)ìœ¼ë¡œ í•™ìŠµ

	- training dataset ì™¸ì˜ ì—¬ëŸ¬ ë¡œë´‡ì„ controlí•  ìˆ˜ ìˆìŒ

- ê¸°ì¡´ì˜ closed modelì¸ RT-2-X(55B)ë³´ë‹¤ í›¨ì”¬ ì‘ì€ í¬ê¸°(7B)ë¡œ í›¨ì”¬ ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±

	- 29ê°œì˜ taskì— ëŒ€í•´ì„œ RT-X-2ë³´ë‹¤ 7ë°° ë‚®ì€ íŒŒë¼ë¯¸í„°ë¡œ 16.5% ì¢‹ì€ ì„±ëŠ¥ì„ ëƒ„

	- íŠ¹íˆ FTì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì„

		- LoRAë¡œ fine-tuningì´ ê°€ëŠ¥í•˜ë©°, quantizationì„ í•´ë„ í° hit ratio dropì´ ì—†ìŒ

- ì–´ë–»ê²Œ ëª¨ë¸ì„ êµ¬ìƒí•˜ë©´ ì¢‹ì€ì§€ì— ëŒ€í•œ insightê°€ ë§ì´ ë‚˜ì™€ ìˆìŒ

- ëª¨ë“  ì½”ë“œê°€ ê³µê°œ ë˜ì–´ ìˆì–´ ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—°êµ¬ë¥¼ ì‹œì‘í•˜ë©´ ì¢‹ìŒ

## Introduction

- robotic datasetì€ ëŒ€ëŸ‰ìœ¼ë¡œ êµ¬ì¶•í•˜ê¸°ê°€ ì–´ë ¤ì›€

	â‡’ ê¸°ì¡´ì˜ vision, language ëª¨ë¸ì˜ ëŠ¥ë ¥ì„ ì‚¬ìš©í•´ì„œ ì¼ë°˜í™”ê°€ ì˜ ë˜ë„ë¡ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì!

- ê¸°ì¡´ ì—°êµ¬

	- VLMì„ í†µí•©í•´ì„œ robotic representation learning í•™ìŠµ

	- task planning, executionì„ ìœ„í•´ modular systemìœ¼ë¡œ ì‚¬ìš©

	- ë”°ë¡œ VLA ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©

		- ë³´í†µ pretrained modelì„ ê°–ê³  ì™€ì„œ PaLIì²˜ëŸ¼ ë°”ë¡œ robot actionì„ ìƒì„±í•˜ë„ë¡ FT

	ğŸ’¡ multi-robot datasetì€ ì¼ì¢…ì˜ multi-ligual datasetëŠë‚Œìœ¼ë¡œ ì ‘ê·¼í•´ë³¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

	- ë…¼ë¬¸ì˜ ì €ìë“¤ì´ ì§€ì í•˜ëŠ” ê¸°ì¡´ ì—°êµ¬ì˜ ë¬¸ì œì 

		- current model are closed

		- ìƒˆë¡œìš´ ë¡œë´‡, í™˜ê²½, íƒœìŠ¤í¬ì— ì‚¬ìš©í•˜ê³  adaptingí•˜ê¸°ì— ìµœê³ ì˜ ë°©ì•ˆì„ ì œê³µí•˜ì§€ ì•ŠìŒ

- 970Kì˜ ë°©ëŒ€í•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ pretrainedëœ fully open source VLA model, OpenVLAë¥¼ ì œì•ˆ

	- FTê²°ê³¼ ê¸°ì¡´ì— FTë¡œ SOTAì˜€ë˜ Octoë³´ë‹¤ ì„±ëŠ¥ì´ í›¨ì”¬ ë†’ìŒ

	- multi-task setting, multi objectì— ëŒ€í•´ì„œ ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ

## Related works

- Visually-Conditioned Language Models

	- patchì„ tokenë‹¨ìœ„ë¡œ ë³´ê³ , pretrained visual transformersì—ì„œ featureë¥¼ ì¶”ì¶œ, language modelì˜ spaceë¡œ projectí•˜ì—¬ ì‚¬ìš©

	- multi-resolution visual featureì—ì„œ í•™ìŠµëœ pretrained backboenì„ ì‚¬ìš©

		- DINOv2ì—ì„œ ì–»ì€ low-levelì˜ spatial infomrationê³¼ SigLIPì—ì„œ ì–»ì€ higher-level semanticì„ ì„ì–´ì„œ visual generalizationì„ ë†’ì´ê³ ì í•¨

- Generalist Robot Policies

	- Octo - 1Bë°‘

		- out-of-the-boxì˜ ì—¬ëŸ¬ ë¡œë´‡ ì»¨íŠ¸ë¡¤ì´ ê°€ëŠ¥í•˜ë„ë¡ generalist policyë¥¼ í•™ìŠµ

		- ìƒˆë¡œìš´ ë¡œë´‡ ì„¸íŒ…ì— ëŒ€í•´ ìœ ì—°í•œ finetuningì´ ê°€ëŠ¥í•˜ë„ë¡ ã…ã…ã…

		- pretrained language embeddingê³¼ visual encoderë¥¼ model componentë¡œ ì‚¬ìš©, model ìì²´ëŠ” scratchë¶€í„° í•™ìŠµ

	- ì´ì™¸ ë‹¬ë¦¬, OpenVLAëŠ” ì¢€ ë” end-to-end ëŠë‚Œìœ¼ë¡œë‹¤ê°€, robot action ìƒì„±í•˜ê¸° ìœ„í•´ VLMì„ finetuning, actinoì„ langauge model vocabì—ì„œì˜ tokenìœ¼ë¡œ ì‚¬ìš©

- Vision-Language-Action Models

	- ê¸°ì¡´ì˜ ì—°êµ¬ë“¤ì€ ë³´í†µ VLM backboneì— robot control actionì„ fuse

		- ì¥ì  : ê¸°ì¡´ VLM componentì™€ align, ë¡œë´‡ë§Œì„ ìœ„í•œ ëª¨ë¸ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— VLMì˜ ì¸í”„ë¼ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŒ

		- ë‹¨ì  : single robot í˜¹ì€ simulated setupì— í•œì •ë˜ì–´ í•™ìŠµ, í‰ê°€ â‡’ generalityê°€ ë–¨ì–´ì§€ê³ , closed model / ëª¨ë¸ì´ í¬ê¸° ë•Œë¬¸ì— ì‘ì€ ì½”ë“œ ìˆ˜ì •ìœ¼ë¡œë„ Bë‹¨ìœ„ì˜ policyë¥¼ ë‹¤ã…£ã…… í•™ìŠµí•´ì•¼ í•¨

			- RT-2-Xì™€ OpenVLAì˜ ì°¨ì´

		- ì‘ì€ ëª¨ë¸ë¡œ ë” ë†’ì€ ì„±ëŠ¥

		- FTì„¸íŒ…ì— ëŒ€í•´ ë¶„ì„

		- PEFTê°€ëŠ¥

		- ê³µê°œ ëª¨ë¸

## Proposed Method

- VLA ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ìì„¸í•œ practiceëŠ” ì•„ì§ ì—°êµ¬ê°€ ëœ ë¨

	- ì–´ë–¤ backbone, datasetì´ ì¢‹ì€ì§€, hyperparameterì— ë”°ë¼ ì„±ëŠ¥ì´ ì–´ë–¤ì§€..

- Preliminaries: Vision-Lnaguage Models

	[ë³´í†µì˜ VLM êµ¬ì¡°]

	- visual encoder : imageì„ patch ë‹¨ìœ„ë¡œ ì„ë² ë”©

	- projector : image embeddingì„ language spaceë¡œ mapping

	- LLM : ë³´í†µ í•™ìŠµì€ paired ë˜ëŠ” interleaved VL dataì— ëŒ€í•´ next token perdictionìœ¼ë¡œ ì§„í–‰

	- ë³¸ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©í•œ backbone VLMì€ Prismatic-7B VLM

		- Prismatic-7B : 600M visual encoder, 2-layer MLP projector, 7B llama2 LM

			- 2íŒŒíŠ¸ì˜ visual encoderì‚¬ìš© : SigLIP, DINOv2

			- LLaVA 1.5 data mixtureë¡œ FTë¨

				- 1M image-text and text-only data samples from open sources

	- OpenVLA training procedure

	- action predictionì„ vision-language taskë¡œ formatting

		- image, textë¥¼ inputìœ¼ë¡œ ë„£ì–´ì„œ string of robot actionì„ ìƒì„±í•˜ë„ë¡ í•¨

	- continous robot actionì„ discrete token actionìœ¼ë¡œ mapping

		- ë¡œë´‡ actionì˜ ê° ì°¨ì›ì„ 256 binìœ¼ë¡œ ìª¼ê°¬

		- ê° action ì°¨ì›ì— ëŒ€í•´ì„œ bin widthë¥¼ ì¼ì •í•˜ê²Œ ì„¤ì •(1st, 99th quntileì— ìˆëŠ” train datasetì— ìˆëŠ” actino ë¶„í¬ ê¸°ë°˜)

		- 256ê°œì˜ ìƒˆë¡œìš´ tokenì„ vocabì— ì¶”ê°€í•¨(ë¶€ì¡±í•œ ìŠ¬ë¡¯ì€ ì˜ ì•ˆì“°ëŠ” tokenì— overwriting)

- Training data

	- Open X datasetì„ ì•Œë§ê²Œ curation

		- ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¼ê´€ëœ input, output space ì‚¬ìš©

			- í•œì†ë¡œë´‡, 1/3 person camera dataë°˜ í™œìš©

		- embodiment ë¹„ìœ¨ ì¡°ì •

			- samplingì‹œ ë§ìœ¼ë©´ down weight, ì ìœ¼ë©´ upweight í™œìš©

		- Octo ì¶œì‹œ ì´í›„ ì¶”ê°€ëœ DROID datasetë„ ê°™ì´ ì„ì–´ì„œ í•™ìŠµí•´ë´¤ìœ¼ë‚˜ ë³„ë¡œ ë„ì›€ ì•ˆëë‹¤ê³  í•¨(10%ë¼ í° ì˜í–¥ì´ ì—†ëŠ”ê±° ê°™ë‹¤ê³  í•¨)

- OpenVLA Design decision

	: BridgeDataV2ë¡œ ë¯¸ë¦¬ ë””ìì¸ ê´€ë ¨ ì‹¤í—˜ì„ í•¨

	- VLM Backbone

		- IDEFICS-1ê³¼ LLaVA, Prismaticìœ¼ë¡œ ì‹¤í—˜

			- prismatic : visually conditioned LM(LLaVAë³´ë‹¤ ë†’ì€ ì„±ëŠ¥, visual encoderëŠ” freeze, projectorì™€ LMë§Œ í•™ìŠµ)

			- IDEFICS < LLaVA < Prismatic ìˆœì˜ ì„±ëŠ¥ì„ ë³´ì„

				- IDEFICSì™€ LLaVAëŠ” ì¥ë©´ì— í•˜ë‚˜ì˜ objectê°€ ìˆì„ ë•ŒëŠ” ì˜ë¨

				- LLaVAê°€ Iì— ë¹„í•´ì„œ ì¢€ ë” language groundingì´ ì˜ ë˜ê¸´ í•¨

				- Prismaticì´ ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¨ ì´ìœ ëŠ” SigLIP-DINOv2 backboneë•Œë¬¸ì¸ê±¸ë¡œ ì¶”ì •

	- Image resolution

		- ì €í™”ì§ˆ(224*224)ì´ë‚˜ ê³ í™”ì§ˆ(384*384)ì´ë‚˜ í° ì„±ëŠ¥ ì°¨ì´ëŠ” ì—†ìœ¼ë‚˜ ê³ í™”ì§ˆì´ í•™ìŠµì‹œê°„ 3ë°° ë” ì†Œìš”

	- Fine-tuning vision encoder

		- VLMí•™ìŠµí•  ë•ŒëŠ” visual encoderë¥¼ í•™ìŠµí•˜ì§€ ì•ŠìŒ

		- í•˜ì§€ë§Œ VLAì—ì„œëŠ” visual encoderë¥¼ FTí•˜ëŠ”ê²Œ ë§¤ìš° ì¤‘ìš”

			â‡’ ê¸°ì¡´ ë¹„ì „ íƒœìŠ¤í¬ì—ì„œ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì™€ ë‹¤ë¥´ê¸° ë•Œë¬¸.. ì´ë¼ ì¶”ì •

	- Training epochs

		- ë§ì´ ëŒë¦´ìˆ˜ë¡ real robot performanceê°€ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒ

		- 27ë²ˆ ëŒë¦¼

	- Learning rate

		- 2e-5(VLMì‚¬ì „ í•™ìŠµí•  ë•Œì™€ ê°™ì€ learning rate)

- Intrastructure for trainign and inference

	- 64ëŒ€ A100ìœ¼ë¡œ 14ì¼, 21500 A100 Hours, batch sizeëŠ” 2048

	- inferenceì‹œ OpenVLAëŠ” 15GBì˜ GPU memí•„ìš”(bf16), RTX 4090 GPUê¸°ì¤€ 6Hzì˜ ì†ë„

	- remote VLA inferenceë„ ì½”ë“œ ìˆë‹¤ê³  í•¨

## Experiments

- Research question

	- OpenVLAê°€ ê¸°ì¡´ ëª¨ë¸ë“¤ì— ë¹„í•´ ì—¬ëŸ¬ ë¡œë´‡ê³¼ ë‹¤ì–‘í•œ ì¡°ê±´ì˜ generalizationì—ì„œ ì„±ëŠ¥ì´ ì–´ë–¤ê°€?

	- OpenVLAë¥¼ ìƒˆë¡œìš´ ë¡œë´‡ ì„¸íŒ…ê³¼ íƒœìŠ¤í¬ì— ëŒ€í•´ íš¨ê³¼ì ìœ¼ë¡œ FTí•  ìˆ˜ ìˆëŠ”ê°€

	- OpenVLAì˜ í•™ìŠµê³¼ inferenceì‹œ PEFTì™€ quantizationë°©ë²•ë¡ ì„ ì ìš©í•´ computational costë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ”ê°€?

- Direct Evaluation on Multiple Robot Platforms

	- Robot setups and task

		- unseen robot : WinowX from BridgeData V2, mobile manipulation robot from RT-2 and RT-2 evaluation(Google robot)

		- generalization axes

			- visual : ìƒ‰, ë°°ê²½, distractor

			- motion : ìœ„ì¹˜, ì‹œì‘ì 

			- physical : í¬ê¸°, ëª¨ì–‘

			- semantic : ëª¨ë¥´ëŠ” target object, instruction

	- Comparision

		- RT-1-X(35M)

		- RT-2-X(55B)

		- Octo(93M)

	- OpenVLAì˜ ì••ë„ì ì¸ ì„±ëŠ¥

		- ë” í° robot action dataset + dataset cleaning, SigLIPê³¼ DINOv2ë¥¼ ê°™ì´ ì¼ê¸° ë•Œë¬¸ì¸ê±¸ë¡œ ì¶”ì •

	- RT-2-XëŠ” ê·¸ë˜ë„ ì–´ëŠì •ë„ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ì„

		- í° ë²”ìœ„ì˜ internet pretraining + robot actionê³¼ internet pretraining dataë¥¼ ê°™ì´ í•™ìŠµ

	- RT-1-Xì™€ OctoëŠ” distractorê°€ ë‚˜íƒ€ë‚˜ë©´ ê°ˆí”¼ë¥¼ ëª»ì¡ìŒ

	- Octo ë…¼ë¬¸ì—ì„œëŠ” octoê°€ RT-2-Xë³´ë‹¤ ì˜í•˜ëŠ”ê±° ê°™ì•˜ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ì•„ë‹˜

- Data-Efficient Adaptation to New Robot Setups

	- ì‹¤í—˜ ì„¸íŒ…

		- 10-150ê°œì˜ demonstrationìœ¼ë¡œ ì´ë¤„ì§„ target datasetì— ëŒ€í•´ì„œ full FT

		- Franka-Tabletop : ì±…ìƒì— 7-DoF robot arm ê½‚ì•„ ë†“ê³  ì‹¤í—˜

		- Franka-DROID : ëª¨ì…˜ë°ìŠ¤í¬ì—ì„œ ì‹¤í—˜

		- ë¹„êµêµ°

			- diffusion policy : SOTA data-efficient imitation learning ë°©ë²•

				- diffusion policy : 3ê°œ ë³´ê³  8ê°œ ì˜ˆì¸¡

				- diffusion policy(matched) : 1ê°œ ë³´ê³  1ê°œ ì˜ˆì¸¡

			- Octo : target taskì— ëŒ€í•´ì„œ FT

			- OpenVLAë„ í•˜ë‚˜ ë„£ì–´ì„œ í•˜ë‚˜ ì˜ˆì¸¡í•˜ëŠ”ê±¸ë¡œ ì•Œê³  ìˆìŒ

	- ê²°ê³¼

		- pretrained modelì— FTí•˜ëŠ”ê²Œ ì„±ëŠ¥ì´ ë†’ì•˜ìŒ

			â‡’ ì–¸ì–´ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ì— ì˜ adaptë˜ë„ë¡ í•¨

		- ë‹¤ë¥¸ ì• ë“¤ì€ narrow single instruction(diffusion policy) or diversity instruction ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì˜í•˜ëŠ”ë°, openVLAëŠ” ë‹¤ ì˜í•¨

	- Parameter-Efficient Fine-tuning

	- ë¹„êµêµ°

		- Full FT

		- last layer only : transformer backboneì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ í•™ìŠµ

		- sandwich FT : visual encoder, token embedding matrix, last layerë§Œ í•™ìŠµ

		- LoRA

	- ì‹¤í—˜ ê²°ê³¼

		- LoRAê°€ ì§±!

			- A100 í•œëŒ€ 10-15ì‹œê°„ì´ë©´ í•™ìŠµ ê°€ëŠ¥, full FTì— ëŒ€í•´ì„œ computation 8ë°° ë‹¨ì¶•

	- Memory-Efficient Inference via quantization

	- 4bit quantizationì„ ì‚¬ìš©í•˜ë©´ GPU memì€ ë°˜ì ˆ, ì„±ëŠ¥ ìœ ì‚¬, HzëŠ” í–¥ìƒ(bf16ê³¼ ë¹„êµì‹œ)

	## Discussion and Limitation

- single-image observationsë§Œ ì§€ì›

- throughtputì„ í–¥ìƒì‹œí‚¤ëŠ”ê²Œ ì¤‘ìš”

- ì„±ëŠ¥ í–¥ìƒì˜ ì—¬ì§€ëŠ” ìˆìŒ - ì•„ì§ 90%ë„ ë‹¬ì„±í•˜ì§€ ëª»í•¨