---
categories: paper-reviews
date: "2025-06-10 00:00:00"
description: " ë…¼ë¬¸ ë¦¬ë·° - DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like
  Training of Large Language Models"
giscus_comments: true
layout: post
related_posts: false
tags: llm paper-review
title: "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training
  of Large Language Models"
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2025-06-10
- **Reviewer**: ê±´ìš° ê¹€

- ìµœê·¼ì— post-trainingì„ ìœ„í•œ RLì—ì„œ **GRPO**ì™€ ê°™ì´ low-resource settingsì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ

  **â†’ GRPOëŠ” solution-levelì˜ scalar reward signalsì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„, samplingëœ ë¬¸ì¥ë“¤ê°„ì˜ semantic diversityë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•¨**

  â†’ ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ reasoning pathë¥¼ ê°–ëŠ” responseë“¤ì´ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ë™ì¼í•œ rewardë¥¼ ë°›ëŠ” (**diversity-quality inconsistency**) ë¬¸ì œê°€ ìˆìŒ

- ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reward computation ê³¼ì •ì—ì„œ semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ” ë°©ë²•ì¸ **Diversity-aware Reward Adjustment (DRA)**ë¥¼ ì œì•ˆí•¨

- DRAëŠ” Submodular Mutual Information (SMI)ë¥¼ í™œìš©í•˜ì—¬

  1.  ì¤‘ë³µëœ responseì˜ rewardëŠ” ê°ì†Œì‹œí‚´

  1.  diverse responseì˜ rewardëŠ” ì¦í­ì‹œí‚´

- 5ê°œ Mathematical Reasoning benchmarkì—ì„œ recent methods ëŒ€ë¹„ outperform ì„±ëŠ¥ ë³´ì—¬ì¤Œ

  (ë‹¨ 7,000ê°œ sampleë¡œë§Œ fine-tuningì„ í•˜ê³ , $55 training costë¡œ í‰ê·  acc 58.2% sota ë‹¬ì„±)

DeepSeek-R1-Zero (Guo et al., 2025)ì—ì„œ ê¸°ì¡´ LLMì— SFTë¥¼ ì ìš©í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜, base LMì— ë°”ë¡œ RLì„ ì ìš©í•  ìˆ˜ ìˆëŠ” R1-Zero training pipelineì„ ì œì•ˆí•¨.

â†’ Group Relative Policy Optimization (GRPO) ì•Œê³ ë¦¬ì¦˜ ë•ë¶„ì— ê°€ëŠ¥í•œ ë°©ë²•

GRPOëŠ” PPOì™€ ë‹¤ë¥´ê²Œ critic model ì—†ì´ ì£¼ì–´ì§„ promptì— ëŒ€í•´ ì—¬ëŸ¬ samplingëœ completionsì˜ relative performanceì— ëŒ€í•œ advantageë¥¼ í‰ê°€í•¨.

<br/>

í•˜ì§€ë§Œ ìµœê·¼ì— ê³µê°œëœ GRPO ë° ê·¸ variants (e.g,. DR. GRPO)ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì •ë‹µ ì—¬ë¶€ì™€ ê°™ì€ <span style='color:red'>**solution-levelì˜ scalar reward signalsì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆì–´, ê°™ì€ ì •ë‹µì´ë¼ë„ diverse reasoning pathì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨**</span>.

â†’ ì´ëŠ” semanticí•˜ê²Œ ë‹¤ë¥¸ completionsë“¤ì´ ì˜¬ë°”ë¥´ê±°ë‚˜ í‹€ë¦° ê²½ìš° ëª¨ë‘ ê±°ì˜ ë™ì¼í•œ rewardsë¥¼ ë°›ì•„, ì˜ë¯¸ ìˆëŠ” reasoning ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” **indistinguishable advantage estimates**ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆìŒ

- **Example1 (Correct Reward)**

  GRPO training processì˜ examplesì„ ì¤€ë¹„í•¨. â†’ ì €ìë“¤ì˜ key motivation of research

  - LLMì€ diverse answerë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŸ° answersë“¤ì€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ reward scoreë¥¼ ë°›ìŒ

    â†’ ì¦‰, solution-level judgementsëŠ” different reasoning pathsë¥¼ êµ¬ë³„í•˜ì§€ ëª»í•¨

  ì•„ë˜ëŠ” questionì— ë™ì¼í•œ ì •ë‹µì„ ìƒì„±í–ˆì§€ë§Œ reasoning pathê°€ ì™„ì „íˆ ë‹¤ë¥¸ ë‘ê°€ì§€ ì‘ë‹µì— ëŒ€í•œ ì¼€ì´ìŠ¤ (ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  rewardëŠ” ë¹„ìŠ·í•¨)

      	- **Example2 (Incorrect Reward)**

  ì´ë²ˆ ì˜ˆì‹œëŠ” Questionì— ëŒ€í•´ ë‘ê°€ì§€ ì‘ë‹µì´ ëª¨ë‘ Incorrectì¸ ë°˜ë©´, reasoning pathëŠ” ì„œë¡œ ë‹¤ë¦„ â†’ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  reward scoreëŠ” ë‘˜ ë‹¤ ë¹„ìŠ·í•˜ê²Œ ë‚®ìŒ

      			â†’ ë˜í•œ, ì´ëŠ” resource-constrained settingsì—ì„œ ë” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ

  - ê° promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” completionsì˜ ê°œìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì—, <span style='color:orange_background'>**rewardê°€ ë†’ì€ outputsì— ëŒ€í•œ exploitationë§Œ reinforceí•˜ë©°, alternativeí•˜ê³  potentially validí•œ reasoning pathsì— ëŒ€í•œ explorationì„ ìœ ë„í•˜ì§€ ëª»í•¨.**</span>

  - (ë¹„ìœ ) ì„ ìƒë‹˜ì´ ìˆ˜í•™ ë¬¸ì œë¥¼ ëª¨ë‘ ì •í™•í•˜ê²Œ í‘¼ í•™ìƒë“¤ì—ê²Œ 100ì ì„ ì£¼ëŠ” ì¼€ì´ìŠ¤. ì •ë‹µì´ ë§ë”ë¼ë„, í•™ìƒì˜ ì´í•´ë„ë‚˜ ì‚¬ê³  ë°©ì‹ì„ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¤ì–‘í•œ ë°©ì‹ì€ í‰ê°€ë˜ì§€ ì•Šê³ , ì˜¤ë‹µì¼ ê²½ìš°ì—ë„ ê·¸ ê³¼ì •ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ë‹¤ì–‘í•œ ì¶”ë¡  ì ‘ê·¼ì„ í‰ê°€í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ê°™ì€ ê°ì ì„ ë°›ìŒ.

<br/>

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì €ìë“¤ì€ <span style='color:orange_background'>**Diversity-aware Reward Adjustment (DRA)**</span>ë¥¼ ì œì•ˆí•¨.

ì´ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ samplingëœ completions ê°„ì˜ _semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ_ ê·¸ë£¹ ë‚´ ë‹¤ë¥¸ *completionsê³¼ì˜ semantic similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° outputì˜ rewardë¥¼ reweight*í•¨.

- <span style='color:orange_background'>**diverse completionsì—ëŠ” ë” ë†’ì€ weight, ì¤‘ë³µëœ completionì—ëŠ” ë” ë‚®ì€ weight ë¶€ì—¬**</span>

### Preliminary

LMì˜ generationì€ token-level Markov Decision Processë¡œ ë³¼ ìˆ˜ ìˆìŒ. ê° generation step $ t $ì—ì„œ state $ s*t $ëŠ” input question $ q $ì™€ ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ partial output sequence $ o*{<t} $ì˜ concatenationì´ê¸°ì—, satesëŠ” ë‹¤ìŒê³¼ ê°™ìŒ $ s*t=[q;o*{<t}] $.

policy $ \pi*{\theta}(.|s_t) $ëŠ” vocab set $ A $ì—ì„œ next token $ o_t $ë¥¼ ì„ íƒí•˜ê³ , ì´ëŠ” deterministic transitionì„ ìœ ë„í•˜ì—¬ next state $ s*{t+1}=[s_t;o_t] $ë¡œ ì´ë™í•¨.

GRPOëŠ” ê° question $ q $ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ responses $ C= ${$ o_1,...o_G $}ë¥¼ samplingí•˜ê³ , ê° responseì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•¨ $ R= ${$ R(q,o_1), ... , R(q,o_G) $}

ê³„ì‚°ëœ reward $ R $ì„ ì´ìš©í•´ advantage $ A\_{i,t} $ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•¨ (normalize)

GRPOì˜ objective function $ J*{GRPO}(\pi*{\theta}) $ë¥¼ optimizeí•¨

<br/>

ì´í›„ ì—°êµ¬ì¸ DR.GRPO (Liu et al., 2025)ì—ì„œëŠ” token efficiencyë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ **GRPOì˜ objective functionì—ì„œ â€˜response lengthâ€™ termê³¼ Advantageì—ì„œ stdë¡œ normalizeí•´ì£¼ëŠ” termì„ ì§€ì›€**

- DR.GRPO (Zichen Liu, et al 2025)

      GRPOì˜ ë‘ ê°€ì§€ í¸í–¥

  1.  **Response-level length bias**

      1.  ê° responseì— ëŒ€í•´ í‰ê· ì„ êµ¬í• ë•Œ, $ 1/|o_i| $ê°€ ê³±í•´ì§€ëŠ”ë°,

          1. Correct responseì¸ ê²½ìš° advantageê°€ ì–‘ìˆ˜ì´ë©´ shorter responseì— ëŒ€í•´ì„œ greater gradient updatesë¥¼ ì•¼ê¸°í•¨ â†’ ì´ PolicyëŠ” correct answerì— ëŒ€í•´ brevity favorìˆìŒ

          1. Incorrect responseì¸ ê²½ìš° advantageê°€ ìŒìˆ˜ longer responseëŠ” $ |o_i| $ê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— penalizedë¥¼ ë” ë°›ìŒ â†’ ì´ PolicyëŠ” Incorrect answerì— ëŒ€í•´ ê¸¸ê²Œ ë§í•˜ëŠ” favor ìˆìŒ

          â‡’ ì‰½ê²Œ ë§í•˜ë©´ GRPOì˜ ê°œë³„ advantageë¥¼ $ A\_{i,t}/|o_i| $ë¡œ ë³´ë©´

              - positive advantage ì— ëŒ€í•´ì„œëŠ” ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ ì‘ì„ìˆ˜ë¡ updateê°€ ì»¤ì§€ê¸°ì— ì§§ì€ responseì— ê°•í•œ signalì„ ì£¼ê³ 

              - negative advantage ì— ëŒ€í•´ì„œ ì—­ì‹œ ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ í´ìˆ˜ë¡ updateê°€ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ê¸´ responseì— ê°•í•œ signalì„ ì¤Œ (=ê¸´ ì˜¤ë‹µì€ under-penalized)

              <span style='color:orange_background'>**â†’ ì¦‰, GRPOëŠ” ì •ë‹µì€ ì§§ê²Œ, ì˜¤ë‹µì€ ê¸¸ê²Œ ë§í•˜ê²Œë” biased policyë¥¼ ìœ ë„í•¨**</span>

  1.  **Question-level difficulty bias**

      1. advantageë¥¼ íŠ¹ì • question ë‚´ì˜ group averageì™€ stdë¡œ normalizationì„ í•˜ê¸° ë•Œë¬¸ì—, stdê°€ ì‘ìœ¼ë©´ ìƒëŒ€ì ìœ¼ë¡œ í•´ë‹¹ questionì— ëŒ€í•œ training signal (weight update)ê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§

         â†’ ì¼ë°˜ì ì¸ RLì—ì„œëŠ” batch ë‹¨ìœ„ë¡œ normalizationë˜ì–´ biasë¥¼ ìƒì‡„ì‹œí‚¤ì§€ë§Œ, GRPOëŠ” question ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë˜ì–´ ê·¸ë ‡ì§€ ëª»í•¨

      â‡’ ìœ„ì™€ ê°™ì€ ë¬¸ì œëŠ” LMì˜ responseê°€ ê¸¸ì–´ì§€ëŠ” ì´ìœ ê°€ reasoning capability ë•Œë¬¸ì¸ì§€ ì•„ë‹ˆë©´ bias ë•Œë¬¸ì¸ì§€ êµ¬ë¶„í•˜ê¸°ê°€ ì–´ë ¤ì›Œì§. ì´ì— ë”°ë¼ <span style='color:orange_background'>**unbiased optimization methodì¸ DR.GRPO ì†Œê°œí•¨**</span>

  **Question1**: ìœ„ì—ì„œ ë¶„ëª… Correct responseì— ëŒ€í•´ì„œëŠ” ì§§ì•„ì§€ëŠ”ë° ì™œ responseê°€ ê¸¸ì–´ì§„ë‹¤ê³  í‘œí˜„í•˜ëŠ”ì§€?

  â‡’ (ê±´í”¼ì…œ) Correct response ê°œìˆ˜ë³´ë‹¤ Incorrect responseì˜ ìˆ˜ê°€ ë” ë§ê¸°ì—, ëŒ€ë¶€ë¶„ì˜ responseëŠ” RL trainingì—ì„œ ì˜¤ë‹µì´ë¼ ê¸¸ê²Œ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìƒê¹€

  **Question2**: GRPOë¥¼ ë³´ë©´ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ reasoning accuracyê°€ ì˜¬ë¼ê°€ëŠ”ë°, ê·¸ëŸ¬ë©´ biasì— ë”°ë¼ response lengthê°€ ì§§ì•„ì ¸ì•¼ í•˜ëŠ”ê±° ì•„ë‹Œê°€?

  â‡’ (ê±´í”¼ì…œ) complex tasksì—ì„œëŠ” ì•„ë¬´ë¦¬ ì˜¤ë˜ í•™ìŠµì‹œì¼œë„ ë†’ì€ accì— ë„ë‹¬í•˜ëŠ” ëª¨ë¸ì´ ì—†ì–´ì„œ ê·¸ë ‡ì§€ ì•Šì„ê¹Œ..?

  <br/>

  DR.GRPOëŠ” GRPOì˜ optimization biasë¥¼ ì—†ì• ê¸° ìœ„í•´ ì•„ë˜ ë‘ê°€ì§€ termsì„ ì—†ì•°

      R1-Zeroì™€ ë¹„ìŠ·í•œ minimal recipeë¡œ í•™ìŠµí•œ Oat-Zero-7B ì„±ëŠ¥

  - minimal recipe

    - base model: Qwen2.5-Math-7B

    - training dataset: MATH datasetì˜ level3~5

    - GPU: 8xA100 27hrs

    ì•„ë˜ (ìš°ì¸¡) ê·¸ë¦¼ì„ ë³´ë©´

  - (solid lines) DR.GRPOëŠ” reasoning accuracyê°€ ì§€ì†ì ìœ¼ë¡œ ì˜¤ë¥´ëŠ” ë°˜ë©´ì—, GRPOëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ (ë¶ˆì•ˆì •í•¨)

  - (dashed lines) DR.GRPOëŠ” response lengthê°€ ì§§ê³  ì•ˆì •ì ì¸ ë°˜ë©´ì—, GRPOëŠ” response lengthê°€ ê³„ì† ê¸¸ì–´ì§

  <br/>

<br/>

### Diversity-Quality Inconsistency

GRPOì™€ DR.GRPOì˜ reward signalì€ **solution-level correctness**ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° completionì— ëŒ€í•´ **sparse scalar judgement**ë¥¼ ê³„ì‚°í•¨.

â†’ ì´ëŸ¬í•œ scalar rewardëŠ” ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ëŠ” diverse reasoning-pathë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, Diversity-Quality Inconsistencyê°€ ë°œìƒí•¨.

ìœ„ì— Example ë§ê³ , ë³´ë‹¤ ì‹¤ì¦ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ statement (â€**_reward alone fails to reflect the underlying variability in reasoning strategies_**â€) ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ embedding distancesë¡œ ì¸¡ì •ëœ completionsì˜ structural dissimilarityë¥¼ ê³„ì‚°í•¨.

- Spearmanâ€™s rank correlationì„ ì‚¬ìš©í•˜ì—¬ sampled completions ì‚¬ì´ì—ì„œ reward differenceì™€ semantic distanceë¥¼ ì¸¡ì •í•¨ â†’semantic distanceê°€ ì»¤ì§ˆìˆ˜ë¡ reward ì°¨ì´ë„ ì»¤ì§€ëŠ”ê°€?

  - 3000ê°œ prompt ë½‘ì•„ì„œ p-value ì¸¡ì •

- Figure2ëŠ” Spearmanâ€™s rank correlationì˜ p-valuesì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ p-valueê°€ significance levelì¸ 0.05 ë³´ë‹¤ í° ê°’ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œë¡œ 80% ì´ìƒì˜ promptì— ëŒ€í•´ statistically significant correlationì´ ì—†ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  â†’ ì¦‰, rewardê°€ semantic diversityì™€ ìƒê´€ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ

  <br/>

  $ \tilde{a} \text{ì´ë ‡ê²Œ} $

### Diversity-aware Reward Adjustment

Diversity-Quality Inconsistency ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° sampleì˜ relative diversity/redundancyì— ë”°ë¼ rewardë¥¼ reweightí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨.

<span style='color:orange_background'>**â†’ diverse completionsì€ ë” ë†’ì€ weight, ì¤‘ë³µëœ responseëŠ” ë‚®ì€ weight**</span>

ë¨¼ì € ê¸°ì¡´ì˜ reward $ R(q,o_i) $ë¥¼ diversity-aware adjusted reward $ \tilde{R}(q,o_i) $ <span style='color:red'>(í‹¸ë‹¤ í‘œì‹œ ì–´ë–»ê²Œ í•˜ë‚˜ìš”â€¦) </span>ìœ¼ë¡œ ëŒ€ì²´í•¨

- SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” completion $ o_i $ì™€ ë‚˜ë¨¸ì§€ group $ C $ \ $ o_i $ ê°„ì˜ Submodular Mutual Informationì„ ë‚˜íƒ€ëƒ„

- Submodular functionsì€ diminishing returns íŠ¹ì„±ì„ ê°–ìœ¼ë©°, diversityì™€ redundancyë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ

  - **\*diminishing returns\*\***: ì–´ë–¤ ì§‘í•©ì— ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ë” ì¶”ê°€í•  ë•Œ, ì´ë¯¸ ë¹„ìŠ·í•œ ìš”ì†Œê°€ ë§ì„ ìˆ˜ë¡ ê·¸ ìš”ì†Œê°€ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê°€ì¹˜(ì •ë³´ ê¸°ì—¬ë„)ëŠ” ì¤„ì–´ë“œëŠ” ì„±ì§ˆ\*

- SMIëŠ” ë‘ ì§‘í•© ê°„ì˜ shared informationì„ ì •ëŸ‰í™”í•˜ë©° (Iyer et al., 2021a,b)ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•¨

      - $ s(o_i,j)=s(j,o_i) $ë¼ê³  ê°€ì •í•˜ë©°, SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” $ o_i $ì™€ ë‚˜ë¨¸ì§€ elementsê°„ì˜ total semmetric simialrityë¥¼ ê³„ì‚°í•¨

  - ì¦‰, $ o_i $ê°€ ë‚˜ë¨¸ì§€ completionsê³¼ ìœ ì‚¬í• ìˆ˜ë¡ SMIê°€ ì»¤ì ¸ reward ê°’ì´ ë‚®ê²Œ reweightë˜ê³ , $ o_i $ê°€ ë‹¤ë¥¼ìˆ˜ë¡ SMIê°€ ì‘ì•„ì ¸ reward ê°’ì´ í¬ê²Œ reweight ë¨.

  - ê° completionì˜ embeddingì€ small LMìœ¼ë¡œ ì–»ê³ , $ s() $ëŠ” cosine similarityë¥¼ ì‚¬ìš©í•¨

<br/>

- SMIë¥¼ ì‰½ê²Œ ë§í•˜ë©´ â€œ<span style='color:orange_background'>**íŠ¹ì • completion í•˜ë‚˜ê°€ group ë‚´ ë‹¤ë¥¸ completionê³¼ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€**</span>â€ë¥¼ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê°’

  - **ì¤‘ë³µì´ ë§ìœ¼ë©´ (high redundancy) â†’ SMIê°€ í¼ â†’ rewardê°€ ì‘ì•„ì§**

  - **ì¤‘ë³µì´ ì ìœ¼ë©´ (high diversity) â†’ SMIê°€ ì‘ìŒ â†’ rewardê°€ ì»¤ì§**

- Submodular í•¨ìˆ˜ëŠ” ìˆ˜í•™ ê°œë…ìœ¼ë¡œ â€œìƒˆë¡œìš´ elementê°€ ê¸°ì¡´ì— ë¹„ìŠ·í•œê²Œ ë§ì„ìˆ˜ë¡ ê¸°ì—¬ë„ê°€ ì¤„ì–´ë“œëŠ” ì„±ì§ˆâ€ì„ ê°–ê³  ìˆìŒ

  ex) ìœ ì‚¬í•œ completionsì´ 9ê°œê°€ ìˆëŠ” ìƒí™©ì—ì„œ, 10ë²ˆì§¸ ë¹„ìŠ·í•œ completionì€ informationì„ ë³„ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ. ë°˜ë©´, ì™„ì „íˆ ë‹¤ë¥¸ completionì´ ë“±ì¥í•˜ë©´ information ê¸°ì—¬ë„ê°€ ì»¤ì§

<br/>

â†’ ì´ë ‡ê²Œ ìƒˆë¡œìš´ rewardë¥¼ êµ¬í•˜ëŠ” ì—°ì‚°ì€ Pytorchì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ

    - $ \Sigma{L_{ij}} $ termì´ simiarlity matrix $ L $ì˜ $ i $th rowì˜ í•©

- Pytorch Code for DAR

  ## 3. Experiment

### 3.1 Experimental Setup

**Training Dataset: **

    - s1 dataset + DeepScaleR dataset with mixed problem difficulties

**Evaluation Dataset: **

    - five mathematical reasoning benchmarks (AIME24, MATH-500, AMC23, Minerva, OlympiadBench)

**Baselines**:

- general purpose large model: Llama-3.1-70B-Instruct, o1-preivew

- Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL

- Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS

**Implementations:**

- ë³¸ ì—°êµ¬ëŠ” DRAì˜ proof-of-conceptë§Œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ëª©ì ì´ê¸°ì— DeepSeek-R1-Distill-Qwen-1.5Bë¥¼ base modelë¡œ ë‘ì–´ í•™ìŠµì‹œí‚´

- 4 x A100 (40GB) GPUs

### 3.2 Empirical Analysis

**Main Results**

- DRA-DR.GRPOëŠ” avg accê°€ 58.2%ë¡œ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜´ (DRA-GRPOì—­ì‹œ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜´)

  - AMC23ì—ì„œ íŠ¹íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ (85.0%)

- DRA-GRPOì™€ DRA-DR.GRPOëŠ” fine-tuning samplesì„ 7,000ê°œ ë°–ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  40,000ê°œ ì‚¬ìš©í•œ DeepScaleR-1.5B-previewë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë³´ì—¬ì¤Œ

  â†’ Low-resource settingsì—ì„œë„ íš¨ê³¼ì ì„

  - DeepScaleR-1.5B-preview

    ë‚˜ë¦„ GOATê¸‰ì˜ ì„±ëŠ¥ ë³´ì—¬ì£¼ëŠ” ì˜ë‚˜ê°€ëŠ” ëª¨ë¸

    ([https://www.notion.so/19681902c1468005bed8ca303013a4e2](https://www.notion.so/19681902c1468005bed8ca303013a4e2])

    [https://github.com/giterinhub/DeepScaleR-1.5B-Preview](https://github.com/giterinhub/DeepScaleR-1.5B-Preview)

    **Ablation Study**

- Base modelì¸ DeepSeek-R1-Distill-Qwen-1.5Bì™€ ë¹„êµí•˜ì—¬ DRA-GRPO, DRA-DR.GRPOëŠ” ê°ê° 7.8%, 9.3% ì„±ëŠ¥ í–¥ìƒë˜ê³  ë‹¨ìˆœ RL (GRPO, DR.GRPO) ëŒ€ë¹„ 1.9%, 2.2% í–¥ìƒ

  â†’ ì´ê²Œ ì™œ Ablation studyë¼ ë§í•˜ëŠ”ê±°ì§€ã…‹ã…‹

**Efficiency**

DRAëŠ” completionsì„ encoding í•´ì•¼í•˜ê¸°ì— over-headê°€ ì¡´ì¬í•˜ì§€ë§Œ, ë³„ë¡œ í¬ì§€ ì•ŠìŒ.

â†’ ì €ìë“¤ì´ ì‹¤í—˜ì— ì‚¬ìš©í•œ GPUìŠ¤í™ì¸ (A100-40GB)ì—ì„œëŠ” ì–´ì°¨í”¼ DRA ì—†ì´ë„ mini-batchë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•´ì„œ DRA ì ìš©í•˜ëŠ” ê²ƒì´ ë³„ ë¬¸ì œê°€ ë˜ì§€ ì•Šë‹¤ê³  í•˜ëŠ”ë°â€¦. â†’ ğŸ¶Â ğŸ”ŠÂ ë¼ê³  ìƒê°í•©ë‹ˆë‹¤

**Training Cost**

500 steps í•™ìŠµì‹œì¼œ 12.5hr ì†Œìš”ë¨ â‡’ $55 ë¹„ìš©

â†’ ë‹¤ë¥¸ ë°©ë²•ëŒ€ë¹„ íš¨ìœ¨ì ì„

### 3.3 Discussion

**Exploration-exploitation Balance**

DRAëŠ” Exploration-exploitation balanceë¥¼ policy gradient ì•ˆì— ì§ì ‘ í†µí•©í•˜ì—¬ ì ìš©í•¨

- Base rewardëŠ” high scoreë¥¼ ë°›ëŠ” completionì„ reinforceí•¨

  â†’ **Exploitation ìœ ë„**

- Diversity weightingì€ semantically novel completionì— learning signalì„ amplify

  â†’ **Exploration ìœ ë„**

ì´ëŸ¬í•œ íƒìƒ‰ì€ low-resource settings (promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” ì‘ë‹µ ìˆ˜ê°€ ì œí•œ ì ì¸ ê²½ìš°)ì—ì„œ ì¤‘ìš”í•¨

â†’ DRAëŠ” mode collapseë¥¼ ë°©ì§€í•˜ê³  ë” ë„“ì€ reasoning strategiesë¥¼ ìœ ë„í•¨

**Ad-hoc vs Post-hoc Diversity**

generated completionsê°„ì˜ diversityë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ Ad-hoc, Post-hoc ë°©ì‹ì´ ìˆìŒ

1. **Ad-hoc**

   1. generation ë‹¨ê³„ì—ì„œ ë‹¤ì–‘ì„±ì„ ìœ ë„í•¨ (temperature ì¡°ì ˆ, decoding ì„¤ì • ë³€ê²½)

   1. ì´ë ‡ê²Œ í•˜ë©´ ê° completionì´ ë…ë¦½ì ìœ¼ë¡œ samplingë˜ì–´ â†’ ì‘ë‹µ ê°„ correlationì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ì—†ìŒ (completionì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ëª…ì‹œì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìŒ)

1. **Post-hoc (ë³¸ ì—°êµ¬ì—ì„œ ì±„íƒí•œ ë°©ë²•)**

   1. diversityë¥¼ reward signalì— ë°”ë¡œ í†µí•©

   1. Semantic redundancyë¥¼ í‰ê°€í•˜ì—¬ policyê°€ íš¨ìœ¨ì ìœ¼ë¡œ learningì„ ì¡°ì •í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ

## 4. Conclusion

- GRPO í˜•ì‹ì˜ RLì—ì„œ completions ê°„ì˜ semantic diversityë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” DRA ì•Œê³ ë¦¬ì¦˜ ì œì•ˆí•¨

  â†’ ê¸°ì¡´ scalar rewardì˜ ë¬¸ì œì¸ exploration-exploitation imbalance issueë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•¨

- ë‘ê°€ì§€ í•œê³„ì ì´ ìˆìŒ

  1.  small-scale models (1.5B) / small group sizes (e.g 6 completions per prompt)

  1.  diversityë¥¼ ì¸¡ì •í•  ë•Œ ì‚¬ìš©ëœ sentence embeddingsì€ ì™¸ë¶€ modelì— ì˜ì¡´í•˜ëŠ” êµ¬ì¡°

- ì´ëŸ° ìª½ë„ ì¬ë°Œë‹¤!ã…‹ã…‹
