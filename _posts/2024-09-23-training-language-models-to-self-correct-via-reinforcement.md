---
categories:
  - paper-reviews
date: "2024-09-23 00:00:00"
description: ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, AGI ê´€ë ¨ ì—°êµ¬
giscus_comments: true
layout: post
related_posts: false
tags:
  - agi
  - alignment
  - fine-tuning
  - gpt
  - language-model
  - llm
  - paper-review
  - reinforcement learning
  - reinforcement-learning
  - vision
thumbnail: assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/thumbnail.jpg
title: Training Language Models to Self-Correct via Reinforcement Learning
---

**ë…¼ë¬¸ ì •ë³´**

- **Date**: 2024-09-23
- **Reviewer**: ì¤€ì› ì¥
- **Property**: Reinforcement Learning, AGI

## 1. Introduction

- ìµœê·¼ì— ë² í¬ëœ LLMì€ â€˜ì•Œê³ ë¦¬ì¦˜=ë¬¸ì œìˆ˜í–‰ëŠ¥ë ¥â€™ì„ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰í•˜ì§€ ëª»í•¨ â‡’ test time queryì— ëŒ€í•´ì„œ LLMì´ ìì²´ì ìœ¼ë¡œ ìƒì„±í•œ responseë¥¼ â€˜self-correctâ€™í•˜ê³  best-possible final responseì„ ìƒì„±í•˜ëŠ” actionì„ ì·¨í•˜ì§€ ëª»í•¨.

  - â€˜self correctionâ€™ì—ì„œ external inputì´ ì—†ëŠ” settingì„ *â€˜intrinsic self-correctionâ€™*ì´ë¼ê³  í˜„ì¬ LLMì€ í•´ë‹¹ ë¬¸ì œì— ì·¨ì•½í•˜ë‹¤ëŠ” ì„ í–‰ì—°êµ¬ê°€ ì¡´ì¬í•¨.

  - ë…¼ë¬¸ì—ì„œëŠ” **_â€˜intrinsic self-correctionâ€™_**ì„ í•´ê²°í•˜ê³ ì í•¨.

- ë…¼ë¬¸ì—ì„œëŠ” LLMì´ ìœ„ì™€ ê°™ì´ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í’€ë•Œ â€œon-the-flyâ€ settingìœ¼ë¡œ mistakeë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë¡ ì„ ì—°êµ¬í•˜ê³ ì í•¨.

  - Snell et al., 2024ì— ë”°ë¥´ë©´, LLMì´ ë¬¸ì œì— ëŒ€í•œ ì˜¤ë‹µì„ ë‚´ë†“ì•˜ì„ë•Œ ì‹¤ì œë¡œëŠ” ì •ë‹µê³¼ ê´€ë ¨ underlying â€œknowledgeâ€ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ê·¸ê²ƒì´ í•„ìš”í• ë•Œ ì •í™•íˆ elicití•˜ê³  draw inferenceí•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤ê³  í•¨

- ê·¸ë ‡ë‹¤ë©´ ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì–´ë–»ê²Œ LLMì—ì„œ self-correction abilitiesë¥¼ ì£¼ì…í–ˆì„ê¹Œ?

  1. Prompt-engineering (Kim et al., 2023; Madaan et al., 2023)

  1. meaningful intrinsic self-correctionì„ ìˆ˜í–‰í•˜ëŠ”ë° í•œê³„ê°€ ì¡´ì¬í•¨

  1. Fine-tuning (Havrilla et al., 2024b; Qu et al., 2024; Welleck et al., 2023; Yuan et al., 2024)

  1. inferenceì‹œì— verifier, refinement model ê°™ì€ multiple modelì´ í•„ìš”í•¨

  1. self-correction training ì¤‘ oracle teacher modelì´ í•„ìš”í•¨

- ì €ìë“¤ì€ ì‹¤í—˜ ê°€ëŠ¥í•œ 2ê°œì˜ baselineì˜ í•œê³„ë¥¼ ì œì‹œí•˜ë©´ì„œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì‹œí•¨

  1. Rejection Sampling ê¸°ë°˜ì˜ STaRëŠ” modelì´ correctionì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ biasë¥¼ ê°•í™”í•œë‹¤.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_000.png" class="img-fluid rounded z-depth-1" %}

1. minimal edit strategyëŠ” self correction ëŠ¥ë ¥ì„ ì£¼ì…í•˜ì§„ ì•Šì§€ë§Œ, â€˜ ë‘ë²ˆì§¸ generationì—ì„œ ì •ë‹µì„ ì˜ëª» ì…ë ¥í•˜ëŠ” í•™ìŠµì„ ì–µì œí•œë‹¤.â€™ â†’ suboptimalì´ë©°, distributional mismatch í™˜ê²½ì—ì„œ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•„ì§

- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ”

  - self generated dataë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œ multi-turn RL ë°©ë²•ë¡ ì„ ì œì‹œ

## 2. Related Works

**#### Prompting for intrinsic self-correction**

- self-correction ê³¼ì •ì¤‘ì— oracle answerì„ í™œìš©í•˜ëŠ” í•œê³„

- first responseë¥¼ ìƒì„±í• ë•ŒëŠ” weak promptë¥¼ ì‚¬ìš©í•˜ê³ , self-correctionì‹œì—ëŠ” strong promptë¥¼ ì‚¬ìš©í•´ overestimate ë¬¸ì œ

**#### Fine-tuning for intrinsic self-correction**

- oracle feedback (revisions directly from human annotators & human)ì„ SFTì— ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•œë‹¤ëŠ” í•œê³„

â‡’ ë³¸ ì—°êµ¬ëŠ” learnerê°€ ì§ì ‘ ìƒì„±í•˜ëŠ” training dataë§Œì„ í™œìš©

## 3. Preliminaries and Problem Setup

> **Problem Define**: External Feedbackì´ ì—†ëŠ” intrinsic self-correctionâ€™ setup

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_001.png" class="img-fluid rounded z-depth-1" %}

- Setting

  - **Multi-turn RL (2-turns)**

  - Dataset: \mathcal{D} = \{(\mathbf{x}_i, \mathbf{y}\_i^\*)\}_{i=1}^{N}

  - What we want to train: LLM Policy \ \pi*\theta(\cdot \mid [\mathbf{x}, \hat{\mathbf{y}}*{1:t}, \mathbf{p}\_{1:t}])

  - Reward Function: \text{verifier } r(\mathbf{y}, \mathbf{y}^\*)

    - ë‹¹ì—°íˆ, inference timeì—ëŠ” ì ‘ê·¼í•  ìˆ˜ ì—†ìœ¼ë©° ëª¨ë¸ì´ ì¶”ë¡ ì„ í†µí•´ì„œ mistakeê°€ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨.

- Standard SFTë‚˜ ì¼ë°˜ì ì¸ RL tuningê³¼ëŠ” ë‹¬ë¦¬, ì—¬ëŸ¬ í„´ì„ ë™ì‹œì— í•™ìŠµ.

- ì¤‘ê°„ í„´ ì‘ë‹µ Å·\_{1:t}ì€ final rewardì˜ intermediate contextë¥¼ ìœ„í•´ ê°„ì ‘ì ìœ¼ë¡œ supervised tuningë¨.

- **A base RL approach for fine-tuning LLMs**

  - on-policy gradient í•™ìŠµ ê¸°ë²•ì„ ì‚¬ìš©

  - REINFORCE with KLD penalty (PPO)

- **Metric**

  - Accuracy@t1: ì²« ë²ˆì§¸ ì‹œë„ì—ì„œì˜ LMì˜ ACC

  - Accuracy@t2: ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œì˜ LMì˜ ACC

  - Î”(t1, t2): ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì‹œë„ ì‚¬ì´ì˜ LMì˜ ACC ìˆœ ê°œì„ , self-correction íš¨ê³¼ë¥¼ ì¸¡ì •

  - Î”^iâ†’c(t1, t2): ì²« ì‹œë„ì—ì„œ í‹€ë ¸ì§€ë§Œ ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œ ë§ì€ ë¬¸ì œì˜ ë¹„ìœ¨, self-correctionì´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë¬¸ì œì˜ ìˆ˜ë¥¼ ì¸¡ì •

  - Î”^câ†’i(t1, t2): ì²« ì‹œë„ì—ì„œ ë§ì•˜ì§€ë§Œ ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œ í‹€ë¦° ë¬¸ì œì˜ ë¹„ìœ¨, ëª¨ë¸ì´ ì •ë‹µì„ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ì¸¡ì • (ë§ì´ ë–¨ì–´ì§€ë©´ ëœ ì´í•´í•˜ëŠ”ê±´ê°€?)

## 4. Supervised Fine-Tuning on Self-Generated Data is Insufficient for Self-Correction

** #### â€˜Intrinsic Self-Correctionâ€™ Settingì´ë‹ˆ ì™¸ë¶€ modelì˜ feedback ì—†ì´ base-modelì„ ê°€ì§€ê³  self correctioní•˜ëŠ” SFTí•˜ëŠ” ë°©ë²•ë¡ ì„ ëŒë ¤ë³´ë©´ ì–´ë–¨ê¹Œ?**

- Baselines SFT

  - STaR

  - Pair-SFT

### 4.1. Analysis Setup: Methods and Dataset Construction

- STaR â‡’ (1) base modelë¡œ two-turn self-correction tracesë¥¼ ìƒì„±í•¨. (2) second attemptsê°€ successfullyí•˜ê²Œ first attemptì˜ incorrect responsesë¥¼ reviseí•  ê²½ìš°ì—ë§Œ filter

  - \mathcal{D}_{\text{STaR}} := \{(x_i, \hat{y}\_i^{-}, \hat{y}\_i^{+})\}_{i=1}^{N}, \text{ where } \hat{y}\_i^{-} \text{ and } \hat{y}\_i^{+} \text{is incorrect and correct answer}

    - ìœ„ì˜ ì„¤ëª… ê·¸ëŒ€ë¡œ ë°ì´í„°ì…‹ êµ¬ì¶•

â‡’ Training 3 iterations of collecting and running SFT.

- Welleck et al. (2023) â‡’ (1) base modelë¡œ two-turn self-correction tracesë¥¼ ìƒì„±í•¨. (2) first attemptsì—ì„œ pairing incorrect responses with correct ones í›„ generates â€œsyntheticâ€ repair traces

  - \mathcal{D}_{\text{SFT}} := \{(x_i, \hat{y}\_i^{-}, \tilde{y}\_i^{+})\}_{i=1}^{N}, \text{ where } \tilde{y}\_i^{+} \text{ is a random correct response for problem ğ’™} \\ \text {randomly sampled from the set of all first-turn and second-turn responses produced} \\ \text{by the model.}

    - two-turn self-correction tracesì—ì„œ ìƒì„±í•œ ì •ë‹µ ì¤‘ ëœë¤í•˜ê²Œ ë°°ì¹˜.

â‡’ Training 1 epoch with SFT.

### 4.2. Empirical Findings

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_002.png" class="img-fluid rounded z-depth-1" %}

- Pair-SFTë¡œ í•™ìŠµí•˜ë©´ base-model ëŒ€ë¹„ ì˜ë„í•œ self-correction í–‰ë™ê²½í–¥ì„±ì´ ë„ì¶œë¨. (Î”(t1, t2)ì˜ ì¦ê°€, incorrect â†’ correctì˜ ì¦ê°€ correct â†’ incorrectì˜ ê°ì†Œ)

- STaRì˜ ê²½ìš° ì˜¤íˆë ¤ self-correctionì´ ì „í˜€ ì¼ì–´ë‚˜ì§€ ì•Šê³  ìˆëŠ”ë°, ì €ìë“¤ì€ â€˜revision trajectoriesâ€™ê°€ limitedëœ spaceì—¬ì„œ ê·¸ë ‡ë‹¤ê³  í•¨ (ì‚¬ì‹¤ ë­”ë§ì¸ì§€ ëª¨ë¥´ê² ..)

- Table 1ì—ì„œ ê°€ì¥ í° ë¬¸ì œëŠ” Correctë¥¼ Incorrectë¡œ ë°”ê¿”ë²„ë¦°ë‹¤ëŠ” ê²ƒ **(ì¤€ì› ìƒê°: ì‚¬ì‹¤ìƒ self-correctionì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ ë¬´ì‘ìœ„ë¡œ alignmentë¥¼ í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì„)**, ì €ìë“¤ì€ ì´ëŸ° ê²½í–¥ì„±ì„ ì–µì§€ë¡œ ì§€ì›Œë²„ë¦¬ê³ ì 2ë²ˆì§¸ attempts ëª¨ë‘ì— ëŒ€í•´ì„œ correct responseë¡œ í•™ìŠµí•˜ëŠ” Table2 ê²°ê³¼ë¥¼ ë‚´ë†“ìŒ

  - STaRëŠ” self-correctionì„ ê±°ì˜í•˜ì§€ ëª»í•˜ê³ , Pair-SFTëŠ” ì•„ì˜ˆ ì •ë‹µì„ ë³€ê²½í•˜ì§€ ì•Šê³  ìˆìŒ.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_003.png" class="img-fluid rounded z-depth-1" %}

- (ì œì•ˆí•œ ë°©ë²•ë¡  SCoReë„ ê±°ì˜ ë³€í™”ê°€ ì—†ê¸´í•œë°..) STaR D+, SFT DëŠ” base model ëŒ€ë¹„ edit distance (first vs second response)ì˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  - base model ëŒ€ë¹„ edit distanceê°€ ê±°ì˜ ì—†ë‹¤ (ë§Œì•½ first attemptì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ ìœ„ì˜ í‘œê°€ ë‚©ë“ì´ ê°€ì•¼í•˜ëŠ”ë° ê·¸ë ‡ì§€ ì•Šìœ¼ë‹ˆ, ë¬¸ì œê°€ ìˆë‹¤ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.)

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_004.png" class="img-fluid rounded z-depth-1" %}

- STaR D+ì™€ëŠ” ë‹¬ë¦¬ SFT DëŠ” trainingê³¼ evaluationì—ì„œ edit distance (first vs second response)ì˜ ë¶„í¬ ì°¨ì´ê°€ ë§ì´ ë‚¨.

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_005.png" class="img-fluid rounded z-depth-1" %}

- Pair-SFTì— ëŒ€í•œ ablation

- fixed ~: first responseëŠ” ê° datasetì— ìˆëŠ” offline responseë¥¼ ê·¸ëŒ€ë¡œ conditionìœ¼ë¡œ ì“°ëŠ” setting

  - trainingì€ ë‹¹ì—°íˆ datasetì— ìˆëŠ” ê±¸ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•´ì•¼ í•˜ë‹ˆ Fixed train

- self-generated: learnerê°€ first responseë¥¼ generatingí•˜ëŠ” setting

- Pair-SFTì—ì„œ Trainingë•Œì²˜ëŸ¼ fixed validation setìœ¼ë¡œ validationì„ ì§„í–‰í•˜ë©´ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

**â‡’ distributional shiftëŠ” self-correctionì— ìˆì–´ì„œ ì¤‘ìš”í•œ issueì´ë©°, fixed offline datasetì€ ì¢‹ì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ ì•„ë‹ˆë‹¤.**

### Takeaways: Insufficiency of SFT (previous method)

1. STaR D+ëŠ” distribution shiftì—ëŠ” ê°•ê±´í•œ ëŒ€ì‹  one mode of correctionì—ë§Œ collapseëœë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í•¨.

1. (i.e., correct â†’ correctì—ì„œë§Œ ê·¸ë‚˜ë§ˆ ì¡°ê¸ˆ ì‘ë™í•¨)

â‡’ what we have to solve: amplification of certain pathological behaviors that seem promising on the training data

1. Pair-SFTëŠ” distribution shiftê°€ ìˆì–´ ì¸í•´ (explorationì€ ì¢‹ì•„ì§€ë‚˜: ì–´ì°Œì €ì°Œ ì„±ëŠ¥ì€ ì¢‹ì•„ì§€ë‚˜) ì •ì‘ LMì˜ first attemptsì— ëŒ€í•œ self correction ëŠ¥ë ¥ì´ ì €í•˜ëœë‹¤.

â‡’ what we have to solve: distribution shift in training data

## 5. SCoRe: Self-Correction via Multi-Turn Reinforcement Learning

### Key Challenges

- ìœ„ì˜ mulit-turn RLì„ í™œìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ë©´ distributional shiftë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ê±´ ìˆ˜ì‹ì ìœ¼ë¡œ ë‹¹ì—°

- ì‹¤í—˜ì„ ì§„í–‰

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_006.png" class="img-fluid rounded z-depth-1" %}

- (left) first responseê³¼ second responseì˜ train accê°€ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ê³„ì† ì›€ì§ì´ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŒ

- (right) ì´ˆë¡ìƒ‰ ì„ ì„ ë³´ë©´ first responseëŒ€ë¹„ second responseê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ answerë¥¼ ìƒì„±í•˜ëŠ”ê°€?ì¸ë°, í•™ìŠµì„ ì§€ì†í•  ìˆ˜ë¡ couplingëœ ì •ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

### Why does this happens?

- Data distributionë‚´ì—ì„œ LMì´ ì·¨í•  ìˆ˜ ìˆëŠ” optimal actionì€ 2ê°€ì§€.

  1.  first responseì—ì„œ second responseë¡œ reviseí•˜ë„ë¡ í•™ìŠµí•˜ê±°ë‚˜ â‡’ Unseen Test distributionì—ì„œ generalization ì‹œì¼œì•¼í•  ì •ì±….

  1.  first responseì—ì„œ ìµœìƒì˜ ì‘ë‹µì„ ìƒì„±í•œ ë‹¤ìŒ second responseì—ì„œ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ í•™ìŠµ

**â‡’ Overparameterizationëœ LLMì€ (Data distributionë‚´ì—ì„œ 1,2ê°€ ë‘˜ë‹¤ ìµœì ì´ë¼ê³  ë¦¬ì›Œë“œê°€ ì„¤ê³„ë˜ì–´ìˆê³  ê·¸ê±¸ ê·¸ëŒ€ë¡œ í•™ìŠµì„ í•˜ë©´) 1.ì— ëŒ€í•œ ì •ì±…ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ.**

**â‡’ ê³¼ê±°ì˜ ì£¼ì–´ì§„ attemptsì— ëŒ€í•´ì„œ self-correctingí•˜ëŠ” ìµœì ì˜ ë°©ë²•ì´ ì•„ë‹ˆë¼, í˜„ì¬ ì£¼ì–´ì§„ responseë¥¼ ê°œì„ í•˜ëŠ” next responseë¥¼ ìƒì„±í•˜ë¼ê³  modelì—ê²Œ í•™ìŠµì‹œì¼œì•¼ í•¨. **

### Method

Objective

1. (ë¹„ìœ ë¥¼ ë“¤ë©´ prior distributionì„ ê¹”ì•„ì¤Œìœ¼ë¡œì¨) LMì´ first attempt distributioní•˜ì—ì„œ second distributionì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ

1. reward-shapingì„ í†µí•´ bias model to self-correct

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_007.png" class="img-fluid rounded z-depth-1" %}

### 5.1. Stage I: Training a Model Initialization to Prevent Collapse

- one-mode collapseë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ RLë¡œ í•™ìŠµ ì§„í–‰

- second responseì—ì„œ ë†’ì€ reward revisionì´ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” ë™ì‹œì— KLDë¥¼ ì‚¬ìš©í•˜ì—¬ first response distributionì´ base-model ìµœëŒ€í•œ ê°€ê¹ê²Œ ì œí•œí•¨

  - ìˆ˜ì •í•  ê²Œ ê±°ì˜ ì—†ëŠ” first responseì„ ë” ë‚˜ì€ second responseìœ¼ë¡œ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ìµœì ì´ ì•„ë‹Œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ëª©ì ì€ first responseì™€ second responseë¥¼ decouplingí•˜ëŠ”ë° ìˆìŒ

    - second responseì—ë„ KLD termì„ ë‘ëŠ”ë° weightë¥¼ 1/10ìœ¼ë¡œ ë‘ 

  - Untitled ì„ ë³´ë©´ Stage 1ì´ ë‹¹ì—°í•˜ê²Œë„ first responseì™€ second responseì— ëŒ€í•œ mode collapse ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ëŠ”ë° ë„ì›€ì´ ë¨

### 5.2. Stage II: Multi-Turn RL with Reward Shaping

- first responseì™€ second responseì— ëŒ€í•œ reward sumì„ maxí•˜ë„ë¡ training

- Reward shaping to incentivize self-correction

  - LMì´ self-correction solutionì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ biasë¥¼ ê±¸ì–´ì£¼ëŠ” ì‘ì—…ì„ ì§„í–‰

  - second responseì— ì•„ë˜ì˜ bonusë¥¼ ë”í•´ì¤Œ. (Î”^iâ†’c(t1, t2)ì™€ Î”^câ†’i(t1, t2)ì— ìœ ë¦¬í•œ reward shapping)

    - \hat{b}(y*2 \mid y_1, y^*) = \alpha \cdot (\hat{r}(y*2, y^*) - \hat{r}(y_1, y^\*))

  - training setì— mode collapseë¥¼ ì™„í™”í•´ì¤„ ê²ƒì„ ê¸°ëŒ€

## 6. Experimental Evaluation

- **Tasks &. Models**

  - MATH â‡’ Gemini 1.5 Flash

    - training set with 4500

  - HumanEval, MBPP-R (offline test: incorrect first-attempt is generated by PaLM2) â‡’ Gemini 1.0 Pro

    - training with MBPP

- **Results**

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_008.png" class="img-fluid rounded z-depth-1" %}

- base model ëŒ€ë¹„ Î”(t1, t2) 15.6%, Accuracy@t2 23.0% ì¦ê°€

- ê°€ì¥ ê³ ë¬´ì ì¸ê±´ ì˜ë„í•œ self-correctionì´ ë™ì‘í•œë‹¤ëŠ” ì 

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_009.png" class="img-fluid rounded z-depth-1" %}

- Pair-SFTê°™ì€ ê²½ìš° offline settingì—ì„œë§Œ ì„±ëŠ¥ì´ ì¢‹ì€ ë°˜ë©´, SCoReëŠ” self-generated settingì—ì„œ self-correction(12.2% intrinsic self-correction delta)ì„ í†µí•´ ì„±ëŠ¥ ì¦ê°€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

  - Accuracy@t1ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ê±´ KLD ë•Œë¬¸ì´ ì•„ë‹ê¹Œ?

{% include figure.liquid loading="eager" path="assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_010.png" class="img-fluid rounded z-depth-1" %}

- (w/o m.t.t) single turnìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ë‹¹ì—°íˆ Accuracy@t1ì€ ë†’ì§€ë§Œ ê·¸ ì™¸ì˜ ì§€í‘œëŠ” ë–¨ì–´ì§€ê³ ,

- (w/o s1, rs) stage 1ì´ë‚˜ reward shapingì„ ì œê±°í•˜ë©´ ì˜ë„í•œ self-correctionì„ ìˆ˜í–‰í•˜ì§€ ëª»í•´ Accuracy@t2ì™€ net increase acc ì—­ì‹œ í•˜ë½í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

- (w STaR) ë§ˆì§€ë§‰ìœ¼ë¡œ stage 2ì—ì„œ on-policyê°€ ì•„ë‹Œ offline dataë¡œ í•™ìŠµí•˜ë©´ distributional shiftë•Œë¬¸ì— spurious solutionì„ í•™ìŠµí•´ ì„±ëŠ¥ì´ í•˜ë½í•œë‹¤ê³  ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤.

## 7. Discussion

- ë…¼ë¬¸ì—ì„œë„ ì–¸ê¸‰í•˜ì§€ë§Œ 1 round ì´ìƒ iterative correction ëª»í•œ ê²ƒì„ limitationìœ¼ë¡œ ì´ì•¼ê¸°í•˜ê³  ìˆìŒ

- ChatGPTë‚˜ ì‹œì¤‘ì— ë‚˜ì˜¨ chatbotë“¤ì„ ì“°ë©´ì„œ ëª‡ turn ëŒ€í™”ë¥¼ ì´ì–´ì„œ ì§„í–‰í•  ë•Œ ê³„ì† ê°™ì€ í•´ë‹µì„ ë‚´ì£¼ëŠ”ê±°ì— ëŒ€í•´ì„œ ì™œ ê·¸ëŸ´ê¹Œ?ì— ëŒ€í•œ ëŒ€ë‹µê³¼ ê·¸ í•´ê²°ì±…ì„ ì œì‹œí•´ì¤€ ë…¼ë¬¸ì´ì—ˆë‹¤.
