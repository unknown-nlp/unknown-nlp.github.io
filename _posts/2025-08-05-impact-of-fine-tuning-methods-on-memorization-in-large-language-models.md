---
categories: paper-reviews
date: '2025-08-05 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - Impact of Fine-Tuning Methods on Memorization in Large Language
  Models'
giscus_comments: true
layout: post
related_posts: false
tags: llm paper-review
title: Impact of Fine-Tuning Methods on Memorization in Large Language Models
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2025-08-05
- **Reviewer**: hyowon Cho

ë§ì€ ì—°êµ¬ë“¤ì´ LLMì´ ì‚¬ì „í•™ìŠµ ë‹¨ê³„ì—ì„œ í•™ìŠµ ë°ì´í„°ë¥¼ ì™¸ìš°ëŠ” ì´ìŠˆì— ëŒ€í•´ì„œ ë³´ê³ í•˜ê³  ìˆëŠ” í•œí¸, finetuningì— ëŒ€í•´ì„œ ë¹„ìŠ·í•œ ì—°êµ¬ëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ ì ë‹¤.

í•˜ì§€ë§Œ, finetuningë„ ë‹¹ì—°íˆ ëª¨ë¸ ëŒ€ë¶€ì˜ ì—…ë°ì´íŠ¸ì™€ ë•Œë•Œë¡œëŠ” êµ¬ì¡°ì ì¸ ë³€í™”ê¹Œì§€ë„ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì—, finetuningì˜ memorization levelì— ëŒ€í•œ ì—°êµ¬ë„ í•„ìš”í•˜ë‹¤.

ê·¸ë ‡ë‹¤ë©´, ì¡´ì¬í•˜ëŠ” ë‹¤ì–‘í•œ finetuning ë°©ë²•ì— ë”°ë¥¸ memorization of fineuning dataì˜ ì˜í–¥ë ¥ì€ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?

<br/>

í•´ë‹¹ ì—°êµ¬ëŠ” ì´ë¥¼ ì‹œí—˜í•˜ê¸° ìœ„í•´ ìš°ì„  finetuning ë°©ë²•ì„ í¬ê²Œ ë‘ ê°€ì§€ë¡œ êµ¬ë¶„í•œë‹¤:

1. Parameter-based finetuning: ëª¨ë¸ íŒŒë¼ ë°”ê¿ˆ

1. Prompt-based fine-tuning: ëª¨ë¸ íŒŒë¼ ê³ ì •, soft token/prefix embeddingâ€¦

<br/>

ê²°ê³¼ì ìœ¼ë¡œ ë‘ ì¹´í…Œê³ ë¦¬ë¥¼ ê³ ë£¨ í¬í•¨í•œ 5ê°€ì§€ ë°©ë²•ì„ ì‹œí—˜í–ˆê³ ,

í‰ê°€ëŠ” ë‹¤ì–‘í•œ MIAs(membership inference attacks )ë¡œ í–ˆê³ ,

ë°ì´í„°ëŠ” Wikitext, WebNLG, Xsum ì„¸ ê°€ì§€ë¡œ í–ˆë‹¤ (ì¢€ ì ê¸´í•˜ë„¤ìš”)

<br/>

ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤

- Parameter-based fine-tuning

	- Model Head Tuning (FT head): fine-tunes only the final output layer

	- Low-Rank Adaptation (LoRA) (Hu et al., 2021)

- Prompt-based fine-tuning:  task-specific prompts only

	- **Prefix Tuning**

		- ê° attention layerì˜ key/valueì— í•™ìŠµ ê°€ëŠ¥í•œ prefix ë²¡í„° ì¶”ê°€.

	- **Prompt Tuning**

		- ëª¨ë¸ ì…ë ¥ ì„ë² ë”© ì•ì— í•™ìŠµ ê°€ëŠ¥í•œ ì—°ì†í˜• í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì¶”ê°€.

	- **P-tuning**

		- ë³„ë„ì˜ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµí•œ ì—°ì†í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ì— ì‚½ì….

<br/>

- ì‚¬ìš©ëœ MIA ê¸°ë²•ê³¼ ì ìˆ˜ ê³„ì‚° ë°©ì‹:

	1. **LOSS** (Yeom et al., 2018)

		- Membership Score = ëª¨ë¸ì˜ ì†ì‹¤

			$ \text{Score} = L(x, M_t) $

			(ì†ì‹¤ì´ ë‚®ì„ìˆ˜ë¡ ë©¤ë²„ì¼ ê°€ëŠ¥ì„± â†‘)

	1. **Reference-based (Ref)** (Mireshghallah et al., 2022a)

		- ê¸°ì¤€ ëª¨ë¸ MrM_rMrì™€ ë¹„êµí•˜ì—¬ ì†ì‹¤ ì°¨ì´ ê³„ì‚°

			$ \text{Score} = L(x, M_t) - L(x, M_r) $

	1. **Zlib Entropy (Zlib)** (Carlini et al., 2021)

		- ì†ì‹¤ì„ zlib ì—”íŠ¸ë¡œí”¼ë¡œ ë‚˜ëˆˆ ë¹„ìœ¨

			$ \text{Score} = \frac{L(x, M_t)}{\text{zlib}(x)} $

	1. **Min-K%** (Shi et al., 2024)

		- í† í° í™•ë¥ ì´ ë‚®ì€ í•˜ìœ„ k% í† í°ë“¤ì˜ í‰ê·  ë¡œê·¸ likelihood

			$ \text{Score} = \frac{1}{E} \sum_{x_i \in \text{Min-}K\%(x)} \log p(x_i | x_{<i}) $

<br/>

- ë°ì´í„°

	- Wikitext-2-raw-1

	- WebNLG

		- tripleë¡œ ì´ë£¨ì–´ì§  (Subject-Predicate-Object)

	- Xsum: ìš”ì•½

		- finetuningì— 5000ê°œë§Œ ì‚¬ìš©

	<br/>

- í‰ê°€

	- training and test setsì—ì„œ ìƒ˜í”Œë§

	<br/>

- ëª¨ë¸

	- LLaMA 2-7B (Touvron et al., 2023)

	- GPT2-series (Radford et al., 2019)

	- LLaMA 3-1B

	â†’ 2025ì˜ ë…¼ë¬¸ì´ë¼ê³  ë¯¿ê¸°ì§€ ì•ŠëŠ”êµ°ì—¬!

	<br/>

- Evaluation Metrics

	- PERF: validation PPL as the primary metric

	- MIA: AUC-ROC

<br/>

- Implementation Details

	- 15 epoch

	- ëª¨ë“  ì„¸íŒ…ì€ ë…¼ë¬¸ì— ë‚˜ì˜¨ê±° ê·¸ëŒ€ë¡œ ë”°ë¼í•¨

	- 4090ì´ë‘ H100 í•œëŒ€ ì‚¬ìš©

<br/>

## Memorization across Tuning Methods

> Does the choice of finetuning strategy affect how much a model memorizes its training data for fine tuning?

<br/>

<br/>

> Observation â™¯1: (ë‹¹ì—°)

	Parameter-based fine-tuning demonstrates a higher tendency to explicitly memorize training data.

<br/>

ëª¨ë“  ë°©ë²•ë¡ ì€ validation PPLê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ ì¢‹ì•˜ìŒ.

í•˜ì§€ë§Œ, prompt-based methods ëŠ” parameter-based ë³´ë‹¤ ì™¸ìš°ëŠ” ì„±ëŠ¥ ë–¨ì–´ì§ (ë‹¹ì—°)

<br/>

> Observation â™¯2:

	Parameter-based fine-tuning exhibits increasing memorization over training epochs, while prompt-based fine-tuning maintains consistently low memorization throughout
training.

<br/>

## Why Prompt-Based Fine-Tuning Exhibits Low Memorization

prompt-based fine-tuning introduces a bias into the modelâ€™s attention mechanism indirectly via
the soft prompt or prefix, rather than altering the attention mechanism itself.

<br/>

- **Prefix Tuning ìˆ˜ì‹ (Petrov et al., 2024)**

	â€…$ â€Št^{pt}_i = A^{pt}_{i0} W_V S_1 + (1 - A^{pt}_{i0})\; t_i $

	- soft-prefixê°€ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ $ A^{pt} $ë¥¼ â€˜ì–´ë””ë¥¼ ë³¼ì§€â€™ë§Œ ì¬ì¡°ì •, **ë³¸ë˜ í† í° ê°„ ìƒëŒ€ ë¶„í¬ëŠ” ê·¸ëŒ€ë¡œ**.

	- ì¦‰ **ìƒˆë¡œìš´ attention íŒ¨í„´ì„ í•™ìŠµ**í•˜ê¸°ë³´ë‹¤ëŠ” **ê¸°ì¡´ ëŠ¥ë ¥ì„ ì¬í™œìš©**.

- ê²°ê³¼ì ìœ¼ë¡œ **í‘œí˜„ ê³µê°„ì˜ ì´ë™(shift) < ì ìŒ** â†’ í•™ìŠµ, ë¹„í•™ìŠµ ìƒ˜í”Œ ë¶„í¬ ì°¨ì´ê°€ ì‘ì•„ MIAê°€ ì–´ë µë‹¤.

	- Petrov et al. (2024) prove that the presence of a prefix does not alter the relative distribution of the input but only shifts the attention to different content.

<br/>

ì´ ê°€ì„¤ì„ í™•ì¸í•˜ê¸° ìœ„í•´:

distributions of non-membership and membership examples on the LLaMA2-7Bë¥¼ ì„¸ ì„¸íŒ…ì—ì„œ ë¹„êµí•¨:

1. pre-trained model,

1. fine-tuned with LoRA

1. fine-tuned with prefix tuning

LoRAëŠ” membership and non-membership samples ì‚¬ì´ ë¶„í¬ ì°¨ì´ê°€ í°ë°, prefix tuningì€ ë¯¸ë¯¸í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

<br/>

## Performance in Different Tuning Paradigms

<br/>

ë‘ ë°©ë²•ë¡ ì´ ìµœì¢…ì ìœ¼ë¡œëŠ” ë¹„ìŠ·í•œ PPLì„ ê°€ì¡ŒìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , Learning trajactoriesëŠ” ê½¤ë‚˜ ë‹¬ëìŒ

<br/>

parameterbased fine-tuning:

- decreases over the first few epochs

- later increases due to overfitting, before eventually converging

prompt-based fine-tuning:

- slightly decreasing validation PPL throughout training,

- converging without the overfitting-induced rise

ì´ëŠ” ì•„ê¹Œë„ ì´ì•¼ê¸° í–ˆë“¯ì´, í›„ìê°€ internal sample distribution of the modelì„ ë°”ê¾¸ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë‹¨ìˆœíˆ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì— ìª¼ë” ë” ë‚˜ì€ biasë¥¼ ì¶”ê°€í•˜ëŠ” ì •ë„ì„ì„ ë‹¤ì‹œí•œë²ˆ ë³´ì¸ë‹¤

<br/>

## Regarding Model Scale

ëª¨ë¸ ì‚¬ì´ì¦ˆê°€ memorizationì— ì¤‘ìš”í•œ ì˜í–¥ë ¥ì„ ì¤„ ê²ƒì„.

â†’ To what extent does model size influence memorization under different fine-tuning strategies?

<br/>

<br/>

> Observation â™¯3

	Model size significantly enhances memorization in parameter-based fine-tuning methods, while prompt-based methods show minimal sensitivity and maintain consistently low memorization.

<br/>

four variants of the GPT-2 architecture:

- GPT-2 (124M),

- GPT-2 Medium (345M),

- GPT2 Large (762M),

- GPT-2 XL (1.5B).

<br/>

LLaMA2-7B vs LLaMA3-1B

<br/>

## ğŸ“ ìš”ì•½ íŒŒë¼ë¯¸í„° ë°”ê¾¸ëŠ” ì• ë“¤ì€ ëª¨ë¸ í¬ê¸° ì»¤ì§ˆìˆ˜ë¡ ë” ì˜ ì™¸ì› ëŠ”ë° ë°˜ëŒ€ëŠ” ë¯¸ë¯¸í•˜ë”ë¼ (low sensitivity of prompt tuning to model scale)

íŠ¹íˆ, gpt2ì˜ ê²½ìš°ë‚˜ 1B ìŠ¤ì¼€ì¼ì—ì„œ LoRAëŠ” ì‚¬ì‹¤ìƒ ê±°ì˜ ëª»ì™¸ì›€

<br/>

<br/>

## Impact of Downstream Tasks

> Observation â™¯4
Prompt-based tuning leads to stronger memorization in structured tasks than in other downstream tasks.

<br/>

ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì˜ ì¢…ë¥˜ì— ë”°ë¼ì„œë„ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. ì´ë¥¼ ìœ„ LLaMA2-7Bë¥¼ ë‹¤ì–‘í•œ ë°©ë²•ì„ í†µí•´ í•™ìŠµì‹œí‚¤ê³  LOSS attack againstì— ëŒ€í•´ì„œ ê°ê°ì„ í‰ê°€í•´ë´„

<br/>

<br/>

Prompt-based ë§Œ ë´¤ì„ ë•Œ, WebNLGê°€ ë‹¤ë¥¸ ê²ƒë“¤ì— ë¹„í•´ì„œ ì„±ëŠ¥ì´ ë†’ë‹¤

ì•„ë§ˆë„ êµ¬ì¡°í™”ëœ patterní•™ìŠµì—ëŠ” ìœ ë¦¬í•œ ê²ƒ ê°™ë‹¤

<br/>

## Impact of LoRA Placement on Memorization

AUCâ†‘ â‡’ ê¸°ì–µ(privacy risk)â†‘

<br/>

1. **Projection > Attention**

	-  LoRAë¥¼ **projection layer**ì—ë§Œ ì ìš©í•  ë•Œ, ë‘ ë°ì´í„°ì…‹ ëª¨ë‘ ë„¤ ê°€ì§€ MIA ì§€í‘œì—ì„œ **AUCê°€ ì¼ê´€ë˜ê²Œ ìƒìŠ¹** â†’ ê¸°ì–µì´ ë” ê°•í•´ì§.

1. **Both layers = ê¸°ì–µ ì œì¼ ê°•í•¨**

	- Attention + Projection ë™ì‹œ ì ìš© ì‹œ **ê°€ì¥ ë†’ì€ AUC** â†’ ìµœëŒ€ ìˆ˜ì¤€ì˜ memorization.

1. **ë©”ì»¤ë‹ˆì¦˜ í•´ì„**

	- Projection layerëŠ” **íŠ¹ì§• ë³€í™˜, ì •ë³´ ì••ì¶•**ì„ ë‹´ë‹¹ â†’ í•™ìŠµ ë°ì´í„°ì˜ êµ¬ì²´ì  íŒ¨í„´ì„ ë” ì˜ â€˜ë¶™ì¡ì•„ ë‘ëŠ”â€™ ìœ„ì¹˜.

	- ê²°ê³¼ëŠ” Meng et al. (ROME)ì˜ Transformer ê¸°ì–µì€ ì£¼ë¡œ projection ì¸µì— ì§‘ì¤‘í•œë‹¤ëŠ” ê°€ì„¤ì„ ì¬í™•ì¸.

	<br/>

	<br/>

Practicalí•œ ê´€ì ì—ì„œâ€¦

- í”„ë¼ì´ë²„ì‹œì— ë¯¼ê°í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œëŠ” LoRAë¥¼ attention ì¸µì—ë§Œ ì‚½ì…í•˜ê±°ë‚˜ rankë¥¼ ë‚®ì¶”ì–´ ìœ„í—˜ì„ ì™„í™”.

- ì„±ëŠ¥ê³¼ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ í•„ìš”í•  ë•Œ, ì‚½ì… ìœ„ì¹˜(attn vs proj)ì™€ ë²”ìœ„(ë‹¨ì¼ vs ë³µí•© ì¸µ)ë¥¼ ì£¼ìš” ì¡°ì ˆ ë³€ìˆ˜ë¡œ í™œìš©í•˜ë©´ íš¨ê³¼ì ì¼ ìˆ˜ ìˆê² ë‹¤!

<br/>

<br/>

ë„ˆë¬´ ë§ì£ ..í•˜ì§€ë§Œ ì €ìê°€ ì´ì•¼ê¸°í•œ ê²ƒë§Œ ë§í•´ë³´ê² ìŠµë‹ˆë‹¤.

1. larger model

1. MoE ê°™ì€ ë‹¤ë¥¸ êµ¬ì¡°

1. ë°ì´í„° ì ìŒ

<br/>