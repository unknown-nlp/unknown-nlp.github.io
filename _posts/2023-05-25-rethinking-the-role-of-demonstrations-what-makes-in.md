---
categories:
- paper-reviews
date: '2023-05-25 00:00:00'
description: ë…¼ë¬¸ ë¦¬ë·° - LLM, ICL ê´€ë ¨ ì—°êµ¬
giscus_comments: true
layout: post
related_posts: false
tags:
- classification
- detection
- gpt
- icl
- language-model
- llm
- nlp
- paper-review
thumbnail: assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/thumbnail.jpg
title: 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2023-05-25
- **Reviewer**: ê±´ìš° ê¹€
- **Property**: LLM, ICL

# Abstract

Large language models (LLMs)ë“¤ì´ in-context learningì„ í†µí•´ downstream taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ì‚¬ì‹¤ì€ ìœ ëª…í•˜ì§€ë§Œ, modelì´ ***ì–´ë–»ê²Œ ***í•™ìŠµì„ í•˜ê³  ***ì–´ë–¤ ì ***ì„ í†µí•´ ì„±ëŠ¥ì„ ë‚´ëŠ”ì§€ í™•ì¸í•˜ëŠ” ì—°êµ¬ëŠ” ê±°ì˜ ì—†ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œ ë‹¤ì–‘í•œ ì‹¤í—˜ì„ í†µí•´ ì²˜ìŒìœ¼ë¡œ LLMsì˜ in-context learningì´ ***ì–´ë–»ê²Œ ***ê·¸ë¦¬ê³  ***ì™œ*** ì‘ë™ì„ í•˜ëŠ” ì§€ì— ëŒ€í•´ì„œ ìˆ˜ë§ì€ ì‹¤í—˜ì„ í†µí•´ ë³´ì—¬ì¤€ë‹¤.

- **Key Aspects**

  - **Label spcae**

  - **Distribtuion of input text**

  - **Overall format**

# Introduction

- LLMsì€ few input-label pairsë¥¼ ê°€ì§€ê³  In-contex learning (ICL)ì„ í†µí•´ downstream tasksì—ì„œ ê½¤ë‚˜ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ì§€ë§Œ, ***ICLì´ ì™œ ì‘ë™í•˜ê³  ì–´ë–»ê²Œ ì‘ë™ì„ í•˜ëŠ” ì§€***ì— ëŒ€í•œ ì—°êµ¬ëŠ” ê±°ì˜ ì—†ë‹¤.

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_000.png" class="img-fluid rounded z-depth-1" %}

- ground turth demonstrationì´ ì‹¤ì œë¡œ ICLì— íš¨ê³¼ì ì´ì§€ ì•ŠìŒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì„

  - Labels in demonstrationì„ random labelsë¡œ ë°”ê¾¸ì–´ë„ classificationê³¼ multi-choice taskì—ì„œ ì„±ëŠ¥ í•˜ë½ì´ ê±°ì˜ ì—†ìŒ. â†’ Modelì€ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒì— ìˆì–´ input-label mappingì— í¬ê²Œ ì˜ì¡´í•˜ì§€ ì•Š.

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_001.png" class="img-fluid rounded z-depth-1" %}

- Demonstationsì˜ ì–´ëŠ ë¶€ë¶„ì´ performanceì— ì§ì ‘ì ì¸ ê¸°ì—¬ë¥¼ í•˜ëŠ”ì§€ ì•Œì•„ë³´ëŠ” ì‹¤í—˜ì„ ì§„í–‰

  1. Label spaceì™€ demonstrationì„ í†µí•´ íŠ¹ì •ëœ input textì˜ distributionì´ ICLì—ì„œ ë˜ê²Œ ì¤‘ìš”í•œ ì—­í• ì„ í•¨ (ê°ê°ì˜ inputsì— ëŒ€í•´ labelì´ ì˜¬ë°”ë¥¸ ê²ƒê³¼ ìƒê´€ ì—†ì´)

  1. Overall formatì€ ì¤‘ìš”í•¨.

    1. label spaceê°€ unknownì¼ ë•Œ, labelì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²ƒ ë³´ë‹¤ randomí•œ English ë‹¨ì–´ë¥¼ labelë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŒ â†’ format ìì²´ë¥¼ ê°–ì¶œ ìˆ˜ ìˆ

  1. ICLì„ objective functionìœ¼ë¡œ ë‘ê³  í•™ìŠµì„ í•˜ëŠ” meta-trainingì€ (1)ê³¼ (2)ì—ì„œ ì–¸ê¸‰í•œ ì ë“¤ì„ ê·¹ëŒ€í™” ì‹œí‚´.

    1. MetaICL (Min et al., 2021)

- ë³¸ ë…¼ë¬¸ì—ì„œ ICLì— ì‚¬ìš©ë˜ëŠ” ***demonstration***ì˜ ì—­í• ì— ëŒ€í•´ ì´í•´í•˜ëŠ” ë¶„ì„ì„ ì œì‹œí•¨.

# Related works

- LLMs, ICL ë‚´ìš© ìƒëµ

- ë³¸ ë…¼ë¬¸ì—ì„œ ì²˜ìŒìœ¼ë¡œ ICLì´ zero-shot ë³´ë‹¤ ì™œ ì„±ëŠ¥ì´ ì¢‹ê²Œ ë‚˜ì˜¤ëŠ”ì§€ ì‹¤í—˜ì ìœ¼ë¡œ ë¶„ì„í•¨

# Experimental Setup

- **Models**:  6 ì¢…ë¥˜ì˜ LMì„ ì‚¬ìš©í•¨ (only-decoder model)

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_002.png" class="img-fluid rounded z-depth-1" %}

  - Inference method (Min et al., 2021) 2ê°œ ì‚¬ìš©í•´ì„œ ì´ 12ê°œì˜ modelsì„ ì‹¤í—˜ì— ì‚¬ìš©

    - **Direct**: ìš°ë¦¬ê°€ í”íˆ ì•Œê³  ìˆëŠ” ë°©ë²•. ì¶œë ¥ì„ ì¶”ì •í•˜ëŠ” ê²ƒì— ì´ˆì ì„ ë‘ 

    - **Channel**: Bayes Ruleì„ í†µí•´ inputê³¼ labelì˜ ìˆœì„œë¥¼ ë’¤ì§‘ì–´ ì¸ê³¼ê´€ê³„ íŒŒì•…

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_003.png" class="img-fluid rounded z-depth-1" %}

  - Evaluation data: 26ê°€ì§€ datasetsì— ëŒ€í•´ í‰ê°€ ì§„í–‰ 

    - (Sentiment analysis, paraphrase detection, NLI, hate speech detection â€¦)

    - dataset ì„ ì • ê¸°ì¤€

      1. low-resource dataset with less than 10K training examples

      1. ì—°êµ¬ì— ë§ì´ ì‚¬ìš©ëœ GLUE and SuperGLUE

      1. ë‹¤ì–‘í•œ domains

# 1. Ground Truth Matters Little

### Gold labels vs Random labels

- **No demonstration**s: zero-shot setting

- **Demonstrations w/ gold labels**: ICL with *k *labeled examples

- **Demonstrations w/ ranodm lables**: ICL with *k* labeled examples (gold â†’ random labels), ì—¬ê¸°ì„œ ì‚¬ìš©í•œ random labelsì´ë€ gold labelê³¼ ë™ì¼í•œ setì„ ê³µìœ í•˜ê³  ìˆìŒ. (ì™„ì „ random ì•„ë‹˜x)

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_004.png" class="img-fluid rounded z-depth-1" %}

- **Results and analysis**

  - Demonstrations w/ gold labels (Yellow)ëŠ” No demonstrations (Blue)ë³´ë‹¤ í•­ìƒ ì„±ëŠ¥ì´ ë†’ìŒ

  - Labelsì„ random (Orange)ê³¼ Labelsì„ gold (Yellow)ë¥¼ ë¹„êµí•˜ë©´ ë¹„ìŠ·í•œ ì„±ëŠ¥ 

â†’ GT input-label pairsëŠ” performance gainì— ìˆì–´ í•„ìˆ˜ì ì´ì§„ ì•ŠìŒ

â†’ ìœ„ ì‹¤í—˜ì€ uniform samplingìœ¼ë¡œ random labelì„ ë½‘ì€ ê±´ë°, true labelsì˜ distribtuionì—ì„œ randomí•˜ê²Œ ì¶”ì¶œí•˜ë©´ ì„±ëŠ¥ì˜ ê°„ê·¹ì„ ë” ì¤„

â†’ label spaceë¥¼ ê³µìœ í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°

  - MetaICLì€ 0.1~0.9% ì„±ëŠ¥ í•˜ë½ë§Œ ìˆì—ˆëŠ”ë°, ICLì„ ëª©ì  í•¨ìˆ˜ë¡œ meta-training ì‹œí‚¨ ê²ƒì´ input-label mappingì„ ë¬´ì‹œí•˜ demonstrationsì˜ ë‹¤ë¥¸ ìš”ì†Œì— ìˆì–´ ì˜í–¥ì„ ë°›ìŒ

### Ablations

- **Does the number of correct labels matter?**

  - Demonstrations w/ a% correct labels (0â‰¤aâ‰¤100)

    - Correct pairs: k x (a/100)

    - Incorrect pairs: k x (1-a/100)

    - a=100 â†’ equal to ICL (demonstrations w/ gold labels)

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_005.png" class="img-fluid rounded z-depth-1" %}

  - **Result and analysis**

    - ì‹¤í—˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ë©´ number of correct labels in demonstrationsì€ í¬ê²Œ ì¤‘ìš” X

    - GPT-J (Classification task) ê²½ìš°ì—ì„œëŠ” 10% ë„˜ê²Œ í•˜ë½ì´ ìˆê¸´ í•˜ì§€ë§Œ, ëŒ€ì²´ë¡œ ë¹„ìŠ·í•œ ì–‘ìƒì„ ë³´ì„

    - No demonstrationsë³´ë‹¨ ê·¸ë˜ë„ incorrect labelsì´ í•­ìƒ ì¢‹ìŒ

- **Is the result consistent with varying k?**

  - input-label pairsì˜ ê°œìˆ˜ â€˜kâ€™ë¥¼ ë°”ê¿” ê°ì— ë”°ë¼ ì‹¤í—˜ì„ ì§„í–‰

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_006.png" class="img-fluid rounded z-depth-1" %}

  - kê°€ 4ê°œì¼ ê²½ìš°ë§Œ ë³´ì•„ë„ No demonstrations (k=0)ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŒ

  - kê°€ 8ì„ ë„˜ì–´ì„œë©´ examplesì˜ ê°œìˆ˜ê°€ ë§ì•„ì ¸ë„ performance ê°œì„ ì´ ì—†ìŒ 

    - ì¼ë°˜ì ì¸ sft ìƒí™©ê³¼ ë‹¤ë¥¸ ì–‘ìƒì„ ë³´

- **Is the result consistent with better templates?**

  - minimal templateì„ defaultë¡œ ì‚¬ìš©í–ˆì§€ë§Œ, manual templateì„ ì ìš©í•´ì„œ ë¹„êµì‹¤í—˜ ì§„í–‰

    - **minimal template**: inputì˜ ë‹¨ìˆœ conatenation

    - **manual template**: dataset-specific ë°©ì‹ìœ¼ë¡œ ì‘ì„±í•œ ë°©ì‹

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_007.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_008.png" class="img-fluid rounded z-depth-1" %}

  - manual tempaltesì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•­ìƒ minimal tempalte ë³´ë‹¤ ì¢‹ì§€ëŠ” ì•ŠìŒ

  - Gold labels â†’ Random labelsë¡œ ë³€ê²½í•  ë•Œ, ì„±ëŠ¥ í•˜ë½ í­ì´ ì‘ì€ trendëŠ” ë¹„ìŠ·í•˜ê²Œ ë³´ì„ 

    - + No demonstrationsë³´ë‹¤ random labelsì´ ë” ì¢‹ì€ trendë„ ë¹„ìŠ·

# 2. Why *does *In-Context Learning work?

- ìœ„ì— ì‹¤í—˜ì—ì„œ demonstrationì—ì„œ GT input-labelì´ ICLì˜ performance gainì— í° ì˜í–¥ì´ ì—†ëŠ” ê²ƒì„ ë³´ì—¬ì¤Œ. ì´ë²ˆ ì‹¤í—˜ì—ì„œ demonstrationsì—ì„œ ë‹¤ë¥¸ ì–´ë–¤ ìš”ì†Œë“¤ì´ ICLì˜ performance gainì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ í™•ì¸.

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_009.png" class="img-fluid rounded z-depth-1" %}

- **Demonstraionì˜ 4ê°€ì§€ aspects**

  1. **Input-label mapping**: input xiê°€ label yiì™€ ì˜¬ë°”ë¥´ê²Œ pairì¸ì§€

    1. ì•ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì´ì§€ë§Œ, ì´ê²ƒë„ aspect ì¤‘ í•˜ë‚˜ë¡œ ë³´ê³  ë³€ì¸ í†µì œ ì§„

  1. **Distribution of input text**: x1~xkì˜ distribution

  1. **Label space**: y1~ykì˜ space

  1. **Format**: the use of input-label pairing as format

### Impact of the distribtuion of the input text

- out-of-domain distribtuion demonstrations ìƒí™©ì—ì„œ ì‹¤í—˜

  - demonstrationì— ì‚¬ìš©ë˜ëŠ” kê°œì˜ xë“¤ì„ ì•„ì˜ˆ ë‹¤ë¥¸ corpusì—ì„œ ìƒ˜í”Œë§í•´ì„œ ì‚¬ìš©

  - Demonstrationì˜ label spaceì™€ formatì€ ìœ ì§€í•˜ë©° input textì˜ distribtuionë§Œ ë°”ê¾¸ë©° ì´ì— ëŒ€í•œ impact í‰ê°€

- **Results**

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_010.png" class="img-fluid rounded z-depth-1" %}

  - Orange barë‘ Purple barë§Œ ë¹„êµí•´ë³´ë©´, OOD inputì„ ì‚¬ìš©í•  ë•Œ í° ì„±ëŠ¥ í•˜ë½ì„ ë³´ì„

    - GPT-J ê²½ìš°ì—ëŠ” No demonstrationë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥

  - Demonstrationsì˜ in-distribtuion inputì€ performance gainì— í° ì˜í–¥ì„ ë¼ì¹¨

    - ê·¼ë°, ë„ˆë¬´ ë‹¹ì—°í•œ ì†Œë¦¬ ì•„ë‹Œê°€ ì‹¶ìŒ. (ë¶„ë¥˜ ë¬¸ì œë¥¼ í‘¸ëŠ”ë°, ìŒ©ëš± ë§ì€ textë¥¼ ê°€ì ¸ì™€ì„œ exampleë¡œ ë³´ì—¬ì£¼ë©´ ì„±ëŠ¥í•˜ë½ì´ ë‹¹ì—°íˆ ìˆì§€ ì•Šì„ê¹Œ..ê·¼ë° ê·¸ ë‹¹ì—°í•  ê±¸ ë˜ ì‹¤í—˜ìœ¼ë¡œ ë³´ì—¬ì£¼ë‹ˆ ì—­ì‹œ ëŒ€ë‹¨í•¨ ğŸ˜€)

### Impact of the label space

- kê°œì˜ labelsì„ randomí•œ English wordë¥¼ ì‚¬ìš©í•´ì„œ ì‹¤í—˜ì„ ì§„í–‰í•¨. 

  - ì›ë˜ì˜ label space í¬ê¸°ì™€ English wordì˜ label space í¬ê¸°ëŠ” ë™ì¼í•˜ê²Œ ì„¤ì •

  - ì „ì²´ English word spaceì—ì„œ randomí•˜ê²Œ label ì¶”ì¶œí•˜ë©° demonstration êµ¬ì¶• 

â†’ ì—¬ê¸°ì„œë„ ì—­ì‹œ input text ë° formatì€ ìœ ì§€

- **Results**

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_011.png" class="img-fluid rounded z-depth-1" %}

  - Orange barì™€ Green barë¥¼ ë¹„êµ

  - **Direct-models  : P(y|x)**

    - random labelì„ ì‚¬ìš©í•  ë•Œì™€, random Enlgish wordsë¥¼ ì‚¬ìš©í•  ë•Œì˜ **ì„±ëŠ¥ ì°¨ì´ê°€ ëª…í™•í•¨**

  - **Channel-models : P(x|y)**

    - random labelì„ ì‚¬ìš©í•  ë•Œì™€, random Enlgish wordsë¥¼ ì‚¬ìš©í•  ë•Œì˜ **ì„±ëŠ¥ ì°¨ì´ê°€ ë¯¸ë¯¸í•¨**

    - Channel-modelsì´ labelì„ conditionìœ¼ë¡œ ë‘ê¸° ë•Œë¬¸ì—, label spaceë¥¼ ì•„ëŠ” ê²ƒì— ìˆì–´ gainì„ ì–»ì„ ìˆ˜ ì—†ë‹¤ê³  í•¨. great!

### Impact of input-label pairing

- Formatì˜ í˜•íƒœë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í—˜ì„ ì§„í–‰í•¨. inputsê³¼ labelsì˜ pairingì„ ì—†ì• ëŠ” ì‹ìœ¼ë¡œ ì‘ì€ ë³€í™”ë§Œ ì¤˜ì„œ formatì˜ í˜•íƒœë¥¼ ë°”ê¿ˆ.

  - Demonstrations with no labels: concatenation of x1~xk

    - Demonstrations with random English wordsì˜ formatì´ ì—†ëŠ” ê²½ìš°ì™€ ëŒ€ì‘

  - Demonstrations with labels only: concatenation of y1~yk

    - Demonstrations with OOD inputsì˜ formatì´ ì—†ëŠ” ê²½ìš°ì™€ ëŒ€ì‘

- **Results**

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_012.png" class="img-fluid rounded z-depth-1" %}

  - Formatì˜ í˜•íƒœë¥¼ ë°”ê¾¼ ê²½ìš° â†’ **ì§„ë³´ë¼ **bar(labelë§Œ ì¡´ì¬), **ì§„ë…¹ìƒ‰ **bar(inputë§Œ ì¡´ì¬)

  - Formatì„ ìœ ì§€í•˜ëŠ” ê²ƒì€ ì¤‘ìš”í•˜ë‹¤. 

    - ì—°ë³´ë¼ vs **ì§„ë³´ë¼**: (Format vs labelì€ ìˆì§€ë§Œ input text xê°€ ì—†ëŠ” ê²½ìš°)

    - ì—°ë…¹ìƒ‰ vs **ì§„ë…¹ìƒ‰**: (Format vs input textì€ ìˆì§€ë§Œ label yê°€ ì—†ëŠ” ê²½ìš°)

  - Formatì„ ì—†ì• ë©´ No demonstrationë³´ë‹¤ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì„

    - ë‹¤ë¥¸ caseë³´ë‹¤ Formatì„ ì—†ì• ëŠ” ê²ƒì´ ê°€ì¥ í° ì„±ëŠ¥ í•˜ë½ì„ ë³´ì„

### Impact of meta-training

- ë‹¤ë¥¸ modelsë“¤ê³¼ ë‹¤ë¥´ê²Œ MetaICLì€ ICLì„ ëª©ì  í•¨ìˆ˜ë¡œ ë‘ì–´ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë‹¤.

  - MetaICL: í•™ìŠµì€ large collection of supervised datasetì„ í†µí•´ multi-task learningìœ¼ë¡œ ì§„í–‰í•˜ê³  InferenceëŠ” ICLê³¼ ë™í•˜ê²Œ ì§„í–‰í•¨. (Unseen taskì— ëŒ€í•´ì„œ generalizabilityë¥¼ ë†’ì´ê¸° ìœ„í•´ ì œì•ˆëœ model)

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_013.png" class="img-fluid rounded z-depth-1" %}

  - ìœ„ì— ì§„í–‰í•œ ì‹¤í—˜ì—ì„œ MetaICLì˜ ë‘ë“œëŸ¬ì§„ ëª‡ ê°€ì§€ íŠ¹ì§•ì´ ë³´ì„

    - Formatì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ë‹¤ë¥¸ modelsì— ë¹„í•´ ë”ìš± ì¤‘ìš”

    - GT Input-label mappingì´ ë‹¤ë¥¸ modelsì— ë¹„í•´ ëœ ì¤‘ìš” 

â†’** meta-training encourages the model to exculsively exploit simpler aspects of the demonstrations and to ignore others**

    - ì´ì— ëŒ€í•´ ì €ìë“¤ì˜ ìƒê°ì€ ë‹¤ìŒê³¼ ê°™ìŒ

      1. input-label mappingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì–´ë µë‹¤

      1. formatì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë¹„êµì  ì‰½ë‹¤

      1. modelì´ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•œ textì˜ spaceëŠ” modelì´ condtionìœ¼ë¡œ ë‘” textì˜ spaceë³´ë‹¤ ì‚¬ìš©í•˜ê¸° ì‰½ë‹¤

â†’ Direct modelì´ input distribtuionë³´ë‹¤ label spaceë¥¼ ì˜ ì´ìš©í•˜ê³ 

â†’ Channel modelì´ label spaceë³´ë‹¤ input distributionì„ ì˜ ì´ìš©í•œë‹¤

(ì´ê²ƒë„ ë‹¹ì—°í•œ ì–˜ê¸°ê°€ ì•„ë‹Œê°€..ê·¸ì¹˜ë§Œ ëŒ€ë°• ğŸ˜ƒ)

{% include figure.liquid loading="eager" path="assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_014.png" class="img-fluid rounded z-depth-1" %}

# Discussion

### Does the model learn at test time?

- Learningì˜ ëŒ€í•œ ì—„ë°€í•œ ì •ì˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë‘ë©´, â€˜caputring the input-label correspondence given in the training dataâ€™

  -  LMsì€ test ì‹œì— ìƒˆë¡œìš´ taskë¥¼ í•™ìŠµí•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. 

  - ì €ìë“¤ì€ modelì´ demonstrationì— ì–¸ê¸‰ëœ taskë¥¼ ë¬´ì‹œí•˜ê³  pretrainingì— ì‚¬ìš©ëœ priorë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤ê³  ìƒê°í•¨. 

- Learningì„ ë„“ì€ ì˜ë¯¸ë¡œ í•´ì„í•˜ë©´

  - íŠ¹ì • inputê³¼ labelì˜ distribtuionsê³¼ íŠ¹ì • formatì„ demonstrationì— ì˜ ë…¹ì—¬ë‚¼ ë•Œ, modelì˜ prediction ê²°ê³¼ëŠ” ë” ì •í™•í•´ì§ˆ ìˆ˜ ìˆê³ , ì´ëŠ” modelì´ demonstrationì„ í†µí•´ ìƒˆë¡œìš´ taskë¥¼ í•™ìŠµí•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ.

  - test ì‹œì— demonstrationì„ ì˜ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ìƒˆë¡œìš´ taskë¼ê³  ë³´ê¸°ë³´ë‹¤ëŠ” ì‚¬ì „í•™ìŠµëœ weightsë¥¼ ì˜ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹ˆ, ì´ê²ƒ ì—­ì‹œ pretrainingì— ì‚¬ìš©ëœ priorë¥¼ ì‚¬ìš©í•˜ëŠ” ê±° ì•„ë‹ê¹Œ? 

### Capacity of LMs

- modelì€ input-label demonstrationì— ì˜ì¡´í•˜ì§€ ì•Šìœ¼ë©° downstream taskë¥¼ ìˆ˜í–‰í•œ ê²ƒì„ ì‹¤í—˜ì—ì„œ ë³´ì„. ì´ë¥¼ í†µí•´ input-label correspondence ìì²´ë¥¼ langauge modeling (pretraining)í•  ë•Œ í•™ìŠµì„ í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ. 

  - ì´ëŠ” langauge modeling ëª©ì  í•¨ìˆ˜ê°€ zero-shot ì„±ëŠ¥ì˜ ì£¼ì—­ì´ë¼ê³  ë´„

  - ë°˜ë©´ì—, ICLì€ LMì—ì„œ í•™ìŠµí•˜ì§€ ëª»í•œ input-label correspondence taskëŠ” ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤ê³  ë´„. ICLì´ í’€ ìˆ˜ ì—†ëŠ” NLP ë¬¸ì œë“¤ì„ ì–´ë–»ê²Œ ë°œì „ì‹œì¼œì•¼ ë  research questionì„ ì§ˆë¬¸í•¨.

â†’ ì´ ì  ì—­ì‹œ LMì˜ knowledge(task) injection ë° updateì™€ ê´€ë ¨ì´ ìˆë‹¤ê³  ìƒê°

### Connection to instruction-following models

- (Instruction) natural languageë¡œ ë¬¸ì œ ì„¤ëª…ì„ ì„¤ëª…í•˜ë©´ inference ê³¼ì •ì—ì„œ ìƒˆë¡œìš´ taskë¥¼ ìˆ˜í–‰ í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì „ ì—°êµ¬ë“¤ì´ ì¡´ì¬. â†’ Demonstrationsê³¼ Instructionì€ LMì—ê²Œ ìˆì–´ ë¹„ìŠ·í•œ ì—­í•  ìˆ˜í–‰

- Instructionì€ modelë¡œ í•˜ì—¬ê¸ˆ model ê°–ê³ ìˆëŠ” capacityë¥¼ ëŒì–´ì˜¬ë¦¬ëŠ” ê²ƒì„ ì´‰ì§„ ì‹œí‚¬ ìˆ˜ëŠ” ìˆì§€ë§Œ, ìƒˆë¡œìš´ taskë¥¼ í•™ìŠµ ì‹œí‚¤ì§€ëŠ” ëª»í•¨.

### 
