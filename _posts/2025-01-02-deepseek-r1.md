---
categories:
- paper-reviews
date: '2025-01-02 00:00:00'
description: 논문 리뷰
giscus_comments: true
layout: post
related_posts: false
tags:
- gpt
- language-model
- llm
- paper-review
- reasoning
- reinforcement-learning
thumbnail: assets/img/posts/2025-01-02-deepseek-r1/thumbnail.jpg
title: DeepSeek R1
---

**논문 정보**
- **Date**: 2025-01-02
- **Reviewer**: 건우 김

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_000.png" class="img-fluid rounded z-depth-1" %}

https://substack.com/home/post/p-153314921

# Introduction

최근에 post-training은 reasoning tasks, align/adapt to user preference 등 성능을 올리기 위해 많이 사용되고 있음. 특히 Reasoning capability 측면에서 보면 GPT-4-o1은 CoT의 reasoning process의 Length를 늘리며 **inference-time 관점에서 scaling-out한** 방법으로 처음 소개된 모델임. test-time scaling을 효과적으로 수행하기 위한 방법으로 다양한 선행 연구들이 있었지만, 이중에 어느 것도 o1과 같은 reasoning model의 성능을 달성하진 못했음.

본 연구에서는 Reinforcement Learning(RL)을 활용해서 Language Model의 reasoning 성능을 끌어올리는 방법을 타묵함. **연구의 주된 목표는 SFT 없이 RL만 활용해서 LLM의 reasoning capabilities를 키우는 것임.**

DeepSeek-V3-Base을 backbone model로 두고 GROP (RL)로만 학습시켜 **DeepSeek-R1-Zero**를 만들었고, 이것의 reasoning 성능은 AIME2024에서 GPT-4-o1의 86.7% 수준의 성능을 보여줌.

하지만 DeepSeek-R1-Zero의 output은 가독성이 떨어지기 때문에, 이를 해결하고자 multi-stage로 학습되는**DeepSeek-R1** 모델을 개발함. 본 모델의 학습 과정은 아래와 같음

1. Cold start

1. Reasoning-oriented RL

1. Rejection Sampling and SFT

1. RL for all Scenarios

더 나아가서 DeepSeek-R1을 활용하여 여러 Open-source models을 대상으로 distillation을 진행시켰고, large model에서 보이는 reasoning pattern이 small models들에 transfer가 어느 정도 되는 것을 보여줌.

본 논문의 주된 **Contributions**은 아래와 같음

1. **Post-Training: Large-Scale Reinforcement Learning on the Base Model**

1. **Distillation: Smaller Models Can Be Powerful Too**

# Approach

(논문의 내용과 jay alammar의 시각 자료를 활용함)

## Overview

DeepSeek-R1은 DeepSeek-V3에서 소개된 모델을 backbone으로 활용함. (이때, DeepSeek-V3의 final model이 아니고 base model임) 

DeepSeek-v3-Base에 SFT를 진행한 뒤에 RL을 적용하면 ⇒ DeepSeek-R1 !!

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_001.png" class="img-fluid rounded z-depth-1" %}

## 1. Long chains of reasoning SFT Data

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_002.png" class="img-fluid rounded z-depth-1" %}

SFT를 진행하기 위해서는 많은 양의 labeled data가 필요한데, 이 만큼의 양을 사람으로부터 얻기는 어렵다. 따라서, 이런 high quality의 long chain-of-thought reasoning examples을 얻기 위해서 “**Interim reasoning LLM**”을 별도로 학습시켜서 활요함.

## 2. An interim high-quality reasoning LLM (but worse at non-reasoning tasks)

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_003.png" class="img-fluid rounded z-depth-1" %}

Interim reasoning model을 별도로 학습시켜서 SFT data를 구축하는데 사용함. 이때, Interim reasoning model은 DeepSeek-R1-Zero과 비슷하게 구축이 됨. (DeepSeek-R1-Zero를 먼저 만들고 난 뒤에, 여기서 영감을 얻어 DeepSeek-R1을 개발함) 

## 3. Creating reasoning models with large-scale RL

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_004.png" class="img-fluid rounded z-depth-1" %}

### **3.1 Large-Scale Reasoning-Oriented RL (R1-Zero)**

여기서 RL은 interim model을 학습시키기 위해 사용하고 해당 모델로 SFT reasoning samples을 구축함. 아래 table에서 보이는 바와 같이 R1-Zero는 o1과 비슷한 수준의 Reasoning 성능을 보여줬기에, 이에 영감을 받아 SFT reasoning samples을 구축하고자 함.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_005.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_006.png" class="img-fluid rounded z-depth-1" %}

기존의 ML 모델들과 다르게, 더 이상 새로운 데이터를 추가하지 않아도 성능이 향상될 수 있음을 보여줌

1. 14.8 Billion high-quality tokens으로 학습한 base model이 있는데, 이미 많은 데이터로 학습시켰기 때문에, 단순히 더 많은 데이터를 추가한다고 해서 성능이 획기적으로 좋아지는 단계는 지났을 가능성이 큼

1. Reasoning 문제는 데이터 없이도 성능이 개선될 수 있음.

**Example**

> Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.

위 문제는 

- linter와 같은 software를 활용하여 생성된 코드가 python 문법을 따르는지 확인할 수 있음

- 코드가 실제로 실행되는지 테스트할 수 있음 (실행되지 않으면 잘못된 코드임을 확인함)

- 실행 속도(퍼포먼스) 측정 가능. 

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_007.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_008.png" class="img-fluid rounded z-depth-1" %}

이때 사용한 System message는 아래와 같음 (명시적으로 <think> tag 사이에 reasoning prcoess를 작성하라고 되어있지만, 논리 과정은 자율 형식)

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_009.png" class="img-fluid rounded z-depth-1" %}

RL과정에서 사용된 두 가지 rewards는 **accuracy reward** (답변이 맞는지 확인)과 **format rewards**( think, answer tag를 잘 사용하고 있는지)가 있고, GRPO 알고리즘을 활용하여 model을 update함

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_010.png" class="img-fluid rounded z-depth-1" %}

### Group Relative Policy Optimization (GRPO)

**PPO**

Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 참조

**Proximal Policy Optimization (PPO): Actor-Critic RL algorithm**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_011.png" class="img-fluid rounded z-depth-1" %}

PPO Advantage를 뜯어보면 아래와 같음

- Advantage는 Rt - V(st)로 산출됨

Gradient Update process

1. Actor update (PPO Objective function)

1. Critic update (Value function)

Actor update → Critic update

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_012.png" class="img-fluid rounded z-depth-1" %}

- PPO에서는 별도의 critic model (=value function)을 사용하여 policy model의 행동을 평가하는데, critic model의 크기는 policy model과 동일하거나 비슷하기에 computational cost가 높은 단점이 있음

- critic model → group scores로 대체하여 baseline을 추정하는 방식

- **Core Ideas**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_013.png" class="img-fluid rounded z-depth-1" %}

(1) PPO의 Clip Trick을 활용함

(2) KL Divergence

(3) Advantage (Ai)

GRPO의 알고리즘은 아래와 같음

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_014.png" class="img-fluid rounded z-depth-1" %}

### **3.2 Creating SFT reasoning data with the interim reasoning model**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_015.png" class="img-fluid rounded z-depth-1" %}

1. **Cold Start**

1. **Reasoning-oriented Reinforcement Learning**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_016.png" class="img-fluid rounded z-depth-1" %}

Interim model을 활용하여 600,000개 samples을 만들고 난 뒤에 DeepSeek-V3에서 사용한 SFT dataset samples 200,000개와 결합하여 최종적으로 **800,000 SFT samples** 구축

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_017.png" class="img-fluid rounded z-depth-1" %}

### 3.3 General RL training phase

위와 같이 SFT로 학습된 R1은 reasoning 및 non-reasoning tasks를 잘 수행하지만, 더 다양한 applications에 확장시키기 위해서 Llama2에서 사용한 helpfulness / safety reward model을 활용하여 일반적인 RL 학습을 진행함

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_018.png" class="img-fluid rounded z-depth-1" %}

여기서는 기존에 사용하던 reasoning modeling을 위한 reward verifier와 llama2에서 사용한 reward model을 **동시에 같이 사용함**

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_019.png" class="img-fluid rounded z-depth-1" %}

### *Distillation: Empower Small Models with Reasoning Capability

DeepSeek-R1은 671B params를 갖고 있기 때문에, 일반 hardware에서 사용하기에 어려움이 있음. 본 연구에서 R1의 reasoning quality를 Qwen-32B와 같은 open source model로 distillation을 진행함.

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_020.png" class="img-fluid rounded z-depth-1" %}

800,000개의 reasoning 및 non-reasoning samples로 학습을 진행함

# Experiment

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_021.png" class="img-fluid rounded z-depth-1" %}

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_022.png" class="img-fluid rounded z-depth-1" %}

# Discussion

## Distillation vs. Reinforcement Learning

*Can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?*

→ Qwen32B-base로 실험한 결과 Zero모델은 R1에 비해 다소 떨어진 성능을 보이고 QwQ32B와 비슷한 성능을 보여줌

{% include figure.liquid loading="eager" path="assets/img/posts/2025-01-02-deepseek-r1/image_023.png" class="img-fluid rounded z-depth-1" %}
