---
categories: paper-reviews
date: '2025-02-04 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - Titans: Learning to Memorize at Test Time'
giscus_comments: true
layout: post
related_posts: false
tags: paper-review
title: 'Titans: Learning to Memorize at Test Time'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2025-02-04
- **Reviewer**: ì¤€ì› ì¥

[//]: # (table_of_contents is not supported)

## 1. Introduction

- Transformer

	â‡’ (1) key-value associationsì„ ì €ì¥ (2) queryë¥¼ í†µí•´ retrieveí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ

	â‡’ current context windowì— ì§ì ‘ì ì¸ dependenciesê°€ í˜•ì„±ë  ìˆ˜ ë°–ì— ì—†ìŒ

- Overcome the scalability issue of Transformers

	â‡’ linear transformer: softmaxëŒ€ì‹  kernel trickìœ¼ë¡œ attention ê³„ì‚°

	â‡’ dataê°€ matrix-valued statesë¡œ mapping/compressedì´ ë˜ê¸° ë•Œë¬¸ì— very long contextì—ì„œ íš¨ìš©X

- Limitation of recurrent neural network

	1. ë‹¨ê¸° ê¸°ì–µ, ì¥ê¸° ê¸°ì–µ, ë©”íƒ€ ê¸°ì–µ, í˜„ì¬ ë§¥ë½ì— ëŒ€í•œ attentionì„ ëª¨ë‘ ì ì ˆí•˜ê²Œ êµ¬í˜„í•œ architectureì˜ ë¶€ì¬

	1. êµ¬ì„± ìš”ì†Œê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆëŠ” ìƒí˜¸ ì—°ê²°ëœ ì‹œìŠ¤í…œì˜ ë¶€ì¬

	1. (LSTM, GRUê°€ ì–´ëŠì •ë„ëŠ” í•˜ì§€ë§Œ) ì—¬ì „íˆ ë°ì´í„°ë¥¼ í†µí•´ ì¶”ìƒí™”ëœ ê³¼ê±° ì—­ì‚¬ë¥¼ ì•”ê¸°í•˜ëŠ” ëŠ¥ë ¥ì´ ê²°ì—¬

<br/>

- Memory Perspective

	â‡’ ë…¼ë¬¸ì—ì„œëŠ” Memory ê´€ì ì—ì„œ ê¸°ì¡´ì˜ Modelì„ ë…ìì—ê²Œ ì´í•´ì‹œí‚¤ê³ ì í•¨

	- <span style='color:blue_background'>**memory: inputì— ì˜í•´ì„œ ë°œìƒí•˜ëŠ” neural update**</span>

	1. RNN

		1. $ ğ‘“ (M_{ğ‘¡ âˆ’1}, ğ‘¥_ğ‘¡ ) $

		1. $ ğ‘”(M_ğ‘¡, ğ‘¥_ğ‘¡ ) $

		â†’ të²ˆì§¸ ì…ë ¥ì— ì˜í•´ì„œ â€˜vector-valued memory moduleâ€™ $ M $ì´ ì—…ë°ì´íŠ¸ ë˜ê³ , retreiving ë˜ëŠ” ê²ƒì˜ ë°˜ë³µ

	1. Transformer

		â†’ RNNê³¼ ë‹¬ë¦¬ past key, valueë¥¼ ê³„ì† appendingí•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸

		â†’ matrix-valued memory Module

	<br/>

- ìœ„ì˜ ë…¼ì˜ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¬¸ì—ì„œëŠ” 5ê°œ RQë¥¼ ë˜ì§

	1. ì¢‹ì€ ë©”ëª¨ë¦¬ êµ¬ì¡°ë€ ë¬´ì—‡ì¼ê¹Œ?

	1. ì ì ˆí•œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì´ë€ ë¬´ì—‡ì¼ê¹Œ?

	1. ì¢‹ì€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ë€ ë¬´ì—‡ì¼ê¹Œ?

	1. (ê¸°ì–µì´ ë‹¨ì¼ê³¼ì •ì´ ì•„ë‹ˆë©° ë‹¨ì¼ ê¸°ëŠ¥ë„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ê°ê° ë‹¤ë¥¸ ì‹ ê²½ êµ¬ì¡°ë¡œ ì„œë¡œ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ë©° ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ) ì„œë¡œ ë‹¤ë¥¸ ìƒí˜¸ ì—°ê²°ëœ ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•©í•˜ëŠ” íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ëŠ” ë¬´ì—‡ì¼ê¹Œ?

	1. (ë°ì´í„°ë¥¼ linear mannerë¡œ ë²¡í„°ë‚˜ í–‰ë ¬ì— ì €ì¥í•œë‹¤ëŠ” ê°€ì •ì€ oversimplificationì¼ ìˆ˜ë„ ìˆë‹¤) long-term memoryë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥/ê¸°ì–µí•˜ë ¤ë©´ deep memory moduleì´ í•„ìš”í•œê°€?

	<br/>

	â‡’ ìœ„ì˜ ë¬¼ìŒì— ëŒ€í•œ í•´ë‹µì„ ì œì‹œí•˜ë©´ì„œ test timeì— memorizeê°€ ê°€ëŠ¥í•œ architecture ì œê³µ

<br/>

## 2. Preliminaries

### Notations

- Input: $ x âˆˆ â„^{(NÃ—d_m)} $

- Neural Network Module: $ \mathcal{M}Â  $

- Attention Mask: $ M $

- Segment

	- ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸: $ S^{(i)} $

	- ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ì˜ jë²ˆì§¸ í† í°, ë²¡í„°, hidden state: $ S_j^{(i)} $

- Neural Network

	- forward pass with weight adjustment: $ \mathcal{N}(x) $

	- forward pass without weight adjustment: $ \mathcal{N}^{*}(x) $

	- forward pass in k-th layer: $ \mathcal{N}^{(k)} $

<br/>

### Backgrounds

- Transformers

	- $ y_i = \sum_{j=1}^i \frac{\exp(Q_i^T K_j/\sqrt{d_m}) V_j}{\sum_{l=1}^i \exp(Q_i^T K_l/\sqrt{d_m})} $

	- ğ‘ Ã— ğ‘‘ operationì´ í•„ìš”í•¨  â†’ ê¸´ ë©”ëª¨ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” larger memory consumption and lower-throughput

- Efficient Attentions (linear attentions)

	- kernel function: $ \phi(x,y) = \phi(x)\phi(y) $

	- attention: $ y_i = \sum_{j=1}^i \frac{\phi(Q_i^T K_j)}{\sum_{l=1}^i \phi(Q_i^T K_l)} V_j = \sum_{j=1}^i \frac{\phi(Q_i)^T \phi(K_j)}{\sum_{l=1}^i \phi(Q_i)^T \phi(K_l)} V_j = \frac{\phi(Q_i)^T \sum_{j=1}^i \phi(K_j)V_j}{\phi(Q_i)^T \sum_{l=1}^i \phi(K_l)} $

	- kernel ì´ identity functionì´ë©´ ë‹¤ìŒê³¼ ê°™ì´ recurrent formatì„ ê°–ëŠ” transformerë¡œ ì „ê°œ

		$ M_t = M_{t-1} + K_t^T V_t $

		$ y_t = Q_t M_t $

- RNN

	- hidden state  = memory units

	- recurrent processë¥¼ memory ê´€ì ì—ì„œ read/writeë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ

		- read(input â†’ hidden) : $ y_t = g(\mathcal{M}_t, x_t) \quad \text{Read Operation} $

		- write(hidden â†’ output): $ \mathcal{M}_t = f(\mathcal{M}_{t-1}, x_t) \quad \text{Write Operation} $

	â‡’ ì´ ê´€ì ì—ì„œ ë³´ë©´ **[Equation](/17feef51b0f08099a4a4f96f0b89ea72#17feef51b0f080019eccc61bc5940ee7)**ì€ matrix-valued memoryì— keyì™€ valueë¥¼ ê³„ì†í•´ì„œ writeí•˜ëŠ” ê³¼ì •ì´ë¼ ë³¼ ìˆ˜ ìˆìŒ

	<br/>

> ê²°êµ­ sequenceê°€ ê¸¸ì–´ì§ì— ë”°ë¼ ëª¨ë¸ì´ forwarding í•˜ë©´ì„œ í’€ì–´ì•¼ í•˜ëŠ” ë¬¸ì œëŠ” 2ê°œë¡œ ì¢í˜€ì§ (memory moduleì„ ì˜ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ê±´ ì—¬ê¸°ì—ì„  ë‹¹ì—°í•œ ë¬¸ì œ)

1. forget mechanismì„ ì˜ ì¶”ê°€í•´ memory ì ì¬ë¥¼ ì¤„ì´ëŠëƒ? (xLSTM, Mamaba2)

1. write operationë¥¼ improvingì‹œí‚¤ëƒ? (ë­ ë…¼ë¬¸ ì„¤ëª…ì„ ë³´ë©´ ì˜ ì§€ìš°ë©´ì„œ writeì‹œí‚¤ëƒ, ë³‘ë ¬ì²˜ë¦¬í•™ìŠµì´ ê°€ëŠ¥í•˜ëƒë¡œ ì„¤ëª…í•¨)

<br/>

## 3. Learning to Memorize at Test Time

ğŸŒ **ë…¼ë¬¸ì´ ì œì•ˆí•˜ê³ ì í•˜ëŠ”ê²ƒì€ â€˜inference timeâ€™ë•Œ long-term memoryë¥¼ ì˜ í™œìš©í•˜ëŠ” meta memory model
â†’ neural network (e.g., LM)ì´ sequenceë¥¼ ì²˜ë¦¬í•  ë•Œ ì´ë¥¼ ì ì ˆíˆ â€˜ì €ì¥í•  í•¨ìˆ˜â€™ë¥¼ íŒŒë¼ë¯¸í„°ë¡œì¨ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ**

<br/>

### 3.1 Long-term Memory

â†’ memorizationì´ ê°€ëŠ¥í•œ learning function, ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ ë°ì´í„°ë¥¼ ëª¨ë“ˆì´ ì–´ë–»ê²Œ ì €ì¥í•˜ëŠ”ì§€ì— ëŒ€í•œ ë°©ë²•ì„ í•™ìŠµ

<br/>

- **Learning Process and Surprise Metric.**

	- Online learningì„ ì°¨ìš©í•œ í›„ ë„ˆë¬´ë‚˜ ì§ê´€ì ì¸ ë°©ë²•ì„ í™œìš©í•´ current sequence input $ x_t $ì´ ê·¸ë™ì•ˆì˜ Memory Moduleì´ ì €ì¥í•´ì˜¨ dataì˜ patternê³¼ ë‹¤ë¥´ë©´ Memory Moduleì„ updateí•˜ëŠ” ì‹ìœ¼ë¡œ í•™ìŠµ

	$$ \mathcal{M}_t = \mathcal{M}_{t-1} - \theta_t \nabla \ell(\mathcal{M}_{t-1}; x_t) $$

	â†’ $ \nabla \ell(\mathcal{M}_{t-1}; x_t) $ì„ `surprise` ë¡œ ì •ì˜í•˜ëŠ”ë° ì‚¬ì‹¤ìƒ past sequenceë‘ ë§ì´ ë‹¤ë¥´ë©´ Memory Moduleì„ ë§ì´ ì—…ë°ì´íŠ¸ í•˜ê² ë‹¤.ë¼ëŠ” ì „í˜•ì ì¸ DL ì—…ë°ì´íŠ¸

	$$ \mathcal{M}_t = \mathcal{M}_{t-1} + S_t $$

	$$ S_t = \eta_{t} \underbrace{S_{t-1}}_{\text{Past Surprise}} - \theta_t \underbrace{\nabla \ell(\mathcal{M}{t-1}; x_t)}_{\text{Momentary Surprise}} $$

	â†’ gradient descent with momentumì˜ í˜•ì‹ê³¼ ë˜‘ê°™ì´ `surprise` $ S_t $ë¥¼ ì •í•¨

	-  $ \eta_{t} $ì™€ $ \theta_{t} $ê°€ ëª¨ë‘ function of input $ x_t $

	- data-dependent mannerë¡œ memory moduleì„ updateí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—

		(e.g., ëª¨ë“  í† í°ì´ ê´€ë ¨ì„±ì´ ìˆê³  ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ì— ìˆì„ ê²½ìš°, recent past tokens ëŒ€ë¹„ input $ x_t $ê°€  $ \eta_{t} \rightarrow 1 $ë¡œ í•´ì•¼ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµì´ ë¨)

	<br/>

	<br/>

- **Objective.**

	â†’ past dataë¥¼ keyì™€ valueì˜ pairë¡œ ì €ì¥í•˜ëŠ” ì´ì „ modelë“¤ì˜ ê´€ì ì„ ë”°ë¼ ì•„ë˜ì˜ lossë¡œ memory moduleì„ í•™ìŠµ

	$$ \mathbf{k}_t = x_t W_K, \quad \mathbf{v}_t = x_t W_V $$

	$$ W_K, W_V \in \mathbb{R}^{d_{\text{in}} \times d_{\text{in}}} $$

	$$ \ell(\mathcal{M}_{t-1}; x_t) = |\mathcal{M}_{t-1}(\mathbf{k}_t) - \mathbf{v}_t|_2^2 $$

	-  input $ x_t $ë¥¼ ì‚¬ì˜ì‹œí‚¨ ì´í›„ memory moduleì´ key â†” valueì˜ ê´€ê³„ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹

	- ì´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê´€ì ì—ì„œ $ W_K, W_V $ëŠ” hyperparameter

	<br/>

- **Forgetting Mechanism.**

	$$ \mathcal{M}_t = (1 - \alpha_t)\mathcal{M}_{t-1} + S_t $$

	$$ S_t = \eta_t S_{t-1} - \theta_t \nabla \ell(\mathcal{M}_{t-1}; x_t) $$

	â†’ GRU, LSTMê°™ì´ forgetting ë¶€ë¶„ ë„ì…. í•´ë‹¹ weight decay/forgetting ë¶€ë¶„ë„ input $ x_t $ì— ì˜í•´ì„œ í•™ìŠµë˜ë„ë¡ ì„¤ê³„

	<br/>

- **Memory Architecture.**

	- vector-valuedë‚˜ matrix-valuedë¥¼ í™œìš©í•´ memory moduleì„ ì„¤ê³„í•  ê²½ìš° â†’  $ \mathcal{M}_t =w_t $

		- ì´ ê²½ìš° ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ëª©í‘œë¥¼ í‘¸ëŠ”ê²Œ ë˜ê³ , ìµœì ì˜ ê°’ì€ ë°ì´í„°ì˜ ì¢…ì†ì„±ì´ ì„ í˜•ì´ë¼ëŠ” ê°€ì •ì„ í•´ì•¼í•¨

	- ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ í¸ì˜ë¥¼ ìœ„í•´ í¸ì˜ìƒ í‘œê¸°ë¥¼ â€˜$ \mathcal{M}_t =w_t $â€™ë¡œ í•˜ì§€ë§Œ expressive powerë¥¼ ìœ„í•´ 2 layer MLPë¥¼ ì¼ë‹¤ê³  í•¨

	<br/>

- **Retrieving a Memory.**

	- ìœ„ì—ì„œ ì„¤ê³„í•œê±´ memory moduleì´ê¸°ì— informationì„ retrieveí•´ì„œ current sequenceí•´ concatí•´ processingì„ í•´ì•¼ ë¹„ë¡œì†Œ ì“¸ëª¨ê°€ ì™„ì„±ì´ ë¨

		$$ \mathbf{q}_t = x_t W_Q $$

		$$ y_t = \mathcal{M}^*(\mathbf{q}_t) $$

<br/>

### 3.2 How to Parallelize the Long-term Memory Training

â†’ long-term memory module í•™ìŠµì‹œì— ê¸´ sequenceë¥¼ parallelí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ì—¬ì¤€ ë¶€ë¶„

$$ \mathcal{M}_t = (1-\alpha_t)\mathcal{M}_{t-1} - \theta_t\nabla\ell(\mathcal{M}_{t-1}; x_t) = \beta_t\mathcal{M}_0 - \sum_{i=1}^t \theta_i \frac{\beta_t}{\beta_i}\nabla\ell(\mathcal{M}_{t'}; x_i) $$

- $ \mathcal{M}_0 $ì—ì„œ í•™ìŠµì‹œì‘

- $ t' $: 0

- $ t $: $ b $

$$ \nabla\ell(W_0; x_t) = (W_0x_t - x_t)x_t^\top \Rightarrow \sum_{i=1}^b \theta_i\frac{\beta_b}{\beta_i}\nabla\ell(W_0; x_i) = \Theta_b B_b(W_0X - X)X^\top $$

- $ \beta_i = \prod_{j=1}^i(1-\alpha_j) $

 â†’ê° ì²­í¬(rank)ì— ê´€ë ¨ëœ í–‰ë ¬ì„ ì €ì¥í•¨ìœ¼ë¡œ ë¶„ì‚°í•™ìŠµ ê°€ëŠ¥

$$ S_t = \eta_t S_{t-1} - \theta_t u_t $$

â†’ ê° chunkì— ëŒ€í•œ $ u_t $ë¥¼ êµ¬í•´ë†“ê³  recurrentí•˜ê²Œ `surprise` valueê°’ êµ¬í•˜ê¸° ê°€ëŠ¥

<br/>

### 3.3 Persistent Memory

â†’ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ input-independentí•œ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ task-related memoryë¡œ í™œìš©í•˜ê³ ì í•¨

(ì—¬ê¸°ì„œë¶€í„° 2016-2019 ëª¨ë¸ë§ ì—°êµ¬ ëŠë‚Œ ë„ˆë¬´ ê°•í•¨;;;)

$$ x_{\text{new}} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}] | x $$

- prefix/prompt tuningì²˜ëŸ¼ sequenceì•ì— task-specific learnable (inferenceì—ì„œëŠ” fixì¸) parameterë¥¼ ë„ì…

- ê·¸ëŸ¼ ì €ìë“¤ì€ ì´ moduleì„ ì™œ ë„ì…í–ˆëƒ?

	1. memorization of the task knowledge (prefix/prompt tuningë‘ ë˜‘ê°™ìŒ)

	1. ì´ parameterë„ ê²°êµ­ attentionì˜ ëŒ€ìƒì´ ë˜ëŠ”ë°, input-independent attention weightsì´ í•„ìš”í•´ì„œ

	1. attention mapì„ ì‚´í´ë³´ë©´ initial biasê°€ ìˆëŠ”ë° input-independent parameterê°€ attention distribution redistributingí•´ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ (ê·¼ë° ë³´í†µ special tokenì— skewedê±¸ë¦¬ì§€ ì•Šë‚˜?)

<br/>

## 4 How to Incorporate Memory?

ğŸŒ **ìœ„ì—ì„œ ì†Œê°œí•œ â€˜neural memoryâ€™ë¥¼ neural networkì— incorporateí•˜ëŠ” 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•¨
â†’ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ neural networkë¥¼ short-term memory modulesë¼ê³  í‘œí˜„í•˜ë©´ì„œ, íŠ¹íˆ transformerëŠ”, key valueë¥¼ ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— long contextì—ì„œëŠ” í•œê³„ê°€ ìˆë‹¤ê³  ì–¸ê¸‰
â†’ í•˜ì§€ë§Œ ì œì•ˆí•˜ëŠ” memory moduleì„ memoryì— read/writeí•˜ë©´ì„œ current key/value representationì„ ê°•í™”**

â†’ ì•„ë˜ ëª¨ë“  frameworkì—ì„œ coreë¥¼ neural network/lmì •ë„ë¡œ ìƒê°í•˜ê³  ë”°ë¼ê°€ë©´ ëœë‹¤.

â†’ ë˜í•œ ì•„ë˜ì˜ ëª¨ë“  frameworkê°€ test timeì— ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë”°ë¼ê°€ì.

<br/>

### 4.1 Memory as a Context (MAC)

â†’ $ S^{(i)} \ (i = 1,\ldots,N/C) $ : sequenceë¥¼ ê³ ì • í¬ê¸° ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ

1. $ h_t = \mathcal{M}_{t-1}^*(\mathbf{q}_t) $ : memory moduleì—ì„œ ê³ ì • ì„¸ê·¸ë¨¼íŠ¸ì™€ ìœ ì‚¬í•œ past information retrieve

1. $ \tilde{S}^{(t)} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}] | h_t | S^{(t)} \\ y_t = \text{Attn}(\tilde{S}^{(t)}) $ : persistent memory, past information, ê³ ì • segmentë¥¼ neural networkì— forwardingí•´ì„œ attention

	â†’ attentionì€ [standard attention](/17feef51b0f08099a4a4f96f0b89ea72#17feef51b0f080598d4cf59d04e34647)ê³¼ ë™ì¼í•¨

		â†’ ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ persistent memory, past informationê°€ ì¶”ê°€ë¨ì— ë”°ë¼ attention windowë§Œ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ

1. $ \mathcal{M}_t = \mathcal{M}_{t-1}(y_t) $ : attention outputì„ í™œìš©í•´ long-term memory moduleì„ update

1. $ o_t = y_t \otimes \mathcal{M}_t^*(y_t) $ : updateë˜ memory moduleì— attention outputì„ í†µê³¼í•œ í›„ ì´ë¥¼ ê¸°ì¡´ attention outputê³¼ tensorê³± ì—°ì‚° í•´ ìµœì¢… output ê³„ì‚°

<br/>

â‡’ í•´ë‹¹ êµ¬ì¡°ì˜ ê°€ì¥ í° ì¥ì ì€ attentionì´ current/longtermì— ë™ì‹œì— attentionì„ ì£¼ê¸° ë•Œë¬¸ì— ì–´ë–¤ ì •ë³´ê°€ ìœ ìš©í•œì§€ íŒŒì•… í›„ ë©”ëª¨ë¦¬ ìš©ëŸ‰ì„ ê´€ë¦¬í•˜ê¸°ì— ìš©ì´í•˜ë‹¤ëŠ” ê²ƒ

<br/>

### 4.2 Gated Memory (MAG)

â†’ ì´ì „ì²˜ëŸ¼ memory moduleì´ current inputì— ì˜í•´ updateë˜ê¸´ í•˜ì§€ë§Œ attentionì— í™œìš©ë˜ì§€ëŠ” ì•ŠìŒ

1. $ \tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}] | x $

1. $ y = \text{SW-Attn}^*(\tilde{x}) $ : sliding window attentionìœ¼ë¡œ attention ì²˜ë¦¬

	1. $ o = y \otimes \mathcal{M}(\tilde{x}) $

	â†’ $  y \ ì™€ \mathcal{M}(\tilde{x}) $ì˜ outputì„ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„° ê°’ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”í•œ í›„ $ \sigma(\cdot) $ ì ìš© í›„ ë¹„ì„ í˜• ê²Œì´íŒ…ì„ í†µí•´ ìµœì¢… outputì„ ë§Œë“¦

	 â†’ (ê·¸ë¦¬ê³  ì´ê²Œ ì•„ë§ˆ ëª‡ê°œì˜ layerë¡œ ìŒ“ì¼ ê²ƒì´ê¸° ë•Œë¬¸ì—) ê·¸ë¦¼ì—ì„œ long-term memoryëŠ” fadingì˜ í˜•íƒœë¡œ attention mapì— ë‚˜íƒ€ë‚¨. ë°˜ë©´, ë‹¤ë¥¸ memory moduleì€ preciseí•˜ê²Œ attentionì´ ë“¤ì–´ê°.

<br/>

### 4.3 Memory as a Layer (MAL)

â†’ attentionê³¼ neural memory moduleì´ ë³¸ì¸ë“¤ì´ ì„¤ê³„í•œ ì˜ë„ë¥¼ 100% í™œìš©í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°.

1. $ \tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}] | x $

1. $ y = \mathcal{M}(\tilde{x}) $

1. $ o = \text{SW-Attn}(y) $

<br/>

â†’ attentionì˜ ì¥ì ì„ í™œìš©ëª»í•˜ë‹ˆ attention ë¶€ë¶„ì„ neural memory moduleë¡œ ë°”ê¾¼ LMMë¡œ ë˜ ë‹¤ë¥¸ ì‹¤í—˜ì„ í•´ë´¤ë‹¤ê³  í•¨.

<br/>

## 5. Experiments

<br/>

### 5.1 Experimental Setup

- Models

	- scale: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. (memory module + coreë¥¼ ì˜ë¯¸í•˜ëŠ” ë“¯, MAC, MAG, MALì˜ í¬ê¸°ê°€ ìœ„ì™€ ê°™ë‹¤ê³  í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì•„)

	- training dataset: FineWeb-Edu dataset (i)~(iii)- 15B / (iv) - 30B

- Training

	- max_lengths: 4K tokens.

	- batch_size: 0.5M tokens

<br/>

### 5.2 Results - Language Modeling

â†’ attentionì´ ë“¤ì–´ê°„ ëª¨ë¸: hybrid model â†’ *í‘œê¸°

â†’ attentionì„ ì•ˆì¼ëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <span style='color:yellow_background'>**model**</span>

â†’ attentionì„ í™œìš©í–ˆëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <span style='color:blue_background'>**model**</span>

- Titanì´ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ë‹¤.

- Mamba, Mamba2, and Gated DeltaNetë„ gating mechanismì„ ì“°ì§€ë§Œ ë³¸ì¸ë“¤ì˜ neural & deep memoryê°€ ë” íš¨ìš©ì´ ë†’ë‹¤ê³  í•˜ëŠ”ë° attention ë•Œë¬¸ì— ì˜ë‚˜ì˜¨ê²Œ ì•„ë‹Œê°€?ë¼ëŠ” ë“¦.

	â†’ ê·¸ë˜ì„œ Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention)ë³´ë‹¤ë„ ì„±ëŠ¥ì´ ì¢‹ê¸° ë•Œë¬¸ì— ë³¸ì¸ë“¤ì´ powerful neural memory moduleë¥¼ ì˜ êµ¬ì¶•í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  ì£¼ì¥

- êµ¬ì¡°ìƒ ë©”ëª¨ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ attentioní•˜ê³  updateí•˜ëŠ” MACì´ long sequence dataì— ëŒ€í•œ dependencyê°€ ê°•í•˜ë‹¤ê³  í•¨

<br/>

### 5.2 Results - Needle in a Haystack

-  TTTì— ë¹„í•´ì„œëŠ” momentumê³¼ forgetting mechanismì´ ìˆì–´ì„œ ìœ ì—°í•œ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥

- forgetting mechanismê°€ ìˆëŠ” Mamba2ì— ë¹„í•´ì„œëŠ” deep non-linear êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë³´ë‹¤ ë” íš¨ìš©ì„± ë†’ì€ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥

â‡’ ê°€ì¥ long context handling ëŠ¥ë ¥ì„ ë§ì´ ìš”êµ¬í•˜ëŠ” S-HIAH taskì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.

<br/>

### 5.2 Results - The Effect of Deep Memory

â†’ memory moduleë¡œë§Œ êµ¬ì¡°ë¥¼ ì§œë„ Mambaë³´ë‹¤ long context ëŒ€í•œ pplì´ ë–¨ì–´ì§

â†’ memory module depthë§Œ ì˜¬ë ¤ë„ pplì´ ë–¨ì–´ì§€ë©°, ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í‚¤ìš¸ìˆ˜ë¡ ê¸´ ê¸¸ì´ì— ëŒ€í•œ pplì´ ëœì–´ì§

<br/>

### 5.2 Results - Time Series & DNA Modeling

â†’ (ìœ„) Mamba moduleë¥¼ neural memoryë¡œ ëŒ€ì²´í–ˆë”ë‹ˆ ì˜ ë‚˜ì˜¤ë”ë¼

â†’ (ì•„ë˜) DNA modeling taskì—ì„œë„ ì„±ëŠ¥ ì˜ ë‚˜ì˜¤ë”ë¼

<br/>

## 5. Conclusion

- test timeë•Œ memory moduleì„ read/writeí•˜ëŠ” meta in-context learnerë¥¼ ë§Œë“¤ê³ ì í–ˆë˜ê±° ê°™ìŒ

- attentionì„ í†µí•´ ëª¨ë“  knowledgeë¥¼ ê¸°ì–µí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ pre-trainingë•Œ ì¼ë¶€ knowledgeëŠ” í•™ìŠµì‹œì¼œë†“ê³ , inference/forwardingë˜ë©´ì„œ í•„ìš”í•œ memoryë¥¼ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ê³ ì í•˜ëŠ”ê²Œ ì´ ë…¼ë¬¸ì˜ ìµœì¢… ëª©í‘œê°€ ì•„ë‹ˆì—ˆì„ê¹Œ?ë¼ëŠ” ìƒê°ì´ ë“¦.