---
categories: paper-reviews
date: '2024-03-26 00:00:00'
description: ' ë…¼ë¬¸ ë¦¬ë·° - Search-in-the-Chain: Interactively Enhancing Large Language
  Models with Search for Knowledge-intensive Tasks'
giscus_comments: true
layout: post
related_posts: false
tags: llm paper-review
title: 'Search-in-the-Chain: Interactively Enhancing Large Language Models with Search
  for Knowledge-intensive Tasks'
---

**ë…¼ë¬¸ ì •ë³´**
- **Date**: 2024-03-26
- **Reviewer**: ìƒì—½
- **Property**: Retrieval

LLMì˜ ì—¬ì „í•œ í•œê³„

1. ë‹¤ì–‘í•œ ì§€ì‹ë“¤ì„ í•©ì³ì„œ ì¶”ë¡ í•˜ëŠ” ê²ƒì— ëŒ€í•œ ì–´ë ¤ì›€.

1. memorization of long-tail and real-time knowledge

1. hallucination

1. referenceì—†ì´ contextë§Œìœ¼ë¡œ ìƒì„±í•˜ëŠ” ê²ƒì€ traceabilityê°€ ë–¨ì–´ì§€ë©´ ì‹ ë¢°í•˜ê¸° ì–´ë ¤ì›€.

â†’ Retrieval-augmented methodëŠ” ìœ„ì˜ ë¬¸ì œë“¤ì„ ì™¸ë¶€ ì§€ì‹ê³¼ ëª¨ë¸ì„ ê²°í•¨ì‹œí‚´ìœ¼ë¡œì¨ ì¼ë¶€ í•´ê²°

<br/>

Retrieval-agumneted methodë¥¼ ë„ì…í•  ë•Œ ì—­ì‹œ ë¬¸ì œ ì¡´ì¬

C-1 : IRì„ LLM reasoning processì— ë°”ë¡œ ì´ìš©í•˜ëŠ” ê²ƒì€ LLMì˜ reasoning chainì„ í›¼ì†í•  ë•Œë„ ìˆìŒ. (LLMì´ local sub-question reasonë§Œ í•˜ê¸° ë•Œë¬¸ì—) â† IRì˜ ê²°ê³¼ë¬¼ì´ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë¬¸ì œì  + sub-questionì— ì§‘ì¤‘í•œ reasoning ë°©ì‹

C-2 : IR ê²°ê³¼ vs LLM ìì²´ ì§€ì‹ì˜ ì¶©ëŒ â†’ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ë§Œë“¤ ê°€ëŠ¥ì„±ì´ ì¡´ì¬.

C-3 : reasoning directionì„ ìˆ˜ì •í•  ë°©ë²•ì´ ì—†ìŒ. (ì¡°ê¸ˆ ì–µì§€)

<br/>

Search-in-the-Chain (SearChain) ì œì•ˆ

- IR-LLM interaction roundë¥¼ ì—¬ëŸ¬ ë²ˆ ì§„í–‰

- IR-LLM interaction : reasoning â†’ (verification â†’ completion) ë°˜ë³µ â†’ tracing

- ê·¸ë¦¼ì„ ë³´ì.

1. CoQë¥¼ ë§Œë“¤ê¸° ìœ„í•´ In-context learning ì´ìš© ë³µì¡í•œ ì§ˆë¬¸ì„ í’€ê¸° ìœ„í•œ IR-oriented query í˜•íƒœë¡œ ë¶„í•´í•´ì„œ êµ¬ì„±

	- **reasoning**

		- **ë…¸ë“œ**(node)** :** IR-oriented query

		- **ì •ë‹µ**(answer)** :** ì¿¼ë¦¬ì— ëŒ€í•œ LLMì˜ ë‹µë³€

		- flag : LLMì´ ì¶”ê°€ ì§€ì‹ì„ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ì•„ë‹Œì§€

		â†’ ê¸°ì¡´ì—ëŠ” IRì„ í™œìš©í•  ê²½ìš° ë…¸ë“œë‹¹ í•œ ë²ˆì˜ reasoningë§Œ ê°€ëŠ¥í–ˆì§€ë§Œ CoQëŠ” ìš°ì„  ì™„ì „í•œ ì²´ì¸ì„ ë¨¼ì € ìƒì„± â†’ C-1 í•´ê²°

		1. ê° ë…¸ë“œì— ëŒ€í•´ IRê³¼ ìƒí˜¸ì‘ìš© ì§„í–‰, êµ¬ì²´ì ìœ¼ë¡œ ì•„ë˜ ë‘ ë‹¨ê³„ë¥¼ ì„ íƒì ìœ¼ë¡œ ë°˜ë³µ

	- **verification**

		- LLMì˜ ì •ë‹µê³¼ retrieved ì •ë³´ê°€ ì¼ì¹˜í•˜ì§€ ì•Šê³  IRì´ **high confidence**ë¥¼ ê°€ì§ˆ ë•Œ â†’ IR í”¼ë“œë°±ì„ ì´ìš©í•´ ì •ë‹µì„ ë‹¤ì‹œ ìƒì„±

	- **completion**

		- flag â†’ IR knowledgeë¥¼ ì´ìš©í•´ ì •ë‹µì„ ë‹¤ì‹œ ìƒì„±

	- ìœ„ì˜ IR interactionì„ ë¼ìš´ë“œë§ˆë‹¤ ë°˜ë³µí•´ CoQë¥¼ ìˆ˜ì • â†’ IRì— ì˜í•œ ì˜¤ë¥˜ë¥¼ ê°ì†Œì‹œí‚´ â†’ C-2 í•´ê²°

1. **tracing** : reasoning processë¥¼ ë§Œë“¤ê³  ê° reasoning stepì„ supportí•˜ëŠ” reference ì„ íƒ

	- ì„ íƒëœ referenceë¥¼ ì´ìš©í•´ ë‹µë³€ ìƒì„± â†’ traceability í–¥ìƒ (RAG ë°©ì‹ì€ ê³µí†µì´ë¼ê³  ìƒê°.)

	- IR interactionì€ reasoning pathë¥¼ chainì—ì„œ **node-identify** Depth-first searchë¡œ ë°”ê¿ˆ (Tree-of-Reasoning, ToR) (ë’¤ì— ì„¤ëª…)

	- IRì„ ì´ìš©í•´ nodeë¥¼ ìˆ˜ì • â†’ C-3 í•´ê²° (ë…¼ë¬¸ì—ì„œëŠ” ì—¬ê¸° ì ê¸´ í–ˆëŠ”ë° verificationì— ì¢€ ë” ê°€ê¹Œìš´ë“¯)

		<br/>

**Main contribution**

- We highlight the challenges in introducing IR into LLM from the perspectives of reasoning and knowledge. â†’ ??? LLMì— IRì„ ë„ì…í•  ë•Œ ìˆëŠ” ë¬¸ì œëŠ” ëŒ€ì²´ë¡œ ì•Œë˜ ê±° ì•„ë‹Œê°€?

- SearChain not only improves the knowledge-reasoning ability of LLM but also uses IR to identify and **give the knowledge that LLM really needs**. Besides, SearChain can **mark references to supporting documents** for the knowledge involved in the generated content.

	â†’ reasoning ì‹¤í—˜ + (Retrieval-augmented ë°©ì‹ + confidence ê°œë…)

- Interaction with IR in **SearChain forms a novel reasoning path: node-identify Depth-first Search on a tree**, which enables LLM to dynamically modify the direction of reasoning.

	â†’ ToR ì œì‹œ

- Experiment shows that SearChain **outperforms state-of-the- art baselines** on complex knowledge-intensive tasks including multi- hop Q&A, slot filling,  fact checking and long-form Q&A. â†’ SOTA

	- CoT prompting

		- CoT, self-consistency ë“±

		- ë³µì¡í•œ ì§ˆë¬¸ì„ ì‘ì€ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ëˆˆ í›„ ê° ì§ˆë¬¸ì„ locally í•´ê²°í•˜ëŠ” ê²ƒì—ë§Œ ì§‘ì¤‘

		â†’ Search planì„ í†µí•œ global reasoningì„ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë°”ê¿ˆ.

	<br/>

	- Retrieval-augmented LM

		- ê¸°ì¡´ ë°©ì‹ë“¤ì—ì„œëŠ” one-step reasoningë§Œì„ ì§„í–‰ â†’ global chainì„ ë§Œë“¤ì–´ì„œ logical relationshipì„ ë” ì˜ ì´í•´í•¨.

		- ê¸°ì¡´ IRì€ ì •ë³´ ì œê³µë§Œ í• ë¿ ìˆ˜ì •ì—ëŠ” ê´€ì—¬í•˜ì§€ ì•ŠìŒ. â†’ IR iteractionì€ missing knowledgeë¥¼ ê°€ì§„ í•­ëª©ë“¤ì— ëŒ€í•´ì„œë§Œ ê²°ê³¼ ìˆ˜ì •ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— IRì˜ negative effectë¥¼ ê°ì†Œì‹œí‚¤ë©° ìˆ˜ì •ë„ ê°€ëŠ¥

		<br/>

<br/>

### Chain-of Query Generation

- In-context learning ì´ìš©

- global reasoning chain for complex question

	$$ \operatorname{CoQ}=\left(q_1, a_1\right) \rightarrow\left(q_2, a_2\right) \rightarrow \ldots \rightarrow\left(q_n, a_n\right) $$

	- branch of Tree-of-Reasoning

	- $ q_i $ : IR-oriented query, $ a_i $ : its answer

- Prompt

		- *Construct a global reasoning chain*

		- main task

		- â€œGlobalâ€ : ì™„ì „í•œ reasoning chainì„ ë§Œë“¤ì–´ë¼.

		ğŸ’¡ "Global" means that LLM needs to plan a complete reasoning chain for the complex question, rather than answer the question directly or only solve "local" sub-questions (comparison shown in Figure 2)

	- *generate a query to the search engine based on what you already know at each step of the reasoning chain*

		- ê° ë…¸ë“œì—ì„œ IR-oriented queryì™€ LLMë§Œì˜ ë‹µë³€ì„ ìƒì„±

		- ë§Œì•½ ë‹µì„ ëª¨ë¥´ê² ë‹¤ë©´ [Unsolved Query] flag ì´ìš© (missing knowledge)

	- ì´í›„ ìˆì„ roundì—ì„œë„ CoQ ìƒì„±ì€ ìœ„ì˜ ë°©ë²•ì„ ë”°ë¦„.

<br/>

### Interaction with IR

- CoQì˜ ê° ë…¸ë“œì— ëŒ€í•´ IRì„ ì´ìš©í•´ ë‹¤ìŒì˜ ì‘ì—…ì„ ë°˜ë³µ - verification, completion

- IRì˜ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ CoQì˜ ê²°ê³¼ë¥¼ ìˆ˜ì • â†’ new branch of ToR (Tree of Reasoning)

- Top-1 retreived documentë¥¼ supporting documentë¡œ ì´ìš©

- ë” ì´ìƒ ë…¸ë“œì— ëŒ€í•œ ìˆ˜ì •ì´ í•„ìš”ì—†ì–´ì§€ë©´ ë¼ìš´ë“œ ì¢…ë£Œ

- correct reasoning pathì™€ supporting documentë¥¼ ì´ìš©í•´ ë‹µë³€ ìƒì„±

- Algorithm

	<br/>

**Verification**

- IR ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ CoQ ê° ë…¸ë“œì˜ ì •ë‹µì´ ë§ëŠ”ì§€ í™•ì¸

1. retrieved Top-1 document $ d_i $ â†’ ODQA ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ reader(DPR) ì´ìš©í•´ answer $ g $ ì¶”ì¶œ

	- confidence $ f $ : predicted value that measures whether $ g $ can answer $ q_i $

	$$ \begin{gathered}s=\arg \max \left(\operatorname{softmax}\left(\mathbf{H} \mathbf{w}_s\right)\right), e=\arg \max \left(\operatorname{softmax}\left(\mathbf{H} \mathbf{w}_e\right)\right), \\g=d_i[s: e], f=\mathbf{H}_{[C L S]} \mathbf{w}_f,\left(\mathbf{w}_s, \mathbf{w}_t, \mathbf{w}_f \in \mathbb{R}^E\right),\end{gathered} $$

	$ \mathbf{H} \in \mathbb{R}^{L \times E} $ :  input text â€œ$ [\mathrm{CLS}] q_i[\mathrm{SEP}] d_i $â€ì˜ last hidden state sequence (L : length, E : hidden dimension)

	$ \mathbf{H}_{[\mathrm{CLS}]} $ : last hidden state of $ [\mathrm{CLS}] $ token

1. $ a_i $ì™€ $ g $ ë¹„êµ

	- short-form generation task

		- $ a_i $ ì•ˆì— $ g $ê°€ ìˆëŠ”ì§€ ì—¬ë¶€

	- long-form generation task

		- ROUGE between $ a_i $ and $ d_i $ > threshold $ \alpha $

1. $ a_i $ì™€ $ g $ì˜ ê²°ê³¼ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ  &  $ f $ > $ \theta $ (IRë¡œ ì¸í•œ ì„±ëŠ¥ ê°ì†Œë¥¼ ë§‰ê¸° ìœ„í•œ threshold)

	- prompt

		ğŸ’¡ According to the Reference, the answer for $ q_i $ should be $ g $, you can change your answer and continue constructing the reasoning chain for [Question]: Q. Reference: $ d_i $.

	- $ a_i $ â†’ $ a^{\prime}_{i} $ ë³€ê²½ í›„, ($ q_i $, $ a^{\prime}_{i} $)ë¥¼ root ë…¸ë“œë¡œ í•˜ì—¬ ìƒˆë¡œìš´ CoQ ì§„í–‰

<br/>

**Completion**

- missing knowledgeë¥¼ ê°–ëŠ” ë…¸ë“œì˜ ì •ë‹µì„ ë³´ì¶©

- [Unsolved Query]ì— ëŒ€í•´ ì§„í–‰

- $ f $ê°’ì— ê´€ê³„ì—†ì´ ë‹¤ìŒ prompt ì‹¤í–‰

	ğŸ’¡ According to the Reference, the answer for $ q^{\star}_{i} $ should be $ g^{\star} $, you can give your answer and continue constructing the reasoning chain for [Question]: Q. Reference: $ d^{\star}_{i} $.

- $ a_i $ â†’ $ a^{\star}_{i} $ ë³€ê²½ í›„, ($ q_i $, $ a^{\star}_{i} $)ë¥¼ root ë…¸ë“œë¡œ í•˜ì—¬ ìƒˆë¡œìš´ CoQ ì§„í–‰

<br/>

**Tracing**

- reasoning process ìƒì„± ë° ê° ë…¸ë“œì— ëŒ€í•´ supporting document mark

- prompt

	ğŸ’¡ You can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final Content]. [Query1]: $ q_1 $ [Answer1]: $ a_1 $ ...[Query m]: $ q_m $  [Answer m]: $ a_m $.

<br/>

**Node-identify Depth-first Search**

- SearChainì´ ë§Œë“œëŠ” reasoing path

- Depth first searchì™€ ê°™ì´ ì •ë‹µì´ í•´ê²°ë˜ê±°ë‚˜ unsolvable sub-questionì´ ì¡´ì¬í•˜ë©´ ê³„ì† reasoningí•œë‹¤ëŠ” ì ì—ì„œ ë™ì¼

- â€œnode-identifyâ€ : í•˜ë‚˜ì˜ search directionì´ ëë‚  ë•Œ parent nodeë¡œ ê°€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ node verificationê³¼ completationì„ í†µí•´ ì‹œì‘ nodeë¥¼ ê²°ì •í•œë‹¤ëŠ” ê²ƒì´ ë‹¤ë¦„.

- LLM - IR ìƒí˜¸ì‘ìš©ì„ í†µí•´ ë™ì ìœ¼ë¡œ reasoning directionì„ ê²°ì •

<br/>

### Experimental setup

- Datasets

	- multi-hop question-answering (HotpotQA (HoPo)

	- Musique (MQ)

	- WikiMulti-HopQA (WQA)

	- StrategyQA (SQA)

	- slotfilling (zsRE)

	- T-REx

	- fact checking (FEVER)

	- long-form question-answering (ELI5)

- Evaluation Metric

	- long and free-form : ROUGE-L

	- cover-EM : ì •ë‹µ í¬í•¨ ì—¬ë¶€

- Baselines

	- reasoning : CoT, CoT-SC, Auto-CoT, Recite-and-answer, Least-to-Most

	- IR : Direct, Self-Ask, ToolFormer, React, DSP, Verify-and-Edit

- implementation

	- LLM : gpt-3.5-turbo

	- retrieval model : ColBERTv2 (V100 ì´ìš©)

	- ìµœëŒ€ interaction ë¼ìš´ë“œ ìˆ˜ : 5

	- $ \alpha $ : 0.35, $ \theta $ : 1.5

<br/>

### Main Results

- **Effect of Chain-of-Query**

	- outperform all baselines

	- local perspective : step by stepìœ¼ë¡œ sub-questionì„ ìƒì„±í•˜ê³  ë‹µë³€í•˜ëŠ” ê²ƒ

	- global perspiective : global reasoning chain of sub-questions

- **Effect of interaction with IR**

	- again outperforms all the baselines.

	- CoQëŠ” IR interactionì„ í†µí•´ LLM reasoningê³¼ IR ê²°ê³¼ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ê¸° ë•Œë¬¸ (IR ê²°ê³¼ë¥¼ ì‚¬ìš©í• ì§€ ë§ì§€ thresholdê°€ ìˆìŒ.)

<br/>

### Analysis

**KnowledgeDecoupling**

- 4 multi-hop QA datasetsì˜ knowledge sourceì— ëŒ€í•´ ë¶„ì„

	1. LLMì˜ knowledge

	1. Corrected by IR (Verification) : IRê³¼ ì¼ì¹˜í•˜ì§€ ì•Šì€ ê²°ê³¼ë¡œ ì¸í•´ ìˆ˜ì •ëœ knowledge

	1. Completed by IR (Completion) : LLMì˜ [Unsolved Query]ì— ëŒ€í•´ ìˆ˜ì •ëœ knowledge

				- ëŒ€ë¶€ë¶„ì€ LLM ìì²´ ì§€ì‹ìœ¼ë¡œ í•´ê²°ë˜ì§€ë§Œ IR ê²°ê³¼ì— ì˜í•´ ìˆ˜ì •ë˜ëŠ” ë¶€ë¶„ë„ ìƒê°ë³´ë‹¤ ë§ì´ ì¡´ì¬

		- ë°ì´í„°ì…‹ì˜ íŠ¹ì§•ì— ë”°ë¼ ê·¸ ë¹„ìœ¨ì€ ë§ì´ ë°”ë€ŒëŠ” ê±° ê°™ìŒ.

<br/>

**Positive and Negative Effects of IR on LLM**

- (a) Positive

	- $ \mathbb{S}_{I R} $ : IR ê²°ê³¼ì— ì˜í•´ ìˆ˜ì •, ë³´ì™„ëœ answerë“¤ë§Œ ì¶”ì¶œ

	- w/o IR ($ \mathbb{S}_{I R} $) < w/o IR ($ \mathbb{S} $) : ì‹¤ì œë¡œ LLMì´ ë‹µë³€ì„ í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªìŒ.

	- w IR ($ \mathbb{S}_{I R} $) > w/o IR ($ \mathbb{S} $) > w/o IR ($ \mathbb{S}_{I R} $) : IRì„ í†µí•œ ë‹µë³€ ì„±ëŠ¥ í–¥ìƒ

- (b) Negative

	- LLMì´ ë§ì•˜ì§€ë§Œ IRì´ ë‹¤ë¥¸ ì •ë‹µì„ ì¤€ ë¹„ìœ¨

	- SearChainì—ì„œ confidenceë¥¼ ì‚¬ìš©í•œ í•„í„°ë§ì´ íš¨ê³¼ì ì„.

<br/>

**CoQ vs Baselines in Reasoning**

ì„±ëŠ¥ ì™¸ reasoningì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì¸¡ë©´ ì¶”ê°€ì ìœ¼ë¡œ ì œì‹œ

- reasoning stepì˜ ìˆ˜

	- ë³µì¡í•œ ì¿¼ë¦¬ë¥¼ ê°€ì§€ëŠ” Musique ì´ìš©

	- ì œì•ˆ ëª¨ë¸ì´ reasoning stepì´ ë§ê³  ì •í™•ë„ ë˜í•œ ë†’ë‹¤.

- ì–´ë ¤ìš´ sub-question í•´ê²°ë ¥

		- local perspective (ì£¼ì–´ì§„ ì¿¼ë¦¬ë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì— ì§‘ì¤‘) ë°©ì‹ìœ¼ë¡œëŠ” reasoningì„ ë©ˆì¶”ëŠ” ê²½í–¥ ì´ í¼. (Figure 4)

	- LLM gloobal chain reasoningì„ ìœ ë„í•¨ìœ¼ë¡œì¨ ì´ëŸ° í˜„ìƒì´ ì¤„ì–´ë“¦.

		- ìƒì—½ ìƒê° : globalì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ promptì—ì„œ ì²« ë¬¸ì¥ ë¿ì´ê³  global chainì´ë¼ëŠ” ìš©ì–´ê°€ ìƒì†Œí• í…ë° ì´ê²Œ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ ì´í•´ê°€ ì˜ ì•ˆë¨.

	- table 4ì˜ ê²°ê³¼ ì—­ì‹œ ë” ë§ì€ reasoningì´ ëŠê¸°ì§€ ì•ŠìŒì„ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°

	- more case study

		<br/>

**SearChain vs New Bing in Tracing**

SearChainê³¼ New bing searchë¥¼ ì•„ë˜ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ë¹„êµ

- Scope of Knowledge Coverage (SKC) [0, +]:

	- ë‹µë³€ì—ì„œ documentì— ì˜í•´ support ëœë‹¤ê³  markëœ ì•„ì´í…œì˜ ìˆ˜

	- SearChain (2.882) is better than New Bing (1.143)

- Accuracy of Marking Position (AMP) [0, 1]

	- reference markì˜ ìœ„ì¹˜ì˜ ì •í™•ë„, 3ëª…ì˜ ëŒ€í•™ì›ìƒì´ í‰ê°€

	- SearChain (0.80) is better than New Bing (0.45)

<br/>

**Efficiency Analysis**

$ n $ : LLMì˜ input

$ m $ : LLMì˜ output

$ r $ : IR-LLM interaction ìˆ˜

$ t $ : ì‘ë‹µ ì‹œê°„

<br/>

<br/>

ë‚˜ì˜ ìƒê°

[Unsolved Query]ê°€ ì •ë§ ì˜ ë‹¬ë¦´ê¹Œ? Table 2ì—ì„œ ì¼ë¶€ í™•ì¸ ê°€ëŠ¥

Appendixì™€ ë³¸ë¬¸ì„ ë´¤ì„ ë•Œ thresholdì™€ alphaë¥¼ ì •í•˜ëŠ” ê³¼ì •ì—ì„œ validationì´ ì•„ë‹Œ test setì„ ì´ìš©í–ˆì„ ê°€ëŠ¥ì„±ë„ ìˆëŠ” ê±° ê°™ë‹¤.