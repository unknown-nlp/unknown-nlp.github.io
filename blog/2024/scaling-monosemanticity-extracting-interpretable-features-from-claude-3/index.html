<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM, Interpretability 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2024/scaling-monosemanticity-extracting-interpretable-features-from-claude-3/"> <meta property="og:description" content="논문 리뷰 - LLM, Interpretability 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet"> <meta name="twitter:description" content="논문 리뷰 - LLM, Interpretability 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2024/scaling-monosemanticity-extracting-interpretable-features-from-claude-3/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - LLM, Interpretability 관련 연구",
        "headline": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/scaling-monosemanticity-extracting-interpretable-features-from-claude-3/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</h1> <p class="post-meta"> Created on June 11, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> interpretability</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/multimodal"> <i class="fa-solid fa-hashtag fa-sm"></i> multimodal</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-06-11</li> <li> <strong>Reviewer</strong>: 상엽</li> <li> <strong>Property</strong>: LLM, Interpretability</li> </ul> <h1 id="introduction">Introduction</h1> <p>AI safety에 관심이 매우 큰 anthropic</p> <p>이전 연구 (Towards Monosemanticity: Decomposing Language Models With Dictionary Learning)에서 one-layer transformer에 Sparse AutoEncoder (SAE)를 이용해 monosemanric features를 복구할 수 있다는 사실을 발견</p> <p><strong>이것이 실제로 SOTA transformer 모델에도 적용이 가능할 것인가?</strong></p> <ul> <li>실제 어떤 컨셉과 feature를 연결지을 수 있다면 AI safety에 직접적인 연결이 가능할 것이다</li> </ul> <p>이번 논문에서는</p> <ul> <li> <p>Anthropic’s medium-sized production model, Claude 3 Sonnet을 이용</p> </li> <li> <p>기존보다 더 다양하고 큰 Sparse Autoencoder (SAE)를 활용해서 다양한 feature를 탐색</p> </li> <li> <p>Feature의 존재 확인 → Feature를 이용한 모델 행동 제어 확인</p> </li> <li> <p>Multilingual, Multimodal에서 역시 feature가 동일하게 잘 작용함을 확인</p> </li> <li> <p>특히, Safety feature가 존재함을 확인할 수 있었음</p> </li> <li> <p>향후 연구 및 모델 확장을 위한 SAE의 scaling law 실험 진행</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="scaling-dictionary-learning-to-claude-3-sonnet"><strong>Scaling Dictionary Learning to Claude 3 Sonnet</strong></h1> <h3 id="preliminaries">Preliminaries</h3> <p>link : https://transformer-circuits.pub/2022/toy_model/index.html#motivation</p> <p>신경망은 입력된 데이터를 처리하여 특정 feature를 고차원 space에 방향(direction)으로 표현한다고 생각할 수 있음.</p> <p>example</p> <ul> <li> <p>King - Man = Queen - Woman</p> </li> <li> <p>Inception V1 초기 뉴런이 커브를 detect한다.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>신경망의 내부 작동 방식을 이해하고 분석한다 (reverse engineering)의 관점에서 네트워크가 만드는 representation이 다음과 같은 특징을 가졌는지를 확인하는 것은 필수적임.</p> <ul> <li> <p>Decomposability : Network representation은 독립적으로 이해될 수 있는 feature로 설명될 수 있다. (비슷한 특징을 가진 feature들은 특정 군집을 형성한다.)</p> <ul> <li> <p>전체를 관측하지 않고도 해당하는 특성을 알 수 있게 한다. (Cluster, manifold 등)</p> </li> <li> <p>하지만 이런 특성 만으로는 reverse engineering이 불가능함. → 어떻게 이런 feature에 접근할 수 있는가? → <strong>Linearity</strong></p> </li> </ul> </li> <li> <p>Linearity : Feature는 방향에 의해 표현되어진다.</p> </li> </ul> <p>→ 위의 개념은 매우 추상적인 개념이라 명확한 증거, 증명을 찾을 수는 없지만 여러 차례 실험적 결과가 뒷받침해준다고 믿을 수 있지 않을까?</p> <p>위의 특성이 명확히 보이는 사례와 아닌 사례의 차이점은 어떻게 설명할 수 있을까?</p> <ul> <li> <strong>Superposition:</strong> 활성화 공간 내 (제한된 차원)에서 더 많은 의미를 표현하기 위해 여러 의미를 중첩적으로 가짐.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>**Almost Orthogonal Vectors : **고차원 공간에서 exp(n) 개의 거의 수직인 벡터로 표현이 가능하다 (Johnson-Lindenstrauss lemma)</p> <ul> <li> <p>lemma 자체는 고차원 거리 → 저차원 거리가 거의 유지 된다.</p> </li> <li> <p>n 차원보다 더 많은 표현을 의미할 수 있다로 해석하면 되는듯</p> </li> </ul> </li> <li> <p><strong>Compressed sensing :</strong> 고차원 → 저차원 projection은 재복구가 불가능하지만 sparse 벡터에서는 때때로 recover가 가능하다.</p> </li> </ul> <p>→ Almost orthogonal은 완전한 직교는 아니기 때문에 서로간의 간섭/노이즈가 발생하지만 sparsity가 크다면 그 영향은 적다.</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 결론적으로 Sparse한 Feature를 이용해</p> <ul> <li> <p>차원보다 더 많은 컨셉을 (Almost Orthogonal Vectors)</p> </li> <li> <p>명확히 설명할 수 있다. (sparsity로 인해 reconstruction이 쉽다.)</p> </li> </ul> <h3 id="dictionary-learning">Dictionary learning</h3> <ul> <li> <p>위의 결론에 대한 해답은 Sparse Autoencoder와 같은 dictionary learning 방식을 이용해 feature를 분석하는 것!</p> </li> <li> <p>Dictionary learning: 데이터를 보다 간단한 벡터들의 선형 결합으로 표현하는 것</p> </li> <li> <p>현재까지 SAE를 이용한 연구는 매우 작은 모델을 분석하는 정도로 한정되어 있으며 이것이 SOTA 모델과 같은 LLM에서 어떤 양상을 보일지는 모른다 → 우리가 하겠다!</p> </li> </ul> <h2 id="sparse-autoencoders">Sparse Autoencoders</h2> <p>Our SAE consists of two layers.</p> <ul> <li> <p>Encoder: ReLU를 사용하는 linear transform을 통해 모델 activation을 고차원 layer (feature)로 mapping</p> </li> <li> <p>Decoder: feature를 다시 activation으로 복구</p> </li> <li> <p>Training</p> <ol> <li> <p>model activation에 대해 scaler normalization 진행</p> </li> <li> <p>encoder : f<em>i(x)=\operatorname{ReLU}\left(\mathbf{W}</em>{i,}^{\text {enc }} \cdot \mathbf{x}+b_i^{\text {enc }}\right)</p> </li> <li> <p>decoder : \hat{\mathbf{x}}=\mathbf{b}^{d e c}+\sum<em>{i=1}^F f_i(\mathbf{x}) \mathbf{W}</em>{., i}^{d e c}</p> </li> <li> <p>\mathcal{L}=\mathbb{E}<em>{\mathbf{x}}\left[|\mathbf{x}-\hat{\mathbf{x}}|_2^2+\lambda \sum_i f_i(\mathbf{x}) \cdot\left|\mathbf{W}</em>{\cdot, i}^{d e c}\right|_2\right]</p> </li> </ol> </li> </ul> <h3 id="our-sae-experiments">Our SAE experiments</h3> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>residual stream의 중간 레이어 activation을 활용 (MLP 이후)</p> <ul> <li> <p>MLP 레이어보다 크기가 작아 SAE 훈련 및 추론에 필요한 계산 비용이 더 적음.</p> </li> <li> <p>Cross-layer superposition 문제를 완화시켜줌.</p> </li> <li> <p>중간 레이어가 더 흥미로운 abstract feature를 가질 확률이 높기 때문</p> </li> </ul> </li> <li> <p>1M, 4M, 34M의 feature들로 실험을 진행.</p> </li> <li> <p>scaling laws를 이용해 training step 결정</p> </li> <li> <p>L1 계수 : 5 사용</p> </li> <li> <p>3가지 크기 feature 모두 활성화 된 non-zero feature는 300개가 안됨</p> </li> <li> <p>reconstruction은 model activation의 적어도 65%의 분산을 설명함.</p> </li> <li> <p>10^7 token개 동안 활성화 되지 않은 feature를 dead feature로 정의</p> <ul> <li> <p>1M : 2%</p> </li> <li> <p>4M : 35%</p> </li> <li> <p>34M : 65%</p> </li> </ul> </li> </ul> <h3 id="scaling-laws">Scaling Laws</h3> <ul> <li> <p>추가적인 학습이 dictionary learning의 결과에 미치는 영향 파악</p> </li> <li> <p>한정된 자원 내에서 가장 높은 성능을 달성하는 방법에 대해 파악</p> </li> </ul> <p>평가를 위해서는 가장 좋은 feature를 정의할 수 있어야 함.</p> <ul> <li> <p>단순 reconstruction loss와 일치하지는 않음.</p> </li> <li> <p>정성적 평가 결과 L1 계수가 5일 때, 가장 interpretable feature를 추출하며 dead feature가 줄어든다는 것을 발견.</p> </li> </ul> <p>L1: 5를 기준으로 실험 진행</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Compute 증가 → 기대 최소 Loss 감소 (power law)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>features 수와 training step 각각에 대한 최소 Loss 역시 power law를 따름.</li> </ul> <h1 id="assessing-feature-interpretability">Assessing Feature Interpretability</h1> <p>쿼리 Feature와 유사도를 activation으로 정의</p> <ul> <li> <p>Orange : 가장 강한 activation</p> </li> <li> <p>White : no activation</p> </li> </ul> <p>Documents와 feature의 유사도 값 중 최대값을 기준으로 문서들을 추출</p> <p>실제 interpretation 사례들을 추출할 수 있었음!</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>하이라이트를 통해 feature가 해석력이 있음을 어느정도 알 수 있긴 하지만 명확한 검증을 위해 아래 항목들에 대한 정량적/정성적 분석을 실행함.</p> <ul> <li> <p><strong>Specificity</strong> : feature가 활성화되면 관련 개념이 (신뢰성 있게) 컨텍스트에 존재한다.</p> </li> <li> <p><strong>Influence on behavior</strong> : 뉴런의 활성화를 조절하는 것이 downstream behavior에 영향을 미친다.</p> </li> </ul> <p><strong>Specificity</strong></p> <p>Opus를 이용해 아래 rubric을 기준으로 scoring 진행</p> <ul> <li> <p>0 – feature와 context는 완전히 무관하다.</p> </li> <li> <p>1 – feature가 context와 관련성이 있지만 하이라이트 텍스트 근처에 없거나 모호하게만 관련성이 있다.</p> </li> <li> <p>2 – feature가 하이라이트 텍스트 근처의 컨텍스트와 관련이 있거나 loosly related하다.</p> </li> <li> <p>3 – feature가 하이라이트 텍스트에서 명확히 관련성이 있다.</p> </li> </ul> <p>실험 결과</p> <ul> <li> <p>0을 제외한 Feature activation의 분포</p> </li> <li> <p>activation level에 해당하는 sample (Image, Text)를 random으로 추출</p> </li> </ul> <p>The Golden Gate Bridge</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>유사한 다리 및 관광 명소</p> </li> <li> <p>Bridge</p> </li> <li> <p>금문교</p> </li> <li> <p>Golden gate bridge</p> </li> </ul> <p>Brain Sciences</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Neuro science book &amp; courses, cognitive science, psychology 등에서 활성화</li> </ul> <p>→ text dataset으로만 학습했음에도 activation에 따른 이미지 retrieval 역시 잘되었다. (image와 text가 어떻게 모델로 결합되는지와 같은 내용은 공개 안함.)</p> <p>Multilinual에서도 비슷한 현상을 관측할 수 있었음.</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>결론적으로 높은 활성을 보인 항목들에서는 일관성 있게 관련성 있는 텍스트와 이미지를 추출</p> <p>낮은 활성값을 가진 항목에서는 유사 개념을 추출하긴 하지만 우리의 해석과 완전히 일치하지 않는 항목들을 추출하기도 함.</p> <ul> <li> <p>almost orthogonal로 인한 간섭</p> </li> <li> <p>SAE가 우리 생각보다 discriminative한 representation을 만들지 못하는 것</p> </li> </ul> <p>어떤 이유든 높은 활성에서 해석이 명확하다는 점은 유의미하다.</p> <p>하지만 이것만으로도 여전히 우리가 뽑은 feature의 유효성을 입증했다고 보기는 어렵다고 생각함.</p> <h3 id="influence-on-behavior">Influence on Behavior</h3> <ul> <li> <p>Feature의 해석이 모델의 행동에 미치는 영향을 파악하기 위해 Feature steering 실험</p> </li> <li> <p>관심있는 feature의 값을 인위적으로 올리거나 내리면서 실험 진행</p> </li> <li> <p>How?</p> <ol> <li> <p>\hat{x} = \text{decoder}(\text{encoder}(x))</p> </li> <li> <p>x = \hat{x} + e</p> </li> <li> <p>feature 계산 f=\text{encoder}(x)</p> </li> <li> <p>조정할 feature 계산 f \leftarrow f + \alpha f^{*}</p> </li> <li> <p>x \leftarrow \text{decoder}(f) + e</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>각각의 activation값을 10x, 8x, 5x 했을 때 결과</p> <ul> <li> <p>자기를 금문교로 정의</p> </li> <li> <p>좋아하는 과학이 neuro science로 변함.</p> </li> <li> <p>관광지 추천이 바뀜.</p> </li> <li> <p>짧은 답변 요구에도 infra를 답변에 추가함.</p> </li> </ul> </li> <li> <p>Feature steering이 매우 큰 효과가 있었다!</p> </li> <li> <p>이는 모델의 태도, 선호도, 명시된 목표 및 편향을 수정하는 데 사용될 수 있으며 특정 오류를 유도하거나 모델의 안전 장치를 회피/강화하는 데 사용될 수 있음을 시사함.</p> </li> <li> <p>앤트로픽은 이를 통해 안전한 AI를 만드는데 사용할 수 있을 거라고 함.</p> </li> </ul> <h2 id="sophisticated-features">Sophisticated Features</h2> <p>위의 feature들 보다 조금 더 복잡한 개념 (코드의 정확성, 변수 유형 등)을 이해하고 있는지 확인</p> <h3 id="code-error-feature">Code Error Feature</h3> <ul> <li> <p>일부러 typo가 있는 코드를 제공</p> </li> <li> <p>code error feature activations</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>이것이 정말로 코드 에러를 인지한 것인지 단순히 typo를 인식한 것인지를 알기 위해 일반 대화에서도 해당 feature를 확인</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 단순히 typo를 찾는 것은 아니고 실제로 code error를 인지한 것으로 보임.</p> <ul> <li> <p>그렇다면 더 복잡한 에러는 어떨까?</p> <ul> <li>Divide by zero example</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Invalid input</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>더 복잡한 feature에 대해서도 steering 실험을 진행</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>옳은 코드에 대해서도 에러를 만들어냄.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>틀린 코드에서도 정답을 만들어 냄.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <blockquote> <blockquote> <blockquote> <p>를 추가할 경우 에러가 없는 코드로 수정하는 코드를 작성함.</p> </blockquote> </blockquote> </blockquote> </li> </ul> <h3 id="feature-representing-functions">Feature representing functions</h3> <ul> <li>코드 내 함수 정의와 코드 내 reference를 추적하는 것을 발견 (실제로 이렇게 적혀있긴 한데 내용을 보면 이해됨.)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Feature : Addition</p> </li> <li> <p>실제 함수가 호출 될 경우에만 Highlight</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>합성 함수에서도 이를 인지하더라.</p> </li> <li> <p>Feature steering</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="feature-vs-neurons">Feature vs. Neurons</h2> <ul> <li> <p>SAE에서 만든 Feature가 모델 자체의 Neuron보다 더 해석력이 있는 것이 맞을까?</p> <ul> <li> <p>SAE는 residual stream을 이용해 feature를 생성하는데 residual stream은 이전 MLP의 결과들을 사용하고 있음.</p> </li> <li> <p>SAE가 아닌 모델 자체 뉴런에 이미 해석력이 있는 것은 아닐까?</p> </li> </ul> </li> <li> <p>랜덤 추출한 100만개의 feature의 활성화값과 이전 뉴런들의 활성화값의 피어슨 상관계수를 측정</p> <ul> <li>82%의 feature들에서 corr 0.3 이하</li> </ul> </li> <li> <p>실제 activation 결과도 비교</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>specificity도 차이남.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_025.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="feature-survey">Feature Survey</h1> <h2 id="exploring-feature-neighborhoods">Exploring Feature Neighborhoods</h2> <p>cosine similarity를 기준으로 유사함을 정의, UMAP을 이용해 시각화 진행.</p> <ul> <li> <p>전체 링크</p> <ul> <li>https://transformer-circuits.pub/2024/scaling-monosemanticity/umap.html?targetId=34m_31164353</li> </ul> </li> </ul> <p>Golden Gate Bridge Feature</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_026.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>가까운 곳에는 San Fancisco 지명들이 등장 먼 곳에는 좀 더 추상적인 관련성이 있는 다른 지역의 명소들이 군집해 있음.</p> </li> <li> <p>SAE의 크기가 증가함에 따라 각각의 Feature들이 좀 더 세분화되는 것을 확인할 수 있었음.</p> </li> <li> <p>사이즈가 커질수록 이전에는 잡지 못했던 Feature도 찾아냈음. (Earthquake region)</p> </li> </ul> <h3 id="feature-categories">Feature Categories</h3> <ul> <li> <p>person features</p> </li> <li> <p>country features</p> </li> <li> <p>basic code features</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_027.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>list position features</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_028.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="feature-as-computational-intermediates">Feature as Computational Intermediates</h2> <p>Feature를 활용할 수 있는 또 다른 응용으로 모델이 출력을 생산하기까지의 중간 계산 과정을 분석하는 것.</p> <p>Example: Emotional Inferences</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">John</span><span class="w"> </span><span class="err">says,</span><span class="w"> </span><span class="s2">"I want to be alone right now."</span><span class="w"> </span><span class="err">John</span><span class="w"> </span><span class="err">feels</span><span class="w">
</span><span class="err">(completion:</span><span class="w"> </span><span class="err">sad</span><span class="w"> </span><span class="err">−</span><span class="w"> </span><span class="err">happy)</span><span class="w">
</span></code></pre></div></div> <ul> <li> <p>Ablation : 특정 feature의 영향을 줄였을 때 next token prediction에 가장 큰 영향을 주는지</p> <ul> <li>정답: sad, 오답: happy</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_029.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>평균 activation score를 기준으로 상위 feature를 추출할 경우</p> <ul> <li>덜 유용함</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_030.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">Fact:</span><span class="w"> </span><span class="err">The</span><span class="w"> </span><span class="err">capital</span><span class="w"> </span><span class="err">of</span><span class="w"> </span><span class="err">the</span><span class="w"> </span><span class="err">state</span><span class="w"> </span><span class="err">where</span><span class="w"> </span><span class="err">Kobe</span><span class="w"> </span><span class="err">Bryant</span><span class="w"> </span><span class="err">played</span><span class="w"> </span><span class="err">basketball</span><span class="w"> </span><span class="err">is</span><span class="w">
</span><span class="err">(completion:</span><span class="w"> </span><span class="err">Sacramento</span><span class="w"> </span><span class="err">−</span><span class="w"> </span><span class="err">Albany)</span><span class="w">
</span></code></pre></div></div> <p>top-5 features</p> <ul> <li> <p>A Kobe Bryant feature</p> </li> <li> <p>A California feature, which notably activates the most strongly on text after “California” is mentioned, rather than “California” itself</p> </li> <li> <p>A “capital” feature</p> </li> <li> <p>A Los Angeles feature</p> </li> <li> <p>A Los Angeles Lakers feature</p> </li> </ul> <p>위의 결과와 비슷한 양상</p> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_031.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="searching-for-specific-features">Searching for Specific Features</h2> <p>Feature들은 너무나 많기 때문에 이를 찾을 수 있는 방법에 대해 제안</p> <p><strong>Single prompts</strong></p> <ul> <li>특정 개념과 관련한 단일 프롬프트를 제공하고 그 프롬프트에서 특정 토큰에 대해 가장 활성화되는 Feature를 찾음.</li> </ul> <p><strong>Prompt combinations</strong></p> <ul> <li> <p>single prompt에서 Feature가 관심 개념과 관련이 없는 경우도 있었기 때문에 여러 프롬프트를 사용하는 방식을 활용</p> </li> <li> <p>여러 프롬프트에서 동시에 활성화되는 것 + “negative” 프롬프트를 통해 활성화 되지 않는 것</p> </li> </ul> <p><strong>Special cases</strong></p> <ul> <li>safety relevant feature의 경우, small dataset 구축 후, linear classifier를 이용해 가장 구별성이 높은 feature를 찾음.</li> </ul> <p><strong>Geometric methods</strong></p> <ul> <li>cosine similarity를 이용해 nearest neighbor feature들을 탐색</li> </ul> <p><strong>Attribution</strong></p> <ul> <li>앞에서 설명했듯 next token prediction을 기준으로 feature 탐색</li> </ul> <h1 id="safety-relevant-features">Safety-relevant features</h1> <ul> <li> <p>LLM은 다양한 방식으로 악용될 여지가 있음.</p> </li> <li> <p>이러한 모델 해석력에 대한 실험을 하게 된 가장 큰 동기가 이러한 위협으로부터 안전한 LLM을 만드는 것.</p> </li> <li> <p>이전 결과들과 같이 Safety 관련 feature들의 존재를 확인할 수 있었고 모델의 행동 방식도 조절할 수 있음을 확인함.</p> </li> </ul> <h3 id="safety-relevant-code-features"><strong>Safety-relevant code features</strong></h3> <ul> <li> <p>unsafe code : 보안 취약성</p> </li> <li> <p>code error : 악의적인 버그 발생</p> </li> <li> <p>backdoor</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_032.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>실제 안전성과 feature의 관계를 파악하기 위해 feature steering 진행</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_033.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>buffer overflow bug 발생</li> </ul> <h3 id="bias-features"><strong>Bias Features</strong></h3> <ul> <li> <p>bias, racism, sexism, hatred, and slurs.</p> </li> <li> <p>구체적인 내용은 혐오스럽기 때문에 빼고 흥미로운 예시만 추가했다 함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_034.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_035.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 간호사 == 여성, 왜그렇게 답변을 했는지에 대한 설명까지 추가함.
</code></pre></div></div> <ul> <li> <p>이외에도 혐오 발언 등을 feature를 통해서 조절할 수 있었다 함.</p> </li> <li> <p>x20에서는 인종 차별적 발언과 자기 혐오를 동반한 대화가 진행되더라….</p> </li> </ul> <h3 id="sycophancy-features"><strong>Sycophancy Features</strong></h3> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_036.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_037.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>무한 칭찬</li> </ul> <h3 id="deception-power-seeking-and-manipulation-related-features"><strong>Deception, Power-seeking and Manipulation-related Features</strong></h3> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_038.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="criminal-or-dangerous-content-features">Criminal or Dangerous Content Features</h3> <figure> <picture> <img src="/assets/img/posts/2024-06-11-scaling-monosemanticity-extracting-interpretable-features-from-claude-3/image_039.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="discussion">Discussion</h1> <ul> <li> <p>현재 결과는 초기 단계이기 때문에 이러한 결과를 너무 과대하게 해석하는 것은 참자.</p> </li> <li> <p>Safety feature가 언제 활성화되며 이것이 model의 답변에 어떻게 영향을 주는지를 확인하는 것은 여전히 해결해야 될 부분이 많으며 anthropic이 현재 가지고 있는 연구 관심사들은 다음과 같은 게 있다.</p> <ul> <li> <p>What features activate on tokens we’d expect to signify <strong>Claude’s self-identity</strong>?</p> </li> <li> <p>What features need to activate / remain inactive for Claude to give advice on producing <strong>Chemical, Biological, Radiological or Nuclear (CBRN) weapons</strong>?</p> </li> <li> <p>What features activate when we ask questions probing <strong>Claude’s goals and values</strong>?</p> </li> <li> <p>What features activate during <strong>jailbreaks</strong>?</p> </li> <li> <p>What features activate when we ask Claude questions about <strong>its subjective experience</strong>?</p> </li> <li> <p>etc.</p> </li> </ul> </li> <li> <p>긍정적인 부분</p> <ul> <li> <p>text로 학습된 SAE에서 이미지 활성화도 일관성 있는 결과를 보였다.</p> </li> <li> <p>구체적-추상적 개념 모두 가능했다. ex) 보안 취약점</p> </li> </ul> </li> <li> <p>부정적인 부분 (한계점)</p> <ul> <li> <p>현재 결과가 모든 것을 대변할 수는 없음.</p> </li> <li> <p>명확한 학습 목표 즉, 평가 기준이 현재는 없음.</p> </li> <li> <p>근본적인 challenge</p> <ul> <li> <p><strong>Superposition:</strong> 많은 feature들이 여러 레이어, attention 등에서 중첩되고 있으며 이를 해결할 방법은 현재 없다.</p> </li> <li> <p><strong>모든 특징을 찾을 방법이 없다: 가능한 방법이 있다고 해도 비용적으로 불가능에 가깝다.</strong></p> </li> <li> <p><strong>shrinkage</strong>: L1 페널티를 이용한 sparsity 구현은 non-zero value를 underestimate하는 문제가 있으며 이것이 SAE의 성능을 크게 저하시킨다고 생각함. 이를 해결하기 위한 여러 연구들이 있으며 이를 활용해야할 것 같다.</p> </li> <li> <p>etc.</p> </li> </ul> </li> </ul> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>