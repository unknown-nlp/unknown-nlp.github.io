<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the performance gap between online and offline alignment algorithms | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content=" 논문 리뷰 - Understanding the performance gap between online and offline alignment algorithms"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/understanding-the-performance-gap-between-online-and-offline-alignment-algorithms/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding the performance gap between online and offline alignment algorithms</h1> <p class="post-meta"> Created on May 27, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-05-27</li> <li> <strong>Reviewer</strong>: 전민진</li> <li> <strong>Property</strong>: RLHF</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>RLHF은 요즘 LLM alignment를 위한 근본 방법론으로 자리 잡음</p> <ul> <li>이에 따라, PPO, DPO, TRPO, self-judge등 엄청나게 많은 variation 방법론이 쏟아지고 있음</li> </ul> </li> <li> <p>RLHF를 크게 online, offline alignmnt algorithm으로 나눠서, 둘의 성능 차이가 어떠한 원인에서 발생하는지 분석해보고자 함</p> </li> </ul> <h2 id="preliminary">Preliminary</h2> <ul> <li> <p><strong>on-policy, off-policy, online, offline</strong></p> <ul> <li> <p>on-policy</p> <ul> <li> <p>behavior policy(데이터 샘플링할 때 사용한 policy) == target policy(학습할 policy)</p> <p>ex) 직접 롤을 하면서 잘하는 방법을 터득함 ⇒ on-policy</p> <p>ex) REINFORCE algorithm, PPO</p> </li> </ul> </li> <li> <p>off-policy</p> <ul> <li> <p>behavior policy(데이터 샘플링할 때 사용한 policy) != target policy(학습할 policy)</p> </li> <li> <p>과거의 자기 모델(파라미터 업데이트하기 전)이 생성한 데이터여도 off-policy</p> <p>ex) 유튜브에서 페이커가 롤하는 영상을 보고 잘하는 방법을 터득함 ⇒ off-policy</p> <p>ex) Q-learning</p> </li> </ul> </li> <li> <p>online</p> <ul> <li> <p>agent가 직접 환경과 상호작용</p> </li> <li> <p>policy를 업데이트할 때 지속적으로 환경과 상호작용하면서 샘플을 수집하는 상황을 가정</p> </li> <li> <p>offline과의 가장 큰 차이가 policy optimization하는 동안 데이터를 샘플링하는가</p> </li> <li> <p>online+on-policy(PPO, figure-a), online+off-policy(figure-b)알고리즘 모두 존재</p> </li> </ul> </li> <li> <p>offline(figure-c)</p> <ul> <li> <p>agent가 직접 환경과 상호작용하지 않음</p> </li> <li> <p>환경과 상호작용 없이 고정된 데이터로만 학습하는 경우</p> </li> <li> <p>즉, policy optimization을 수행하는 동안 추가적인 샘플링 과정이나 데이터 수집 과정이 없음 ⇒ 주어진 데이터만으로 학습</p> <p>ex) DPO</p> </li> </ul> <h2 id="introduction">Introduction</h2> </li> </ul> </li> </ul> <blockquote> <p><strong><em>Is online RL necessary for AI alignment?</em></strong></p> </blockquote> <ul> <li> <p>offline RL(DPO)같은 방법론의 경우 online RLHF에 비해서 훨씬 간단하고 연산량도 적음</p> </li> <li> <p>과연 offline RL로도 충분히 alingment가 가능한지를 분석</p> </li> <li> <p>online vs offline algorithms</p> <ul> <li> <p>online이 연산량이 훨신 큼</p> <ul> <li>due to sampling and training an extra reward model</li> </ul> </li> <li> <p>budget(KLD with SFT model)을 기준으로 각 방법론이 어느 정도의 성능을 내는지를 비교</p> </li> <li> <p>일반적인 RL setting에서는 당연히 online이 우세하나, RLHF에서 reward에 bottle neck이 발생하는걸 고려하면 애매</p> <ul> <li> <p>LLM을RL로 학습할 때는 reward modeling을 human preference data로 한 후에, policy만을 업데이트</p> <p>⇒ policy가 업데이트 되면서 분포가 reward model과 상이해질 수 있음</p> </li> </ul> </li> </ul> </li> </ul> <h2 id="comparing-online-and-offline-performance-under-goodharts-law">Comparing online and offline performance under Goodhart’s law</h2> <ul> <li> <p>우선 online과 offline alignment method가 성능 차이가 나는지를 확인</p> </li> <li> <p>면밀한 비교를 위해 둘다 IPO loss 사용, 차이는 $ \mu = \pi_\theta $(online), $ \mu = D $(offline)</p> </li> <li> <p>Online achieves better budget and performance trade-off than offline</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>- 실험 세팅

		- golden policy : 기존의 human preference데이터로 golden preference model(reward model)을 학습, 이를 바탕으로 policy를 학습한 모델(T5X-XXL, 11B 사용)

- 다른 모델은 모두 golden preference model로 학습데이터를 다시 라벨링한 데이터셋 $ D_{golden} $으로 학습(T5X-Large, 770M)

	- $ D_{golden} $은 기존 데이터셋에 비해 다소 너프된, 노이즈가 있는 데이터셋이라고 보면 됨

- 각각의 점은 학습된 policy 성능을 의미(미리 정해놓은 hyper-parameter + 학습 정도)
</code></pre></div> </div> <ul> <li> <p>실험 결과, <strong>online algorithm이 offline에 비해서 더 높은 trade-off performance를 보임</strong></p> </li> <li> <p>online, offline 모두 overoptimization 현상이 발생하는 것을 알 수 있음</p> <ul> <li>두 가지 모델 모두 특정 지점에서 peak를 찍고 다시 성능이 하락함</li> </ul> </li> </ul> </li> </ul> <h2 id="hypotheses-for-the-performance-discrepancy">Hypotheses for the performance discrepancy</h2> <p>같은 데이터셋으로 학습하는데 왜 on, offline 모델이 성능 차이가 나는지 확인하기 위해 가설 설정</p> <ul> <li> <p>Hypothesis 1 : Data coverage</p> <ul> <li> <p><em>online algorithm are better because the dataset coverage is more diverse than the offline dataset</em></p> <ul> <li> <p>offline은 이미 구축된 데이터셋으로만 학습되는 대신, online의 경우 현재 모델로 response sampling, reward model로 분류한 뒤에 학습</p> </li> <li> <p>당연히 데이터 다양성은 online algorithm이 압살</p> <ul> <li>online버전은 학습하면서 모델이 좀 더 aligment된 답변을 내놓을 것이기 때문</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Hypothesis 2 : sub-optimal offlifne dataset</p> <ul> <li> <p><em>offline algorithms are at a disadvantage because the initial preference dataset is generated by a sub-optimal policy</em></p> <ul> <li> <p>preference dataset을 구축할 때 사용된 모델이 sub-optimal하기 때문에, 이걸로만 학습하는 offline 방식은 더 불리할 것</p> </li> <li> <p>offline 방식은 사실상 SFT의 contrastive version으로 볼 수 있음</p> <ul> <li>즉, 데이터의 품질에 더 영향을 받을 수 밖에 없다</li> </ul> </li> <li> <p>Hypothesis 3 : Better classification better performance</p> </li> </ul> </li> <li> <p><em>Offline algorithms typically train policies as classifiers</em></p> </li> <li> <p>_However, as classifiers they might not be as accurate as proxy preference models (due to effectively different ways to parameterize the classification). _</p> </li> <li> <p><em>If the accuracy improves, the performance will improve too</em></p> <ul> <li> <p>offline방식은 사실 policy를 preference classifier로 학습하는 것과 같음</p> </li> <li> <p>만약, offline으로 학습한 policy의 classifier성능이 떨어진다면, 당연히 policy 자체의 성능도 향상될 수 없을 것</p> <ul> <li>이런 상황이라면, classifier의 accuracy를 올렸을 때, policy 자체의 성능도 올라야 함</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Hypothesis 4 : Non-contrastive loss function</p> <ul> <li> <p><em>How much of the performance gap is attributed to the loss function being contrastive, rather than samples being offline?</em></p> <ul> <li> <p>둘의 성능 차이가 sampling아니라 loss function이 contrastive에서 발생하는건지 확인</p> <ul> <li> <p>CL의 경우 hard negative와 같은 데이터가 중요한데, 그러면 당연히 계속 데이터를 sampling하는 online방식이 효과적일 것</p> </li> <li> <p>SFT로 loss식을 바꾸면 on,off 성능차가 줄지 않을까?라는 의문에서 시작된 가설</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Hypothesis 5 : Scaling policy is all you need</p> <ul> <li> <p><em>Scaling policy size upwards is all you need to bridge the gap between online and offline algorithms</em></p> <ul> <li>모델 사이즈를 키우면 on,off 성능 차가 줄지 않을까?</li> </ul> </li> </ul> </li> </ul> <p>*<strong>* 실험 세팅</strong></p> <ul> <li> <p>controlled setting to study KL vs. performance trade-off</p> <ul> <li> <p>한정된 세팅에서 실험하기 위해 $ D_{golden} $을 학습 데이터셋으로 사용</p> </li> <li> <p>scaling experiment을 제외하고는 policy와 proxy preference model로 Large T5X(with 770MM params)을 사용</p> </li> <li> <p>learned policy와 SFT policy의 거리를 측정하는 메트릭으로 KLD(sft,learned policy)를 사용</p> </li> </ul> </li> <li> <p>supervised fine-tuning</p> <ul> <li> <p>SFT시, (x,y_w), (x,y_l)을 모두 데이터셋으로 사용</p> <ul> <li> <p>모델의 퀄리티를 향상시키는걸 목표가 아니라 RLHF의 시작을 보장하기 위한 용도</p> </li> <li> <p>현실적인 상황을 가정</p> </li> </ul> </li> </ul> </li> <li> <p>evaluation</p> <ul> <li> <p>2048에 대한 prompt에 대해 fixed policy baseline에 대한 win rate를 평가지표로 사용</p> </li> <li> <p>fixed policy는 golden preference model를 바탕으로 online algorithm으로 학습한 모델</p> </li> <li> <p>win rate는 golden preference model이 결정</p> </li> </ul> </li> <li> <p>hyper-parameter</p> <ul> <li> <p>baseline</p> <ul> <li>lr : 1e-5, beta : 0.1, gradient step : 4k</li> </ul> </li> <li> <p>offline algorithm</p> <ul> <li>lr : (3e-6, 1e-5, 3e-5), beta : (0.1, 0.5, 1), training step : (4k, 20k steps)</li> </ul> </li> </ul> </li> </ul> <h2 id="investigating-the-hypotheses">Investigating the hypotheses</h2> <h3 id="hypothesis-1--data-coverage">Hypothesis 1 : Data coverage</h3> <p>: on, off의 성능 차는 데이터의 다양성에서 기인할 것</p> <p>⇒ online 학습에서 사용되는 데이터셋을 shuffle, 이를 바탕으로 offline 학습을 해보자</p> <p>(shuffle안하면 online과 똑같음)</p> <ul> <li> <p>offline with d_online-shufflfe이 offline에 비해 근소한 성능 향상을 보였으나, 기존과 큰 차이 없었음</p> <ul> <li> <p>그나마 Chat arena에서 준수한 성능을 보임 → data coverage가 충분하면 data order는 크게 중요하지 않다..(고 하기엔 사실 online dataset을 활용하는거 자체가 현실성이 없음 ㅎㅎ…)</p> </li> <li> <p>shuffle을 덜하면 성능이 향상됨</p> <p>→ (민진생각) 모델의 상태에 알맞는 학습 데이터를 제공하는 것이 젤 중요한거 같음</p> <ul> <li>결론 : data coverage때문은 아님</li> </ul> </li> </ul> </li> </ul> <h3 id="hypothesis-2--sub-optimal-offline-dataset">Hypothesis 2 : Sub-optimal offline dataset</h3> <p>: offline dataset이 sub-optimal해서 offline 성능이 낮은 것이다</p> <ul> <li> <p>red dot : online policy로 4k step만큼 학습한 모델로 response sampling, golden preference로 다시 라벨링한 데이터로 학습</p> <ul> <li>기존의 golden dataset에 비해 좀 더 tricky한 데이터라고 볼 수 있음</li> </ul> </li> <li> <p>실험 결과, 성능에 도움 안됨</p> </li> </ul> <h3 id="hypothesis-3--better-classification-better-performance">Hypothesis 3 : Better classification, better performance</h3> <p>: preference classification을 잘하면, policy의 성능이 높을 것이다</p> <p>→ 1) proxy preference model이 policy를 classifier로 쓰는 것보다 높은 classification 성능을 보일 것</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2) online과 offline의 성능 차이는 이러한 classification accuracy의 차이에서 기인했을 수도 있다
</code></pre></div></div> <p>** policy는 preference classifier라고 볼 수 있음</p> <p>**preference model은 policy를 classifier로 쓰는거보다 더 expressive version이라고 볼 수 있음</p> <blockquote> <blockquote> <p>왜지??? 그냥 likelihood가 아니라 score를 학습하도록 해서? 잘 모르겠다…</p> </blockquote> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>각 response r(x,y)마다 single scalar를 부여
</code></pre></div></div> <ul> <li> <p>실험 세팅</p> <ul> <li> <p>proxy preference model : online에서 사용되는 reward model</p> </li> <li> <p>online@4k, offline@4k : policy를 classifier로 사용한 경우</p> </li> <li> <p>각 학습 스텝에서 나오는 online dataset으로 classifier의 성능을 측정</p> </li> </ul> </li> <li> <p>실험 결과, 학습할수록 on-policy dataset이 기존의 preference dataset과 멀어짐</p> <p>→ proxy preference model 성능이 하락</p> <p>→ 맨 처음 실험 장표에서 봤던 online algorithms의 over-optimization 문제를 부분적으로 설명</p> </li> <li> <p>classifier 성능에 따른 policy자체의 성능(win rate)에 관한 장표</p> </li> <li> <p>실험 결과, classifier의 성능과 policy 자체의 성능은 큰 연관성이 없었다..</p> </li> <li> <p>각 방법론의 학습 정도에 따른 classifier 성능과, D_golden에서 y_w의 relative log probs를 측정</p> <ul> <li> <p>log probs of the winning reponses from the pairwise dataset, relative to the SFT policy</p> </li> <li> <table> <tbody> <tr> <td>$ \mathbb{E}<em>{(x,y_w,y_l)\sim D</em>{golden}}[log\pi_{\theta}(y_w</td> <td>x)-log\pi_{sft}(y_w</td> <td>x)] $</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>(figure 8, top row) online의 classification accuracy가 낮은 것을 확인할 수 있음</p> <ul> <li> <p>online의 경우 계속 data distribution이 이동</p> <p>→ 고정된 분포($ D_{golden} $)에 대한 classification은 낮지만 generative는 상대적으로 잘됨</p> </li> </ul> </li> <li> <p>(figure 8, bottom row) offline방식의 경우 winning response의 logit을 높이는 방식이 아니라 둘다 logit을 낮추되, losing response의 logit을 훨씬 크게 낮추는 방식으로 학습</p> <ul> <li> <p>물론 likelihood가 낮아진다고 해서, offline algorithm이 학습에 실패했다는 것은 아님</p> <ul> <li> <p>단지 offline optimization은 offline dataset에서 샘플링 된 winning response쪽으로 더 확률이 커지도록 확률 분포를 움직이는 게 아니라는 뜻</p> <p>→ 우리가 일반적으로 생각하는 것보다 더 간접적으로 optimize..</p> </li> </ul> </li> <li> <p>online 방식의 경우 현재 정책에서 발생할 가능성이 높은 응답만 관찰, offline과 다른 최적화 과정을 거친다는 것을 알 수 있음</p> </li> </ul> </li> </ul> <h3 id="hypothesis-4--non-contrastive-loss-function">Hypothesis 4 : Non-contrastive loss function</h3> <p>: off가 on보다 성능이 낮은 이유는 loss function이 contrastive 구조이기 때문</p> <p>→ 데이터의 품질에 더 많은 영향을 받음</p> <p>⇒ loss fuction을 SFT느낌으로 바꿔서 실험해보자</p> <ul> <li> <p>Bo2에서도 on,offline의 성능 차이는 비슷</p> <ul> <li>다만 Bo2에선 data coverage를 높이면 성능이 향상됨</li> </ul> </li> <li> <p>chat arena sxs에서는 Bo2를 사용한 on, off성능이 유사</p> <p>→ winning response에 대한 SFT만으로도 성능을 충분히 낼 수 있음</p> </li> </ul> <h3 id="hypothesis-5--non-contrastive-loss-function">Hypothesis 5 : Non-contrastive loss function</h3> <p>: policy model을 scaling up → 3B, 11B로도 학습 (자원때문에 batch size 낮춤)</p> <ul> <li> <p>scaling up하면 model의 peak성능이 높아지긴 함</p> </li> <li> <p>scaling up해서 실험한 결과, 모델이 작을 때와 유사하게 overoptimization문제를 발견</p> </li> <li> <p>모델 크기를 키웠더니 offine with D_online-shuffle 성능이 크게 향상되긴 함</p> <ul> <li>모델이 커지면 on,off 성능 차이를 data coverage로 설명할 수도 있다!</li> </ul> </li> </ul> <h2 id="making-the-dataset-more-on-policy-improves-offline-learning">Making the dataset more on-policy improves offline learning</h2> <p>어떻게 데이터셋을 구축해야 offline learning의 성능을 향상시킬 수 있을까? 에 대한 ablation study</p> <ul> <li> <dl> <dt>3가지 버전의 데이터셋을 구축</dt> <dd> <p>각각의 데이터셋은 D_golden에서 prompt를 샘플링, 버전에 맞는 모델로 response sampling, golden preference model로 라벨링</p> </dd> <dd> <p>구축한 후 D_golden과 비교</p> </dd> </dl> <ul> <li> <p>D_sft vs 800 : Two sides of the responses are generated by the SFT policy and the online algorithms’ learned policy at 800 step</p> </li> <li> <p>D_800 vs 4k : Two sides of the responses are generated by the online algorithms’ learned policy at 800 step and 4k step</p> </li> <li> <p>D_4k vs 4k : Two sides of the responses are both generated by the online policy at 4k step</p> </li> </ul> </li> <li> <p>실험 결과, 학습데이터가 어느정도 SFT와 분포가 유사해야 잘 작동</p> <ul> <li>분포가 offline초기 단계와 비슷해야 좀 더 on-policy와 유사하게 작동하기 때문으로 추정</li> </ul> </li> <li>응답 간의 퀄 차이만으로는 성능이 향상되지 않았음</li> </ul> <h2 id="summary">Summary</h2> <ul> <li> <p>Online과 offline의 성능을 budget 측면에서 비교했을 때, online이 성능이 우월</p> <ul> <li>on-policy data generation이 학습 효율 향상의 핵심인 것으로 추정됨</li> </ul> </li> <li> <p>요즘 하도 다양한 RL방법론이 등장하고 있어서, 이들 사이에 차이가 발생해 성능이 차이나는지 분석하는 것도.. 좋을듯</p> </li> </ul> <p><br></p> <p><br></p> <p><br></p> <p><br></p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>