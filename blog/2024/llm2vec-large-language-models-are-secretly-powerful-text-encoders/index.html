<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content=" ë…¼ë¬¸ ë¦¬ë·° - LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/llm2vec-large-language-models-are-secretly-powerful-text-encoders/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</h1> <p class="post-meta"> Created on September 02, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> Â  Â· Â  <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> Â  <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a> Â  Â· Â  <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2024-09-02</li> <li> <strong>Reviewer</strong>: ê¹€ì¬í¬</li> <li> <strong>Property</strong>: Embeddings, LLM</li> </ul> <hr> <hr> <h2 id="1-intro">1. Intro</h2> <p>ğŸ’¡ Scalingì„ í†µí•´ ì¢‹ì€ ëŠ¥ë ¥ì„ ë³´ìœ í•œ Decoder ëª¨ë¸ì„ Encoderë¡œ <span style="color:red"><strong>ê°„ë‹¨íˆ</strong></span> Adaptation í•  ìˆ˜ ìˆë‹¤!</p> <ul> <li> <p>BERTì™€ GPT ì´í›„ ì´ì–´ì§„ Encoder vs Decoder ëŒ€ì „</p> <ul> <li> <p>BERTëŠ” íŒ¨ë°°í•˜ì˜€ë‹¤â€¦ â†’ Encoder modelì˜ ê²½ìš° scalingì„ í†µí•œ ì„±ëŠ¥ ê°œì„  ë° í™œìš©ì²˜ê°€ ë¶„ëª…í•˜ì§€ ì•ŠìŒ</p> </li> <li> <p>Decoder-only: <span style="color:blue">scaling</span>ì„ í†µí•œ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ í™•ì‹¤</p> <p>â‡’ ì„±ëŠ¥ ê°œì„ ì„ í†µí•´ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¡œ í™•ì¥ ë° ë„ë©”ì¸ í™•ì¥ì´ ê°€ëŠ¥</p> </li> </ul> </li> <li> <p>Embedding: ì—¬ì „íˆ ê´‘ë²”ìœ„í•œ íƒœìŠ¤í¬ì—ì„œ í™œìš©</p> <ul> <li>Information Retrieval, Sentence Classification, Clustering ë“±ë“±</li> </ul> <p>â‡’ Encoder ëª¨ë¸ì´ ì—¬ì „íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆëŠ” ë¶„ì•¼</p> <p>â‡’ PLMì˜ ì„±ëŠ¥ ê°œì„ ì´ ì–´ë ¤ì›Œ ë¹„ì•½ì  ì„±ëŠ¥ê°œì„ ì´ ë§¤ìš° ì–´ë ¤ì›€</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>â†’ Scalingì„ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ ì‹œí‚¤ëŠ” Decoder ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ?

MNTP + UCLì„ í†µí•´ Decoderë¥¼ Encoderë¡œ í™œìš© ê°€ëŠ¥
</code></pre></div> </div> </li> </ul> <h2 id="2-method">2<span style="color:green_background">. Method</span> </h2> <p><strong>3ë‹¨ê³„ ê³¼ì •ìœ¼ë¡œ êµ¬ì„±</strong></p> <h3 id="2-1-bidirectional-attention">2-1. Bidirectional Attention</h3> <p>**ê¸°ì¡´ Decoder only ëª¨ë¸ì˜ Attention layerë¥¼ bi-Directional Attentionìœ¼ë¡œ ë³€ê²½ **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>â†’ ê¸°ì¡´ pretrained weightëŠ” ê·¸ëŒ€ë¡œ í™œìš©
</code></pre></div></div> <p><br></p> <h3 id="2-2-mntpmasked-next-token-prediction">2-2. MNTP(Masked Next Token Prediction)</h3> <p><strong>Decoderê°€ ì ì ˆíˆ Embedding taskë¥¼ í•™ìŠµí•˜ë„ë¡ ìœ ë„</strong></p> <ol> <li> <p>ë¬¸ì¥ ë‚´ ì„ì˜ì˜ í† í°ì„ ë§ˆìŠ¤í‚¹í•˜ê³ , ì´ì „ ì‹œì ì˜ representationìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡</p> </li> <li> <p>BERTì˜ MTP(Masked Token Prediction)ê³¼ GPTì˜ NTP(Next Token Prediction)ì˜ ì¤‘ê°„ ìˆ˜ì¤€</p> <ul> <li> <p>MTP: ë¬¸ì¥ ì¤‘ê°„ í† í°ì„ Maskingí•˜ê³  ì˜ˆì¸¡</p> <ul> <li> <p>bi-directional ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ contextë¥¼ ë°˜ì˜í•œ Representation ìƒì„± í•™ìŠµ</p> <p>â†’ Encoderì— ì í•©í•œ í•™ìŠµ ë°©ì‹</p> </li> </ul> </li> <li> <p>NTP: ë§¤ í† í°ì˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡</p> <ul> <li> <p>Scalingì— ìš©ì´í•˜ê³  ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±ì´ ê°€ëŠ¥</p> <p>â†’ Pretrain ì‹œ í•™ìŠµí•œ íƒœìŠ¤í¬ë¥¼ ìœ ì§€</p> </li> </ul> </li> </ul> </li> </ol> <h3 id="2-3-unsupervised-contrastive-learning">2-3. Unsupervised Contrastive Learning</h3> <p><strong>Sentence Levelì˜ Representationì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ</strong></p> <ol> <li> <p>SimCSE í•™ìŠµ ë°©ë²•ë¡  ì´ìš©</p> <ol> <li> <p>query: ì„ì˜ì˜ ë¬¸ì¥</p> </li> <li> <p>positive: query ë¬¸ì¥ì— ëŒ€í•´ dropoutì„ ë‹¤ë¥´ê²Œ ì ìš©í•œ Representation</p> </li> <li> <p>negative: in-batch negatives</p> </li> </ol> </li> </ol> <h2 id="3-experimental-setup">3. Experimental Setup</h2> <h3 id="masking-token-_">Masking Token: â€œ_â€</h3> <ul> <li> <p>Decoder modelì€ masking tokenì´ ì—†ìŒ</p> </li> <li> <p>ë‚˜ëŠ” ë°”ë³´ê°€ ì•„ë‹ˆë‹¤. â†’ ë‚˜ëŠ” _ ì•„ë‹ˆë‹¤.</p> <ul> <li>ë‚˜ëŠ” <mask> ì•„ë‹ˆë‹¤.</mask> </li> </ul> </li> </ul> <h3 id="training-step-1-a100">training step (1 A100)</h3> <ul> <li> <p>MNTP: 1,000 step, 100 minutes</p> </li> <li> <p>UCL: 1,000 step, 3 hours</p> </li> </ul> <h3 id="training-method-lora">Training Method: LoRA</h3> <h3 id="training-dataset">Training Dataset</h3> <p>Wikipedia ë°ì´í„° ì´ìš©: ëª¨ë“  LLMì˜ ì‚¬ì „í•™ìŠµì— í¬í•¨</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>**â†’ ëª¨ë¸ì—ê²Œ ìƒˆë¡œìš´ ì§€ì‹ ì£¼ì… X, Encoderë¡œì„œì˜ íƒœìŠ¤í¬ í•™ìŠµ**
</code></pre></div></div> <ul> <li> <p>MNTP: Wikitext-103</p> </li> <li> <p>UCL: Wikipedia sentences</p> </li> </ul> <h2 id="4-experiments">4. Experiments</h2> <h3 id="4-1-word-level-tasks">4-1. Word Level Tasks</h3> <ul> <li> <p>token ìˆ˜ì¤€ì˜ representationì„ í•„ìš”ë¡œ í•˜ëŠ” Task</p> </li> <li> <p>ì‹¤í—˜ ëª¨ë¸</p> <ul> <li> <p>Uni: Decoder only LLM (ì§ì„ )</p> </li> <li> <p>DeBERTa-v3-large: Pretrained Encoder (ì ì„ )</p> </li> <li> <p>Bi: LLMì˜ attnì„ bi-directional attnìœ¼ë¡œ ë³€ê²½, í•™ìŠµ X</p> </li> <li> <p>Bi + MNTP : MNTP í•™ìŠµëœ ëª¨ë¸</p> </li> <li> <p>Bi+ MNTP + SimCSE: MNTPë¡œ í•™ìŠµëœ ëª¨ë¸ + SimCSE í›ˆë ¨</p> </li> </ul> </li> <li> <p>ì‹¤í—˜ í•´ì„</p> <ul> <li> <p>Bi: ì‹¬ê°í•œ í•™ìŠµ ì €í•˜ ì–‘ìƒì„ ë³´ì„ â†’ LLMì€ attnë§Œ ë°”ê¾¸ì–´ì„œëŠ” ì ì ˆí•œ representation ìƒì„± X</p> </li> <li> <p>MNTP: ì„±ëŠ¥ ê°œì„  ê´€ì°° â†’ 1,000 step í•™ìŠµìœ¼ë¡œ ì¢‹ì€ Representation í˜•ì„± ê°€ëŠ¥</p> </li> <li> <p>SimCSE: word-level taskì—ì„œë„ MNTP ëŒ€ë¹„ ì„±ëŠ¥ì €í•˜ ì ìŒ</p> </li> </ul> </li> </ul> <h3 id="4-2-sentence-level-task">4-2. Sentence Level task</h3> <p>MTEB ë²¤ì¹˜ë§ˆí¬ ë‚´ 15ê°œ íƒœìŠ¤í¬ì— ëŒ€í•œ í‰ê· ê°’</p> <ul> <li> <p>Pooling ë°©ë²•ë¡ ê³¼ ê´€ê³„ì—†ì´ SimCSEê°€ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„</p> </li> <li> <p>Mistial: Bidirectional Attnë§Œ ì ìš©í•˜ë”ë¼ë„ ì„±ëŠ¥ ê°œì„ ì´ ê´€ì°°ë˜ëŠ” ëª¨ìŠµì„ ë³´ì„</p> <ul> <li> <p>ë‹¤ë¥¸ LLMê³¼ ë‹¤ë¥¸ ê²½í–¥ì„±</p> <p>â‡’ í•™ìŠµ ë°ì´í„° or pretrain methodì— ìˆì–´ ì°¨ì´ì ì´ ì¡´ì¬í•œë‹¤ê³  ì¶”ì¸¡ But ê³µê°œ X</p> <p>(ì¬í¬) Mistral ì‚¬ì „ í•™ìŠµ ê³¼ì •ì—ì„œ token level taskê°€ ì ìš©â€¦?</p> </li> </ul> </li> </ul> <p>ğŸ’¡</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Bi Directional Attnë§Œìœ¼ë¡œëŠ” Decoder ëª¨ë¸ì˜ Encoder ì „í™˜ ë¶ˆê°€

1. MNTP íƒœìŠ¤í¬ë¡œ ì ì€ stepë§Œ í•™ìŠµí•˜ë”ë¼ë„ ëª¨ë¸ì€ ì‰½ê²Œ Encoderë¡œ ì „í™˜ë¨

1. Unsupervised Contrastive Learning ì ìš© ì‹œ sentence levelì—ì„œ ë” ë†’ì€ ì„±ëŠ¥ ë‹¬ì„±
</code></pre></div></div> <p><br></p> <h3 id="4-3-mteb-benchmark">4-3. MTEB Benchmark</h3> <ul> <li> <p>MTEB: Retrieval, Rerank, Clustering, Sentence Pair Classification, STS, Extractive Summarization ë“± Encoderìš© íƒœìŠ¤í¬ì— ëŒ€í•œ ë²”ìš© ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹</p> </li> <li> <p>Echo Embedding: ë™ì¼ ë¬¸ì¥ì„ ë‘ë²ˆ ë°˜ë³µí•˜ì—¬ ì…ë ¥í•˜ê³ , ë’· ë¬¸ì¥ì—ì„œ Representationì„ íšë“í•˜ëŠ” ë°©ë²•</p> <p>â†’ ë’·ë¬¸ì¥ì˜ ëª¨ë“  í† í°ì€ ì•ë¬¸ì¥ì„ í†µí•´ ëª¨ë“  í† í°ì— ëŒ€í•´ attní•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ bi-directional attnì˜ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ</p> <ul> <li> <p>Uni + Mean: Decoder ëª¨ë¸ì˜ Representationì— ëŒ€í•´ mean pooling ì ìš©</p> </li> <li> <p>BERT + SimCSEì™€ ìœ ì‚¬í•œ ì„±ëŠ¥ ê°€ëŠ¥</p> </li> <li> <p>Decoder Modelì´ Scaling upë˜ë©´ì„œ ëª¨ë¸ êµ¬ì¡°ì˜ í•œê³„ë¥¼ ë„˜ì„ ìˆ˜ ìˆìŒ</p> <p>(ì¬í¬): Decoder ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ Encoderë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ë°©í–¥ì„±ì˜ ê·¼ê±°</p> </li> </ul> </li> <li> <p>Bi + Mean: Decoder ëª¨ë¸ì— ëŒ€í•´ Bi Attnë§Œ ì ìš©í•˜ê³  ëª¨ë“  tokenì— ëŒ€í•œ Mean pooling ì´ìš©</p> <ul> <li>Mistralì„ ì œì™¸í•œ ëª¨ë¸ì˜ ê²½ìš° ë§¤ìš° ì•ˆì¢‹ì€ ì„±ëŠ¥ ê¸°ë¡</li> </ul> </li> <li> <p>LLM2Vec(w/o SimCSE): ê±°ì˜ ëª¨ë“  íƒœìŠ¤í¬ì—ì„œ Uni + Mean ëŒ€ë¹„ ë†’ì€ ì„±ëŠ¥ ë„ì¶œ</p> <ul> <li>Sentence Level íƒœìŠ¤í¬ë¼ëŠ” ì ì„ ê³ ë ¤í•˜ë©´ LLM2Vecì´ ë‹¨ìˆœí•˜ë©´ì„œ ê°•ë ¥í•œ ë°©ë²•ë¡ ì„ì„ ë³´ì—¬ì¤Œ</li> </ul> </li> <li> <p>LLM2Vec: ë§¤ìš° ë†’ì€ ì„±ëŠ¥ ë„ì¶œ</p> <ul> <li> <p>ì ì€ í•™ìŠµìœ¼ë¡œë„ Decoder ëª¨ë¸ì´ Sentence ë‹¨ìœ„ë¡œ representationì„ ë§¤ìš° ëŠ¥ìˆ™í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŒ</p> <p>â‡’ ìµœê·¼ MTEB ìƒìœ„ ë°©ë²•ë¡ ë“¤ì´ ëª¨ë‘ ë™ì¼í•œ ê²½í–¥ì„±ì„ ë³´ì„ (LLM + Bi Attn + Contr. Learning)</p> <p><br></p> </li> </ul> </li> </ul> <h3 id="4-4-analysis-1mntpì˜-íš¨ê³¼">4-4. Analysis 1(MNTPì˜ íš¨ê³¼)</h3> <p><strong>LLM2Vecì„ í†µí•´ í•™ìŠµë˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?</strong></p> \[q_i = (A_i, B_i), s^+\_i = (A_i, C_i) ,s^-\_i = (A_i, D_i)\] <ul> <li> <p>B, C: ë¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë¬¸ì¥</p> </li> <li> <p>B, D: ë‹¤ë¥¸ ì˜ë¯¸ì˜ ë¬¸ì¥</p> </li> <li> <p>A ë¬¸ì¥ì˜ í† í°ì—ì„œ poolingí•˜ì—¬ representationì„ ì‚°ì¶œ</p> <ul> <li>Bidirectionalì´ ì˜ëœë‹¤ë©´: $ q_i, s^+_i $ëŠ” ë¹„ìŠ·í•œ Representationì´ì–´ì•¼ í•¨. â†’ ë’·ë¬¸ì¥ì˜ ì •ë³´ê°€ A ë¬¸ì¥ìœ¼ë¡œ í˜ë €ì–´ì•¼ í•¨ìœ¼ë¡œ</li> </ul> </li> <li> <p>ëª¨ë¸ í¬ê¸°ì— ê´€ê³„ì—†ì´ MNTP ì´í›„ posì™€ Neg ê°„ ê±°ë¦¬ê°€ ë²Œì–´ì§</p> <ul> <li>MNTP: ìœ„ì¹˜ì— ê´€ê³„ì—†ì´ Tokenë“¤ì´ ì„œë¡œ attní•˜ë„ë¡ í•™ìŠµ</li> </ul> </li> </ul> <h3 id="4-5-analysis-2why-mistral-works">4-5. Analysis 2(why Mistral works)</h3> <ul> <li> <p>ë™ì¼ ë°ì´í„°ì— ëŒ€í•œ original attention(llm)ê³¼ bi directional attention ì‹œì˜ token/layer ë‹¨ìœ„ Representation ë¹„êµ</p> <ul> <li> <p>sheared LLaMA, LLaMA 2 ëª¨ë‘ ìƒìœ„ ë ˆì´ì–´ë¡œ ê°ˆìˆ˜ë¡ ë‘ attn ê°„ ë‹¤ë¥¸ Representationì´ ìƒì„±ë¨</p> </li> <li> <p>Mistral: bi directional attn ì ìš© ì‹œì—ë„ ëª¨ë“  ë ˆì´ì–´ì—ì„œ ê¸°ì¡´ attnê³¼ ë¹„ìŠ·í•œ representationì´ í˜•ì„± ë¨</p> <p>â†’ Mistralì´ ë­”ì§“ì„ í•œ ê²ƒ ê°™ì€ë°â€¦ ë­”ì§€ëŠ” ëª°ë¼ìœ â€¦</p> </li> </ul> </li> </ul> <h2 id="5-conclusion">5. Conclusion</h2> <ul> <li> <p>Decoder ëª¨ë¸ì„ Encoderë¡œ ì „í™˜í•˜ëŠ” ë°©ë²•ë¡  ì œì•ˆ</p> <ul> <li>MNTP: NTPì™€ MTP ì‚¬ì´ì˜ task</li> </ul> </li> <li> <p>ë‹¨ìˆœ: 1,000 step, 128 batch size, 1 a100 80GBë©´ ì¶©ë¶„íˆ í•™ìŠµì´ ë¨</p> </li> <li> <p>ë²”ìš©: MNTP + SimCSEë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë²”ìš©ì ì¸ encoderë¡œì„œ ì‚¬ìš© ê°€ëŠ¥</p> <ul> <li>ë‹¤ë¥¸ encoder í•™ìŠµ ë°©ë²•ë¡  ì—­ì‹œ ì¶”ê°€ì ìœ¼ë¡œ ì ìš© ê°€ëŠ¥ â‡’ MTEB ìƒìœ„ ëª¨ë¸ ëŒ€ë¶€ë¶„ì˜ baseline</li> </ul> </li> <li> <p>Encoder Pretraian ì‹œ scaling íš¨ê³¼ê°€ ì—†ëŠ” ìƒí™©ì—ì„œ Encoder ë¶„ì•¼ í–¥í›„ ë°œì „ ë°©í–¥</p> <ul> <li> <p>DecoderëŠ” Scaling íš¨ê³¼ê°€ ìˆìŒ</p> </li> <li> <p>Scalingí•˜ì—¬ Decoder í›ˆë ¨ â†’ LLM2Vec ì ìš© â‡’ Scaling íš¨ê³¼ê°€ ì ìš©ëœ Encoder ê°œë°œ ê°€ëŠ¥</p> </li> </ul> </li> </ul> <p><br></p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>