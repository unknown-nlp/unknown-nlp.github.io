<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pyspark - How to preprocess Large Scale Data with Python | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content=" 논문 리뷰 - Pyspark - How to preprocess Large Scale Data with Python"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/al-folio/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alshedivat.github.io/al-folio/blog/2024/pyspark-how-to-preprocess-large-scale-data-with-python/"> <script src="/al-folio/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/al-folio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/al-folio/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/al-folio/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pyspark - How to preprocess Large Scale Data with Python</h1> <p class="post-meta"> Created on July 23, 2024 </p> <p class="post-tags"> <a href="/al-folio/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/al-folio/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   ·   <a href="/al-folio/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-07-23</li> <li> <strong>Reviewer</strong>: 준원 장</li> <li> <strong>Property</strong>: spark</li> </ul> <h2 id="0-prerequisites-for-practice">0. Prerequisites For Practice</h2> <ul> <li> <p>Spark runs on Java 8, 11, or 17.</p> <p>-</p> </li> <li> <p>Install Apache Spark (Framework that powered the pyspark)</p> <p>-</p> <ul> <li>Latest Version의 Pre-built for Apache Hadoop 3.3 and later download</li> </ul> </li> <li> <p>Spark runs on Python 3.7+</p> </li> </ul> <p><br></p> <ul> <li>Install the following libraries in Python</li> </ul> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pip</span> <span class="nx">install</span> <span class="nx">pyspark</span> <span class="nx">findspark</span>
<span class="nx">conda</span> <span class="nx">install</span> <span class="nx">jupyter</span>

</code></pre></div></div> <p><br></p> <ul> <li>Launch jupyter lab in python</li> </ul> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">jupyter</span> <span class="o">-</span> <span class="nx">lab</span><span class="p">;</span>
</code></pre></div></div> <h2 id="1-introduction-to-spark">1. Introduction to Spark</h2> <h3 id="spark">Spark</h3> <ul> <li> <p>대규모 데이터 처리를 위한 분석 엔진 (unified analytics engine for large-scale data processing)</p> </li> <li> <p><strong>In-Memory (RAM)</strong> (Hadoop은 Disk에서 처리되기 때문에 여기서 속도차이가 발생)</p> </li> <li> <p>분산 병렬처리</p> </li> <li> <p>사용하기 쉬움</p> <ul> <li>Write applications quickly in Java, Scala, Python, R,and SQL.</li> </ul> <p><br></p> </li> </ul> <h3 id="mapreduce란">MapReduce란?</h3> <ul> <li> <p>Spark를 알기 위해서는 MapReduce라는거에 대해서 알면 매우 좋다</p> </li> <li> <p><strong>MapReduce는 대규모 데이터 세트를 처리하기 위한 프로그래밍 모델이자 구현 (i.e., distributed data processing)</strong></p> </li> <li> <p>이름에도 명시되어 있듯이, 해당 모델은 두 가지 주요 단계, 즉 <code class="language-plaintext highlighter-rouge">Map</code> 단계와 <code class="language-plaintext highlighter-rouge">Reduce</code> 단계로 구성</p> <ol> <li> <p><strong>Map</strong>:</p> <ul> <li> <p>입력 데이터를 키-값 쌍으로 변환하고, 각 데이터 조각에 지정된 Map 함수를 적용</p> <p>(예시)</p> <ul> <li> <p>각 단어를 <code class="language-plaintext highlighter-rouge">(단어, 1)</code>의 키-값 쌍으로 변환</p> </li> <li> <p>“hello world hello”라는 텍스트가 있다면, 이는 <code class="language-plaintext highlighter-rouge">[(hello, 1), (world, 1), (hello, 1)]</code>으로 변환</p> </li> </ul> </li> </ul> </li> <li> <p><strong>Reduce</strong>:</p> <ul> <li> <p>맵 단계에서 생성된 키-값 쌍을 키에 따라 그룹화 → 그룹화된 각 키에 대해 리듀스 함수를 적용하여 최종 결과를 생성</p> <p>(예시)</p> <ul> <li> <code class="language-plaintext highlighter-rouge">[(hello, 1), (hello, 1)]</code> 및 <code class="language-plaintext highlighter-rouge">[(world, 1)]</code>로 그룹화 → <code class="language-plaintext highlighter-rouge">hello</code>는 2번, <code class="language-plaintext highlighter-rouge">world</code>는 1번 출현</li> </ul> </li> </ul> </li> </ol> </li> </ul> <p><u>_⇒ MapReduce 모델은 하둡(Hadoop)과 같은 시스템에서 널리 사용됨. 데이터를 디스크에 저장하고, 중간 결과도 디스크에 쓰기 때문에 대량의 데이터 처리에 적합하지만, 입출력 작업으로 인해 속도가 느려짐._</u></p> <p>⇒ MapReduce는 YARN (Hadoop에서 사용되는 resource manager - CPU나 메모리 등의 계산 리소스가 관리되며, 아래 그림에서 어떤 호스트에 컨테이너를 어떻게 할당할 것인가를 결정)상에서 동작하는 분산 애플리케이션 중 하나며, 분산 시스템에서 데이터를 처리하는 데 사용됨.</p> <p>⇒ 또한, SQL 같은 쿼리 언어를 위해 Apache Hive라는 쿼리 엔진을 사용할 수 있는데, 이들은 입력한 쿼리를 자동으로 MapReduce 프로그램으로 변환 해주기에 <span style="color:red"><strong>아래 그림에서 확인할 수 있듯이 위 2개의 애플리케이션 (Hadoop, YARN) 모두 대량의 데이터를 배치 처리에 적합하지만, 애드 훅 쿼리를 여러 번 실행하는 복잡한 쿼리에는 부적합하며데이터 처리의 스테이지가 바꿜 때마다 약간의 대기 시간이 필요</strong></span> (이건 Hive on Tez같은 기술적인 해결책이 존재)</p> <p>⇒ 하지만 점점 RAM 가격이 저렴해졌고, (Disk IO로 빠질것을) In-Memory에서 돌아가는 SPARK 등장!</p> <p><br></p> <h2 id="2-spark-how-spark-work--spark-session">2. Spark (How Spark Work &amp; Spark Session)</h2> <h3 id="how-spark-work">How Spark Work</h3> <ul> <li> <p>Spark는 기본적으로 multi-node (향후 글에서는 이를 <strong>CLUSTER 모드/CLUSTER</strong>라는 용어로 자주 사용됩니다!)에서 데이터를 처리하는 것을 원칙으로 함</p> </li> <li> <p>따라서 해당 섹션에서는 CLUSTER 모드에서 Spark가 어떻게 동작하는지를 중점적으로 설명하나, 코드 실습은 환경적인 한계로 인해 단일노드 환경에서 진행했음을 양해해주시길 바랍니다</p> </li> <li> <p>(<a href="https://www.notion.so/56ee4c6eb92e4f2ab93329bb3ce02afc]" rel="external nofollow noopener" target="_blank">https://www.notion.so/56ee4c6eb92e4f2ab93329bb3ce02afc</a> 가 spark-submit이라는 resource를 얼마나 사용할지 config파일 같은 메뉴얼을 user code를 통해 제출하면 Driver Process에서 ‘(<a href="https://www.notion.so/29f09ded8c3741cdba7291e8e7c444ce]" rel="external nofollow noopener" target="_blank">https://www.notion.so/29f09ded8c3741cdba7291e8e7c444ce</a> ’을 생성</p> </li> <li> <p>이때, 할당받은 Executor에서 실질적인 데이터가 처리됨</p> </li> <li> <p>Spark 자체는 JVM으로 구동이 되지만, 다수의 언어가 Interface 형태로 지원됨!</p> </li> </ul> <p><br></p> <h3 id="spark-context-vs-spark-session">Spark Context vs. Spark Session</h3> <p>→ 이전에 Spark Session에 대해서 언급을 했는데, Spark Context라는 것도 있다. 둘의 차이를 알아보자</p> <p>**#### Spark Context **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Represents the connection to a Spark cluster (기억하자! Spark는 멀티노드가 디폴트 환경이다)

	- Spark 애플리케이션과 Spark 클러스터 간의 연결을 담당. 즉 Spark functionality의 가장 큰 Entry Point!

- Coordinates task execution across the cluster

	- 클러스터 전반에 걸쳐 태스크를 조정하고 실행을 관리

	-  데이터를 파티션으로 나누거나, 각 노드에 작업을 분배, 실행 중인 작업의 상태를 모니터링 역할

- Entry point in earlier versions of Spark (1.x)

- ***Functionalities***

	- Core functionality for low-level programming and cluster interaction

	- Creates RDDs (Resilient Distributed Datasets)

		- RDD는 Spark의 기본 데이터 구조로, 데이터를 클러스터의 여러 노드에 걸쳐 분산시켜 처리할 수 있도록 설계된 불변성 분산 컬렉션.

		- RDD는 데이터의 장애 허용성과 병렬 처리를 지원

	- Performs transformations and defines actions

		- RDD에 대한 다양한 변환(예: `map`, `filter`, `reduceByKey` 등)을 수행하고, 액션(예: `collect`, `count`, `saveAsTextFile` 등)을 정의하여 실제 연산 수행

- Fully supported for backward compatibility

	- SparkContext는 이전 버전과의 완벽한 호환성을 제공. 예를 들어, 기존에 Spark 1.x 버전에서 작성된 애플리케이션이 Spark의 최신 버전에서도 계속 작동할 수 있음

- Use in specific scenarios or with libraries/APIs reliant on it

	- 특정 라이브러리나 API가 여전히 SparkContext를 필요로 하는 경우가 있음 (e.g., 일부 low-level API나 특수한 작업을 수행하는 라이브러리는 SparkContext를 직접 사용)
</code></pre></div></div> <p><br></p> <p><strong>#### Spark Session</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Introduced in Spark 2.0

- Unified entry point for interacting with Spark

	- 다양한 기능(SQL 쿼리, DataFrame 작업, 스트리밍 작업 등)을 하나의 인터페이스에서 사용할 수 있게 됨

- Combines functionalities of SparkContext, SQLContext, HiveContext, and StreamingContext

	- SQLContext의 SQL 작업, HiveContext의 Hive 작업, 그리고 StreamingContext의 스트리밍 작업 등을 SparkSession 하나로 처리할 수 있게 되었음 → 사용자의 편의성 증대

- Supports multiple programming languages (Scala, Java, Python, R)

- ***Functionalities***

	- Extends SparkContext functionality

		- SparkSession은 SparkContext의 모든 기능을 포함하며, 추가적인 기능을 제공

	- Higher-level abstractions like DataFrames and Datasets

	- Provides data source APIs, machine learning algorithms, and streaming capabilities
</code></pre></div></div> <p><br></p> <p><strong>##### Summary</strong></p> <p>SparkContext를 사용하던 초기 Spark 버전에서는 주로 RDD(Resilient Distributed Datasets)를 사용하여 데이터를 처리. 이후에 조금 더 자세히 설명하겠지만 RDD는 Spark의 가장 기본적인 데이터 구조로, low-level API를 통해 데이터를 다루며, 큰 유연성을 제공하지만, 사용자가 많은 세부 사항을 직접 관리해야 하는 단점이 존재</p> <p>Spark 2.0 이후 도입된 SparkSession은 DataFrame과 같은 high-level API를 제공함으로써, 사용자가 보다 쉽게 데이터를 구조화하고 SQL과 유사한 쿼리를 사용할 수 있게 해줌. 이 high-level API는 내부적으로 RDD를 사용하지만, 개발자가 직접 RDD를 다루는 것보다 훨씬 간단하고 효율적인 데이터 처리가 가능하도록 설계</p> <p><br></p> <h3 id="spark의-cluster-구성">Spark의 Cluster 구성</h3> <p>→ (<a href="https://www.notion.so/a9cd823fd7fd45abaeb10a9a7a89436d]" rel="external nofollow noopener" target="_blank">https://www.notion.so/a9cd823fd7fd45abaeb10a9a7a89436d</a> 에서 어떻게 Spark가 구동되는지 간단히 알아보았으니, Spark가 작동되는 Cluster, 즉 multi-node 환경에서 실제로 앞서 설명한 노드들이 어떤 구조로 이루어져있는지, SparkSession과의 관계성도 고려해서 유기적으로 설명해보겠습니다.</p> <ol> <li> <p><strong>Master Node와 클러스터 매니저</strong></p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>- Hadoop HDFS 구조

- **Master 노드 (NameNode)**:

	- HDFS에서 NameNode은 메타데이터를 관리. 이 메타데이터에는 파일 시스템의 디렉토리 구조, 파일 위치 정보 등이 포함되어 있어, 클러스터의 데이터 노드에서 데이터를 어디에서 찾을 수 있는지를 알려주는 역할을 수행

	- Master 노드는 시스템의 중앙 집중식 관리 노드로서, 전체 파일 시스템의 구조와 상태를 관리

- **Slave 노드 (DataNode)**:

	- 실제 데이터 파일은 DataNode에 분산 저장.

	- 해당 노드들은 NameNode의 지시에 따라 데이터를 저장하고, 데이터 요청에 응답

	- 데이터 노드는 클러스터 내에서 파일 데이터의 실제 저장과 처리를 담당

- **Secondary NameNode (위의 그림에는 표기 X)**:

- 주로  NameNode의 메타데이터를 주기적으로 백업하는 역할을 수행
</code></pre></div> </div> <ul> <li> <p><strong>클러스터 구조</strong></p> <ul> <li> <p><strong>Master 노드</strong>:</p> <ul> <li>Spark에서는 Master 노드가 <a href="/1f7345e855b7412d9922776e3e5e1139#56ee4c6eb92e4f2ab93329bb3ce02afc">클러스터 매니저</a> 역할을 수행</li> </ul> </li> <li> <p><strong>Worker 노드</strong>:</p> <ul> <li> <p>Worker 노드는 실제 데이터 처리 작업을 수행하는 노드</p> </li> <li> <p>Master 노드로부터 할당받은 태스크를 수행하며, Spark의 연산과 데이터 저장을 처리</p> </li> </ul> </li> <li> <p><strong>클러스터 매니저(위의 그림의 하단 3개 상자)</strong>: Spark에서 클러스터 매니저는 클러스터의 리소스(메모리, CPU 등)를 관리</p> </li> <li> <p>클러스터 매니저에는 여러 유형이 있으며(Standalone, YARN, Mesos 등), 이들은 클러스터의 리소스 할당 및 태스크 관리를 담당</p> </li> </ul> </li> </ul> <p><strong>⇒ 어?! 로컬/서버 1대에서는 클러스터라는 개념이 성립이 될 수 없는데라는 의문이 들었다면 아래를 계속 읽어주세요!</strong></p> </li> <li> <p>(<a href="https://www.notion.so/29f09ded8c3741cdba7291e8e7c444ce]" rel="external nofollow noopener" target="_blank">https://www.notion.so/29f09ded8c3741cdba7291e8e7c444ce</a></p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">SparkSession</code>은 사용자가 Spark와 상호작용하기 위한 Entry Point. (i.e., 해당 세션을 통해 사용자는 데이터 로드, 처리, 저장 등의 작업을 제출) 하지만, SparkSession은 Local모드. Cluster모드 2가지에서 각각 다르게 동작</p> <ul> <li> <p><strong>Local 모드 (싱글 머신)</strong></p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">SparkSession</code>이 <code class="language-plaintext highlighter-rouge">.master("local[*]")</code>로 설정된 경우, <strong>Master 노드</strong>와 <strong>Worker 노드</strong> 모두 로컬 컴퓨터에서 실행. 이 경우, 단일 컴퓨터에서 모든 Spark 작업이 처리. 클러스터 매니저는 내부적으로 이를 관리하지만, <u>_별도의 물리적 _</u><u>**_Master 노드_**</u><u>_와 _</u><u>**_Worker 노드_**</u><u>_는 존재하지 않음 (싱글머신에 컨테이너 N대 띄우고 클러스터모드로 돌리는 환경을 많이 사용하는것 같습니다,,!)_</u>.</p> </li> <li> <p><strong>Local 모드</strong>에서 <span style="color:red">Spark는 싱글 머신에서 실행되며, 사용 가능한 CPU 코어들을 Worker 노드로 간주. </span>각 코어는 병렬 처리를 수행하는 하나의 “논리적” 워커로 작동</p> </li> <li> <p><strong>Local 모드</strong>는 주로 개발과 테스팅을 목적으로 사용되며, 실제 클러스터 환경에서의 실행을 모의하기 위한 방법으로 사용</p> </li> <li> <p><strong>Executor</strong>: 로컬 모드에서는 <span style="color:red"><em>전체 JVM(Spark Session은 JVM으로 돌아감)이 하나의 큰 실행자(executor)로 볼 수 있으며, 여러 스레드가 데이터를 처리</em></span></p> </li> </ul> </li> </ul> </li> </ul> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">MySparkApplication</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.memory</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">2g</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.sql.shuffle.partitions</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">4</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	- **CLUSTER 모드 (멀티 머신)**

		- **CLUSTER 모드**에서는 **Master 노드**가 하나의 서버(또는 컴퓨터)에서 실행되고, 여러 워커 노드가 다른 서버에서 실행. 이 경우, **Master 노드**는 클러스터 매니저를 통해 클러스터 전체의 리소스를 관리하고 워커 노드에게 작업을 할당

		- **CLUSTER 모드**에서는 &lt;span style='color:red'&gt;*여러 물리적 머신이 Worker 노드로 구성되어 각기 다른 데이터 세트의 처리를 담당*&lt;/span&gt;. 이 각 머신은 독립적인 리소스(CPU, 메모리)를 장착

		- **CLUSTER 모드**에서 Spark는 다수의 Worker Node를 통해 데이터를 분산 처리하며, &lt;u&gt;*각 Worker Node는 하나 이상의 executor를 가질 수 있음*&lt;/u&gt;

			- 이 &lt;u&gt;*executor*&lt;/u&gt;들이 실제 데이터 처리 작업을 담당하며, 각 &lt;u&gt;*executor*&lt;/u&gt;는 Worker 노드의 리소스를 할당받아 독립적으로 작업을 수행

		- 클러스터 매니저(예: YARN, Mesos, Spark 자체의 Standalone 클러스터 매니저)가 전체 자원을 관리하고 워커 노드에 작업을 할당
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">Example on Standalone Cluster</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">master</span><span class="p">(</span><span class="sh">"</span><span class="s">spark://master_node_address:7077</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.memory</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">4g</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.cores</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">2</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

</code></pre></div></div> <p><br></p> <h2 id="3-rdds-resilient-distributed-datasets">3. RDDs (Resilient Distributed Datasets)</h2> <ul> <li> <p>Resilient Distributed Datasets (RDDs)는 Apache Spark의 핵심 데이터 구조로서, 분산 환경에서 대규모 데이터셋의 효율적인 처리를 가능</p> </li> <li> <p>SPARK라는 생태계의 가장 low-level의 데이터 처리 단위라고 필자는 이해하였다.</p> <p>(하지만 element(요소)단위로 실질적인 transformation이 적용되는거 같음)</p> </li> </ul> <h3 id="about-rdds">About RDDs</h3> <ol> <li> <p><strong>Backbone of data processing in Spark</strong></p> <ul> <li> <p>RDD는 Spark에서 데이터 처리 작업의 기본 단위로 사용</p> </li> <li> <p>데이터는 클러스터 전체의 여러 노드에 분산 저장되며, 각 노드는 할당된 데이터 부분에 대한 작업을 병렬로 수행할 수 있음</p> </li> </ul> </li> <li> <p><strong>Distributed, fault-tolerant, parallelizable data structure, and in-memory</strong></p> <ul> <li> <p><strong>분산(Distributed)</strong>: 데이터는 네트워크상의 여러 컴퓨터(노드)에 걸쳐 분산되어 저장. 이 구조는 데이터를 여러 노드에 나누어 처리함으로써, 처리 속도를 높이고, 한 노드의 실패가 전체 시스템에 미치는 영향을 최소화</p> </li> <li> <p><strong>내결함성(Fault-tolerant)</strong>: RDD는 데이터의 파티션을 여러 노드에 복제하거나, 데이터의 메타데이터(예: tranformation 연산 기록)를 사용하여 실패한 노드의 데이터를 다시 계산할 수 있게 만듦. 이를 통해 데이터 손실 없이 시스템의 장애를 극복할 수 있음</p> </li> <li> <p><strong>병렬 처리 가능(Parallelizable)</strong>: 데이터의 각 파티션은 독립적으로 처리될 수 있어, 여러 처리 작업을 동시에 수행</p> </li> <li> <p><span style="color:red"><strong>메모리 저장(In-Memory)</strong></span><span style="color:red">: RDD는 데이터를 메모리에 저장하고, 여러 연산을 메모리 상에서 직접 수행함으로써 데이터 접근 시간을 단축</span></p> </li> </ul> </li> <li> <p><strong>Efficiently processes large datasets across a cluster</strong></p> <ul> <li>RDD를 사용하면 매우 큰 데이터셋을 효율적으로 처리. 데이터는 파티션 단위로 분할되고, 각 파티션은 클러스터의 다양한 노드에서 동시에 처리.</li> </ul> </li> <li> <p><strong>Key characteristics: immutable, distributed, resilient, lazily evaluated, fault-tolerant</strong>:</p> <ul> <li> <p><strong>불변성(Immutable)</strong>: 한 번 생성된 RDD는 변경할 수 없음. 데이터에 transformation을 가하려면, 새로운 RDD를 생성하는 transformation 연산을 적용해야 함. 이는 데이터의 일관성을 보장하고, 복잡한 데이터 파이프라인을 안정적으로 관리할 수 있게 함</p> </li> <li> <p><strong>Distributed</strong>: 데이터를 여러 노드에 나누어 (partitioned) 분산처리되도록 함</p> </li> <li> <p><span style="color:red"><strong>Lazily evaluated</strong></span><span style="color:red">: RDD 연산은 실제로 액션(action)이 호출될 때까지 실행되지 않음. 불필요한 계산을 피하고 최적화된 방식으로 데이터를 처리</span></p> </li> <li> <p>**Fault-tolerant: **<code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">filter</code>, <code class="language-plaintext highlighter-rouge">reduce</code>, <code class="language-plaintext highlighter-rouge">collect</code>, <code class="language-plaintext highlighter-rouge">count</code>, <code class="language-plaintext highlighter-rouge">save</code> 등의 연산을 지원하며, 해당 연산들은 모두 내결함성을 갖음. 예를 들어, 하나의 워커 노드가 위의 연산을 처리하는 도중에 문제가 발생해도 해당 노드의 처리가 필요한 RDD 파티션은 다른 노드에서 재처리가 가능함</p> </li> </ul> </li> </ol> <p><br></p> <h3 id="how-to-make-rdds">How to Make RDDs?</h3> <p>**#### **(<a href="https://www.notion.so/fb84785de42b4495ac8a26cac7c7dbf0]" rel="external nofollow noopener" target="_blank">https://www.notion.so/fb84785de42b4495ac8a26cac7c7dbf0</a>** **</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">SPARK_HOME</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">/Users/name/App/Spark</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">PYSPARK_DRIVER_PYTHON</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">jupyter</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">PYSPARK_DRIVER_PYTHON_OPTS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">lab</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">PYSPARK_PYTHON</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">python</span><span class="sh">'</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">RDD-Demo</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="sh">'''</span><span class="s">
현재 Spark 세션이 어떤 마스터 설정을 사용하고 있는지를 출력
### LOCAL
- local[*]이면 로컬 모드에서 모든 사용 가능한 코어를 사용하고 있다는 것을 의미
### CLUSTER
spark://host:port 형식이면 특정 클러스터 매니저(Standalone, Mesos, YARN 등)에 연결되어 있다는 것을 의미
</span><span class="sh">'''</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">RDD-Demo</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">master</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
로컬 환경에서 실행 중인 경우, 스레드의 수(즉, 병렬 처리 가능한 작업의 수)는 다음과 같이 확인
</span><span class="sh">'''</span>
<span class="kn">import</span> <span class="n">multiprocessing</span>

<span class="n">num_cores</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="p">.</span><span class="nf">cpu_count</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of available cores:</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_cores</span><span class="p">)</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">numbers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="n">num_partitions</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">getNumPartitions</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of partitions:</span><span class="sh">"</span><span class="p">,</span> <span class="n">num_partitions</span><span class="p">)</span>

<span class="n">partitioned_data</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">glom</span><span class="p">().</span><span class="nf">collect</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Data in each partition:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">partitioned_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Partition </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>

</code></pre></div></div> <p>**#### **(<a href="https://www.notion.so/f64e61864c3d414d9de7780e4fddd5ad]" rel="external nofollow noopener" target="_blank">https://www.notion.so/f64e61864c3d414d9de7780e4fddd5ad</a>** **</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
    <span class="p">.</span><span class="nf">master</span><span class="p">(</span><span class="sh">"</span><span class="s">spark://master_url:7077</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">RDD-Demo</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.instances</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">6</span><span class="sh">"</span><span class="p">)</span> \ <span class="c1"># 클러스터 전체에서 워커 노드들에 걸쳐 분배
</span>    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.memory</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">2g</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">config</span><span class="p">(</span><span class="sh">"</span><span class="s">spark.executor.cores</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">2</span><span class="sh">"</span><span class="p">)</span> \
    <span class="p">.</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="n">numbers</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">))</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">(</span><span class="n">numbers</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>  <span class="c1"># 명시적으로 파티션 수를 6으로 설정
</span>
<span class="n">partitioned_data</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">glom</span><span class="p">().</span><span class="nf">collect</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Data in each partition:</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">partitioned_data</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Partition </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">data</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>


<span class="sh">'''</span><span class="s">
(예상 출력 결과)
Data in each partition:
Partition 0: [1, 2, 3, 4]
Partition 1: [5, 6, 7, 8]
Partition 2: [9, 10, 11, 12]
Partition 3: [13, 14, 15, 16]
Partition 4: [17, 18]
Partition 5: [19, 20]
</span><span class="sh">'''</span>

</code></pre></div></div> <p><br></p> <h3 id="transformations-vs-actions">Transformations vs. Actions</h3> <ul> <li> <p>Transformation과 Action은 RDDs를 조작하고 결과를 도출하는 방법</p> </li> <li> <p>RDD operation은 Spark의 low-level API 작업이기에 <code class="language-plaintext highlighter-rouge">SparkSession</code>을 통해 접근 가능한 <code class="language-plaintext highlighter-rouge">SparkContext</code>의 인스턴스(<code class="language-plaintext highlighter-rouge">spark.sparkContext</code>)를 사용.</p> </li> </ul> <p><strong>#### Transformations</strong></p> <ul> <li> <p><strong>개념</strong>: Transformations은 RDD에 적용되는 연산으로, <span style="color:red">새로운 RDD를 생성</span>. <span style="color:red"><em>Transformations은 ‘지연 평가(lazy evaluation)’ 모델을 따르기 때문에, 실제 연산은 관련된 동작(Action)이 호출될 때까지 실행X</em></span></p> </li> <li> <p><strong>특성</strong>: Transformations을 통해 생성된 새 RDD는 원본 RDD의 변경 불가능한(immutable) 특성을 유지하며, 원본 데이터를 수정하지 않고 새로운 데이터셋을 생성</p> </li> <li> <p><strong>예시</strong>: <code class="language-plaintext highlighter-rouge">map</code>, <code class="language-plaintext highlighter-rouge">filter</code>, <code class="language-plaintext highlighter-rouge">flatMap</code>, <code class="language-plaintext highlighter-rouge">reduceByKey</code>, <code class="language-plaintext highlighter-rouge">sortBy</code>, <code class="language-plaintext highlighter-rouge">join</code> → <code class="language-plaintext highlighter-rouge">filter</code> 변환은 조건에 맞는 데이터만을 포함하는 새 RDD를 생성</p> </li> </ul> <p><strong>#### Actions</strong></p> <ul> <li> <p><strong>개념</strong>: Actions은 RDD에 적용되며, 변환된 데이터에 대해 계산을 실행하고 결과를 반환. <span style="color:red">Actions은 ‘적극적 평가(eager evaluation)’을 통해 즉시 결과를 도출</span></p> </li> <li> <p><strong>특성</strong>: <span style="color:red">Actions은 Spark의 연산 과정에서 </span><span style="color:red"><em>lazy evaluation</em></span><span style="color:red">된 transformations들을 trigger</span>하고, <span style="color:red">최종 결과를 계산하기 위해 데이터를 드라이버 프로그램으로 가져오거나 외부 시스템에 저장</span></p> </li> <li> <p><strong>예시</strong>: <code class="language-plaintext highlighter-rouge">collect</code>, <code class="language-plaintext highlighter-rouge">count</code>, <code class="language-plaintext highlighter-rouge">first</code>, <code class="language-plaintext highlighter-rouge">take</code>, <code class="language-plaintext highlighter-rouge">save</code>, <code class="language-plaintext highlighter-rouge">foreach</code> → <code class="language-plaintext highlighter-rouge">collect</code> 동작은 RDD의 모든 요소를 드라이버 프로그램으로 반환하여 사용하도록 함</p> </li> </ul> <p><br></p> <p><strong>#### Examples</strong></p> <ol> <li> <p><strong>Transformation</strong>:</p> <p><strong>#### Big Picture</strong></p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">filtered_customers</span> <span class="o">=</span> <span class="n">customers_rdd</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">'</span><span class="s">age</span><span class="sh">'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 즉시 실행 X (i.e., lazy evaluation). 대신, 필요한 계산을 정의하고, 실행 계획을 준비

- action을 통해서 실행시켜줘야 결과물 확인 가능
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### LOCAL 노드 환경 가정
</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Charlie</span><span class="sh">"</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">40</span><span class="p">)]</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">mapped_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">upper</span><span class="p">(),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">mapped_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">rdd with uppercease name: </span><span class="sh">"</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>
<span class="c1">#rdd with uppercease name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]
</span>
<span class="n">filtered_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">filtered_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="c1">#[('Charlie', 35), ('Alice', 40)]
</span>
<span class="n">reduced_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
<span class="n">reduced_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="c1">#[('Alice', 65), ('Bob', 30), ('Charlie', 35)]
</span>
<span class="n">sorted_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">sortBy</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">sorted_rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="c1">#[('Alice', 40), ('Charlie', 35), ('Bob', 30), ('Alice', 25)]
</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. `map()`

	- `map()`  RDD의 각 요소에 주어진 함수를 적용, &lt;u&gt;그 결과로 새로운 RDD를 생성&lt;/u&gt;

1.  `filter()`

	- `filter()` 주어진 조건 함수를 만족하는 요소들만을 포함하는 &lt;u&gt;새로운 RDD를 생성&lt;/u&gt;

	- 예시 코드에서는 나이가 30 초과인 사람들만 필터링

1. `reduceByKey()`

	- `reduceByKey()`키-값 쌍(pair)을 가진 RDD에 사용되며, 같은 키를 가진 값들을 주어진 reduce()함수로 합쳐서 &lt;u&gt;새로운 RDD를 생성&lt;/u&gt;

	- 예시 코드에서는 이름을 키로 하여 나이를 합산

1.  `sortBy()`

	-  `sortBy()`  주어진 키 함수의 결과에 따라 요소를 정렬한 &lt;u&gt;새로운 RDD를 생성 &lt;/u&gt;

	- 예시 코드에서는 나이를 기준으로 내림차순 정렬

&lt;br/&gt;
</code></pre></div></div> <ol> <li> <p><strong>Action</strong>:</p> <p><strong>#### Big Picture</strong></p> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">result</span> <span class="o">=</span> <span class="n">filtered_customers</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>이제 `collect()` 동작이 호출되면서, `filter` 변환에 의해 정의된 모든 ‘***tranformation’ ***연산이 실행, 이후 그 결과가 반환
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### LOCAL 노드 환경 가정
</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Bob</span><span class="sh">"</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Charlie</span><span class="sh">"</span><span class="p">,</span> <span class="mi">35</span><span class="p">),</span> <span class="p">(</span><span class="sh">"</span><span class="s">Alice</span><span class="sh">"</span><span class="p">,</span> <span class="mi">40</span><span class="p">)]</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">All elements of the rdd: </span><span class="sh">"</span><span class="p">,</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">collect</span><span class="p">())</span>

<span class="n">count</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">count</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">The total number of elements in rdd: </span><span class="sh">"</span><span class="p">,</span> <span class="n">count</span><span class="p">)</span>

<span class="n">first_element</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">first</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">The first element of the rdd: </span><span class="sh">"</span><span class="p">,</span> <span class="n">first_element</span><span class="p">)</span>

<span class="n">taken_elements</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">take</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">The first two elements of the rdd: </span><span class="sh">"</span><span class="p">,</span> <span class="n">taken_elements</span><span class="p">)</span>
<span class="c1">#The first two elements of the rdd:  [('Alice', 25), ('Bob', 30)]
</span>
<span class="n">rdd</span><span class="p">.</span><span class="nf">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. `collect()`

	- `collect()`  RDD에 포함된 모든 요소를 드라이버 프로그램(여기서는 사용자의 Local 머신)으로 반환

	-  collect()는 RDD 전체를 메모리에 로드하므로, 큰 데이터셋에서는 메모리 오버플로를 발생 가능

1.  `count()`

	- `count()` 함수는 RDD에 포함된 요소의 총 수를 count해서 반환

	- 예시 코드에서는 `rdd.count()`를 호출하여 데이터의 총 수를 계산하고 그 결과를 출력

1.  `first()`

	- `first()` 함수는 RDD의 첫 번째 요소를 반환

	- 예시 코드에서는 `rdd.first()`를 사용하여 첫 번째 데이터 요소를 검색하고 그 결과를 출력

1. `take(n)`

	- `take(n)` 함수는 RDD에서 처음 `n`개의 요소를 반환 (상위 `n`개 데이터 샘플을 보고자 할 때 사용)

	- 예시 코드에서는 `rdd.take(2)`를 호출하여 상위 2개 데이터 요소를 반환

1. `foreach()`

	- `foreach()` 함수는 RDD의 각 요소에 대해 지정된 함수를 실행 (RDD의 각 요소에 대해 부작용(side effect)를 가진 작업을 수행할 때 사용 (예: 데이터베이스에 저장, 출력 등))

	- 예시 코드에서는 `rdd.foreach(lambda x: print(x))`를 사용하여 RDD의 모든 요소를 출력

&lt;br/&gt;
</code></pre></div></div> <h3 id="additional-operations">Additional Operations</h3> <ul> <li> <p><code class="language-plaintext highlighter-rouge">saveAsTextFile()</code></p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">saveAsTextFile("output.txt")</code> RDD의 내용을 외부 파일 시스템에 텍스트 파일 형식으로 저장</p> </li> <li> <p>로컬 파일 시스템, HDFS(Hadoop Distributed File System), S3 같은 클라우드 스토리지 등 다양한 파일 시스템을 지원</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">output.txt</code>라는 이름으로 저장하면, 실제로는 <code class="language-plaintext highlighter-rouge">output.txt/part-00000</code>, <code class="language-plaintext highlighter-rouge">output.txt/part-00001</code> 등의 형태로 여러 파일에 걸쳐 저장 (RDD가 분산되어 처리되기 때문)</p> </li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">textFile()</code></p> <ul> <li> <code class="language-plaintext highlighter-rouge">spark.sparkContext.textFile("output.txt")</code> 텍스트 파일로부터 새로운 RDD를 생성. 파일의 각 라인을 RDD의 한 요소로 로드</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rdd_text</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="sh">"</span><span class="s">output.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">rdd_text</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>

</code></pre></div></div> <p><br></p> <h2 id="4-dataframe-in-spark">4. DataFrame in Spark</h2> <h3 id="dataframes-in-apache-spark"><strong>DataFrames in Apache Spark</strong></h3> <ul> <li> <p>Spark DataFrame은 분산 데이터 컬렉션으로, 구조화된 데이터를 테이블 형태로 저장</p> </li> <li> <p>각 컬럼에는 이름과 데이터 타입이 정의되어 있어 SQL 데이터베이스의 테이블과 유사</p> </li> <li> <p>RDD보다 high-level operation 수행가능, 직관적</p> </li> </ul> <p><br></p> <p><strong>#### DataFrame Structure</strong></p> <ul> <li> <p>각 행은 데이터 레코드(데이터 인스턴스 1개)를 나타내며, 열은 해당 레코드의 특정 필드(속성)를 나타냄</p> </li> <li> <p>사실상 우리가 평소에 접하는 pandas dataframe가 유사, spark에서도 SQL 쿼리를 실행하는 것처럼, DataFrame을 사용하여 데이터에 대한 쿼리, 필터링, 집계 등을 수행 가능</p> </li> </ul> <p><br></p> <p><strong>#### Schema Information</strong></p> <ul> <li> <p><strong>스키마의 역할</strong>: DataFrame의 스키마는 각 컬럼의 이름과 데이터 타입 정보를 포함 (SQL배울때의 그 스키마와 사실상 동일함…)</p> </li> <li> <p>스키마는 데이터를 읽고 쓰는 동안 타입 안정성을 보장하고, SQL 쿼리 및 데이터 처리 작업의 최적화</p> </li> </ul> <p><br></p> <p><strong>#### Advantages of DataFrames</strong></p> <ul> <li> <p>Optimized Execution</p> <ul> <li> <p><strong>Schema Information</strong></p> <ul> <li>DataFrames의 형태이기에 각 컬럼의 데이터 유형을 미리 알고 있음. <u>_따라서 더 효율적인 데이터 처리와 쿼리 최적화가 가능. 스키마 정보는 query planner에 의해 사용되어 더 빠르고 효율적인 실행 계획을 생성._</u> </li> </ul> </li> <li> <p><strong>Predicate Pushdown</strong></p> <ul> <li>쿼리의 필터링 조건을 가능한 한 데이터 소스에 가깝게 적용하여 불필요한 데이터의 처리와 이동을 줄이는 최적화도 가능하게 함. 예를 들어, 데이터베이스에서 데이터를 가져올 때 필요한 데이터만 추출하여 전송량을 줄이고 처리 속도를 높임.</li> </ul> </li> </ul> </li> <li> <p>Ease of Use</p> <ul> <li> <p><strong>SQL-like Interface (사용편의성)</strong></p> </li> <li> <p><strong>Simplified API</strong></p> <ul> <li> <strong>API 단순화</strong>: 복잡한 RDD 변환과 액션 대신, DataFrames API는 직관적이고 선언적인 데이터 조작을 지원 → 사실상 사용자 입장에서 가장 큰 장점 중 하나</li> </ul> </li> </ul> </li> <li> <p>Integration with Ecosystem</p> <ul> <li> <p><span style="color:red"><strong>Seamless Integration</strong></span></p> <ul> <li>Spark SQL, MLLib, GraphX 등 Spark의 다른 라이브러리들과의 원활한 통합을 통해, 복잡한 데이터 파이프라인을 구축할 때 일관된 API를 사용 가능 → 마찬가지로 사용자 입장에서 가장 큰 장점 중 하나</li> </ul> </li> </ul> </li> <li> <p>Built-in Optimization</p> <ul> <li> <p><strong>Catalyst Optimizer</strong></p> <ul> <li>Spark의 고급 최적화 엔진인 Catalyst는 실행 계획을 동적으로 컴파일하고 최적화하여 실행 성능을 크게 향상.</li> </ul> </li> </ul> </li> <li> <p>Interoperability</p> <ul> <li> <p><span style="color:red"><strong>Data Format Conversion</strong></span></p> <ul> <li>DataFrames는 다양한 데이터 소스와 포맷(Pandas DataFrames, Parquet, JSON 등)으로부터 쉽게 데이터를 읽고 쓸 수 있으며, 다른 데이터 처리 도구와의 연동이 용이 → 전처리하는 입장에서 가장 큰 장점 중 하나</li> </ul> </li> </ul> </li> </ul> <p><br></p> <h3 id="difference-from-pandas-dataframes-mutable-vs-immutable-dataframes">Difference from Pandas Dataframes (Mutable vs. Immutable DataFrames)</h3> <ul> <li> <p>많은 사람들이 처음 데이터를 접할때 배우는 Pandas의 Dataframe과 Spark의 DataFrame은 어떻게 다를까?</p> </li> <li> <p><strong>pandas DataFrame</strong></p> <ul> <li> <p>pandas에서의 DataFrame은 mutable(가변)</p> </li> <li> <p>즉, 데이터 프레임 내의 데이터를 직접 변경할 수 있음, 대부분의 사용자가 경험해보았듯이 데이터를 조작하고 업데이트할 때 매우 유연한 구조를 가지고 있음</p> </li> <li> <p>method에 <code class="language-plaintext highlighter-rouge">inplace</code> 매개변수를 제공하여, 원본 데이터 프레임을 직접 수정할지 여부를 사용자가 선택할 수 있음</p> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">df_pandas</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span>
    <span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
    <span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">df_pandas</span><span class="p">[</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_pandas</span><span class="p">[</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">]</span> <span class="o">+</span> <span class="n">df_pandas</span><span class="p">[</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">]</span>

<span class="n">df_pandas</span><span class="p">.</span><span class="nf">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Alpha</span><span class="sh">'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df_pandas</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
   Alpha  B  C
0      1  4  5
1      2  5  7
2      3  6  9
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li> <p><strong>Spark DataFrame</strong></p> <ul> <li> <p>Spark에서의 DataFrame은 immutable(불변)</p> </li> <li> <p><u>즉, 한 번 생성되면 그 내용을 변경할 수 없으며, 데이터에 변형을 가하고자 할 때는 새로운 DataFrame이 생성 </u></p> </li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">example</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="n">df_spark</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="nf">createDataFrame</span><span class="p">([</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="p">],</span> <span class="p">[</span><span class="sh">"</span><span class="s">A</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">])</span>

<span class="n">new_df_spark</span> <span class="o">=</span> <span class="n">df_spark</span><span class="p">.</span><span class="nf">withColumn</span><span class="p">(</span><span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">,</span> <span class="nf">col</span><span class="p">(</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span> <span class="o">+</span> <span class="nf">col</span><span class="p">(</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">))</span>

<span class="n">final_df_spark</span> <span class="o">=</span> <span class="n">new_df_spark</span><span class="p">.</span><span class="nf">withColumnRenamed</span><span class="p">(</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Alpha</span><span class="sh">'</span><span class="p">)</span>

<span class="n">final_df_spark</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="sh">'''</span><span class="s">
+-----+---+---+
|Alpha|  B|  C|
+-----+---+---+
|    1|  4|  5|
|    2|  5|  7|
|    3|  6|  9|
+-----+---+---+
</span><span class="sh">'''</span>

</code></pre></div></div> <p><br></p> <h3 id="example-rdd-vs-dataframe">Example (RDD vs DataFrame)</h3> <ul> <li>txt file에서 word count를 하고 most frequent word를 찾는 example</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">from</span> <span class="n">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">desc</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">DataFrame-Demo</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

</code></pre></div></div> <p><strong>#### RDD</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/data.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result_rdd</span> <span class="o">=</span> <span class="n">rdd</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">))</span> \
                <span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">word</span><span class="p">:</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> \
                <span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> \
                <span class="p">.</span><span class="nf">sortBy</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">**spark.sparkContext.textFile**</code>: 파일 시스템에서 텍스트 파일을 읽어서 RDD를 생성. 각 줄은 RDD의 요소로 변환 (각 line을 index하나로 간주한다고 생각하면 좋을듯…?)</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**flatMap(lambda line: line.split(" "))**</code>: 각 줄을 공백으로 분리하여 단어를 추출</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**flatMap**</code>은 각 입력 요소에 대해 여러 개의 출력을 생성할 수 있으며, 이 모든 출력을 단일 RDD로 평탄화합니다.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**map(lambda word: (word, 1))**</code>: 각 단어를 <code class="language-plaintext highlighter-rouge">(단어, 1)</code>의 tuple 형식으로 매핑</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**reduceByKey(lambda a, b: a + b)**</code>: 같은 키(단어)를 가진 값들을 합산 → 각 단어의 출현 횟수를 계산</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**sortBy(lambda x: x[1], ascending=False)**</code>: 계산된 단어의 빈도 수에 따라 내림차순으로 정렬</p> </li> </ul> <p><strong>#### DataFrame</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="sh">"</span><span class="s">./data/data.txt</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">selectExpr</span><span class="p">(</span><span class="sh">"</span><span class="s">explode(split(value, </span><span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="s">)) as word</span><span class="sh">"</span><span class="p">)</span> \
              <span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">"</span><span class="s">word</span><span class="sh">"</span><span class="p">)</span> \
              <span class="p">.</span><span class="nf">count</span><span class="p">()</span> \
              <span class="p">.</span><span class="nf">orderBy</span><span class="p">(</span><span class="nf">desc</span><span class="p">(</span><span class="sh">"</span><span class="s">count</span><span class="sh">"</span><span class="p">))</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">**spark.read.text**</code>: 파일 시스템에서 텍스트 파일을 읽어 DataFrame을 생성. 각 line은 DataFrame의 ‘value’라는 이름의 컬럼에 저장</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**selectExpr("explode(split(value, ' ')) as word")**</code>: <code class="language-plaintext highlighter-rouge">selectExpr</code>은 SQL 표현식을 사용해 데이터를 변환 → ‘value’를 공백으로 분리하고, <code class="language-plaintext highlighter-rouge">explode</code> 함수를 사용해 각 단어를 별도의 row로 확장</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**groupBy("word")**</code>: ‘word’ 컬럼의 값에 따라 데이터를 그룹화</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**count()**</code>: 각 그룹의 데이터 수를 세어 ‘count’ 컬럼에 저장 → 각 단어의 출현 횟수를 계산</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**orderBy(desc("count"))**</code>: ‘count’ 컬럼을 기준으로 내림차순으로 정렬</p> </li> </ul> <p><span style="color:red">**→ RDD와 마찬가지로 당연히 DataFrame도 Lazy Evaluation의 작동 원리에 따라 Action( **</span><span style="color:red"><code class="language-plaintext highlighter-rouge">**count**</code></span><span style="color:red">**, **</span><span style="color:red"><code class="language-plaintext highlighter-rouge">**collect**</code></span><span style="color:red">**, **</span><span style="color:red"><code class="language-plaintext highlighter-rouge">**take**</code></span><span style="color:red">** ) 호출하는 순간, Spark는 필요한 데이터만을 계산하여 자원 사용과 처리 시간을 최적화 **</span></p> <p><br></p> <h3 id="example-create-spark-dataframe-from-csv">Example (Create Spark DataFrame from CSV)</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Read CSV with header
</span>
<span class="n">csv_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data/products.csv</span><span class="sh">"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">csv_file_path</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">df</span><span class="p">.</span><span class="nf">printSchema</span><span class="p">()</span>

<span class="n">df</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
root
 |-- id: string (nullable = true)
 |-- name: string (nullable = true)
 |-- category: string (nullable = true)
 |-- quantity: string (nullable = true)
 |-- price: string (nullable = true)

+---+--------------------+---------------+--------+------+
| id|                name|       category|quantity| price|
+---+--------------------+---------------+--------+------+
|  1|           iPhone 12|    Electronics|      10|899.99|
|  2|     Nike Air Max 90|       Clothing|      25|119.99|
|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|
|  4|    The Great Gatsby|          Books|      50| 12.99|
|  5|L</span><span class="sh">'</span><span class="s">Oreal Paris Mas...|         Beauty|     100|  9.99|
+---+--------------------+---------------+--------+------+
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li>Spark로 csv file을 불러올 수 있으나 schema를 제대로 읽지 못하는 것을 확인할 수 있음</li> </ul> <p><br></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Read CSV with an explicit schema definition
</span>
<span class="kn">from</span> <span class="n">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StructType</span><span class="p">,</span> <span class="n">StructField</span><span class="p">,</span> <span class="n">StringType</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="n">DoubleType</span>
<span class="n">schema</span> <span class="o">=</span> <span class="nc">StructType</span><span class="p">([</span>
    <span class="nc">StructField</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataType</span><span class="o">=</span><span class="nc">IntegerType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="nc">StructField</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataType</span><span class="o">=</span><span class="nc">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="nc">StructField</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">category</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataType</span><span class="o">=</span><span class="nc">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="nc">StructField</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">quantity</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataType</span><span class="o">=</span><span class="nc">IntegerType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="nc">StructField</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">price</span><span class="sh">"</span><span class="p">,</span> <span class="n">dataType</span><span class="o">=</span><span class="nc">DoubleType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">csv_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data/products.csv</span><span class="sh">"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">csv_file_path</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">printSchema</span><span class="p">()</span>

<span class="n">df</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- category: string (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- price: double (nullable = true)

+---+--------------------+---------------+--------+------+
| id|                name|       category|quantity| price|
+---+--------------------+---------------+--------+------+
|  1|           iPhone 12|    Electronics|      10|899.99|
|  2|     Nike Air Max 90|       Clothing|      25|119.99|
|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|
|  4|    The Great Gatsby|          Books|      50| 12.99|
|  5|L</span><span class="sh">'</span><span class="s">Oreal Paris Mas...|         Beauty|     100|  9.99|
+---+--------------------+---------------+--------+------+
only showing top 5 rows
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li>명시적 schema를 제공함으로써 제대로된 데이터타입을 가질 수 있도록 할 수 있음</li> </ul> <p><br></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### Read CSV file into DataFrame with inferSchema
</span><span class="n">csv_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data/products.csv</span><span class="sh">"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">csv_file_path</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">printSchema</span><span class="p">()</span>

<span class="n">df</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
root
 |-- id: integer (nullable = true)
 |-- name: string (nullable = true)
 |-- category: string (nullable = true)
 |-- quantity: integer (nullable = true)
 |-- price: double (nullable = true)

+---+--------------------+---------------+--------+------+
| id|                name|       category|quantity| price|
+---+--------------------+---------------+--------+------+
|  1|           iPhone 12|    Electronics|      10|899.99|
|  2|     Nike Air Max 90|       Clothing|      25|119.99|
|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|
|  4|    The Great Gatsby|          Books|      50| 12.99|
|  5|L</span><span class="sh">'</span><span class="s">Oreal Paris Mas...|         Beauty|     100|  9.99|
+---+--------------------+---------------+--------+------+
only showing top 5 rows
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li>inferSchema=True를 추가해서 spark가 자동으로 datatype을 추적하도록 할 수 있음.</li> </ul> <p><br></p> <h3 id="transformations--actions">Transformations &amp; Actions</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="nf">appName</span><span class="p">(</span><span class="sh">"</span><span class="s">DataFrame-Operations</span><span class="sh">"</span><span class="p">).</span><span class="nf">getOrCreate</span><span class="p">()</span>

<span class="n">data_file_path</span> <span class="o">=</span> <span class="sh">"</span><span class="s">./data/stocks.txt</span><span class="sh">"</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nf">csv</span><span class="p">(</span><span class="n">data_file_path</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inferSchema</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">selected_columns</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="sh">"</span><span class="s">id</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">price</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Selected Columns:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">selected_columns</span><span class="p">.</span><span class="nf">show</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>

<span class="sh">'''</span><span class="s">
Selected Columns:
+---+----------------+-------+
| id|            name|  price|
+---+----------------+-------+
|  1|          iPhone| 899.99|
|  2|         Macbook|1299.99|
|  3|            iPad| 499.99|
|  4|      Samsung TV| 799.99|
|  5|           LG TV| 699.99|
|  6|      Nike Shoes|  99.99|
|  7|    Adidas Shoes|  89.99|
|  8| Sony Headphones| 149.99|
|  9|Beats Headphones| 199.99|
| 10|    Dining Table| 249.99|
+---+----------------+-------+
only showing top 10 rows
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">**select**</code>: 표현식 집합을 선택하고 새로운 DataFrame을 반환</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">filtered_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">quantity</span> <span class="o">&gt;</span> <span class="mi">20</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Filtered Data:</span><span class="sh">"</span><span class="p">,</span> <span class="n">filtered_data</span><span class="p">.</span><span class="nf">count</span><span class="p">())</span>
<span class="n">filtered_data</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="sh">'''</span><span class="s">
Filtered Data: 12
+---+--------------+-----------+--------+-----+
| id|          name|   category|quantity|price|
+---+--------------+-----------+--------+-----+
|  6|    Nike Shoes|   Clothing|      30|99.99|
|  7|  Adidas Shoes|   Clothing|      25|89.99|
| 12|        Apples|       Food|     100|  0.5|
| 13|       Bananas|       Food|     150| 0.25|
| 14|       Oranges|       Food|     120| 0.75|
| 15|Chicken Breast|       Food|      50| 3.99|
| 16| Salmon Fillet|       Food|      30| 5.99|
| 24|    Laptop Bag|Accessories|      25|29.99|
| 25|      Backpack|Accessories|      30|24.99|
| 28|         Jeans|   Clothing|      30|59.99|
| 29|       T-shirt|   Clothing|      50|14.99|
| 30|      Sneakers|   Clothing|      40|79.99|
+---+--------------+-----------+--------+-----+
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">**filter**</code>: 주어진 조건을 만족하는 행만 필터링</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">grouped_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">groupBy</span><span class="p">(</span><span class="sh">"</span><span class="s">category</span><span class="sh">"</span><span class="p">).</span><span class="nf">agg</span><span class="p">({</span><span class="sh">"</span><span class="s">quantity</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">sum</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">price</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">avg</span><span class="sh">"</span><span class="p">})</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Grouped and Aggregated Data:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">grouped_data</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="sh">'''</span><span class="s">
Grouped and Aggregated Data:
+-----------+-------------+------------------+
|   category|sum(quantity)|        avg(price)|
+-----------+-------------+------------------+
|       Food|          450|2.2960000000000003|
|     Sports|           35|             34.99|
|Electronics|           98| 586.6566666666665|
|   Clothing|          200|  99.2757142857143|
|  Furniture|           41|            141.99|
|Accessories|           55|             27.49|
+-----------+-------------+------------------+
</span><span class="sh">'''</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">**groupBy**</code>: 지정된 열을 기준으로 DataFrame을 그룹화</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**agg**</code>: 그룹화된 데이터에 대해 집계 함수(sum, avg)를 수행</p> </li> </ul> <p><br></p> <p>… 이외에도 <code class="language-plaintext highlighter-rouge">**join**</code>** , **<code class="language-plaintext highlighter-rouge">**orderBy**</code>** **등의 transformation 적용 가능</p> <p><br></p> <h2 id="5-code-review---deduplication-data-with-pyspark-nlp">5. Code Review - Deduplication Data with PySpark (NLP)</h2> <ul> <li> <p>LLM Training을 위해서는 대용량 Corpus 수집 후 전처리가 필수적</p> <p>(실제로 NLP Researcher들이 Spark를 가장 많이 활용할 부분)</p> </li> <li> <p>polyglot 모델 학습시에 Deduplication, 전처리코드 활용되었던 레포지토리내의 Deduplication 코드를 상세하게 리뷰해보면서 PySpark가 어떻게 사용되는지 이해를 높혀보고자 한다!</p> </li> <li> <p>아래는 dps/spark/jobs/dedup_job.py 전문</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Run this from project root path

python bin/sparkapp.py dedup_job --config_path=./configs/dedup_job.yaml
</span><span class="sh">"""</span>

<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">combinations</span>

<span class="kn">import</span> <span class="n">yaml</span>
<span class="kn">from</span> <span class="n">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>
<span class="kn">from</span> <span class="n">pyspark.rdd</span> <span class="kn">import</span> <span class="n">RDD</span>

<span class="kn">from</span> <span class="n">dps.spark.spark_session</span> <span class="kn">import</span> <span class="n">spark_session</span>
<span class="kn">from</span> <span class="n">dps.spark.utils.io_utils</span> <span class="kn">import</span> <span class="n">read_line</span><span class="p">,</span> <span class="n">to_json</span>
<span class="kn">from</span> <span class="n">dps.spark.prep.dedup_prep</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">shingle_word</span><span class="p">,</span>
    <span class="n">generate_minhash</span><span class="p">,</span>
    <span class="n">jaccard_by_hashvalues</span><span class="p">,</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">expand_instances_by_minhash</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">expand_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_gram</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">char_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">):</span>
    <span class="n">shingles</span> <span class="o">=</span> <span class="nf">shingle_word</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">n_gram</span><span class="o">=</span><span class="n">n_gram</span><span class="p">,</span> <span class="n">char_level</span><span class="o">=</span><span class="n">char_level</span><span class="p">)</span>
    <span class="n">minhashes</span> <span class="o">=</span> <span class="nf">generate_minhash</span><span class="p">(</span><span class="n">shingles</span><span class="p">,</span> <span class="n">num_perm</span><span class="o">=</span><span class="n">expand_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">mh</span> <span class="ow">in</span> <span class="n">minhashes</span><span class="p">.</span><span class="nf">tolist</span><span class="p">():</span>
        <span class="nf">yield </span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">mh</span><span class="p">),</span> <span class="p">[</span><span class="nf">dict</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">,</span> <span class="n">shingles</span><span class="o">=</span><span class="n">shingles</span><span class="p">,</span> <span class="n">hashvalues</span><span class="o">=</span><span class="n">minhashes</span><span class="p">)])</span>

<span class="k">def</span> <span class="nf">explore_dedup_instance</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">group_represent_text</span> <span class="o">=</span> <span class="n">hash_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span>
        <span class="sh">"</span><span class="s">text</span><span class="sh">"</span>
    <span class="p">]</span>  <span class="c1"># not to remove all text instances in group.
</span>    <span class="n">pairs</span> <span class="o">=</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">d_1</span><span class="p">,</span> <span class="n">d_2</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">sim_score</span> <span class="o">=</span> <span class="nf">jaccard_by_hashvalues</span><span class="p">(</span><span class="n">d_1</span><span class="p">[</span><span class="sh">"</span><span class="s">hashvalues</span><span class="sh">"</span><span class="p">],</span> <span class="n">d_2</span><span class="p">[</span><span class="sh">"</span><span class="s">hashvalues</span><span class="sh">"</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">sim_score</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="n">dedup_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_1</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">d_2</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">group_represent_text</span> <span class="ow">in</span> <span class="n">dedup_text</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">dedup_text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">dedup_text</span><span class="p">[</span>
                    <span class="mi">0</span>
                <span class="p">]</span> <span class="o">!=</span> <span class="n">group_represent_text</span> <span class="k">else</span> <span class="n">dedup_text</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">dedup_text</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dedup_job</span><span class="p">(</span><span class="n">config_path</span><span class="p">):</span>
    <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">conf</span> <span class="o">=</span> <span class="n">yaml</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">Loader</span><span class="o">=</span><span class="n">yaml</span><span class="p">.</span><span class="n">FullLoader</span><span class="p">)</span>

    <span class="n">input_paths</span> <span class="o">=</span> <span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">base_dir</span><span class="sh">"</span><span class="p">]</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="sh">'</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">targets</span><span class="sh">"</span><span class="p">]])</span>

    <span class="k">with</span> <span class="nf">spark_session</span><span class="p">(</span><span class="sa">f</span><span class="sh">""</span><span class="p">)</span> <span class="k">as</span> <span class="n">spark</span><span class="p">:</span>
        <span class="n">sc</span><span class="p">:</span> <span class="n">SparkContext</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span>

        <span class="n">proc_rdd</span><span class="p">:</span> <span class="n">RDD</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">sc</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="n">input_paths</span><span class="p">)</span>
            <span class="p">.</span><span class="nf">repartition</span><span class="p">(</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_dist</span><span class="sh">"</span><span class="p">])</span>
            <span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="n">read_line</span><span class="p">)</span>
            <span class="p">.</span><span class="nf">cache</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">overlap_kv_rdd</span><span class="p">:</span> <span class="n">RDD</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">proc_rdd</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">expand_instances_by_minhash</span><span class="p">(</span>
                    <span class="n">x</span><span class="p">,</span>
                    <span class="n">expand_size</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">num_expand</span><span class="sh">"</span><span class="p">],</span>
                    <span class="n">n_gram</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_gram</span><span class="sh">"</span><span class="p">],</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">],</span>
                    <span class="n">char_level</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">char_level</span><span class="sh">"</span><span class="p">],</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
            <span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">explore_dedup_instance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">threshold</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">sim_threshold</span><span class="sh">"</span><span class="p">])</span>
            <span class="p">)</span>
            <span class="p">.</span><span class="nf">distinct</span><span class="p">()</span>
            <span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nf">dict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">x</span><span class="p">)))</span>
            <span class="p">.</span><span class="nf">cache</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="n">proc_rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">x</span><span class="p">)).</span><span class="nf">subtractByKey</span><span class="p">(</span><span class="n">overlap_kv_rdd</span><span class="p">).</span><span class="nf">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">).</span><span class="nf">repartition</span><span class="p">(</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_output</span><span class="sh">"</span><span class="p">]).</span><span class="nf">flatMap</span><span class="p">(</span><span class="n">to_json</span><span class="p">).</span><span class="nf">saveAsTextFile</span><span class="p">(</span>
            <span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">output_dir</span><span class="sh">"</span><span class="p">]</span>
        <span class="p">)</span>

</code></pre></div></div> <p><br></p> <p><strong>#### Function</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">expand_instances_by_minhash</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">expand_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_gram</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">char_level</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span>
<span class="p">):</span>
    <span class="n">shingles</span> <span class="o">=</span> <span class="nf">shingle_word</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">n_gram</span><span class="o">=</span><span class="n">n_gram</span><span class="p">,</span> <span class="n">char_level</span><span class="o">=</span><span class="n">char_level</span><span class="p">)</span>
    <span class="n">minhashes</span> <span class="o">=</span> <span class="nf">generate_minhash</span><span class="p">(</span><span class="n">shingles</span><span class="p">,</span> <span class="n">num_perm</span><span class="o">=</span><span class="n">expand_size</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">mh</span> <span class="ow">in</span> <span class="n">minhashes</span><span class="p">.</span><span class="nf">tolist</span><span class="p">():</span>
        <span class="nf">yield </span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">mh</span><span class="p">),</span> <span class="p">[</span><span class="nf">dict</span><span class="p">(</span><span class="o">**</span><span class="n">data</span><span class="p">,</span> <span class="n">shingles</span><span class="o">=</span><span class="n">shingles</span><span class="p">,</span> <span class="n">hashvalues</span><span class="o">=</span><span class="n">minhashes</span><span class="p">)])</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">shingle_word</code>: data를 n-gram 또는 문자 단위로 분할</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">generate_minhash</code>: 분할된 data로부터 MinHash 값을 생성</p> <ul> <li>실제로 저 함수는 hash_values = np.array([])라는 것을 반환</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">expand_instances_by_minhash</code>: 를 거치면 각 데이터는 하나의 min_hash value로 mapping됨 (yield (str(mh), [dict(**data, shingles=shingles, hashvalues=minhashes)]))</p> </li> </ul> <p><strong>⇒ Example</strong></p> <ul> <li> <p>text: “안녕하세요 여러분 안녕하세요”</p> </li> <li> <p>n_gram: 2,</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">shingle_word</code>: [“안녕하세요_여러분”, “여러분_안녕하세요”]</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">minhases</code>: [11,23, … ]</p> </li> </ul> <p><br></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">explore_dedup_instance</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">,</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">):</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="n">group_represent_text</span> <span class="o">=</span> <span class="n">hash_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="nf">combinations</span><span class="p">(</span><span class="n">hash_groups</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">d_1</span><span class="p">,</span> <span class="n">d_2</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">sim_score</span> <span class="o">=</span> <span class="nf">jaccard_by_hashvalues</span><span class="p">(</span><span class="n">d_1</span><span class="p">[</span><span class="sh">"</span><span class="s">hashvalues</span><span class="sh">"</span><span class="p">],</span> <span class="n">d_2</span><span class="p">[</span><span class="sh">"</span><span class="s">hashvalues</span><span class="sh">"</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">sim_score</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="n">dedup_text</span> <span class="o">=</span> <span class="p">[</span><span class="n">d_1</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">d_2</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">group_represent_text</span> <span class="ow">in</span> <span class="n">dedup_text</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">dedup_text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">dedup_text</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">group_represent_text</span> <span class="k">else</span> <span class="n">dedup_text</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">dedup_text</span><span class="p">)</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">combinations</code>: 가능한 모든 텍스트 쌍을 생성</p> <ul> <li>이때 (<a href="https://www.notion.so/ab77c70d558e4dbbade6bf99de13c937]" rel="external nofollow noopener" target="_blank">https://www.notion.so/ab77c70d558e4dbbade6bf99de13c937</a> 에서 생성한 str(mh)끼리만 combinations를 생성 (그룹 내의 모든 데이터 쌍의 조합을 생성)</li> </ul> </li> <li> <p><code class="language-plaintext highlighter-rouge">jaccard_by_hashvalues</code>: 위에서 생성한 combinations 텍스트 사이의 유사도를 계산</p> </li> <li> <p>theshold가 높을 경우 두 instance 중 하나만 반환</p> </li> </ul> <p><br></p> <p><strong>#### Spark</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proc_rdd</span><span class="p">:</span> <span class="n">RDD</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">sc</span><span class="p">.</span><span class="nf">textFile</span><span class="p">(</span><span class="n">input_paths</span><span class="p">)</span>
    <span class="p">.</span><span class="nf">repartition</span><span class="p">(</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_dist</span><span class="sh">"</span><span class="p">])</span>
    <span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span><span class="n">read_line</span><span class="p">)</span>
    <span class="p">.</span><span class="nf">cache</span><span class="p">()</span>
<span class="p">)</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">sc.textFile</code>: 지정된 경로에서 텍스트 파일을 읽어 RDD를 생성</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">repartition</code>: <code class="language-plaintext highlighter-rouge">n_dist</code> 설정에 따라 RDD의 파티션 수를 조정하여 데이터 분포를 최적화</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">flatMap</code>: <code class="language-plaintext highlighter-rouge">read_line</code> 함수를 각 입력 라인에 적용하여 각 줄의 데이터를 단일 요소로 읽음</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">cache</code>: RDD를 메모리에 캐시하여 다중 작업에서 성능을 향상</p> </li> </ul> <p><br></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">overlap_kv_rdd</span><span class="p">:</span> <span class="n">RDD</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">proc_rdd</span><span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">expand_instances_by_minhash</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span>
            <span class="n">expand_size</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">num_expand</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">n_gram</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_gram</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">seed</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">char_level</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">char_level</span><span class="sh">"</span><span class="p">],</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="p">.</span><span class="nf">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
    <span class="p">.</span><span class="nf">flatMap</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">explore_dedup_instance</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">threshold</span><span class="o">=</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">sim_threshold</span><span class="sh">"</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="p">.</span><span class="nf">distinct</span><span class="p">()</span>
    <span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nf">dict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">x</span><span class="p">)))</span>
    <span class="p">.</span><span class="nf">cache</span><span class="p">()</span>
<span class="p">)</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">expand_instances_by_minhash</code>: 각 데이터 인스턴스에 대해 (<a href="https://www.notion.so/2abe1c7400cc4dc886ac911a2af91415]" rel="external nofollow noopener" target="_blank">https://www.notion.so/2abe1c7400cc4dc886ac911a2af91415</a> 적용</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">reduceByKey</code>: 동일한 키를 가진 데이터를 합침 → 데이터를 그룹화가 일어나기 때문에 str(mh)별로 묶임</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">explore_dedup_instance</code>: 설정된 유사도 임계값을 기반으로 중복을 식별하고 제거</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">distinct</code>: 중복된 결과를 제거</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">map</code>: 각 결과에 대한 추가적인 데이터 구조를 생성</p> <p>(예시: [(“apple”, {‘text’: ‘apple’}), (“banana”, {‘text’: ‘banana’}), (“cherry”, {‘text’: ‘cherry’})])</p> </li> </ul> <p>⇒ 위 코드는 중복된 instance/RDD를 찾아내는 함수</p> <p><br></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">proc_rdd</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">],</span> <span class="n">x</span><span class="p">)).</span><span class="nf">subtractByKey</span><span class="p">(</span><span class="n">overlap_kv_rdd</span><span class="p">).</span><span class="nf">map</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="p">).</span><span class="nf">repartition</span><span class="p">(</span><span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">n_output</span><span class="sh">"</span><span class="p">]).</span><span class="nf">flatMap</span><span class="p">(</span><span class="n">to_json</span><span class="p">).</span><span class="nf">saveAsTextFile</span><span class="p">(</span>
    <span class="n">conf</span><span class="p">[</span><span class="sh">"</span><span class="s">output_dir</span><span class="sh">"</span><span class="p">]</span>
<span class="p">)</span>

</code></pre></div></div> <ul> <li> <p><code class="language-plaintext highlighter-rouge">subtractByKey</code>: 중복된 데이터를 제거, 즉 <code class="language-plaintext highlighter-rouge">overlap_kv_rdd</code>에 있는 키를 가진 데이터는 제거</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">repartition</code>: 결과 데이터의 파티션 수를 조절</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">flatMap(to_json)</code>: 결과 데이터를 JSON 형식으로 변환</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">saveAsTextFile</code>: 최종 결과를 텍스트 파일로 저장</p> </li> </ul> <p><br></p> <h2 id="6-references">6. References</h2> <p><br></p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/al-folio/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/al-folio/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/al-folio/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/al-folio/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/al-folio/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/al-folio/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/al-folio/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/al-folio/assets/js/search-data.js"></script> <script src="/al-folio/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>