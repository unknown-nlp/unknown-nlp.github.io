<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Train LLM? - From Data Parallel To Fully Sharded Data Parallel | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content=" 논문 리뷰 - How to Train LLM? - From Data Parallel To Fully Sharded Data Parallel"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/how-to-train-llm-from-data-parallel-to-fully-sharded-data-parallel/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How to Train LLM? - From Data Parallel To Fully Sharded Data Parallel</h1> <p class="post-meta"> Created on May 07, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-05-07</li> <li> <strong>Reviewer</strong>: 준원 장</li> <li> <strong>Property</strong>: LLM, Pre-Training, torch</li> </ul> <h3 id="-본-내용에-잘못된-내용이-있을-수도-있으니-혹시나-잘못된-부분-있다면-언제든-지적해주시면-감사하겠습니다">! 본 내용에 잘못된 내용이 있을 수도 있으니, 혹시나 잘못된 부분 있다면 언제든 지적해주시면 감사하겠습니다!</h3> <h2 id="0-floating-point">0. Floating Point</h2> <ul> <li> <p>컴퓨터에서 실수를 표현하는 방법으로, 수를 Exponent와 Fraction로 분리하여 표현.</p> </li> <li> <p>IEEE 754 표준은 floating 포인트 수를 표현하는 데 널리 사용되는 표준. 해당 표준에서는 다양한 정밀도를 제공.</p> </li> <li> <p>floating exampling</p> <ul> <li> <p>십진수 -0.5625</p> </li> <li> <p>이진수 -0.1001</p> <ul> <li>→ -1.001 X x 2^(-1)</li> </ul> </li> <li> <p><span style="color:red"><strong>Sign</strong></span>: 1 (음수는 1임)</p> </li> <li> <p><span style="color:green"><strong>Exponent</strong></span>: 127 + (-1) = 126</p> </li> <li> <p><span style="color:blue"><strong>Fraction</strong></span>: 0010 0…</p> </li> <li> <p>**fp32: **<span style="color:red">**1 **</span><span style="color:green"><strong>01111110</strong></span>** **<span style="color:blue">**00100000000000000000000**</span></p> </li> </ul> </li> </ul> <p><br></p> <ul> <li>FP16</li> </ul> <p>→ fp32 대비 Exponent와 Fraction을 둘다 줄여 DL모델의 inference시 연구자들이 주로 사용하는 precision. (일반적으로 fp32으로 모델학습, fp16으로 inference을 하나 실험적으로 fp32대비 분명한 성능하락 확인)</p> <p><br></p> <ul> <li>BF16</li> </ul> <p>→ BF16은 fp16과 다르게 자릿수를 대표하는 exponent 동일한 8비트로 설정함으로써 더 넓은 범위의 수를 표현할 수 있음.</p> <p>→ 대신 fraction의 자릿수를 희생했기 때문에 수의 정밀도가 떨어진다는 한계가 존재</p> <p>(ex)</p> <p>십진수 0.1을 이진수로 표현하면 무한 반복 이진수: <code class="language-plaintext highlighter-rouge">**0.0001100110011001100...**</code> (반복)</p> <ul> <li> <p>fp16</p> </li> <li> <p>fraction는 10비트를 사용하여 이 값을 가능한 한 정확하게 근사: <code class="language-plaintext highlighter-rouge">**0.0001100110**</code></p> </li> <li> <p>exponent는 이진 표현에서 소수점을 왼쪽으로 몇 칸 이동시켜야 하는지에 따라 결정되며, 이 경우 -4. exponent 필드 값은 −4+15=11 (이진수 <code class="language-plaintext highlighter-rouge">**01011**</code>).</p> </li> <li> <p>bf16</p> </li> <li> <p>fraction는 7bit를 사용하여 값을 근사: <code class="language-plaintext highlighter-rouge">**0.0001100**</code></p> </li> <li> <p>exponent는 FP16과 동일하게 -4이고, 지수 필드 값은 −4+127=123 (이진수 <code class="language-plaintext highlighter-rouge">**01111011**</code>).</p> </li> </ul> <p><br></p> <p><br></p> <ul> <li>결론</li> </ul> <p>⇒ Layer가 깊어질수록 logit값이 커지는 transformer계열에서는 BF16을 활용해 모델을 training하는 것이 적합해보임. input의 스케일이 크게 다른 경우, BF16은 underflow나 overflow를 방지할 수 있음.</p> <p>⇒ Normalization, Optimizer state 연산 (e.g., momentum) 등 term을 계속해서 나눠주는 연산의 경우 정밀도를 희생한 BF16은 오차를 유발할 수 있기 때문에 mixed precision 같은 방법론들을 도입해서 한계를 방지해야함.</p> <h2 id="1-mixed-precision">1. Mixed Precision</h2> <ul> <li> <p>Mixed Precision은 DL 학습 시 다양한 Precision을 혼합하여 사용하는 기술.</p> </li> <li> <p>대표적으로 NVIDIA의 Tensor Cores에서는 fp16과 fp32를 혼합하여 더 빠른 수행 시간과 높은 수치 정밀도를 제공</p> </li> <li> <p>fp16으로는 표현할 수 있는 숫자체계가 제한적이기 때문에 gradient update시에 underflow 문제 발생 가능</p> <p>→ 빨간선 왼쪽이 fp16으로 표현할 수 없는 범위</p> </li> </ul> <p>→fp16으로 tensor 연산 시의 문제점</p> <p><br></p> <ul> <li> <p>위를 해결하기 위해 Precision을 혼합해서 활용하는 Mixed Precision 기법이 나옴</p> <ul> <li>BWD-Actv &amp; BWD-Weight 계산 예시</li> </ul> <p><br></p> </li> <li> <p>가정: 우리의 Model은 fp32이다. 해당 모델이 가지고 있는 weights을 Master weights라고 명명.</p> </li> </ul> <ol> <li> <p>fp32로 표현된 Master weights을 복사하여 fp16 weights로 가져옴</p> </li> <li> <p>fp16 weights로 forward &amp; loss 계산</p> </li> <li> <p>loss값이 scaling factor를 곱한다.</p> <p>(fp16으로 표현된 상태에서 Gradient을 구하면 backprop 과정 중 underflow 발생 가능)</p> <p><strong>[FROM]</strong></p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>**[To]**
</code></pre></div> </div> <ol> <li>fp16으로 표현된 loss에서 backward (loss.backward()) → backpropagate 한다.</li> </ol> </li> <li> <p>upscaling한 gradient(XS)를 optimizer.step()을 위해서 다시 unscale (X1/S)해준다.</p> <ol> <li> <p>이때 nan value, 0 되는지 check</p> </li> <li> <p>Gradient clipping, weight decay 등을 적용</p> </li> <li> <p><strong>optimizer의 state term등은 전부 fp32 상태로 저장</strong></p> <p>(optimizer는 state initialization할때 default로 fp32를 사용)</p> </li> </ol> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># note(crcrpar): Deliberately host `step` on CPU if both capturable and fused are off.
</span>                <span class="c1"># This is because kernel launches are costly on CUDA and XLA.
</span>                <span class="n">state</span><span class="p">[</span><span class="sh">"</span><span class="s">step</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((),</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">_get_scalar_dtype</span><span class="p">(</span><span class="n">is_fused</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="sh">"</span><span class="s">fused</span><span class="sh">"</span><span class="p">]),</span> <span class="n">device</span><span class="o">=</span><span class="n">p</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="sh">"</span><span class="s">capturable</span><span class="sh">"</span><span class="p">]</span> <span class="ow">or</span> <span class="n">group</span><span class="p">[</span><span class="sh">"</span><span class="s">fused</span><span class="sh">"</span><span class="p">]</span>
                    <span class="k">else</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nf">_get_scalar_dtype</span><span class="p">())</span>
                <span class="p">)</span>

</code></pre></div></div> <ol> <li>optimizer.step()은 fp32에서 이루어지며 master weight을 업데이트 함.</li> </ol> <p><br></p> <ul> <li>Torch AMP(<strong>Automatic Mixed Precision</strong>)에 위의 설명 및 코드가 그대로 반영되어 있음</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">().</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="p">...)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="nc">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># Runs the forward pass with autocasting.
</span>        <span class="k">with</span> <span class="nf">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># Scales loss. Calls backward() on scaled loss to create scaled gradients.
</span>        <span class="c1"># Backward passes under autocast are not recommended.
</span>        <span class="c1"># Backward ops run in the same dtype autocast chose for corresponding forward ops.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># scaler.step() first unscales the gradients of the optimizer's assigned params.
</span>        <span class="c1"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,
</span>        <span class="c1"># otherwise, optimizer.step() is skipped.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="c1"># Updates the scale for next iteration.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>

</code></pre></div></div> <h2 id="2-list-of-nccl-operation">2. List of NCCL Operation</h2> <ul> <li> <p>2개의 프로세스간의 통신 패턴을 point-to-point communication(점대점 통신)라고 명명한다면, 여러 개의 프로세스간의 통신을 collective communication(집합 통신)이라고 명명한다.</p> <p>→ point-to-point communication는 sender는 데이터를 보내고, receiver는 데이터를 받도록 설계하면 되기 때문에 구현 난이도가 상대적으로 쉬움</p> </li> </ul> <p>→ multiple senders와 multiple receivers가 있는 collective communication의 경우, topology들이 구성이 매우 다양하기 때문에 (위 그림처럼) 데이터 통신을 최적화하기 매우 어렵다는 한계점이 존재한다.</p> <ul> <li>조금 더 자세히 말하면, 몇 개의 GPU가 있고, 어떤 GPU가 어떤 GPU와 어떻게 연결(PCIe, NVLink, IB, ethernet 등등) 되어 있는 지와 같은 topology 정보는 통신 최적화에 필수로 고려해야 하는 요소인데, 그 조합이 너무나도 많기에 이것을 다 만족하는 최적화된 솔루션을 찾아 구현하기가 매우 어렵다.</li> </ul> <p><br></p> <ul> <li> <p>이러한 문제점을 해결하기 위해 ‘Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J. Parallel Distrib. Comput., 69:117–124, 2009.’에서 모든 어떤 topology라도 <strong>Ring(s) topology</strong>로 생각하고 구현할 경우 최적 성능을 달성할 수 있음을 보여줌.</p> <p>→ 위는 Broadcast 연산인데 GPU0이 GPU[1:3]에 data를 뿌릴때 순차적으로 (GPU0→ GPU1 이 끝나면 GPU0 → GPU2 …)으로 하는게 아니라 데이터를 작은 조각으로 나누어서 GPU0 → GPU1 &amp; GPU1 → GPU2 &amp; GPU2 → GPU3 이렇게 인접한 모든 프로세스가 데이터를 전송하도록 구현하였음.</p> </li> </ul> <p><br></p> <ul> <li> <p><strong>NCCL(NVIDIA Collective Communications Library)</strong>은 <strong><em>멀티 GPU</em></strong> 및 <strong><em>멀티 노드 환경</em></strong>에서 고성능을 제공하는 통신 라이브러리로 다양한 네트워크 topology에서도 최적 성능을 달성하는 것을 목표로 개발되었으며 이전에 언급한 Ring-based 집합통신 알고리즘을 기반으로 최적화된 집합 통신을 구현</p> </li> <li> <p>NCCL은 DL training, inference에서의 다양한 집합 연산(예: All-reduce, All-gather, Broadcast 등)을 최적화하여 각 GPU 사이에 일관되고 효율적인 통신을 제공하여 병렬 처리 성능을 극대화하는것에 목표를 두고 있음.</p> </li> <li> <p>동기 및 비동기 통신을 지원하며, CUDA 스트림을 통해 다른 연산과 겹쳐서 수행이 가능하다고 함.</p> </li> </ul> <p><br></p> <p><br></p> <p>→ (1) GPU를 이용한 연산 GPGPU(CUDA)으로 가속화 (2) NCCL을 활용해 하나의 GPU가 아닌 multiple GPUs, 나아가 multi-node 상의 multiple GPUs의 통신을 통한 연산 가속화</p> <ul> <li>General-Purpse computing on Graphic Processing Unit: 일반적으로 컴퓨터 그래픽스를 위한 계산만 맡았던 그래픽 처리 장치를, 전통적으로 중앙 처리 장치가 맡았던 응용 프로그램들의 계산에 사용하는 기술</li> </ul> <p><br></p> <ul> <li> <p>간단한 용어 정리</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>→ Node : 2개
</code></pre></div> </div> <p>→ Global World Size : 4 X 2 = 8</p> <p>→ Local World Size : 4 X 1 = 4</p> <p>→ Rank: process_id</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> : 일반적으로 DDP에서 1개 node안에서 각 node에서 각 gpu마다 1개 process를 배정함

 :  Local World Size = # of Rank
</code></pre></div> </div> </li> </ul> <p><br></p> <ul> <li><strong>NCCL Operations (DP나 DDP 쓰면서 다들 호출해보셨을 operations들)</strong></li> </ul> <ol> <li> <p><strong>AllReduce</strong></p> <ol> <li> <p>모든 GPU(rank)의 데이터를 결합하여 그 결과를 모든 GPU에 다시 배포</p> </li> <li> <p>예를 들어, 각 GPU가 데이터 조각을 가지고 있을 때, 이 데이터를 합산하여 모든 GPU에 결과를 제공</p> </li> <li> <p>mini batch별로 GPU에 forwarding하고 Loss 평균내기 전에 가장 많이 쓰는 operation</p> </li> <li> <p><strong>Broadcast</strong></p> </li> <li> <p>한 GPU에서 다른 모든 GPU로 데이터를 보냄.</p> </li> <li> <p>예를 들어, 첫 번째 GPU의 데이터를 나머지 3개의 GPU와 공유.</p> </li> <li> <p><strong>Reduce</strong></p> </li> <li> <p>모든 GPU의 데이터를 결합하고 그 결과를 하나의 GPU로 보냄</p> </li> <li> <p>예를 들어, 모든 GPU의 데이터를 합산하고 이를 첫 번째 GPU(root)에만 전달.</p> </li> <li> <p><strong>AllGather</strong></p> </li> <li> <p>모든 GPU로부터 데이터를 수집하여 모든 GPU에 분배</p> </li> <li> <p>각 GPU가 데이터 조각을 가지고 있을 때, 이들을 모두 모아서 모든 GPU에 전달</p> </li> <li> <p>Fully Sharded Data Parallel에 사용</p> <ul> <li>노드 0 : <code class="language-plaintext highlighter-rouge">**[in0[0], in0[1], in0[2]]**</code> </li> </ul> </li> </ol> <ul> <li> <p>target node: <code class="language-plaintext highlighter-rouge">**out**</code>에서 <code class="language-plaintext highlighter-rouge">**[out[0], out[1], out[2]]**</code>에 저장</p> </li> <li> <p>여기서 <code class="language-plaintext highlighter-rouge">**Y=0**</code>, <code class="language-plaintext highlighter-rouge">**count=3**</code>이므로,</p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">**out[0*3+0] = in0[0]**</code></p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**out[0*3+1] = in0[1]**</code></p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**out[0*3+2] = in0[2]**</code></p> </li> </ul> </li> </ul> </li> </ol> <p><br></p> <ol> <li> <p><strong>ReduceScatter</strong></p> <ol> <li> <p>모든 GPU의 데이터를 결합하고 그 결과를 모든 GPU에 분산.</p> </li> <li> <p>데이터를 결합한 후, 이를 여러 조각으로 나누어 각각의 GPU에 할당.</p> </li> </ol> <ul> <li> <strong>inXY</strong>: 노드 X에서 노드 Y로 보내진 데이터.</li> </ul> </li> </ol> <ul> <li> <p><strong>out[Y]</strong>: 노드 Y에서 모든 입력 데이터를 합산한 결과를 저장하는 배열.</p> </li> <li> <p><strong>count</strong>: 각 노드에서 전송된 데이터 요소 수.</p> </li> <li> <p><strong>i</strong>: 각 노드의 데이터 배열에서의 인덱스.</p> </li> <li> <p><strong>노드 0</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in00, in01, in02, in03]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 1</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in10, in11, in12, in13]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 2</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in20, in21, in22, in23]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 3</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in30, in31, in32, in33]**</code>의 데이터 존재</p> </li> </ul> <p>노드 0의 출력 <code class="language-plaintext highlighter-rouge">**out0**</code>을 구하는 방법:</p> <ul> <li> <p>노드 0으로 보내진 모든 데이터 요소를 합산.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**out0 = sum(in00*4+0, in10*4+0, in20*4+0, in30*4+0)**</code></p> <ul> <li>여기서 <code class="language-plaintext highlighter-rouge">**inX0**</code>는 각 노드 X에서 노드 0으로 보낸 첫 번째 데이터 요소를 의미.</li> </ul> </li> </ul> <h2 id="3-dp-dataparallel--ddp-distributeddataparallel">3. DP (DataParallel) &amp; DDP (DistributedDataParallel)</h2> <ul> <li> <p>두 방법론 모두 효율적으로 모델을 학습하기 위해 등장한 방법론.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DataParallel</code>은 단일 작업, 멀티쓰레드 방법론으로 GPU에 입력 데이터를 부분적으로 할당(mini-batch를 분할)하고 동일한 신경망 모델을 복제하여 이용하는 방식</p> </li> <li> <p>반면, <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>은 다중 작업이며 단일 및 다중 기기 학습을 전부 지원하는 방식</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DataParallel</code>가 가진 스레드간 GIL 경합, 복제 모델의 반복 당 생성, 입력 및 수집 출력으로 인한 추가적인 오버헤드를 <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>가 보완해 현재는 DDP를 주로 활용,</p> </li> </ul> <p><br></p> <ul> <li><strong><em>DP와 DDP 모두 (당연히 ) Multi-GPUs training setting을 가정함</em></strong></li> </ul> <p><br></p> <ul> <li><strong>DP (DataParallel)</strong></li> </ul> <ol> <li> <p><strong>Scatter</strong></p> <ol> <li> <p>mini-batch의 data: 128</p> </li> <li> <p>GPU0이 rank_size(4)로 나누어서 GPU[0:3]에 32개씩 전송</p> </li> </ol> </li> <li> <p><strong>Replicate</strong></p> <ol> <li> <p>GPU0에 처음에 model.paramaters()를</p> </li> <li> <p>model parameter를 GPU[0:3]에 broadcast</p> </li> </ol> </li> <li> <p><strong>Forward</strong></p> <ol> <li> <p>각 GPU에서 replicate model로 forward 진행</p> </li> <li> <p>logit 계산</p> </li> </ol> </li> <li> <p><strong>Gather</strong></p> <ol> <li>GPU[0:3]에 있는 logit을 GPU0에 계산해서 loss 계산</li> </ol> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">def</span> <span class="nf">data_parallel</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">,</span>
                  <span class="n">output_device</span><span class="p">):</span>
  <span class="c1"># (1) [Scatter] data를 device들에 scatter
</span>  <span class="n">inputs</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>

  <span class="c1"># (2) [Replicate] model weight을 device_ids들에 복제
</span>  <span class="n">replicas</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="nf">replicate</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">device_ids</span><span class="p">)</span>

  <span class="c1"># (3) [Forward] 각 device 에 복제된 model이 각 device의 data를 forward
</span>  <span class="n">logit</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="nf">parallel_apply</span><span class="p">(</span><span class="n">replicas</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

  <span class="c1"># (4) [Gather] 모델의 logit을 output_device(하나의 device) 로 모음
</span>  <span class="n">logit</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">parallel</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">output_device</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">logits</span>

</code></pre></div></div> <ol> <li> <p><strong>Scatter</strong></p> <ol> <li>GPU0에 계산된 loss를 각 device에 scatter</li> </ol> </li> <li> <p><strong>Backward</strong></p> <ol> <li>각 device(replicate model)은, 각자 전달받은 loss를 사용하여 각자 gradient 계산</li> </ol> </li> <li> <p><strong>Reduce</strong></p> <ol> <li>계산된 모든 graidents를 GPU0으로 reduce</li> </ol> </li> <li> <p><strong>Update</strong></p> <ol> <li> <p>Gradient를 이용해서 GPU0에 있는 모델을 업데이트</p> </li> <li> <p>optimizer.step()</p> </li> <li> <p>이후 다시 minibatch, model을 broadcast</p> </li> </ol> </li> </ol> <p><br></p> <p><strong>### DP의 문제점</strong></p> <p>→ 메모리 사용량 : 0번 GPU &gt; 다른 GPU</p> <p>→ 매 step마다 다른 device로 복제 (broadcast) 하는 문제</p> <p>→ Python의 GIL(Global Interpreter Lock)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>: 여러 개의 thread가 동시에 실행되지 못하도록 막는 기능

⇒ DDP는 multi-process로 DP의 한계를 극복하고자 함

&lt;br/&gt;
</code></pre></div></div> <ul> <li><strong>DDP (DistributedDataParallel)</strong></li> </ul> <p>⇒ 모델 학습 시 가장 큰 병목현상을 자랑했던 step-wise broadcasting을 해결할 수 있는 방법이 무엇이 있을까?</p> <p>⇒ device에 있는 replicate들을 optimizer.step()해줄 수 있으면 모든 문제가 해결이 된다.</p> <p>⇒ 어떻게 하면 모든 replicate들의 gradient들을 한 GPU에 모으지 않고 ( <code class="language-plaintext highlighter-rouge">Gather</code>연산) 모든 device에 한번에 뿌려줄 수 있을까? :** All-Reduce**</p> <p><br></p> <ul> <li> <p>**이전에 언급한 Ring 알고리즘 기반 All-reduce으로 **특정 device로 부하가 쏠리지 않는 프로세스 통신이 가능해짐</p> <p><br></p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>

<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="sh">"</span><span class="s">nccl</span><span class="sh">"</span><span class="p">)</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">())</span> <span class="o">*</span> <span class="n">rank</span>

<span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">tensor</span><span class="si">}</span><span class="se">\n</span><span class="s">)
rank 1: tensor([[6., 6.],
        [6., 6.]], device=</span><span class="sh">'</span><span class="s">cuda:1</span><span class="sh">'</span><span class="s">)

rank 2: tensor([[6., 6.],
        [6., 6.]], device=</span><span class="sh">'</span><span class="s">cuda:2</span><span class="sh">'</span><span class="s">)
rank 0: tensor([[6., 6.],
        [6., 6.]], device=</span><span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="s">)

rank 3: tensor([[6., 6.],
        [6., 6.]], device=</span><span class="sh">'</span><span class="s">cuda:3</span><span class="sh">'</span><span class="s">)

</span></code></pre></div></div> <p><br></p> <p><br></p> <ol> <li> <p><strong>Scatter</strong></p> <ol> <li> <p>DistributedSampler을 활용해 각 rank(process)에 mini_batch를 scatter</p> </li> <li> <p>일반적으로 데이터를 메모리에 올릴 때 CPU + DRAM(pageable Memory) → Pinned Memory → VRAM에 올리는데, 속도 향상으로 위해 data load시 바로 Pinned Memory를 통해 VRAM으로 올리는 option을 사용하기도 함. (pin_memory=True)</p> <ol> <li><strong>Process-wise Forwarding</strong></li> </ol> </li> <li> <p>각 process마다 mini-batch로 forward 진행</p> </li> <li> <p>loss 계산</p> </li> </ol> </li> <li> <p><strong>Process-wise Backward + All Reduce</strong></p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">backward()</code> 연산은 프로세스별로 뒤쪽 레이어부터 순차적으로 이루어지다가 ‘Gradient Bucketing’을 수행함.</p> </li> <li> <p>‘Gradient Bucketing’은 Gradient를 일정한 사이즈의 Bucket에 저장해두고 가득차면 다른 프로세스로 전송하는 방식</p> </li> <li> <p>쉽게 말해, 모든 process들이 정해놓은 일정 크기의 layer (bucket) <code class="language-plaintext highlighter-rouge">backward()</code> 연산이 완료되면, (1) 각 process들의 진행 중인 <code class="language-plaintext highlighter-rouge">backward()</code> 은 진행시켜놓고 (2)bucket-wise로 All-reduce 를 수행한 후 device개수로 나눠 average gradient를 계산하여 모든 device들의 gradient들을 동기화 시켜놓는것</p> <ol> <li><strong>Update</strong></li> </ol> </li> <li> <p>마스터 process 없이 data parallel이 가능함</p> </li> </ol> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.distributed</span> <span class="k">as</span> <span class="n">dist</span>
<span class="kn">from</span> <span class="n">torch.nn.parallel</span> <span class="kn">import</span> <span class="n">DistributedDataParallel</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">AdamW</span>
<span class="kn">from</span> <span class="n">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">DistributedSampler</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span>
<span class="kn">from</span> <span class="n">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dist</span><span class="p">.</span><span class="nf">init_process_group</span><span class="p">(</span><span class="sh">"</span><span class="s">nccl</span><span class="sh">"</span><span class="p">)</span>
<span class="n">rank</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">()</span>
<span class="n">world_size</span> <span class="o">=</span> <span class="n">dist</span><span class="p">.</span><span class="nf">get_world_size</span><span class="p">()</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">"</span><span class="s">multi_nli</span><span class="sh">"</span><span class="p">).</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">]</span>
<span class="n">datasets</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">premise</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">hypothesis</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">h</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">labels</span><span class="sh">"</span><span class="p">:</span> <span class="n">l</span><span class="p">.</span><span class="nf">as_py</span><span class="p">(),</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">datasets</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">datasets</span><span class="p">[</span><span class="mi">9</span><span class="p">])</span>
<span class="p">]</span>

<span class="n">sampler</span> <span class="o">=</span> <span class="nc">DistributedSampler</span><span class="p">(</span>
    <span class="n">datasets</span><span class="p">,</span>
    <span class="n">num_replicas</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span>
    <span class="n">datasets</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">sampler</span><span class="o">=</span><span class="n">sampler</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">bert-base-cased</span><span class="sh">"</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">3</span><span class="p">).</span><span class="nf">cuda</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">device</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="nf">tokenizer</span><span class="p">(</span>
        <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">premise</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">hypothesis</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">tokens</span><span class="p">.</span><span class="n">input_ids</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(),</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">tokens</span><span class="p">.</span><span class="n">attention_mask</span><span class="p">.</span><span class="nf">cuda</span><span class="p">(),</span>
        <span class="n">labels</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">labels</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">).</span><span class="n">loss</span>

    <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">step: </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">300</span><span class="p">:</span>
        <span class="k">break</span>

</code></pre></div></div> <p><br></p> <h2 id="4-fsdp-fully-sharded-data-parallel">4. FSDP (Fully Sharded Data Parallel)</h2> <ul> <li> <p>DDP에서는 model의 parameter, gradient, optimizer에서 사용하는 states 등을 모두 각 GPU에서 보관하고 GPU 통신을 통해서 model의 state를 동기화 함</p> </li> <li> <p>하지만, 바야흐로 13B, 40B, 70B LLM의 시대에서 A100-80GB 1장으로 Full-Fine Tuning을 할 수 있는 시대가 되지 않았다.</p> </li> <li> <p>Model의 각 Unit을 GPU에 Sharding하고 Forwarding/Backwarding할때 마다 Collective Communication을 수행해서 GPU간 통신을 증가시키지만 Large Model Training을 가능하게 한 Fully Sharded Data Parallel 방법론이 등장</p> </li> </ul> <p><br></p> <p><strong>### 용어 정리</strong></p> <ul> <li> <p><strong>FSDP Unit</strong></p> <ul> <li> <p>Model Structure에서 Split할 대상들</p> <p>(e.g., layers, TransformerBlocks)</p> </li> <li> <p>collective communication의 대상</p> </li> <li> <p>FSDP Unit ≠ sharding의 대상이 X</p> </li> <li> <p>예시에서 [layer0,3], [layer1,2], [layer4,5]가 각각 FSDP Unit</p> </li> </ul> </li> <li> <p>** Sharding**</p> <ul> <li> <p><strong>FSDP Unit(e.g., a single layer/stage)을 *</strong>FlatParameter*로 저장하는 과정</p> </li> <li> <p><em>FlatParameter</em>는 1D tensor로 sharding을 통해 각 FSDP unit이 각 rank에 균등하게 분배됨</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>	→ (16개의 Rank가 있다고 가정) 각 rank에 weight, bias flat parameter하나씩 store하고 15번 device에 padding tensor store해서 균등있게 store되도록 함 (NCCL이 equal하게 input을 뿌려야 성능이 향상된다고 논문에 표 제공)
</code></pre></div> </div> </li> <li> <p>위의 예시로 설명하면, [layer0,3]가 2개의 device에 sharding</p> </li> </ul> </li> </ul> <p><br></p> <ul> <li> <table> <tbody> <tr> <td>**[Recap] All-Gather</td> <td>Reduce scatter**</td> </tr> </tbody> </table> </li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[//]: # (column is not supported)

		[//]: # (column is not supported)

	&lt;br/&gt;
</code></pre></div></div> <ul> <li>자 그럼, 용어도 정리했으니 각 FSDP Unit이 어떻게 Forwarding/Backwarding되는지 알아보자.</li> </ul> <ol> <li> <p><strong>Forwarding</strong></p> <ol> <li> <p>All-Gather연산으로 각 device에 sharded 되어 있는 각 FSDP Unit의 shard들 (w1~w4)을 모든 device에 뿌린다.</p> </li> <li> <p>결론적으로, FSDP의 memory requirements는 sharded model (w1) &amp; 가장 큰 FSDP unit(아래 예시에는 [w1:w4])이 device에 loaded되었을때에 비례한다.</p> </li> <li> <p>모든 device에 동일한 weight이 있으면 (FSDP Unit이 존재하면) Forwarding 진행</p> </li> <li> <p>각 shard에 원래 있어야할 weight 제외하고 나머지 weight (Peer Shard) Free</p> </li> </ol> </li> <li> <p><strong>Backwarding</strong></p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>1. Forwarding과 마찬가지로 All-Gather연산 수행
</code></pre></div> </div> <ol> <li> <p>Loss.backward()연산으로 gradient 수행</p> <ol> <li> <p>각 device(예시에서는 node)에 FSDP unit의 gradient가 존재</p> </li> <li> <p>mini-batch가 sampling되어 올라가는 상황이기에 각 device (예시에서는 node)가 가지고 있는 gradient는 다를 수 밖에 없음</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>	1. 따라서 sum한 후 각 shard에 할당하는 reduce-scatter 연산을 통해 gradient를 계산
</code></pre></div> </div> </li> </ol> </li> <li> <p>모든 FSDP unit에 대해서 순차적으로 back propagation</p> </li> </ol> </li> </ol> <p><br></p> <ul> <li><strong>Overall Example</strong></li> </ul> <p><strong>[Model]</strong></p> <p><strong>[Overlap Communication and Computation]</strong></p> <ol> <li> <p>FSDP Unit_0 - [Layer0;Layer3] All-Gather</p> </li> <li> <p>FSDP Unit_0 - [Layer0] Forwarding &amp; FSDP Unit_1 - [Layer1;Layer2] All-Gather</p> </li> <li> <p>FSDP Unit*2 - [Layer4;Layer5] All-Gather &amp; FSDP Unit_1 - [Layer1;Layer2] _Forwarding &amp; FSDP Unit_1 - [Layer1;Layer2] &amp; FSDP Unit_1 Parameter Free (Forwarding 끝) *&amp; FSDP Unit_0 - [Layer3] Forwarding</p> </li> <li> <p>FSDP Unit<em>2 - [Layer4;Layer5] All-Gather (BackProp용) &amp; FSDP Unit_2 - [Layer4;Layer5] Forwarding &amp; _FSDP Unit 2 Parameter Free (Forwarding 끝)</em></p> </li> <li> <p>(4에서 All-Gather한) FSDP Unit_2 - [Layer4;Laye5] Backward &amp; *FSDP Unit 2 Parameter Free (Backward 끝) &amp; ReduceScatter &amp; *FSDP Unit_0 - [Layer3] BackProp</p> </li> <li> <p>FSDP Unit_1 - [Layer1,2] All-Gather</p> </li> <li> <p>FSDP Unit*1 - [Layer1,2] Backward (Layer 3 Gradient 있기 때문에) &amp; FSDP Unit_1 - [Layer1,2] *ReduceScatter &amp; _FSDP Unit_1 - [Layer1,2] _ Parameter Free (Backward 끝) &amp; FSDP Unit 0 _[Layer1] BackProp</p> </li> <li> <p><em>FSDP Unit 0 Parameter Free (Layer1까지 Backprop 완료)</em></p> </li> <li> <p><em>FSDP Unit 0 Reduce Scatter</em></p> </li> </ol> <p><br></p> <h2 id="5-llama2-recipes---training-code-review">5. Llama2 Recipes - Training Code Review</h2> <ul> <li> <p>Meta에서 제공하는 Llama2 Recipes는 Fine-tuning Recipe를 제공하는데, 기본적으로 llama-recipes/src/llama_recipes.fine-tuning.py가 from llama_recipes.utils.train_utils import train을 호출해서 학습을 진행</p> </li> <li> <p><a href="http://fine-tuning.py/" rel="external nofollow noopener" target="_blank">fine-tuning.py</a> 위주로 간단한 코드 리뷰를 진행</p> </li> </ul> <h3 id="fine-tuningpy">fine-tuning.py</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">os</span>

<span class="kn">import</span> <span class="n">dataclasses</span>
<span class="kn">import</span> <span class="n">fire</span>
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">from</span> <span class="n">peft</span> <span class="kn">import</span> <span class="n">get_peft_model</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span>
<span class="kn">from</span> <span class="n">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">FullyShardedDataParallel</span> <span class="k">as</span> <span class="n">FSDP</span><span class="p">,</span>
    <span class="n">ShardingStrategy</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">torch.distributed.fsdp.fully_sharded_data_parallel</span> <span class="kn">import</span> <span class="n">CPUOffload</span>
<span class="kn">from</span> <span class="n">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">StepLR</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AutoTokenizer</span><span class="p">,</span>
    <span class="n">LlamaForCausalLM</span><span class="p">,</span>
    <span class="n">LlamaConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">transformers.models.llama.modeling_llama</span> <span class="kn">import</span> <span class="n">LlamaDecoderLayer</span>

<span class="kn">from</span> <span class="n">llama_recipes.configs</span> <span class="kn">import</span> <span class="n">fsdp_config</span> <span class="k">as</span> <span class="n">FSDP_CONFIG</span>
<span class="kn">from</span> <span class="n">llama_recipes.configs</span> <span class="kn">import</span> <span class="n">train_config</span> <span class="k">as</span> <span class="n">TRAIN_CONFIG</span>
<span class="kn">from</span> <span class="n">llama_recipes.data.concatenator</span> <span class="kn">import</span> <span class="n">ConcatDataset</span>
<span class="kn">from</span> <span class="n">llama_recipes.policies</span> <span class="kn">import</span> <span class="n">AnyPrecisionAdamW</span><span class="p">,</span> <span class="n">apply_fsdp_checkpointing</span>

<span class="kn">from</span> <span class="n">llama_recipes.utils</span> <span class="kn">import</span> <span class="n">fsdp_auto_wrap_policy</span>
<span class="kn">from</span> <span class="n">llama_recipes.utils.config_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">update_config</span><span class="p">,</span>
    <span class="n">generate_peft_config</span><span class="p">,</span>
    <span class="n">generate_dataset_config</span><span class="p">,</span>
    <span class="n">get_dataloader_kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">llama_recipes.utils.dataset_utils</span> <span class="kn">import</span> <span class="n">get_preprocessed_dataset</span>

<span class="kn">from</span> <span class="n">llama_recipes.utils.fsdp_utils</span> <span class="kn">import</span> <span class="n">hsdp_device_mesh</span>
<span class="kn">from</span> <span class="n">llama_recipes.utils.train_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">train</span><span class="p">,</span>
    <span class="n">freeze_transformer_layers</span><span class="p">,</span>
    <span class="n">setup</span><span class="p">,</span>
    <span class="n">setup_environ_flags</span><span class="p">,</span>
    <span class="n">clear_gpu_cache</span><span class="p">,</span>
    <span class="n">print_model_size</span><span class="p">,</span>
    <span class="n">get_policies</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">accelerate.utils</span> <span class="kn">import</span> <span class="n">is_xpu_available</span>

<span class="k">def</span> <span class="nf">setup_wandb</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">fsdp_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="n">wandb</span>
    <span class="k">except</span> <span class="nb">ImportError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ImportError</span><span class="p">(</span>
            <span class="sh">"</span><span class="s">You are trying to use wandb which is not currently installed. </span><span class="sh">"</span>
            <span class="sh">"</span><span class="s">Please install it using pip install wandb</span><span class="sh">"</span>
        <span class="p">)</span>
    <span class="kn">from</span> <span class="n">llama_recipes.configs</span> <span class="kn">import</span> <span class="n">wandb_config</span> <span class="k">as</span> <span class="n">WANDB_CONFIG</span>
    <span class="n">wandb_config</span> <span class="o">=</span> <span class="nc">WANDB_CONFIG</span><span class="p">()</span>
    <span class="nf">update_config</span><span class="p">(</span><span class="n">wandb_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">init_dict</span> <span class="o">=</span> <span class="n">dataclasses</span><span class="p">.</span><span class="nf">asdict</span><span class="p">(</span><span class="n">wandb_config</span><span class="p">)</span>
    <span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="o">**</span><span class="n">init_dict</span><span class="p">)</span>
    <span class="n">run</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">train_config</span><span class="p">)</span>
    <span class="n">run</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">fsdp_config</span><span class="p">,</span> <span class="n">allow_val_change</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">run</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Update the configuration for the training and sharding process
</span>    <span class="n">train_config</span><span class="p">,</span> <span class="n">fsdp_config</span> <span class="o">=</span> <span class="nc">TRAIN_CONFIG</span><span class="p">(),</span> <span class="nc">FSDP_CONFIG</span><span class="p">()</span>
    <span class="nf">update_config</span><span class="p">((</span><span class="n">train_config</span><span class="p">,</span> <span class="n">fsdp_config</span><span class="p">),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="c1"># Set the seeds for reproducibility
</span>    <span class="k">if</span> <span class="nf">is_xpu_available</span><span class="p">():</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">xpu</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span><span class="p">:</span>
        <span class="nf">setup</span><span class="p">()</span>
        <span class="c1"># torchrun specific
</span>        <span class="n">local_rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">LOCAL_RANK</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">world_size</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">is_xpu_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">xpu</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
            <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">set_device</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
        <span class="nf">clear_gpu_cache</span><span class="p">(</span><span class="n">local_rank</span><span class="p">)</span>
        <span class="nf">setup_environ_flags</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

    <span class="n">wandb_run</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_wandb</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">or</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">wandb_run</span> <span class="o">=</span> <span class="nf">setup_wandb</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">fsdp_config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Load the pre-trained model and setup its configuration
</span>    <span class="n">use_cache</span> <span class="o">=</span> <span class="bp">False</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="k">else</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">and</span> <span class="n">train_config</span><span class="p">.</span><span class="n">low_cpu_fsdp</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        for FSDP, we can save cpu memory by loading pretrained model on rank0 only.
        this avoids cpu oom when loading large models like llama 70B, in which case
        model alone would consume 2+TB cpu mem (70 * 4 * 8). This will add some comms
        overhead and currently requires latest nightly.
        </span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
                <span class="n">train_config</span><span class="p">.</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">load_in_8bit</span><span class="o">=</span><span class="bp">True</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
                <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
                <span class="n">attn_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">sdpa</span><span class="sh">"</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_fast_kernels</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">llama_config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">model_name</span><span class="p">)</span>
            <span class="n">llama_config</span><span class="p">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="n">use_cache</span>
            <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">meta</span><span class="sh">"</span><span class="p">):</span>
                <span class="n">model</span> <span class="o">=</span> <span class="nc">LlamaForCausalLM</span><span class="p">(</span><span class="n">llama_config</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
            <span class="n">train_config</span><span class="p">.</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">load_in_8bit</span><span class="o">=</span><span class="bp">True</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">,</span>
            <span class="n">attn_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">sdpa</span><span class="sh">"</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_fast_kernels</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Load the tokenizer and add special tokens
</span>    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">model_name</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">tokenizer_name</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">train_config</span><span class="p">.</span><span class="n">tokenizer_name</span><span class="p">)</span>
    <span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">eos_token_id</span>

    <span class="c1"># If there is a mismatch between tokenizer vocab size and embedding matrix,
</span>    <span class="c1"># throw a warning and then expand the embedding matrix
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">model</span><span class="p">.</span><span class="nf">get_input_embeddings</span><span class="p">().</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">WARNING: Resizing the embedding matrix to match the tokenizer vocab size.</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">resize_token_embeddings</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">))</span>

    <span class="nf">print_model_size</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_config</span><span class="p">,</span> <span class="n">rank</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Prepare the model for int8 training if quantization is enabled
</span>    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span><span class="p">:</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

    <span class="c1"># Convert the model to bfloat16 if fsdp and pure_bf16 is enabled
</span>    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">and</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">pure_bf16</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_peft</span><span class="p">:</span>
        <span class="n">peft_config</span> <span class="o">=</span> <span class="nf">generate_peft_config</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nf">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">print_trainable_parameters</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">wandb_run</span><span class="p">:</span>
            <span class="n">wandb_run</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">peft_config</span><span class="p">)</span>

    <span class="n">hsdp_device_mesh</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">hsdp</span> <span class="ow">and</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">sharding_strategy</span> <span class="o">==</span> <span class="n">ShardingStrategy</span><span class="p">.</span><span class="n">HYBRID_SHARD</span><span class="p">:</span>
        <span class="n">hsdp_device_mesh</span> <span class="o">=</span> <span class="nf">hsdp_device_mesh</span><span class="p">(</span><span class="n">replica_group_size</span><span class="o">=</span><span class="n">fsdp_config</span><span class="p">.</span><span class="n">replica_group_size</span><span class="p">,</span> <span class="n">sharding_group_size</span><span class="o">=</span><span class="n">fsdp_config</span><span class="p">.</span><span class="n">sharding_group_size</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">HSDP device mesh is ready</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1">#setting up FSDP if enable_fsdp is enabled
</span>    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_peft</span> <span class="ow">and</span> <span class="n">train_config</span><span class="p">.</span><span class="n">freeze_layers</span><span class="p">:</span>

            <span class="nf">freeze_transformer_layers</span><span class="p">(</span><span class="n">train_config</span><span class="p">.</span><span class="n">num_freeze_layers</span><span class="p">)</span>

        <span class="n">mixed_precision_policy</span><span class="p">,</span> <span class="n">wrapping_policy</span> <span class="o">=</span> <span class="nf">get_policies</span><span class="p">(</span><span class="n">fsdp_config</span><span class="p">,</span> <span class="n">rank</span><span class="p">)</span>
        <span class="n">my_auto_wrapping_policy</span> <span class="o">=</span> <span class="nf">fsdp_auto_wrap_policy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">LlamaDecoderLayer</span><span class="p">)</span>

        <span class="n">device_id</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="nf">is_xpu_available</span><span class="p">():</span>
            <span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">xpu</span><span class="p">.</span><span class="nf">current_device</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
            <span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">current_device</span><span class="p">()</span>

        <span class="n">model</span> <span class="o">=</span> <span class="nc">FSDP</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="n">auto_wrap_policy</span><span class="o">=</span> <span class="n">my_auto_wrapping_policy</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_peft</span> <span class="k">else</span> <span class="n">wrapping_policy</span><span class="p">,</span>
            <span class="n">cpu_offload</span><span class="o">=</span><span class="nc">CPUOffload</span><span class="p">(</span><span class="n">offload_params</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">fsdp_cpu_offload</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">mixed_precision</span><span class="o">=</span><span class="n">mixed_precision_policy</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">pure_bf16</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
            <span class="n">sharding_strategy</span><span class="o">=</span><span class="n">fsdp_config</span><span class="p">.</span><span class="n">sharding_strategy</span><span class="p">,</span>
            <span class="n">device_mesh</span><span class="o">=</span><span class="n">hsdp_device_mesh</span><span class="p">,</span>
            <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
            <span class="n">limit_all_gathers</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">sync_module_states</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">low_cpu_fsdp</span><span class="p">,</span>
            <span class="n">param_init_fn</span><span class="o">=</span><span class="k">lambda</span> <span class="n">module</span><span class="p">:</span> <span class="n">module</span><span class="p">.</span><span class="nf">to_empty</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">),</span> <span class="n">recurse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">low_cpu_fsdp</span> <span class="ow">and</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">fsdp_activation_checkpointing</span><span class="p">:</span>
            <span class="nf">apply_fsdp_checkpointing</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">quantization</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span><span class="p">:</span>
        <span class="k">if</span> <span class="nf">is_xpu_available</span><span class="p">():</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">xpu:0</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">dataset_config</span> <span class="o">=</span> <span class="nf">generate_dataset_config</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>

     <span class="c1"># Load and preprocess the dataset for training and validation
</span>    <span class="n">dataset_train</span> <span class="o">=</span> <span class="nf">get_preprocessed_dataset</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">dataset_config</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">--&gt; Training Set Length = </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">dataset_val</span> <span class="o">=</span> <span class="nf">get_preprocessed_dataset</span><span class="p">(</span>
        <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">dataset_config</span><span class="p">,</span>
        <span class="n">split</span><span class="o">=</span><span class="sh">"</span><span class="s">test</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">or</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">--&gt; Validation Set Length = </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset_val</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">batching_strategy</span> <span class="o">==</span> <span class="sh">"</span><span class="s">packing</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">dataset_train</span> <span class="o">=</span> <span class="nc">ConcatDataset</span><span class="p">(</span><span class="n">dataset_train</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">context_length</span><span class="p">)</span>

    <span class="n">train_dl_kwargs</span> <span class="o">=</span> <span class="nf">get_dataloader_kwargs</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">dataset_train</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Create DataLoaders for the training and validation dataset
</span>    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
        <span class="n">dataset_train</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">num_workers_dataloader</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">train_dl_kwargs</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">run_validation</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">batching_strategy</span> <span class="o">==</span> <span class="sh">"</span><span class="s">packing</span><span class="sh">"</span><span class="p">:</span>
            <span class="n">dataset_val</span> <span class="o">=</span> <span class="nc">ConcatDataset</span><span class="p">(</span><span class="n">dataset_val</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">context_length</span><span class="p">)</span>

        <span class="n">val_dl_kwargs</span> <span class="o">=</span> <span class="nf">get_dataloader_kwargs</span><span class="p">(</span><span class="n">train_config</span><span class="p">,</span> <span class="n">dataset_val</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="sh">"</span><span class="s">val</span><span class="sh">"</span><span class="p">)</span>

        <span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span>
            <span class="n">dataset_val</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">num_workers_dataloader</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="o">**</span><span class="n">val_dl_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Initialize the optimizer and learning rate scheduler
</span>    <span class="k">if</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">pure_bf16</span> <span class="ow">and</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="sh">"</span><span class="s">anyprecision</span><span class="sh">"</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="nc">AnyPrecisionAdamW</span><span class="p">(</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">momentum_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">variance_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">,</span>
            <span class="n">use_kahan_summation</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span>
            <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">weight_decay</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="n">scheduler</span> <span class="o">=</span> <span class="nc">StepLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="n">train_config</span><span class="p">.</span><span class="n">gamma</span><span class="p">)</span>

    <span class="c1"># Start the training process
</span>    <span class="n">results</span> <span class="o">=</span> <span class="nf">train</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">train_dataloader</span><span class="p">,</span>
        <span class="n">eval_dataloader</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">scheduler</span><span class="p">,</span>
        <span class="n">train_config</span><span class="p">.</span><span class="n">gradient_accumulation_steps</span><span class="p">,</span>
        <span class="n">train_config</span><span class="p">,</span>
        <span class="n">fsdp_config</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">local_rank</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">rank</span> <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="k">else</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">wandb_run</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">train_config</span><span class="p">.</span><span class="n">enable_fsdp</span> <span class="ow">or</span> <span class="n">rank</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="p">[</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Key: </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">, Value: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">items</span><span class="p">()]</span>
        <span class="k">if</span> <span class="n">train_config</span><span class="p">.</span><span class="n">use_wandb</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
                <span class="n">wandb_run</span><span class="p">.</span><span class="n">summary</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="n">fire</span><span class="p">.</span><span class="nc">Fire</span><span class="p">(</span><span class="n">main</span><span class="p">)</span>

</code></pre></div></div> <p><br></p> <h2 id="6-references">6. References</h2> <ul> <li>Floating Point</li> </ul> <p><a href="https://thecho7.tistory.com/entry/Mixed-Precision-BF16%EC%9D%98-%ED%8A%B9%EC%A7%95%EA%B3%BC-%EC%9E%A5%EB%8B%A8%EC%A0%90" rel="external nofollow noopener" target="_blank">https://thecho7.tistory.com/entry/Mixed-Precision-BF16의-특징과-장단점</a></p> <ul> <li>AMP</li> </ul> <p><a href="https://pytorch.org/docs/stable/notes/amp_examples.html" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/notes/amp_examples.html</a></p> <ul> <li>NVIDIA - AMP</li> </ul> <ul> <li>Mixed Precision Training</li> </ul> <p><a href="https://thecho7.tistory.com/entry/%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Mixed-Precision-Training-MP-AMP" rel="external nofollow noopener" target="_blank">https://thecho7.tistory.com/entry/논문-리뷰-Mixed-Precision-Training-MP-AMP</a></p> <ul> <li>AdamW Source Code</li> </ul> <p><a href="https://pytorch.org/docs/stable/_modules/torch/optim/adamw.html#AdamW" rel="external nofollow noopener" target="_blank">https://pytorch.org/docs/stable/_modules/torch/optim/adamw.html#AdamW</a></p> <ul> <li>NCCL Operations</li> </ul> <p><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html" rel="external nofollow noopener" target="_blank">https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html</a></p> <ul> <li>NCCL - Ring based Algorithms</li> </ul> <p><a href="https://computing-jhson.tistory.com/81" rel="external nofollow noopener" target="_blank">https://computing-jhson.tistory.com/81</a></p> <ul> <li>DP</li> </ul> <p><a href="https://seunghan96.github.io/python/dlf/DPP/" rel="external nofollow noopener" target="_blank">https://seunghan96.github.io/python/dlf/DPP/</a></p> <ul> <li>DDP</li> </ul> <p><a href="https://algopoolja.tistory.com/95" rel="external nofollow noopener" target="_blank">https://algopoolja.tistory.com/95</a></p> <ul> <li>Pinned Memory</li> </ul> <p><a href="https://mopipe.tistory.com/191" rel="external nofollow noopener" target="_blank">https://mopipe.tistory.com/191</a></p> <ul> <li>PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</li> </ul> <p><a href="https://arxiv.org/pdf/2304.11277" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2304.11277</a></p> <ul> <li>How Fully Sharded Data Parallel (FSDP) works?</li> </ul> <p><a href="https://www.youtube.com/watch?v=By_O0k102PY&amp;t=1225s" rel="external nofollow noopener" target="_blank">https://www.youtube.com/watch?v=By_O0k102PY&amp;t=1225s</a></p> <ul> <li>다중 GPU를 효율적으로 사용하는 방법: DP부터 FSDP까지</li> </ul> <p><a href="https://medium.com/tesser-team/%EB%8B%A4%EC%A4%91-gpu%EB%A5%BC-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9C%BC%EB%A1%9C-%EC%82%AC%EC%9A%A9%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95-dp%EB%B6%80%ED%84%B0-fsdp%EA%B9%8C%EC%A7%80-3057d31150b6" rel="external nofollow noopener" target="_blank">https://medium.com/tesser-team/다중-gpu를-효율적으로-사용하는-방법-dp부터-fsdp까지-3057d31150b6</a></p> <ul> <li>Llama2 code</li> </ul> <p><a href="https://github.com/meta-llama/llama-recipes" rel="external nofollow noopener" target="_blank">https://github.com/meta-llama/llama-recipes</a></p> <p><br></p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>