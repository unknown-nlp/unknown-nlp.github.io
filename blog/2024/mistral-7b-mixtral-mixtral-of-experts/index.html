<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mistral 7B &amp; Mixtral (Mixtral of Experts) | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Mistral 7B &amp; Mixtral (Mixtral of Experts)"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2024/mistral-7b-mixtral-mixtral-of-experts/"> <meta property="og:description" content="논문 리뷰 - LLM 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Mistral 7B &amp; Mixtral (Mixtral of Experts)"> <meta name="twitter:description" content="논문 리뷰 - LLM 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2024/mistral-7b-mixtral-mixtral-of-experts/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - LLM 관련 연구",
        "headline": "Mistral 7B & Mixtral (Mixtral of Experts)",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/mistral-7b-mixtral-mixtral-of-experts/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mistral 7B &amp; Mixtral (Mixtral of Experts)</h1> <p class="post-meta"> Created on January 16, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/detection"> <i class="fa-solid fa-hashtag fa-sm"></i> detection</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-01-16</li> <li> <strong>Reviewer</strong>: 준원 장</li> <li> <strong>Property</strong>: LLM</li> </ul> <h2 id="1-mistral-7b">1. Mistral 7B</h2> <h3 id="introduction">Introduction</h3> <p>→ Higher Model Performance는 model size의 escalation을 필수로 요구하는 시대.</p> <p>→ Scaling trends가 computational cost, inference latency로 deployment 환경에서 방해물이 되기 때문에 high-level performance와 efficiency를 동시에 달성할 수 있는 balanced model을 굽는게 필요.</p> <ul> <li> <p>이를 동시에 달성하는 Mistral 7B를 Apache 2.0 license로 베포함</p> <ul> <li><strong>Apache 2.0 license - open-weight model</strong></li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Mistral 7B는 Llama2 13B model을 측정한 모든 benchmark에서 이겼으며 mathematics와 code generation에서는 Llama1 34B보다도 좋은 성능을 보임.</p> </li> <li> <p>Mistral 7B는 기본적으로 Transformer Decoder Architecture를 채택하며 아래 2가지 techniques을 채택하여 efficiency를 달성</p> <ul> <li> <p>Grouped-query attention - accelerated the inference speed.</p> </li> <li> <p>Sliding window attention - handle longer sequences more effectively at a reduced computational cost.</p> </li> </ul> </li> </ul> <p>(+ vLLM inference server와 SkyPilot을 활용해서 cloud에서 쉽게 쓸 수 있도록 베포했다고 함…)</p> <h3 id="architectural-details"><strong>Architectural details</strong></h3> <p><strong>#### Overall Model Architecture</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>#### Sliding Window Attention</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Attention</p> <ul> <li> <p>number of operations in vanilla attention is <em>quadratic</em> in the sequence length.</p> </li> <li> <p>memory increases <em>linearly</em> with the number of tokens.</p> </li> </ul> <p>→ Sliding Window Attention</p> <ul> <li> <p>receptive field : k(attention layer) * w (window size)</p> </li> <li> <p>이론적으로 4096*32 \approx 132k을 attention span으로 처리할 수 있는데, w=4096으로 두고 16K sequence을 Flash attention&amp;Xformers을 적용해서 처리하면 vanllia attention대비 2배 빠른 성능을 보인다고 함.</p> </li> </ul> <p>→ Rolling Buffer Cache</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>attention span이 고정되어 있다 = rolling buffer (데이터를 담는 공간) cache를 limit할 수 있다.</p> </li> <li> <p>(아마 가상화한) cache 메모리를 W로 설정한 다음 timestep i의 key, value 값을 cache memory에 저장하면서 cache 메모리를 효율적으로 사용할 수 있음.</p> </li> <li> <p>position는 ‘i mod W’로 두고 W보다 큰 token이 들어오면 cache memory를 overwritten해서 (e.g., 4097 % 4096 = 1 → 1번 position overwritten) cache 메모리가 커지는걸 방지할 수 있음.</p> </li> <li> <p>Sequence length가 길어질 때 model quality impact 영향 안미치고 처리가 가능하다고 함.</p> </li> </ul> <p><strong>#### Pre-fill and Chunking (GQA)</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>generation시에 token을 one-by-one으로 generation하는데 (as each token is conditioned on the previous ones)</p> </li> <li> <p>주어진 prompt는 미리 알기 때문에 cache에 prompt의 k,v값을 pre-fill해서 inference 속도를 향상시킬 수 있음.</p> </li> <li> <p>prompt가 크다면, chunk-size = window-size로 자르고 k,v를 계산하고 해당 chunck 내에서의 attention (third chunk in figure) 과 sliding window attention과 caching 사용한 attention(center block)을 동시에 사용해서 2W구간에 빠르게 연산처리.</p> <ul> <li> <p>“The cat sat on”, “the mat and saw”, “the dog go to”으로 chunk 나눔</p> </li> <li> <p>현재 “the dog go to”에 대해서 casual mask를 씌우면서 generation을 할때 pre-fill (cache) 된 window-size 만큼의 “the mat and saw”에 대해서도 sliding attention이 적용.</p> </li> </ul> </li> </ul> <h3 id="results">Results</h3> <p>→ Evaluation Pipeline을 직접 구축해서 Llama와 Mistral 7B를 아래의 benchmark에 재측정</p> <ul> <li> <p>**Commonsense Reasoning (0-shot): **Hellaswag [28], Winogrande [21], PIQA [4], SIQA [22], OpenbookQA [19], ARC-Easy, ARC-Challenge [9], CommonsenseQA [24]</p> </li> <li> <p>**World Knowledge (5-shot): **NaturalQuestions [16], TriviaQA [15]</p> </li> <li> <p>**Reading Comprehension (0-shot): **BoolQ [8], QuAC [7]</p> </li> <li> <p>**Math: **GSM8K [10] (8-shot) with maj@8 and MATH [13] (4-shot) with maj@4</p> </li> <li> <p>**Code: **Humaneval [5] (0-shot) and MBPP [2] (3-shot)</p> </li> <li> <p>**Popular aggregated results: **MMLU [12] (5-shot), BBH [23] (3-shot), and AGI Eval [29] (3-5-shot, English multiple-choice questions only)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ (Evaluation Pipeline이 Llama 원문과는 다르지만) Llama 7, 13B보다 성능 훨씬 좋음</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Code-Llama와 달리 MBPP(Code generation task)에서의 성능을 위해 나머지 task의 성능을 해치지 않음.</p> <ul> <li>Llama2 family군과 비교했을 때 동일한 size대비 Mistral은 어느정도의 성능을 보이는가?</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ reasoning, comprehension, STEM reasoning (specifically MMLU)에서 3배나 큰 모델이 달성했을 법한 성능을 달성한다.</p> <p><strong>#### Instruction Finetuning</strong></p> <ul> <li>Hugging Face repository에 public하게 풀려있는 instruction sft dataset으로 sft를 진행.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Llama2-13B보다 성능이 좋다.</p> <h2 id="2-routing">2. Routing</h2> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Routing이란, 컴퓨터 네트워크에서 정보나 데이터 패킷이 출발지부터 목적지까지 최적의 경로를 따라 전송되는 과정.</p> </li> <li> <p>우리가 인터넷에서 웹사이트를 방문하거나 이메일을 보낼 때, 그 정보는 여러 대의 컴퓨터나 스위치(Gateway, L2Access), 라우터 등을 거쳐 수신자에게 도달함.</p> </li> <li> <p>Routing은 이러한 네트워크 장비들이 각각의 정보 패킷을 어디로 보내야 할지를 결정하는 메커니즘을 뜻함. 우편 시스템이나 도로 교통망과 비슷함. (일종의 교차로? 고속도로의 톨게이트? 물류체계의 물류거점?을 기반으로 데이터가 유통되는 과정)</p> </li> </ul> <h2 id="3-mixtral-8x7b">3. Mixtral 8X7B</h2> <h3 id="introduction-1">Introduction</h3> <p>→ Mistral 7B과 동일한 Apache 2.0 license 아래에 open-weight인 sparse mixture of experts model (SMoE)인 Mixtral 8X7B를 베포함.</p> <p>→ Mixtral 8X7B는 대부분 benchmark에서 Llama2 70B와 latest gpt3.5보다 좋은 성능을 보임.</p> <p>→ Decoder only model이며, 매 token마다 router network가 8개 중 2개의 expert만 active시켜서 연산을 시킨다는 점에서 늘어나는 parameter대비 실제로 연산하는 active하는 parameter는 적은 sparse model.</p> <p>→ 32k context size를 활용한 Multilingual data로 pre-training.</p> <p>→ chat-bot model은 SFT → DPO.</p> <p>(Megablocks CUDA kernels을 통합해서 쓸 수 있게 해줬다라고 함… :: 뭔소리인지 모르겠…)</p> <h3 id="architectural-details-1"><strong>Architectural details</strong></h3> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>#### Overall Model Architecture</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Mistral과 다른점은</p> <ul> <li> <p>Fully dense context length of 32k tokens. (Sliding Window 적용 X)</p> </li> <li> <p>MLP layer가 SMoE Layer로 대체가 되었다.</p> </li> </ul> <p><strong>#### Sparse Mixture of Experts</strong></p> <ul> <li> <p>n개의 Expert E가 아래와 같이 있다고 할 =때,</p> </li> <li> <p>MoE 식은 다음과 같이 작성할 수 있다. (x는 각 token)</p> </li> <li> <p>Gating Vector를 Sparse하게 만들면 → router가 sparse한 value를 갖는 Expert로 x를 forwarding 시킬 필요가 없음 → Experts에 대한 computation cost save</p> </li> </ul> <p>: <strong>직관적으로 각 token x를 가지고 expert classification을 한다고 생각하자.</strong></p> <p>→ Total parameter count = n이 증가하더라도, K를 고정하면 individual token을 처리하는 active parameter count는 k는 증가하지 않음 (물론 network latency는 증가 당연히..)</p> <ul> <li> <p>MoE Layer를 Efficient하게 처리하는 방법</p> <ul> <li> <p>Megablocks</p> <ul> <li>MoE Layer내부에 있는 feed-forward network (FFN) 연산을 sparse matrix multiplications로 치환해서 연산량을 줄이고, 다양한 expert들에게 할당되는 token 연산을 처리할 수 있도록 한다.</li> </ul> </li> <li> <p>Expert Parallelism (sharded)</p> <ul> <li> <p>Expert를 각 GPU에 allocation</p> </li> <li> <p>EX. Expert 4개라면 id-0,1,2,3 rank에 expert를 allocation, attention을 통과한 각 token들의 hidden representation들을 0,1,2,3중 선택된 expert가 allocation된 gpu에 allocate되어서 연산. Expert forwarding이 끝나면 기존 위치로 복귀 (load balancing 때문에 GPU에 고르게 분산하는 테크닉이 필요하다고 함)</p> </li> </ul> </li> </ul> </li> <li> <p>Mixtral 8X7B는 MLP 대신 SwiGLU를 expert function E(x)로 활용함</p> <ul> <li>**SwiGLU **</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ <strong>Unbounded above, Bounded below, and Self-gated Function.</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Motivated by gating Function in the NLP area.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Llama1,2등 Foundation Backbone의 non-linear function으로 많이 활용되는중</p> <h3 id="results-1">Results</h3> <p>→ Mistral 7B와 마찬가지로 Evaluation Pipeline을 직접 구축해서 Llama2와 Mistral 7B를 아래의 benchmark에 재측정</p> <ul> <li> <p>**Commonsense Reasoning (0-shot): **Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]</p> </li> <li> <p>**World Knowledge (5-shot): **NaturalQuestions [20], TriviaQA [19]</p> </li> <li> <p>**Reading Comprehension (0-shot): **BoolQ [7], QuAC [5]</p> </li> <li> <p>**Math: **GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4</p> </li> <li> <p>**Code: **Humaneval [4] (0-shot) and MBPP [1] (3-shot)</p> </li> <li> <p>**Popular aggregated results: **MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34] (3-5-shot, English multiple-choice questions only)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Code generation &amp; Math에서 특히 성능이 좋다.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 특히 Llama2 70B 대비 Mixtral은 Token당 활용하는 active parameter가 13B임을 강조하면서 SMoE의 cost-efficiency를 또 한번 강조함.</p> <ul> <li> <p>Inference compute cost가 낮은거지, memory &amp; hardware utilization cost가 적지 않음을 이야기함</p> </li> <li> <p>sparse parameter count는 47B</p> </li> <li> <p>routing mechanism에 의한 utilization overhead, device당 2개 이상의 expert를 실행하려고 하는 경우의 메모리 문제 등등 언급</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ GPT3.5와 비교해도 밀리지 않는 성능, 특히 instruct following model은 MT-Bench에서 정말 크게 안밀림.</p> <p>→ Pretrain data를 전혀 open하지 않았지만, Mistral 7B 대비 multi-lingual도 upsample해서 English Benchmark 성능을 해치지 않고 좋은 성능 달성.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>: 뭐로 학습한거냐…</strong></p> <p><strong>#### Long Range</strong></p> <p>→ 임의의 길이를 갖는 long prompt내에서 passkey를 retrieve하는 task에서 context 길이 상관 없이 100% 성능 달성 (좌)</p> <p>→ Proof-pile dataset에서의 context 증가에 따른 PPL의 단조 감소 경향세 (우)</p> <figure> <picture> <img src="/assets/img/posts/2024-01-16-mistral-7b-mixtral-mixtral-of-experts/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li> <p>유튜브에서 paper 리뷰하는 사람이 ‘요즘 같이 학습데이터로 저작권 소송이 판치는 시대에 오히려 학습데이터 오픈하지 않은게 현명한 것 같다’라고 했는데 맞는말 같다..</p> </li> <li> <p>이럴 수록 저번에 발표한 pre-training detection 방법의 효용성이 빛을 발하는 것 같기도?!</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>