<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Knowledge Distillation, LLM, Limited Budget 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/smaller-weaker-yet-better-training-llm-reasoners-via/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</h1> <p class="post-meta"> Created on September 09, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/knowledge-distillation"> <i class="fa-solid fa-hashtag fa-sm"></i> knowledge distillation</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/limited-budget"> <i class="fa-solid fa-hashtag fa-sm"></i> limited budget</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-09-09</li> <li> <strong>Reviewer</strong>: hyowon Cho</li> <li> <strong>Property</strong>: Knowledge Distillation, LLM, Limited Budget</li> </ul> <blockquote> <p>Google DeepMind (2024-08-30)</p> </blockquote> <h1 id="introduction">Introduction</h1> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>많은 연구들이 이미 언어모델을 학습시키는 데 synthetic 데이터를 사용하고 있다. 이들 중, reasoning task에서 가장 일반적인 방법은 하나의 질문에 대해서 여러 가지 후보 답변들을 생성하게 하고, 이를 gold answer와 비교해, 맞는 정답을 가진 것들만 남기고 나머지는 버리는 것이다.</p> <p>하지만, 이렇게 여러 개의 데이터를 strong LMs로 부터 생성해 사용하는 것은 비싸고, resource-intensive하다. 또한 현실적으로 우리가 사용할 수 있는 예산은 정해져있기 때문에 만들 수 있는 solution도 그리 많지는 못하다.</p> <p>이 논문에서는 fixed compute budget 상황에서, weaker but cheaper (WC) model이 우리가 일반적으로 생각하는 것과 다르게 stronger but more expensive (SE) model을 사용하는 것보다 낫다고 주장한다.</p> <p>이를 증명하기 위해 저자들은 크게 3가지 축에서 데이터들에 대한 비교를 진행한다.</p> <ol> <li> <p>coverage, the number of unique problems that are solved,</p> </li> <li> <p>diversity, the average number of unique solutions we obtain per problem,</p> </li> <li> <p>false positive rate (FPR), the percentage of problems that arrive at the correct final answer but with a wrong reasoning.</p> </li> </ol> <p>당연히 고정된 예산 하에서, WC model이 SE model보다 더 많은 데이터를 만들어낼 수 있다. 하지만 SE가 당연히 퀄리티는 높을 것. 그렇기 때문에, WC가 더 높은 coverage and diversity 그리고 동시에 higher FPR를 가질 것이라고 이야기한다.</p> <p>이후, 저자들은 이 추측을 검증하기 위해서 SE and WC로 만든 데이터를 이용해서 모델들을 finetuning한다. 단순 하나의 방법이 아니라 여러 가지로.</p> <ol> <li> <p>knowledge distillation, where a student LM learns from a teacher LM (Hinton et al., 2015);</p> </li> <li> <p>self-improvement, where an LM learns from self-generated data (Huang et al., 2022); and</p> </li> <li> <p>a new paradigm we introduce called Weak-to-Strong Improvement, where a strong student LM improves using synthetic data from a weaker teacher LM.</p> </li> </ol> <p>저자들은 여러 개의 벤치마크에서 아주 일관적 SE-generated data 보다 WC-generated data로 학습한 결과가 훨씬 좋음을 보인다 (일반적인 믿음과 달리) 즉, WC-generated data에서 샘플링하는 것이 훨씬 compute optimal하다는 것.</p> <p>어느 정도 큰 스케일에서 더 작은 모델과 큰 모델의 성능 갭이 점차 줄어들고 있는 요즘, LM reasoners를 어떤 방식으로 학습할지 생각을 하게 한다.</p> <h1 id="preliminaries">Preliminaries</h1> <ul> <li> <p>D = {𝑞𝑖, 𝑎𝑖} = training</p> </li> <li> <p>reasoning questions = 𝑞𝑖</p> </li> <li> <p>final answers = 𝑎𝑖</p> </li> </ul> <p>다 아시겠지만, 이들을 가지고 synthetic data를 만드는 방법은 다음과 같다:</p> <ol> <li> <p>sample multiple solutions for each 𝑞𝑖 at a non-zero temperature</p> </li> <li> <p>create the synthetic data with reasoning chain &amp; generated answer</p> </li> <li> <p>filter the incorrect solutions by comparing 𝑎ˆ𝑖 𝑗 to 𝑎𝑖 and removing the solutions whose final answer do not match that of the gold answer</p> </li> </ol> <h3 id="metric">Metric</h3> <ul> <li> <p>𝑐𝑜𝑣𝑒𝑟𝑎𝑔𝑒@𝑘 (aka 𝑝𝑎𝑠𝑠@𝑘)</p> <ul> <li>k개의 솔루션을 생성했을 때, 최소 하나 이상이 정답을 맞춤</li> </ul> </li> <li> <p>𝑑𝑖𝑣𝑒𝑟𝑠𝑖𝑡𝑦@𝑘</p> <ul> <li>k개의 답변을 생성했을 때, k개중 정답을 맞춘 solution 개수의 평균</li> </ul> </li> <li> <p>false positive rate</p> <ul> <li>답은 맞았는데, reasoning이 잘못된 비율</li> </ul> </li> </ul> <h1 id="compute-matched-sampling-and-training">Compute-Matched Sampling and Training</h1> <p>당연히 고정된 예산, 고정된 sampling budget (FLOPs) 상에서, 사람들은 더 weaker but cheaper (WC) model을 통해서 더 많은 샘플들을 만들어낼 수도 있고 혹은 stronger but more expensive (SE) model을 통해 더 적지만 양질의 데이터를 만들 수 있다.</p> <p>WC model가 𝑃_𝑊𝐶 parameters를 가지고, SE가 𝑃_𝑆𝐸 parameters를 가진다고 하고, 두 모델을 이용해서 무조건 같은 예산만을 사용할 수 있다고 하면, 만들 수 있는 데이터의 개수는 다음과 같이 차이가 난다.</p> <ul> <li> <p>Following (Kaplan et al., 2020), FLOPs per inference token = 2𝑃</p> </li> <li> <p>FLOPs for 𝑇 inference tokens = 2𝑃𝑇</p> </li> <li> <p>assume that generating each solution requires an average of 𝑊 inference tokens</p> </li> <li> <p>𝑆_𝑊𝐶 and 𝑆_𝑆𝐸 = number of samples we generate per question</p> </li> <li> <p>total cost</p> <ul> <li> <p>𝐶𝑜𝑠𝑡𝑊𝐶 = 𝑛×𝑆_𝑊𝐶 ×𝑊 × (2𝑃_𝑊𝐶)</p> </li> <li> <p>𝐶𝑜𝑠𝑡𝑆𝐸 = 𝑛×𝑆_𝑆𝐸 ×𝑊 × (2𝑃_𝑆𝐸)</p> </li> <li> <p>𝑆_𝑊𝐶 =(𝑃_𝑆𝐸/𝑃_𝑊𝐶)* 𝑆_𝑆𝐸</p> </li> </ul> </li> </ul> <p>즉, 고정 예산에서 𝑃_𝑆𝐸/𝑃_WC 만큼 WC에서 더 데이터를 만들어낼 수 있다는 말. 이렇게 둘 중 하나로 데이터를 만든 이후, 고정된 스텝으로 모델들을 학습시켜보고 비교하여 데이터들의 유용성을 판단할 수 있다.</p> <p>언급했듯 학습 방법은 3가지:</p> <ol> <li>knowledge distillation(Student-LM finetuning)</li> </ol> <ul> <li>일반적으로, student 모델의 학습용으로 만들어지는 데이터는 더 똑똑하고 강한 모델에서 만들어지는 데이터를 사용한다. 높은 퀄리티를 보장하기 위해서.</li> </ul> <ol> <li>self-improvement</li> </ol> <ul> <li>Prior work (Singh et al., 2023)는 finetuning a WC model through self-generated data는 1의 방식보다 훨씬 별로라고 증명해냈다. 하지만, 해당 연구의 비교 방식은 같은 예산을 사용하지 않았기 때문에 정당하지 않다고 언급한다. 따라서, 다시 정당하게 동일한 세팅으로 다시 하여 실험을 재개한다.</li> </ul> <ol> <li>그리고 이들이 제안하는 novel weak-to-strong improvement paradigm</li> </ol> <ul> <li> <p>weak-to-strong improvement (W2S-I)은 일반적인 방법과 달리, 강한 모델은 약한 모델의 데이터로 학습하는 방식이다.</p> </li> <li> <p>즉, 약한 모델도 강한 모델을 발전시킬 수 있다</p> </li> </ul> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>Datasets</p> <ul> <li> <p>MATH</p> </li> <li> <p>GSM-8K</p> </li> </ul> </li> <li> <p>Data Generation</p> <ul> <li> <p>Gemma2 models for synthetic data generation</p> </li> <li> <p>Gemma2-9B : Gemma2-27B = WC : SE models</p> </li> <li> <p>MATH using a 4-shot prompt</p> </li> <li> <p>GSM-8K using an 8-shot prompt</p> </li> <li> <p>9B model가 27B와 3배 정도 크기 차이가 나므로, 데이터를 3배 정도 더 만들 수 있다.</p> </li> <li> <p>실험에서는 둘 나 낮은 예산의 경우: a low budget, where we generate 1 and 3 candidate solutions per problem from Gemma2-27B and Gemma2-9B</p> </li> <li> <p>높은 예산의 경우: high budget, where we generate 10 and 30 candidate solutions per problem</p> </li> </ul> </li> <li> <p>Synthetic Data Evaluation</p> <ul> <li> <p>같은 비용을 가지는 개수끼리 coverge/diversity@k 계산</p> </li> <li> <p>FRP는 50개 for each model에 대한 human eval &amp; 500개에 대한 LLM eval</p> </li> </ul> </li> <li> <p>Evaluating Finetuned Models:</p> <ul> <li>pass@1 accuracy</li> </ul> </li> </ul> <h1 id="experiments-and-results">Experiments and Results</h1> <ul> <li> <p>(§5.1) we analyze the data along various quality metrics .</p> </li> <li> <p>(§5.2) Subsequently, we present the supervised finetuning results for the different setups .</p> </li> <li> <p>(§5.3) Finally, we perform ablation studies to study the impact of dataset size, sampling strategy, and the role of quality dimensions in the model performance.</p> </li> </ul> <h2 id="1-synthetic-data-analysis">1. Synthetic Data Analysis</h2> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="1-coverage">1) Coverage</h3> <p>결론: Gemma2-9B (WC)이 Gemma2-27B (SE)보다 훨씬 좋았다.</p> <ul> <li> <p>MATH:</p> </li> <li> <p>11% and 6% at the low and high sampling budgets,</p> </li> <li> <p>8% and 1% for GSM-8K.</p> </li> </ul> <p>즉, 더 많이 만드는 것이 퀄리티가 더 낮더라도 더 많이 문제를 푸는데 도움이 되었다. converge trend는 다양.</p> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>추가적으로, 더 많이 데이터를 만드는 것은 비단 낮은 난이도의 문제 뿐만 아니라 높은 난이도에서도 공통적이었다.</p> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>오히려 반대로, 더 큰 모델로 10개 만들 때는 풀지 못했던 어려운 문제도 더 약한 모델로 30번 만드는 경우에 자주 풀렸다고.</p> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="2-diversity">2) Diversity</h3> <p>Gemma2-9B에서 생성된 데이터의 다양성은 Gemma2-27B보다 MATH 데이터셋의 저예산에서 86%, 고예산에서 125% 더 높았으며, GSM-8K 데이터셋에서는 각각 134%와 158% 더 높음.</p> <h3 id="3-fpr">3) FPR</h3> <p>human 평가에 따르면 WC 모델이 생성한 해결책의 FPR이 MATH에서는 SE 모델보다 7% 높았고, GSM-8K에서는 2% 높았음. -&gt; 생각보다 차이는 안나지만, 그럼에도 불구하고 좋지는 않음.</p> <h2 id="2-compute-optimality-results-for-training">2. Compute-Optimality Results for Training</h2> <p>그렇다면 이들은 다양하게 학습해봤을 때는 어떨까.</p> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>Student-LM Finetuning Gemma-7B를 WC와 SC로 학습한 결과. WC consistently outperforms the one finetuned on data from SC. 일반적인 믿음과 다르다.</p> </li> <li> <p>WC-LM Finetuning Gemma2-9B를 Gemma2-9B가 만든 WC와 27B가 만든 SC로 학습한 결과. 대부분 WC가 더 나았지만, 그렇지 않은 경우도 존재. 저자들은 이것이 데이터가 너무 쉬워서라고 주장.</p> </li> <li> <p>SE-LM finetuning Gemma2-27B를 양 데이터로 학습. 더 작은 모델로 만든 데이터가 놀랍게도 더 도움이 되었다.</p> </li> </ol> <blockquote> <p>Takeaway: Overall, our findings challenge the conventional wisdom that advocates training on samples from the SE model, by showing that training on samples from the WC model may be more compute-optimal across various tasks and setups.</p> </blockquote> <h2 id="3-ablation-studies">3. Ablation Studies</h2> <h3 id="impact-of-dataset-size">Impact of Dataset Size</h3> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="default-vs-compute-optimal-sampling-from-cheap-lms">Default vs Compute-Optimal Sampling from Cheap LMs</h3> <p>그렇다면, 기존에 SE를 사용하던 일반적인 방법처럼 데이터 개수를 통일하면?</p> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>볼 수 있는 것처럼, 더 퀄리티 좋은 것을 사용하는게 더 좋은 것은 확실하다.</p> <h3 id="coverage-and-diversity">Coverage and Diversity:</h3> <p>데이터의 정답률과 다양성의 측면에서 얼마나 영향력이 큰가를 small scale로 실험한 결과물. 30개의 질문으로 구성.</p> <ol> <li> <p>high coverage, high diversity</p> </li> <li> <p>high coverage, low diversity</p> </li> </ol> <ul> <li> <p>하나의 질문 당 맞는거 하나만</p> </li> <li> <p>reduces the diversity of the original WC dataset from 11 to 1, while maintaining the coverage.</p> </li> </ul> <ol> <li>low coverage, low diversity</li> </ol> <ul> <li>one solution per problem from the WC model + 낮은 정답률 가지도록 일부러 필터</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="scaling-to-state-of-the-art-language-models">Scaling to state-of-the-art language models</h1> <p>위의 부분에서는 open LM들에 대해서 실험한 결과. 이 섹션에서는 Gemini-1.5-Pro and Gemini-1.5-Flash를 사용해본다.</p> <p>모델 사이즈가 공식적으로 공개되지 않았기에, compute-matched sampling에는 pricing per output token을 Proxy로 사용한다고..</p> <ul> <li> <p>synthetic data from the Pro (SE) - $10.5</p> </li> <li> <p>Flash (WC) models. - $0.3</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <blockquote> <p>Takeaway: We demonstrate that price-matched sampling from weaker SoTA LMs produces superior reasoners compared to finetuning with data from stronger SoTA models.</p> </blockquote> <h1 id="conclusion">Conclusion</h1> <figure> <picture> <img src="/assets/img/posts/2024-09-09-smaller-weaker-yet-better-training-llm-reasoners-via/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>