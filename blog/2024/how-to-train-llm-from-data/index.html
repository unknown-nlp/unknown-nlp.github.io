<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How to Train LLM? - From Data Parallel To Fully Sharded Data Parallel | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM, Pre-Training, torch 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/how-to-train-llm-from-data/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How to Train LLM? - From Data Parallel To Fully Sharded Data Parallel</h1> <p class="post-meta"> Created on May 07, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/torch"> <i class="fa-solid fa-hashtag fa-sm"></i> torch</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-05-07</li> <li> <strong>Reviewer</strong>: 준원 장</li> <li> <strong>Property</strong>: LLM, Pre-Training, torch</li> </ul> <h3 id="-본-내용에-잘못된-내용이-있을-수도-있으니-혹시나-잘못된-부분-있다면-언제든-지적해주시면-감사하겠습니다">! 본 내용에 잘못된 내용이 있을 수도 있으니, 혹시나 잘못된 부분 있다면 언제든 지적해주시면 감사하겠습니다!</h3> <h2 id="0-floating-point">0. Floating Point</h2> <ul> <li> <p>컴퓨터에서 실수를 표현하는 방법으로, 수를 Exponent와 Fraction로 분리하여 표현.</p> </li> <li> <p>IEEE 754 표준은 floating 포인트 수를 표현하는 데 널리 사용되는 표준. 해당 표준에서는 다양한 정밀도를 제공.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>floating exampling</p> </li> <li> <p>FP16</p> </li> </ul> <p>→ fp32 대비 Exponent와 Fraction을 둘다 줄여 DL모델의 inference시 연구자들이 주로 사용하는 precision. (일반적으로 fp32으로 모델학습, fp16으로 inference을 하나 실험적으로 fp32대비 분명한 성능하락 확인)</p> <ul> <li>BF16</li> </ul> <p>→ BF16은 fp16과 다르게 자릿수를 대표하는 exponent 동일한 8비트로 설정함으로써 더 넓은 범위의 수를 표현할 수 있음.</p> <p>→ 대신 fraction의 자릿수를 희생했기 때문에 수의 정밀도가 떨어진다는 한계가 존재</p> <p>(ex)</p> <p>십진수 0.1을 이진수로 표현하면 무한 반복 이진수: <code class="language-plaintext highlighter-rouge">**0.0001100110011001100...**</code> (반복)</p> <ul> <li> <p>fp16</p> </li> <li> <p>fraction는 10비트를 사용하여 이 값을 가능한 한 정확하게 근사: <code class="language-plaintext highlighter-rouge">**0.0001100110**</code></p> </li> <li> <p>exponent는 이진 표현에서 소수점을 왼쪽으로 몇 칸 이동시켜야 하는지에 따라 결정되며, 이 경우 -4. exponent 필드 값은 −4+15=11 (이진수 <code class="language-plaintext highlighter-rouge">**01011**</code>).</p> </li> <li> <p>bf16</p> </li> <li> <p>fraction는 7bit를 사용하여 값을 근사: <code class="language-plaintext highlighter-rouge">**0.0001100**</code></p> </li> <li> <p>exponent는 FP16과 동일하게 -4이고, 지수 필드 값은 −4+127=123 (이진수 <code class="language-plaintext highlighter-rouge">**01111011**</code>).</p> </li> <li> <p>결론</p> </li> </ul> <p>⇒ Layer가 깊어질수록 logit값이 커지는 transformer계열에서는 BF16을 활용해 모델을 training하는 것이 적합해보임. input의 스케일이 크게 다른 경우, BF16은 underflow나 overflow를 방지할 수 있음.</p> <p>⇒ Normalization, Optimizer state 연산 (e.g., momentum) 등 term을 계속해서 나눠주는 연산의 경우 정밀도를 희생한 BF16은 오차를 유발할 수 있기 때문에 mixed precision 같은 방법론들을 도입해서 한계를 방지해야함.</p> <h2 id="1-mixed-precision">1. Mixed Precision</h2> <ul> <li> <p>Mixed Precision은 DL 학습 시 다양한 Precision을 혼합하여 사용하는 기술.</p> </li> <li> <p>대표적으로 NVIDIA의 Tensor Cores에서는 fp16과 fp32를 혼합하여 더 빠른 수행 시간과 높은 수치 정밀도를 제공</p> </li> <li> <p>fp16으로는 표현할 수 있는 숫자체계가 제한적이기 때문에 gradient update시에 underflow 문제 발생 가능</p> </li> </ul> <p>→ 빨간선 왼쪽이 fp16으로 표현할 수 없는 범위</p> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→fp16으로 tensor 연산 시의 문제점</p> <ul> <li> <p>위를 해결하기 위해 Precision을 혼합해서 활용하는 Mixed Precision 기법이 나옴</p> </li> <li> <p>가정: 우리의 Model은 fp32이다. 해당 모델이 가지고 있는 weights을 Master weights라고 명명.</p> </li> </ul> <ol> <li> <p>fp32로 표현된 Master weights을 복사하여 fp16 weights로 가져옴</p> </li> <li> <p>fp16 weights로 forward &amp; loss 계산</p> </li> <li> <p>loss값이 scaling factor를 곱한다.</p> </li> <li> <p>fp16으로 표현된 loss에서 backward (loss.backward()) → backpropagate 한다.</p> </li> <li> <p>upscaling한 gradient(XS)를 optimizer.step()을 위해서 다시 unscale (X1/S)해준다.</p> </li> <li> <p>optimizer.step()은 fp32에서 이루어지며 master weight을 업데이트 함.</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Torch AMP(<strong>Automatic Mixed Precision</strong>)에 위의 설명 및 코드가 그대로 반영되어 있음</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creates model and optimizer in default precision
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">().</span><span class="nf">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="p">...)</span>

<span class="c1"># Creates a GradScaler once at the beginning of training.
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epochs</span><span class="p">:</span>
    <span class="k">for</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

        <span class="c1"># Runs the forward pass with autocasting.
</span>        <span class="k">with</span> <span class="nf">autocast</span><span class="p">(</span><span class="n">device_type</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">):</span>
            <span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

        <span class="c1"># Scales loss. Calls backward() on scaled loss to create scaled gradients.
</span>        <span class="c1"># Backward passes under autocast are not recommended.
</span>        <span class="c1"># Backward ops run in the same dtype autocast chose for corresponding forward ops.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

        <span class="c1"># scaler.step() first unscales the gradients of the optimizer's assigned params.
</span>        <span class="c1"># If these gradients do not contain infs or NaNs, optimizer.step() is then called,
</span>        <span class="c1"># otherwise, optimizer.step() is skipped.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>

        <span class="c1"># Updates the scale for next iteration.
</span>        <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
</code></pre></div></div> <h2 id="2-list-of-nccl-operation">2. List of NCCL Operation</h2> <ul> <li>2개의 프로세스간의 통신 패턴을 point-to-point communication(점대점 통신)라고 명명한다면, 여러 개의 프로세스간의 통신을 collective communication(집합 통신)이라고 명명한다.</li> </ul> <p>→ point-to-point communication는 sender는 데이터를 보내고, receiver는 데이터를 받도록 설계하면 되기 때문에 구현 난이도가 상대적으로 쉬움</p> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ multiple senders와 multiple receivers가 있는 collective communication의 경우, topology들이 구성이 매우 다양하기 때문에 (위 그림처럼) 데이터 통신을 최적화하기 매우 어렵다는 한계점이 존재한다.</p> <ul> <li> <p>조금 더 자세히 말하면, 몇 개의 GPU가 있고, 어떤 GPU가 어떤 GPU와 어떻게 연결(PCIe, NVLink, IB, ethernet 등등) 되어 있는 지와 같은 topology 정보는 통신 최적화에 필수로 고려해야 하는 요소인데, 그 조합이 너무나도 많기에 이것을 다 만족하는 최적화된 솔루션을 찾아 구현하기가 매우 어렵다.</p> </li> <li> <p>이러한 문제점을 해결하기 위해 ‘Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of workstations. J. Parallel Distrib. Comput., 69:117–124, 2009.’에서 모든 어떤 topology라도 <strong>Ring(s) topology</strong>로 생각하고 구현할 경우 최적 성능을 달성할 수 있음을 보여줌.</p> </li> </ul> <p>→ 위는 Broadcast 연산인데 GPU0이 GPU[1:3]에 data를 뿌릴때 순차적으로 (GPU0→ GPU1 이 끝나면 GPU0 → GPU2 …)으로 하는게 아니라 데이터를 작은 조각으로 나누어서 GPU0 → GPU1 &amp; GPU1 → GPU2 &amp; GPU2 → GPU3 이렇게 인접한 모든 프로세스가 데이터를 전송하도록 구현하였음.</p> <ul> <li> <p><strong>NCCL(NVIDIA Collective Communications Library)</strong>은 <strong><em>멀티 GPU</em></strong> 및 <strong><em>멀티 노드 환경</em></strong>에서 고성능을 제공하는 통신 라이브러리로 다양한 네트워크 topology에서도 최적 성능을 달성하는 것을 목표로 개발되었으며 이전에 언급한 Ring-based 집합통신 알고리즘을 기반으로 최적화된 집합 통신을 구현</p> </li> <li> <p>NCCL은 DL training, inference에서의 다양한 집합 연산(예: All-reduce, All-gather, Broadcast 등)을 최적화하여 각 GPU 사이에 일관되고 효율적인 통신을 제공하여 병렬 처리 성능을 극대화하는것에 목표를 두고 있음.</p> </li> <li> <p>동기 및 비동기 통신을 지원하며, CUDA 스트림을 통해 다른 연산과 겹쳐서 수행이 가능하다고 함.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ (1) GPU를 이용한 연산 GPGPU(CUDA)으로 가속화 (2) NCCL을 활용해 하나의 GPU가 아닌 multiple GPUs, 나아가 multi-node 상의 multiple GPUs의 통신을 통한 연산 가속화</p> <ul> <li> <p>General-Purpse computing on Graphic Processing Unit: 일반적으로 컴퓨터 그래픽스를 위한 계산만 맡았던 그래픽 처리 장치를, 전통적으로 중앙 처리 장치가 맡았던 응용 프로그램들의 계산에 사용하는 기술</p> </li> <li> <p>간단한 용어 정리</p> </li> <li> <p><strong>NCCL Operations (DP나 DDP 쓰면서 다들 호출해보셨을 operations들)</strong></p> </li> </ul> <ol> <li> <p><strong>AllReduce</strong></p> </li> <li> <p><strong>Broadcast</strong></p> </li> <li> <p><strong>Reduce</strong></p> </li> <li> <p><strong>AllGather</strong></p> </li> <li> <p><strong>ReduceScatter</strong></p> </li> </ol> <ul> <li> <p><strong>inXY</strong>: 노드 X에서 노드 Y로 보내진 데이터.</p> </li> <li> <p><strong>out[Y]</strong>: 노드 Y에서 모든 입력 데이터를 합산한 결과를 저장하는 배열.</p> </li> <li> <p><strong>count</strong>: 각 노드에서 전송된 데이터 요소 수.</p> </li> <li> <p><strong>i</strong>: 각 노드의 데이터 배열에서의 인덱스.</p> </li> <li> <p><strong>노드 0</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in00, in01, in02, in03]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 1</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in10, in11, in12, in13]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 2</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in20, in21, in22, in23]**</code>의 데이터 존재</p> </li> <li> <p><strong>노드 3</strong>에서는 <code class="language-plaintext highlighter-rouge">**[in30, in31, in32, in33]**</code>의 데이터 존재</p> </li> </ul> <p>노드 0의 출력 <code class="language-plaintext highlighter-rouge">**out0**</code>을 구하는 방법:</p> <ul> <li> <p>노드 0으로 보내진 모든 데이터 요소를 합산.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">**out0 = sum(in00*4+0, in10*4+0, in20*4+0, in30*4+0)**</code></p> </li> </ul> <h2 id="3-dp-dataparallel--ddp-distributeddataparallel">3. DP (DataParallel) &amp; DDP (DistributedDataParallel)</h2> <ul> <li> <p>두 방법론 모두 효율적으로 모델을 학습하기 위해 등장한 방법론.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DataParallel</code>은 단일 작업, 멀티쓰레드 방법론으로 GPU에 입력 데이터를 부분적으로 할당(mini-batch를 분할)하고 동일한 신경망 모델을 복제하여 이용하는 방식</p> </li> <li> <p>반면, <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>은 다중 작업이며 단일 및 다중 기기 학습을 전부 지원하는 방식</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">DataParallel</code>가 가진 스레드간 GIL 경합, 복제 모델의 반복 당 생성, 입력 및 수집 출력으로 인한 추가적인 오버헤드를 <code class="language-plaintext highlighter-rouge">DistributedDataParallel</code>가 보완해 현재는 DDP를 주로 활용,</p> </li> <li> <p><strong><em>DP와 DDP 모두 (당연히 ) Multi-GPUs training setting을 가정함</em></strong></p> </li> <li> <p><strong>DP (DataParallel)</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-05-07-how-to-train-llm---from-data/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p><strong>Scatter</strong></p> </li> <li> <p><strong>Replicate</strong></p> </li> <li> <p><strong>Forward</strong> </p> </li> </ol> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>