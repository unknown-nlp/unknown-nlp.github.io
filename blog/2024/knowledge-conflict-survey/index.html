<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Knowledge conflict survey | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Knowledge conflict survey"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2024/knowledge-conflict-survey/"> <meta property="og:description" content="논문 리뷰"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Knowledge conflict survey"> <meta name="twitter:description" content="논문 리뷰"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2024/knowledge-conflict-survey/",
        "@type": "BlogPosting",
        "description": "논문 리뷰",
        "headline": "Knowledge conflict survey",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/knowledge-conflict-survey/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Knowledge conflict survey</h1> <p class="post-meta"> Created on August 13, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a>   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/embedding"> <i class="fa-solid fa-hashtag fa-sm"></i> embedding</a>   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-08-13</li> <li> <strong>Reviewer</strong>: yukyung lee</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <p><strong><em>Definition of Knowledge Conflict</em></strong></p> <ul> <li> <p>In-depth analysis of knowledge conflicts for LLM</p> <ul> <li>blending contextual and parametric knowledge</li> </ul> </li> <li> <p>Three types of knowledge conflicts</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Context-memory (CM) : Context ↔ Parametric memory</p> </li> <li> <p>Inter-context (IC): external document간에 일어나는 conflict</p> </li> <li> <p>Intra-memory (IM): 모델 내부의 memory들끼리 conflict (원인→다양한 pre-trained dataset)</p> </li> <li> <p>knowledge conflict가 일어나는 상황을 noise나 misinformation이 있는 상황에서 parametric knowledge와의 충돌로 보고있는듯 (abstract)</p> </li> <li> <p>이 논문의 궁극적인 목표는 conflict를 해결해서 LLM의 robustness를 향상시키는 것으로 보임</p> </li> </ul> <p><strong><em>Key terms</em></strong></p> <ul> <li> <p>Parametric knowledge (memory): LM’s world knowledge</p> </li> <li> <p>External contextual knowledge (context): user prompt, dialogues, retrieved documents</p> </li> <li> <p>knowledge conflict: The discrepancies among the contexts and the model’s parametric knowledge are referred to as knowledge conflicts</p> </li> </ul> <dl> <dt><strong><em>Knowledge Conflict (Causes - Phenomenon- Behaviors)</em></strong></dt> <dd> <p>lifecycle of knowledge conflicts as both a cause leading to various behaviors, and an effect emerges from the intricate nature of knowledge</p> </dd> </dl> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Knowledge conflict is originally rooted in ODQA (answer→ short / yellow)</p> <ul> <li> <p>전반적으로 external knowledge가 잘못된 지식인 경우에 문제가 발생할 수 있다는 이야기를 하고 있음</p> </li> <li> <p>resolving knowledge conflicts;</p> </li> </ul> </li> <li> <p>Timing relative to potential conflicts: pre-hoc, post-hoc strategies</p> </li> </ul> <p><strong><em>Taxonomy</em></strong></p> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong><em>Related Dataset</em></strong></p> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="2-context-memory-conflict">2. Context-Memory Conflict</h2> <blockquote> <p><em>This static parametric knowledge stands in stark contrast to the dynamic nature of external information, which evolves at a rapid pace (De Cao et al., 2021; Kasai et al., 2022)</em></p> </blockquote> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Causes: <em>Why do context-memory conflicts happen</em>?</p> <ul> <li> <p><strong>Temporal Misalignment</strong>: trained on past data (shifts in language use, cultural changes, updates in knowledge) → up-to-date contextual information accurate (Faithful to Context 방법이 더 적합)</p> <ul> <li> <p>Knowledge Editing (KE): directly update the parametric knowledge of an existing pre-trained model</p> </li> <li> <p><strong>Retrieval Augmented Generation (RAG)</strong>: leverages a retrieval module to fetch relevant documents from external sources (e.g., database, the Web) to supplement the model’s knowledge without altering its parameters</p> </li> </ul> </li> </ul> </li> </ul> <blockquote> <p><em>For RAG, it is inevitable to encounter knowledge conflicts since the model’s parameters are not updated (Chen et al., 2021; Zhang and Choi, 2021)</em></p> </blockquote> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Continue learning: update the internal knowledge through continual pre-training on new and updated data
</code></pre></div></div> <ul> <li>Misinformation Pollution: false or misleading information (particularly for time-invariant knowledge) → contextual information considered incorrect (Faithful to Memory 방법이 더 적합)</li> </ul> <blockquote> <p><em>This vulnerability poses a real threat, as models might unknowingly propagate misinformation if they incorporate deceptive inputs without scrutiny (Xie et al., 2023; Pan et al., 2023b; Xu et al., 2023)</em></p> </blockquote> <ul> <li> <p>Analysis: <em>How do LLMs navigate context-memory conflicts?</em></p> <ul> <li>Open-domain question answering (ODQA)</li> </ul> </li> </ul> <p><strong><em>Longpre et al. (2021):</em></strong> The authors create an automated framework that identifies QA instances with named entity answers, then substitutes mentions of the entity in the gold document with an alternate entity, thus creating the conflict context. <strong>(parametric knowledge)</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- entity substitution approach used by Longpre et al. (2021) potentially reduces the semantic coherence of the perturbed passages

- Longpre et al. (2021) based their research on single evidence passages
</code></pre></div></div> <p><strong><em>Chen et al. (2022):</em></strong> revisit this setup while reporting differing observations <strong>(contextual knowledge)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Chen et al. (2022) utilizes multiple evidence passages
</code></pre></div></div> <p>— <em>Emergence of really Large Language Models (ChatGPT, Llama 2)—</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- ***Tan et al. (2024):*** examine how LLMs blend retrieved context with generated knowledge in the ODQA setup **(parametric knowledge)**
</code></pre></div></div> <ul> <li>General QA</li> </ul> <p><strong>_Xie et al. (2023): _</strong>generate conflicting context alongside the memorized knowledge <strong>(contextual knowledge), **Meanwhile, they also identify a strong confirmation bias (Nickerson, 1998) in LLMs, i.e., the models tend to favsor information consistent with their internal memory **(parametric knowledge)</strong></p> <p><strong>_Wang et al. (2023h): _</strong>desired behaviors when an LLM encounters conflicts should be to pinpoint the conflicts and provide distinct answers. While LLMs perform well in identifying the existence of knowledge conflicts, they struggle to determine the specific conflicting segments and produce a responsge with distinct answers amidst conflicting information.</p> <p><strong>_Ying et al. (2023): _</strong>two perspectives: factual robustness (the ability to identify correct facts from prompts or memory) and decision style (categorizing LLMs’ behavior as intuitive, dependent, or rational-based on cognitive theory) → LLMs are highly susceptible to misleading prompts, especially in the context of commonsense knowledge.</p> <p><strong><em>Qian et al. (2023):</em></strong> evaluate the potential interaction between parametric and external knowledge more systematically, cooperating knowledge graph (KG) → LLMs often deviate from their parametric knowledge when presented with direct conflicts or detailed contextual changes</p> <p><strong><em>Xu et al. (2023):</em></strong> study how LLMs respond to knowledge conflicts during interactive sessions → LLMs tend to favor logically structured knowledge, even when it contradicts factual accuracy.</p> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Remarks</strong></p> <p>*I. Crafting Conflicting Knowledge. *Model’s behavior under context-memory conflict is analyzed by <strong>artificially creating conflicting knowledge</strong>, in early years through <strong>entity-level substitutions</strong> and more recently by <strong>employing LLMs to generate semantically coherent conflicts</strong>.</p> <p><em>II. What is the conclusion?</em> <strong>No definitive rule exists for whether a model prioritizes contextual or parametric knowledge.</strong> Yet, knowledge that is <strong>semantically coherent, logical, and compelling</strong> is typically favored by models over generic conflicting information.</p> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Solution: _ What strategies are there to deal with context-memory conflicts_?</p> <ul> <li> <p><strong>Faithful to Context: Context Prioritization</strong></p> <ul> <li>Fine-tuning</li> </ul> </li> </ul> </li> </ul> <p><strong>_Li et al. (2022a): _</strong>Introduce Knowledge Aware FineTuning (KAFT) to strengthen the two properties (controllability and robustness) by incorporating counterfactual and irrelevant contexts into standard training datasets</p> <p><strong>_Gekhman et al., 2023): _</strong>TrueTeacher focuses on improving factual consistency in summarization by annotating model-generated summaries with LLMs</p> <p><strong>_Xue et al., (2023): _</strong>DIAL<strong>* *</strong>improves factual consistency in dialogue systems</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- **Prompting**
</code></pre></div></div> <p><strong>_Zhou et al. (2023d): _</strong>specialized prompting strategies, specifically opinion-based prompts and counterfactual demonstrations</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Decoding
</code></pre></div></div> <p><strong>_Shi et al. (2023a): _</strong>Contextaware Decoding (CAD) to reduce hallucinations by amplifying the difference in output probabilities with and without context</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Knowledge Plug-in
</code></pre></div></div> <p><strong>_Lee et al. (2022a): _</strong>uses plug-and-play modules to store updated knowledge, ensuring the original model remains unaffected</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Pre-training
</code></pre></div></div> <p><strong>_Shi et al., (2023b): _</strong>extends LLMs’ ability to handle long and varied contexts across multiple documents</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Predict Fact Validity
</code></pre></div></div> <p><strong>_Zhang and Choi (2023): _</strong>introducing fact duration prediction to identify and discard outdated facts in LLMs</p> <ul> <li> <p>Discriminating Misinformation(Faithful to Memory): Parametric Prioritization</p> <ul> <li> <p>Prompting</p> </li> <li> <p>Query Augmentation</p> </li> <li> <p>Training Discriminator</p> </li> </ul> </li> <li> <p><strong>Disentangling Sources: treat context and knowledge separately and provide disentangled answers</strong></p> </li> </ul> <p><strong>_Neeman et al., 2022: _</strong>DisentQA trains a model that predicts two types of answers for a given question: one based on contextual knowledge and one on parametric knowledge</p> <p><strong>_Wang et al. (2023h): _</strong>three-step process designed to help LLMs detect conflicts, accurately identify the conflicting segments, and generate distinct, informed responses based on the conflicting data</p> <ul> <li>Improving Factuality: Integrated response leveraging both context and parametric knowledge</li> </ul> <p><strong>_Zhang et al. (2023e): _</strong>COMBO uses discriminators trained on silver labels to assess passage compatibility, improving ODQA performance by leveraging both LLM-generated (parametric) and external retrieved knowledge</p> <p><strong>_Jin et al. (2024a): _</strong>CD2 maximizes the difference between various logits under knowledge conflicts and calibrates the model’s confidence in the truthful answer</p> <p><strong>Remarks</strong></p> <p>Some researchers regard that LLM should not rely solely on either parametric or contextual information but instead grant LLM users the agency to make informed decisions based on distinct answers (Wang et al., 2023h; Floridi, 2023)</p> <h2 id="3-inter-context-conflict">3. Inter-Context Conflict</h2> <p><strong><em>Outdated Information</em></strong></p> <ul> <li> <p>Facts can evolve !</p> <ul> <li>Addressing conflicts that arise from documents bearing different timestamps, especially when a user’s prompt specifies a particular time period.</li> </ul> </li> <li> <p>Some findings</p> <ul> <li> <p>knowledge source간의 inconsistency는 model의 confidence level에 거의 영향을 미치지않는다고 발견함 [1]</p> </li> <li> <p>모델들은 질문과 관련된 context, model parametric knowledge와 일치하는 context를 선호 [1]</p> </li> <li> <p>LLM이 모델의 parametric memory와 일치하는 evidence에 bias를 보인다고 주장 [2]</p> </li> <li> <p>주어진 문맥 내에서 더 많은 문서에 의해 입증된 답변을 선호 [2]</p> </li> <li> <p>데이터가 도입되는 순서에 대해 민감 [2]</p> </li> </ul> </li> <li> <p><strong>References</strong></p> </li> </ul> <p>[1] Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence.</p> <p>[2] Adaptive chameleon or stubborn sloth: Unraveling the behavior of large language models in knowledge conflicts 논문에서도 위와 같은 결과를 얻었다고 reporting</p> <p><strong><em>Solutions</em></strong></p> <ul> <li> <p>Eliminating Conflict (유의미하게 참고할 것들은 없는듯)</p> <ul> <li>LLM with tool</li> </ul> </li> <li> <p>Improving Robustness</p> <ul> <li>Query Augmentation (현실적이지만 novelty가 없음)</li> </ul> </li> </ul> <h2 id="4-intra-memory-conflict">4. Intra-Memory Conflict</h2> <figure> <picture> <img src="/assets/img/posts/2024-08-13-knowledge-conflict-survey/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Definition: LLM의 **parametric knowledge 내부 **(latent representation)에서의 inconsistency들로 인해 발생되는 문제</p> </li> <li> <p>Causes: IM이 발생하는 원인은 어떤게 있을까?</p> <ol> <li><strong>Bias in Training Corpora</strong></li> </ol> </li> </ul> <p>Training 과정에서 주목해야 하는 conflict 원인이다.</p> <p>당연하게도 LLM이 parametric knowledge를 형성함에 pre-training 과정이 가장 많이 기여한다.</p> <p>The training data 내부의 어떤 편향이나 논란의 소지가 있는 정보가 있을 때 conflict이 발생한다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- ***examples***
</code></pre></div></div> <p><strong>Wang et al. (2023d)</strong> Parametric knowledge의 bias와 인과관계에 있는 <strong>Entity bias와 해결 방안</strong>에 대해 이야기한다.</p> <p>Entity bias의 예로는 ‘빌 게이츠’라는 엔티티에 대해, ‘마이크로소프트’의 ‘visitor’(현재의 상태) 대신 ‘founder’라고 대답하는 것이다.</p> <p>논문은 3가지 이런 Entity bias를 해결하기 위한 세 가지 방법을 제안한다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Structured Causal Model: Parameter estimation을 더 용이하게 하기 위한 방법이다.

  - Training-time intervention: 내부를 확인할 수 있는 모델에 적용할 수 있는 방법으로, 학습 도중에 엔티티의 embedding을 주변 엔티티와 섞어 bias를 줄이는 방법이다.

  - In-context intervention: GPT-3.5 같이 모델 내부를 확인할 수 없는 경우 활용하는 방법으로, inference 과정에서 엔티티들을 유사한 다른 엔티티로 교체해 bias를 줄이는 방법이다.
</code></pre></div></div> <p>논문에서 제안하는 가능한 Future work로는 Better benchmark for comprehensive evaluation, Causal interventions (제안된 3가지 방법들) 의 응용 등을 제안했다.</p> <p><strong>Xu et al. (2023d) **해당 논문에서는</strong> Entity typing (엔티티의 type을 결정하는 task) model과 모델 내부의 여러 상관관계들 그리고 발생하는 Bias와 해결방안**에 대해 이야기한다.</p> <p>Bias의 종류</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Mention-Context Bias: 맥락(context)보다 엔티티 언급 자체에 너무 편향되는 경우

  - Lexical Overlapping Bias: 엔티티를 언급하는 부분들에서의 어휘적 유사성에 의해 발생되는 편향

  - Named Entity Bias: 너무 자주 언급되었던 엔티티의 경우 기존의 parametric knowledge에 너무 의존하는 편향

  - Pronoun Bias: 고유명사에 비해 대명사를 사용한 inference에 더 좋은 성능을 보이는 편향

  - Dependency Bias: 문맥 상 dependency 들을 잘못해석하는 경우 나타나는 편향

  - Overgeneralization Bias: 많은 엔티티를 갖는 라벨쪽으로의 편향
</code></pre></div></div> <p>Bias 해결방안: Counterfactual Data Augmentation</p> <p>제안하는 Future work: 추가적인 bias나 debiasing technique의 발견, 더 robust한 모델 개발</p> <ol> <li><strong>Decoding Strategy</strong></li> </ol> <p>Inference 과정에서 주목해야 하는 conflict 원인이다.</p> <p>LLM의 인퍼런스의 직접적인 결과는 다음 토큰에 대한 probability distribution 으로부터 샘플링한 것이다.</p> <p>최근 가장 널리 사용되고 있는 샘플링 방식은 stochastic 한데, stochastic sampling 때문에 인퍼런스 과정에서 inconsistency가 발생한다.</p> <p><em>e.g.) 다른 사안에 대해 similarly phrased prompt를 사용 했을때, 혹은 같은 사안에 대해 다른 표현으로 질문했을 때 parametric knowledge 내에서의 충돌 발생</em></p> <p><em>LLMs produce entirely different content, even when provided with the same context</em></p> <ol> <li><strong>Knowledge Editing</strong></li> </ol> <p>모델의 parametric knowledge를 바꾸는 과정(e.g. fine-tuning)에서 주목해야 하는 conflict 원인이다.</p> <p>파인튜닝이 코스트가 큰 만큼 최대한 작은 scope에서 parametric knowledge를 변경하는 여러 시도들이 있었다.하지만 조금씩 parametric knowledge를 수정하는 과정에서 일반화가 충분히 일어나지 않는 문제들이 발생했다.</p> <p>→ 정리: <strong>Bias in Training Corpora</strong>가 IM conflict에 가장 많이 기여하고 <strong>Decoding strategy</strong>가 이를 악화시키는 것으로 나타난다. <strong>Knowledge editing</strong>을 하는 과정에서 기존 모델의 지식과 충돌이 일어나는 것은 자명하다.</p> <ul> <li> <p>Analysis</p> <ol> <li><strong>Self Inconsistency</strong></li> </ol> </li> </ul> <p>모델의 parametric model을 활용하는 과정에서 나타나는 knowledge inconsistency로 다음과 같은 경향을 가진다.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- LLM은 Uncommon knowledge에 대해 더 강하게 나타나는 경향을 보인다.

- 처음 내놓은 응답에 대해 계속 진위여부를 확인할 때 번복하는 경향을 보인다.

- 인코딩 기반의 모델들은 [가까이 있는 단어], [자주 등장하는 단어] 에 많은 영향을 받아 사실이 아닌 단어를 생성하는 경향을 보인다. → [지식의 진위여부와 관계 있는 단어]에 집중해야 하는데..!
</code></pre></div></div> <p><em>co-occurence bias: LLM이 정답보다 바이럴을 더 선호하는 편향</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Pretraining 과정에서 함께 등장하지 않은 ‘대상’과 ‘역할’에 대해 추후에 Knowledge editing을 해줘도 잘 성능을 보이지 못하는 경향을 보인다. (예: 빌게이츠) → LLM은 Pretrain 단계에서, 즉 parametric knowledge가 형성되는 단계에서 training corpora의 단어 occurence에 취약하다.
</code></pre></div></div> <ol> <li><strong>Latent Representation of Knowledge</strong></li> </ol> <p>LLM의 기반인 multi-layer transformer 구조는 inter-memory conflict를 유발한다.</p> <p>LLM의 memory</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Shallow level memory: Low-level information

- Deeper level memory: Semantic information

- Factual information이 특정 트랜스포머 레이어에 집중돼있고, 나머지 레이어에 비슷한 내용에 대해 inconsistent하고 not reliable한 정보들이 저장되어있다.

- 올바른 정보가 latent space에 저장돼있다 한들, 그것을 불러오고 응답을 generate하는 과정에서 정확히 표현되지 않는 경우가 존재한다.
</code></pre></div></div> <ol> <li><strong>Cross-lingual Inconsistency</strong></li> </ol> <p>언어별로 knowledge set이 달라 같은 사안에 대해 사실여부가 다르게 저장되어있다.</p> <p>즉 같은 사실(semantically identical)이라도 다른 언어로 표현돼있는 knowledge의 경우, 모델 파라미터 내부에 서로 다른 영역에 저장되어있다.</p> <ul> <li> <p>Solutions</p> <ol> <li> <p><strong>Improving Consistency</strong></p> </li> <li> <p>Fine-tuning (pre-hoc)</p> </li> <li> <p><strong>Plug-in</strong> (pre-hoc)</p> </li> </ol> </li> </ul> <p>단어-뜻 piar를 통해 구성한 데이터셋으로 모델을 retrain한다</p> <p>retrain한 모델의 파라미터와 기존 모델의 파라미터를 융합해 variance를 줄이는 방식</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. **Ouptut Ensemble **(post-hoc)
</code></pre></div></div> <p>두개의 모델로부터 나온 output을 ensemble하는 방식</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Base model: 질문에 대한 답변 셋을 생성

  - Relation model: 답변의 논리적 일관/일치성을 평가
</code></pre></div></div> <ol> <li> <p><strong>Improving Factuality</strong></p> </li> <li> <p><strong>DoLa</strong> (post-hoc)</p> </li> </ol> <p>Contrastive Decoding Approach Steps</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Premature layer &amp; Mature layer: 내재돼있는 parametric knowledge의 신뢰도에 따라 둘을 구분한다.

  1. 모델이 다음 단어를 샘플링 할 때 확률을 premature layers와 mature layers 사이의 log probability의 차이 *(KLD, XE ..?) *를 통해 계산한 후 샘플링한다.

1. **ITI** (post-hoc)
</code></pre></div></div> <p>DoLa와 유사한 방식</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. Ground truth일 가능성이 높은 모델의 영역(attention heads)을 찾아낸다. *TruthfulQA 벤치마크를 활용*

  1. 모델이 답변을 생성할 때 신뢰도가 높은 정보와 높은 상관관계를 가지는 방향으로 모델의 activation들을 조정한다.

  1. 답변의 모든 단어에 대해 이 과정을 반복한다.
</code></pre></div></div> <h2 id="5-challenges-and-future-directions">5. Challenges and Future Directions</h2> <p><strong>Knowledge Conflicts in the Wild</strong></p> <ul> <li> <p>Retrieval Augmented Language Model에서 자주 일어날 수 있는 상황</p> <ul> <li>conflict : retrieved documents (from web)</li> </ul> </li> </ul> <p><strong>Solution at a Finer Resolution</strong></p> <ul> <li> <p>User query is important : subjective or debatable questions lead to conflicts</p> </li> <li> <p>the source of conflicting information can vary ; mis info/ outdated facts/ partially corrected data</p> </li> <li> <p>User expectation</p> </li> </ul> <p><strong>Evaluation on Downstream Tasks</strong></p> <ul> <li>대부분의 연구들이 QA를 사용하는데, broder implication을 고려하여 다양한 task를 평가해봐야 한다고 주장</li> </ul> <p><strong>Interplay among the Conflicts</strong></p> <ul> <li>internal knowledge inconsistency를 잘 잡아내는게 중요</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>