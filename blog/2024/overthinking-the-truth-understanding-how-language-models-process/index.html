<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> OVERTHINKING THE TRUTH: UNDERSTANDING HOW LANGUAGE MODELS PROCESS FALSE DEMONSTRATIONS | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Papers"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Papers | OVERTHINKING THE TRUTH: UNDERSTANDING HOW LANGUAGE MODELS PROCESS FALSE DEMONSTRATIONS"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2024/overthinking-the-truth-understanding-how-language-models-process/"> <meta property="og:description" content="논문 리뷰"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="OVERTHINKING THE TRUTH: UNDERSTANDING HOW LANGUAGE MODELS PROCESS FALSE DEMONSTRATIONS"> <meta name="twitter:description" content="논문 리뷰"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2024/overthinking-the-truth-understanding-how-language-models-process/",
        "@type": "BlogPosting",
        "description": "논문 리뷰",
        "headline": "OVERTHINKING THE TRUTH: UNDERSTANDING HOW LANGUAGE MODELS PROCESS FALSE DEMONSTRATIONS",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/overthinking-the-truth-understanding-how-language-models-process/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">OVERTHINKING THE TRUTH: UNDERSTANDING HOW LANGUAGE MODELS PROCESS FALSE DEMONSTRATIONS</h1> <p class="post-meta"> Created on January 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-01-23</li> <li> <strong>Reviewer</strong>: hyowon Cho</li> </ul> <h1 id="introduction">Introduction</h1> <p>최근의 언어모델들의 핵심은 context-following이 가능하다는 것이다. 때때로 그들은 finetuning 없이도 그에 준하는 성능을 보인다. 이 때문에, 최신 연구들은 context가 성능에 미치는 영향을 연구하고 더 발달된 프롬프트를 만들기 위한 연구 혹은 그것에 대한 Internal mechanism에 집중하고 있다.</p> <p>하지만, context-following이 가능하는 말은 즉, incorrect, toxic, unsafe한 모델 출력을 만들어낸다는 말과도 동일하다. 즉, user error의 패턴을 받아들여 그것을 재생산하는 문제가 있다는 것이다. 다른 말로, context-following learns too much. 의도한 속성 외에도 다른 것까지 모조리 학습해버리는 문제가 있다.</p> <p>오늘 소개할 논문에서는 모델이 이미 zero-shot에서 정답을 알고 있지만 context에 의해 잘못된 답을 뱉는 경우를 좀 더 파고 든다.</p> <ol> <li> <p>how incorrect imitations emerge over the course of the model’s processing</p> </li> <li> <p>look for the model components that cause them.</p> </li> </ol> <p>전체 결과를 요약하면 다음과 같다:</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>언어모델은 분명 context를 imitate하고</p> </li> <li> <p>demonstration을 처리하는 critical layer가 존재하며, 이는 일반적으로 후반부 layer에 위치</p> </li> <li> <p>이것에 기여하는 특정 attention head들이 존재한다. 이 head들은 일반적으로 critical layer의 뒤에 몰려있다.</p> </li> </ol> <h1 id="preliminaries-few-shot-learning-with-false-demonstrations">PRELIMINARIES: FEW-SHOT LEARNING WITH FALSE DEMONSTRATIONS</h1> <ul> <li> <p>Task: few-shot learning for classification tasks</p> </li> <li> <p>Datasets</p> <ul> <li> <p>SST-2</p> </li> <li> <p>Poem Sentiment</p> </li> <li> <p>Financial Phrasebank</p> </li> <li> <p>Ethos</p> </li> <li> <p>TweetEval-Hate, -Atheism, and -Feminist</p> </li> <li> <p>Medical Question Pairs</p> </li> <li> <p>MRPC</p> </li> <li> <p>SICK</p> </li> <li> <p>RTE</p> </li> <li> <p>AGNews</p> </li> <li> <p>TREC</p> </li> <li> <p>DBpedia</p> </li> <li> <p>Unnatural: demonstrations are of the form“[object]: [label]” and the labels are “plant/vegetable”, “sport”, and “animal”.</p> </li> </ul> </li> <li> <p>Models</p> <ul> <li> <p>GPT-J-6B</p> </li> <li> <p>GPT2-XL-1.5B</p> </li> <li> <p>GPT-NeoX-20B</p> </li> <li> <p>GPT-J-6B (intruction tuned)</p> </li> <li> <p>GPT2-XL-1.5B (intructoin tuned)</p> </li> <li> <p>GPT-NeoX-20B (intructoin tuned)</p> </li> <li> <p>Pythia - 410M, 2.8B, 6.9B, and 12B</p> </li> <li> <p>Llama2-7B</p> </li> </ul> </li> <li> <p>Evaluation metrics.</p> <ul> <li> <p>calibrated classification accuracy</p> <ul> <li>we measure how often the correct label has a higher probability than its median probability over the dataset</li> </ul> </li> </ul> </li> </ul> <h2 id="false-demonstration-labels-decrease-accuracy">FALSE DEMONSTRATION LABELS DECREASE ACCURACY</h2> <p>첫 번째로 보장한 것은, demonstratoin label이 모두 맞을 경우와, 모두 틀린 경우의 성능 차이이다. 잘못된 레이블을 매핑할 때, 같은 클래스의 데이터는 모두 같은 레이블을 가지도록 한다. 즉, 레이블만 permute됨으로, 저자들은 이 세팅을 permuted labels setting이라고 부른다.</p> <p>결과는 다음과 같다:</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>왼쪽의 figure를 통해 알 수 있듯, 성능 차이가 커진다. 하지만 이는 모델이 random label을 선택함으로써 이루어졌을 가능성도 존재함으로, 어떤 레이블을 선택했는가를 확인한다 (오른쪽). 보다시피, demonstration을 점차 잘 따라가고 있음을 확인할 수 있다.</p> <h2 id="random-and-partially-correct-labels-lead-to-lower-accuracy-than-correct-labels">RANDOM AND PARTIALLY CORRECT LABELS LEAD TO LOWER ACCURACY THAN CORRECT LABELS</h2> <p>다른 세팅들에 대한 실험도 추가적으로 진행한다.</p> <ol> <li> <p>half correct labels half permuted labels –&gt; acc gap: 0.12</p> </li> <li> <p>random labels –&gt; acc gap: 0.15</p> </li> <li> <p>permuted labels –&gt; acc gap: 0.28</p> </li> </ol> <ul> <li>-&gt; 생각: 랜덤일때는 오히려 학습을 거부하고 본래에 stick? 랜덤하게 하다가 맞춰버린건지, 오류 결과를 봤으면 좋았을텐데 없다ㅠㅠ</li> </ul> <h1 id="zeroing-later-layers-improves-accuracy">ZEROING LATER LAYERS IMPROVES ACCURACY</h1> <p>false context-following에 대해 더 잘 알아보기 위해, 저자들은 model prediction을 intermediate layer에서 직접적으로 decode해본다.</p> <p>이 섹션에서는 크게 두 가지 finding이 있다.</p> <ol> <li> <p>the model’s accuracies given correct and incorrect demonstrations sharply diverge at the same “critical layers” across tasks</p> </li> <li> <p>on incorrect demonstrations the model “overthinks” – it performs better midway through processing</p> </li> </ol> <h2 id="intermediate-layer-predictions-the-logit-lens">Intermediate layer predictions: the logit lens.</h2> <p>먼저, 각 layer에서 decoding은 logit lens 방법을 이용해 수행한다.</p> <p>당연히, 중간 레이어를 거쳐 나온 distribution은 앞선 L개의 레이어를 거친 뒤의 모델 prediction일 것이다.</p> <p>즉, 본래 . For a sequence of tokens t1, …, tn \in V , the logits of the full model’s predictive distribution p(tn+1 | t1, …, tn) are given by: [logit_1, …, logit_{|V|}] = W_U . LayerNorm(h^{(n)}_L).</p> <p>여기서 h_L을 intermediate hidden state인 h_l^{(n)}으로 바꿨다고 생각하면 된다.</p> <h2 id="overall-result">Overall Result</h2> <p>average accuracy of 3 of our 11 models over the fourteen non-toy datasets</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="accurate-and-incorrect-demonstrations-sharply-diverge-at-critical-layers">Accurate and incorrect demonstrations sharply diverge at “critical layers”</h2> <p>올바른 demonstration을 주었을 때, 정확도는 layer가 깊어질수록 높아졌다.</p> <p>놀랍게도 permuted이나 random에서도 early에서는 비슷한 양상을 보였지만, 후반부에 갈수록 양상이 변화하며 정확도가 떨어졌다. 이 트렌드는 다양한 데이터셋에서 공통적으로 나타났다.</p> <p>즉, 모든 실험에서 올바른 그리고 올바르지 않은 프롬프트에 대한 정확도는 모두 동일한 레이어 구간에서 나타났다. 저자들은 이를 critical layer라고 부른다.</p> <p>예를 들어, GPT-J에서는 13-14, pythia는 7-8, llama는 15-17가 해당 레이어이다.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="early-exiting-improves-classification-performance-given-incorrect-demonstrations">Early-exiting improves classification performance given incorrect demonstrations.</h2> <p>올바르지 않은 demonstration을 주었을 때, 저자들은 overthinking을 발견한다: decoding from earlier layers performs better than decoding from the final layer.</p> <p>위의 figure에서 확인할 수 있듯, 16을 넘어가는 순간부터는 full model을 계속해서 이긴다.</p> <ul> <li>-&gt; 개인적으로는 되려 잘못된 정보를 지속적으로 넣었는데 early stage에서 acc이 오르는게 더 신기한데! 다들 저자가 이야기한게 더 놀라운 사실인지,, 오히려 overthinking이 아니라, counterfactual한걸 잘 받아들일 가능성을 보여줬다고도 할 수 있지 않나 싶은데! 어떻게 생각하시나요?</li> </ul> <h2 id="ablating-attention-heads-only-improves-accuracy-further">Ablating attention heads only improves accuracy further</h2> <p>올바른 정보와 잘못된 정보를 보여주는 demonstration이 critical layer에서부터 차이가 나기 시작한다는 것은, 해당 레이어 이후에서야 demonstration에 대한 정보가 제대로 인코딩된다고도 해석할 수 있다. 즉, late attention layers가 overthinking을 유발한다는 것이다.</p> <p>이를 확인하기 위해, 뒤의 Layer에서 attention head들을 zero-out 해본다 (MLP는 건들지 않는다).</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>표에서 확인할 수 있듯,</p> <ul> <li> <p>ablating just the attention heads has a similar effect to ablating the entire layer</p> </li> <li> <p>ablating just MLPs has a much smaller effect</p> </li> </ul> <p>즉, overthinking은 late attention head로부터 발생한다는 것이다!</p> <h1 id="zooming-into-attention-heads">ZOOMING INTO ATTENTION HEADS</h1> <p>가설은 다음과 같다: there are false induction heads that attend to false labels in similar past demonstrations, and make the model more likely to output them.</p> <p>이를 formal하게 표현하기 위해, 저자들은 attention head가 false induction head가 되게 하는 3가지 properties를 언급한다.</p> <ol> <li>label-attending</li> </ol> <ul> <li>concentrate its attention on labels in the previous demonstrations</li> </ul> <ol> <li>class-sensitive</li> </ol> <ul> <li> <p>meaning it attends specifically to labels that follow inputs from the same class</p> </li> <li> <p>(e.g “tomato”, “garlic” and “kale” in Figure 5).</p> </li> </ul> <ol> <li>label-promoting</li> </ol> <ul> <li>meaning it increases the probability of the labels it attends to.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>false induction head를 판별하는 공식 prefix-matching score는 다음과 같다:</p> <p>PM^h = \sum ^n<em>{i=1}Att^h(x, yi) · 1</em>{class(x)=class(x<em>i)} − \frac{1}{#labels − 1} \sum^n</em>{i=1}Att^h (x, y<em>i) · 1</em>{class(x)\neq class(x_i)}</p> <p>첫번째 텀에서 head가 class x의 label에 잘 attend하는가를 포착하고, 그렇지 않으면 작아지도록 뒤의 텀에서 값을 줄인다.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>보다시피, early layer에서는 점수가 낮게 유지되다가 critical layer를 지나면 증가하기 시작한다.</p> <h2 id="ablating-false-induction-heads">Ablating false induction heads</h2> <p>가장 높은 점수를 가지는 attention head를 zeroing했을 때, 성능을 크게 증가하는 것을 알 수 있었다. 랜덤한 head를 ablate했을 때는 오히려 성능이 낮아지는 양상을 보였다.</p> <figure> <picture> <img src="/assets/img/posts/2024-01-23-overthinking-the-truth-understanding-how-language-models-process/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="verifying-that-our-heads-are-label-promoting">Verifying that our heads are label-promoting.</h2> <p>여기서는 label promoting (i.e. that they increase the probability of the false labels they attend to)한지를 확인한다.</p> <p>이를 위해 각 head들에게 logit lens를 적용해본다. 이후, 정답 레이블과 permuted 레이블의 차를 false label promoting score로 지정한다. 높은 숫자는 permuted label의 확률이 높아졌다는 뜻이다.</p> <p>위에 언급한 5 heads: average false label promoting score of 6.5를 기록했다. 즉, 그들은 정답 레이블에 비해 permuted label logit을 6.5나 더 증가시킨 것이다.</p> <p>반면, 랜덤하게 레드들을 추출했을 때는 average score of −0.04, with a standard deviation of 0.41를 기록했다.</p> <p>요약하면, later layer에 소수의 false induction heads가 있으며, 그들은 false labels in past demonstrations에 attend, and increasing their probability함으로써 잘못된 컨텍스트를 따르는데 기여한다.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>