<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> KNOWLEDGE ENTROPY DECAY DURING LANGUAGE MODEL PRETRAINING HINDERS NEW KNOWLEDGE ACQUISITION | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Interpretability, Continual Learning 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/knowledge-entropy-decay-during-language-model-pretraining-hinders/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">KNOWLEDGE ENTROPY DECAY DURING LANGUAGE MODEL PRETRAINING HINDERS NEW KNOWLEDGE ACQUISITION</h1> <p class="post-meta"> Created on October 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/continual-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> continual learning</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> interpretability</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-10-17</li> <li> <strong>Reviewer</strong>: hyowon Cho</li> <li> <strong>Property</strong>: Interpretability, Continual Learning</li> </ul> <h2 id="1-intro">1. Intro</h2> <p>최근 연구들은 언어 모델이 매개변수 내에 저장된 지식을 어떻게 활용하여 응답을 생성하는지 분석해 왔으나, 모델이 사전학습(pretraining) 동안 이러한 지식을 통합하는 방식이 어떻게 변화하는지는 잘 알려져 있지 않다.</p> <p>본 연구는 모델의 다양한 지식 통합 특성이 사전학습 중 어떻게 변화하며, 이러한 변화가 지속 학습(continual learning) 상황에서의 지식 습득 및 망각에 미치는 영향을 분석한다.</p> <p>우리는 모델의 지식 통합 특성을 측정하기 위해 <em>knowledge entropy</em> 개념을 도입한다. 이는 모델이 메모리 벡터를 얼마나 폭넓게 통합하는지를 나타냅니다. 높은 지식 엔트로피는 폭넓은 메모리 벡터를 사용하는 것을, 낮은 엔트로피는 특정 메모리 벡터를 집중적으로 사용하는 것을 의미한다. 사전학습 단계별로 지식 엔트로피를 분석한 결과, 학습 후반부의 모델들은 낮은 지식 엔트로피를 보여, 폭넓은 메모리 벡터를 사용하던 초기와 달리 소수의 메모리 벡터에 집중하는 경향이 나타났다.</p> <p>우리는 이러한 변화가 새로운 지식을 습득하는 모델의 행동에 영향을 미칠 것으로 가정하고, 다양한 사전학습 단계에서 시작한 모델들을 대상으로 새로운 도메인 코퍼스를 활용한 실험을 진행했다. 그 결과, <strong>지식 엔트로피와 지식 습득 및 유지 능력 사이에 강한 상관관계가 있음을 확인했으며, 사전학습이 진행됨에 따라 두 요소가 감소</strong>한다는 사실을 발견했다.</p> <p>우리는 이러한 양상이, 지식 엔트로피가 낮아지면 활성 메모리 벡터의 수가 줄어들어, 새로운 지식을 저장할 때 기존 메모리 벡터가 자주 덮어쓰이기 때문이라고 가정한다. 이를 테스트하기 위해, 비활성 메모리 벡터를 인위적으로 활성화하여 새로운 지식을 더 넓은 범위의 메모리 벡터에 저장하도록 유도한 실험을 수행했으며, 이를 통해 모델의 지식 습득 능력과 망각이 개선되는 결과를 얻었다.</p> <p>결론적으로, 우리가 찾아낸 바는 다음과 같다:</p> <ul> <li> <p>사전학습이 진행될수록 모델은 메모리 벡터의 사용/통합 폭이 좁아져 지식 습득 및 유지 능력이 저하되는 것을 확인</p> </li> <li> <p>초기 모델은 높은 지식 엔트로피로 인해 지식 습득 및 유지 능력이 뛰어나지만 언어 모델링 성능이 제한적이다.</p> </li> <li> <p>중기 모델은 균형 잡힌 성능을 보여 새로운 지식을 추가 학습하기에 적합한 것으로 나타났다.</p> </li> <li> <p>비활성 메모리 벡터를 인위적으로 활성화하여 새로운 지식을 더 넓은 범위의 메모리 벡터에 저장하도록 유도하면, 지식 습득 및 유지 능력이 향상된다.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="2-related-works">2. Related Works</h2> <h3 id="dynamics-of-knowledge-in-language-models">Dynamics of Knowledge in Language Models</h3> <ul> <li> <p>언어 모델은 매개변수에 지식을 저장하고, 이를 통해 응답을 생성함 (Yang, 2024; Petroni et al., 2019; Wang et al., 2021).</p> </li> <li> <p>연구 목적: 모델이 학습과 추론 과정에서 지식을 어떻게 학습하고 저장하며 사용하는지 이해.</p> </li> <li> <p>주요 연구:</p> </li> <li> <p><strong>본 연구의 차별점</strong>: 언어 모델이 사전학습 중 지식을 통합하는 방식의 변화와 이로 인한 지속 학습 시 지식 습득 및 망각에 대한 영향 분석.</p> </li> </ul> <h3 id="entropy-in-natural-language-processing">Entropy in Natural Language Processing</h3> <ul> <li> <p><strong>엔트로피 개념</strong>: 정보 이론에서 예측 가능한 사건은 낮은 엔트로피, 예측이 어려운 사건은 높은 엔트로피를 가짐 (Lairez, 2022; Majenz, 2018).</p> </li> <li> <p><strong>NLP에서 엔트로피의 사용</strong>:</p> </li> <li> <p><strong>본 연구에서의 엔트로피</strong>:</p> </li> </ul> <h2 id="3-knowledge-entropy">3. Knowledge Entropy</h2> <h3 id="31-정의">3.1 정의</h3> <p>본 연구에서는 모델의 매개변수 지식 통합 범위를 분석하기 위해 <em>knowledge entropy</em>라는 새로운 개념을 도입한다. 낮은 지식 엔트로피는 모델이 특정한 지식 소스에 의존하는 반면, 높은 지식 엔트로피는 다양한 지식 소스를 통합함을 의미한다.</p> <p>기존 연구들이 피드포워드 레이어(FFN)를 키-값 메모리로 간주한 것에 착안해, 메모리 벡터(FFN의 두 번째 투영 행렬)를 지식 소스로 보고, 이를 통합하는 정도를 메모리 계수로 측정한다. 지식 엔트로피는 계수의 확률 분포를 이용해 계산되며, 이는 사전학습된 데이터셋을 기반으로 측정된다.</p> <p>구체적인 설명은 다음과 같다:</p> <p>Geva et al. (2021, Transformer feed-forward layers are key-value memories.) 은 FFNs이 ey-value neural memories (Sukhbaatar et al., 2015)와 비슷하게 동작한다는 key-value memory의 개념을 소개한 바 있다. FFNs는 두 개의 projection layer와 중간에 하나의 activation으로 구성되어 있다.</p> <p>FFN(x) = f(x · K^T) · V</p> <p>이때, 첫 번째 projection matrix는 key, 두 번째 projection matrix는 values, 또는 memory vector로 구성된 memories라고 볼 수 있다고 주장한다. 출력값인 FFN(x)는 메모리 벡터 v들의 linear combination이라고 할 수 있으며, coefficient는 C는 f(x · K^T)라고 볼 수 있다. 이는 다시 말해, coefficient가 memory vector를 어떻게 조합할지, 어느 정도의 중요도를 볼 지 결정한다고 볼 수 있다.</p> <p>따라서, 우리의 knowledge entropy, H(θ)는 sum of layer-wise entropy로 정의하며, 이는 D라는 데이터셋의 모든 토큰들에 대한 average coefficient C¯^l를 구한 것이라고 할 수 있다.</p> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>C^{(l)}_{n,j} = coefficient of j-th token position of n-th instance at layer l</p> </li> <li> <p>c¯^l_i = i-th element from average coefficient C¯^l</p> </li> <li> <p>T_n = sequence length of the n-th instance in the dataset D,</p> </li> <li> <p>m is the inner dimension of feed-forward layer</p> </li> <li> <p>L denotes the number of layers in the model.</p> </li> </ul> <h3 id="32-실험-설정">3.2 실험 설정</h3> <p>OLMo (1B 및 7B) 모델들을 사용해 실험을 진행하며, Dolma 데이터셋을 기반으로 지식 엔트로피를 측정한다. 실험에는 Dolma의 일부 데이터(2천 개 인스턴스)를 사용해 모델의 사전학습 단계에서의 엔트로피 변화를 분석했다.</p> <p>모델의 메모리 계수 (C)는 SwiGLU 활성화 함수를 사용해 계산하며, 절대값을 사용해 메모리 벡터의 기여도를 확률 분포로 변환한다.</p> <p>C^{(l)}_{n,j} = abs(SwiGLU(x_j))</p> <p>절대값을 사용한 이유는 linear combination을 할 때, 그것의 기여도를 알고싶었기 때문이다 (magnitude).</p> <blockquote> <p>Please note that the trend persists across other corpora as well (Figure 6 in Appendix A.1); however, since we are analyzing the model’s behavior throughout training, we define knowledge entropy based on calculations using the training dataset.</p> </blockquote> <p>이러한 지식 엔트로피는 dataset, activation function의 선택과 무관하게 일정한 trend를 가졌다.</p> <h3 id="33-사전학습-후반부-모델은-낮은-지식-엔트로피를-보임">3.3 사전학습 후반부 모델은 낮은 지식 엔트로피를 보임</h3> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>사전학습 단계별 지식 엔트로피의 변화를 분석한 결과, 사전학습이 진행될수록 지식 엔트로피가 감소함을 확인했다. 이는 모델이 학습 후반부에 특정 메모리 벡터에 더 집중하게 되며, 폭넓은 메모리 벡터를 사용하는 경향이 줄어듦을 의미한다. 이러한 감소는 대부분 레이어에서 관찰되었으며, 마지막 레이어에서 가장 뚜렷했다. (Appendix A.1)</p> <h3 id="34-다른-엔트로피-정의에서도-유사한-경향이-관찰됨">3.4 다른 엔트로피 정의에서도 유사한 경향이 관찰됨</h3> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>지식 엔트로피 외에도 어텐션 레이어와 Next Token Prediction의 엔트로피를 분석했다. 어텐션 엔트로피는 어텐션 가중치의 불확실성을 측정하며, 사전학습 초기 급격히 감소한 후 점진적으로 줄어드는 경향을 보였다. 이는 모델이 점차 중요한 토큰에 집중하게 됨을 시사한다. 또한, Next Token Prediction의 엔트로피는 사전학습이 진행될수록 감소하여 모델이 예측에 대해 더 확신을 가지게 됨을 나타냈다.</p> <p>.</p> <h2 id="4-knowledge-acquisition-and-forgetting">4. KNOWLEDGE ACQUISITION AND FORGETTING</h2> <h3 id="41-실험-설정">4.1 실험 설정</h3> <ul> <li> <p><strong>모델 및 하이퍼파라미터</strong>: OLMo 모델의 사전학습 중간 체크포인트를 사용하며, continual knowledge learning 연구를 참고하여 하이퍼파라미터를 설정했다. 배치 크기, 학습률, 학습 기간 등 다양한 조합을 테스트했고, 기본 설정은 배치 크기 128, 학습률 4e-4, 1 epoch으로 설정했다.</p> </li> <li> <p><strong>데이터셋</strong>: PubMed는 새로운 지식이 많아 continual learning에 적합하다. 또한, FICTIONAL KNOWLEDGE dataset을 이용해, 모델의 새로운 정보 습득 능력을 평가한다. 학습 후에는 평가용 프롬프트를 이용해 지식 습득을 측정하고, 6개의 다운스트림 태스크를 통해 지식 망각을 평가한다.</p> </li> <li> <p><strong>Metric</strong>: 지식 습득은 주입된 지식을 얼마나 잘 불러내는지, 지곤 지식 C에 대한 각기 다른 paraphrased 혹은 연관된 probing 프롬프트 15개를 이용해 측정한다 (P). 또한, initial model에 비해서 얼마나 좋아졌는지를 A로 표기한다. 지식 망각은 사전학습 단계에서의 성능 감소로 측정한다.</p> </li> </ul> <h3 id="42-지식-습득-및-유지-능력의-감소">4.2 지식 습득 및 유지 능력의 감소</h3> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>모델의 사전학습 단계별 성능을 분석한 결과, 사전학습 후반부의 모델은 새로운 지식을 습득하는 데 어려움을 겪으며, 더 많은 망각을 보였다.</p> <p>사전학습 중간 단계에서 지속 학습을 시작한 모델이 지식 회상과 다운스트림 태스크 성능에서 가장 우수한 결과를 나타냈다.</p> <p>초기 모델은 높은 지식 습득 능력을 보이지만, 언어 모델링 능력이 제한적이었으며, 반대로 후반부 모델은 더 큰 데이터셋으로 학습되었음에도 불구하고 낮은 지식 습득과 높은 망각 비율을 보였다.</p> <p>이는 사전학습 모델이 새로운 지식을 학습하는 데 어려움을 겪는 이유를 설명하는 연구와 일치한다.</p> <p>따라서 중간 단계 체크포인트를 사용하면 학습과 성능의 균형을 맞추기 위한 적절한 선택임을 제안한다.</p> <p>이러한 양상은 batch size, learning rate, training corpus, and the number of epochs 등 다양한 하이퍼파라미터 세팅에서 모두 동일하였다.</p> <h3 id="43-비활성화된-메모리-벡터의-활성화가-지식-습득을-증가시킴">4.3 비활성화된 메모리 벡터의 활성화가 지식 습득을 증가시킴</h3> <p>지식 엔트로피 (Figure 2)와 모델의 지식 습득 및 유지 능력(Figure 4a) 사이에 강한 상관관계가 관찰되었다.</p> <p>모델이 제한된 메모리 벡터에 의존할수록 (decrease in knowledge entropy), 모델의 추가적인 학습 과정에서 해당 벡터들만 업데이트하고, 이것이 새로운 지식 습득이 어려워지고, 망각률이 높아지는 원인이 된다는 가설을 검증하기 위해, 비활성 메모리 벡터를 인위적으로 활성화하는 실험을 수행했다.</p> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>비활성 메모리 벡터를 활성화하기 위해, 우리는 메모리 계수 C¯를 생성하는 up-projection matrix K를 수정한다. 구체적으로, Algorithm 1에 따라 가장 적게 사용되는 p%의 메모리 계수를 찾아내고, 그 부분에 특정 수치 m을 곱해준다. 이 수치는 각 레이어의 평균 계수 값을 해당 위치의 계수 값으로 나눈 후, 증폭 인수 q를 곱하는 방식이다. 이렇게 q의 값을 조정하면, 적게 활성화된 메모리 계수들이 얼마나 되살아나는지를 조절할 수 있고, 이를 통해 파라미터 업데이트의 크기를 조절하게 된다.</p> <p>우리의 실험에서는 p=50, q는 변화하면서 실험을 진행한다.</p> <figure> <picture> <img src="/assets/img/posts/2024-10-17-knowledge-entropy-decay-during-language-model-pretraining-hinders/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>실험한 결과, 비활성 메모리 벡터를 활성화시킬수록 지식 습득 및 유지 성능이 향상되었다 (Figure 5a). 특히, q 값을 1 이상으로 설정한 경우 성능이 개선되었으나, q 값을 0.5로 설정했을 때는 성능이 저하되었다. 이는 특정 메모리 위치에 집중된 업데이트가 성능을 저하시킬 수 있음을 의미한다.</p> <p>추가적으로, q 값을 고정하고, p 값을 변경해 비활성화된 매개변수의 더 큰 부분을 활성화했을 때, 일반적으로 성능이 향상되는 경향을 확인했다.</p> <p>다양한 단계의 모델들을 사용했을 때, 모두 비슷한 양상을 보였지만, 학습의 후반부로 갈수록 그 효과가 더욱 두드러졌다.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>