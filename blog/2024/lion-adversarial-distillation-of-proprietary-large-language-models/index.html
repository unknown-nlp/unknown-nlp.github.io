<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Lion: Adversarial Distillation of Proprietary Large Language Models | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Knowledge Distillation 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Lion: Adversarial Distillation of Proprietary Large Language Models"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2024/lion-adversarial-distillation-of-proprietary-large-language-models/"> <meta property="og:description" content="논문 리뷰 - Knowledge Distillation 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Lion: Adversarial Distillation of Proprietary Large Language Models"> <meta name="twitter:description" content="논문 리뷰 - Knowledge Distillation 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2024/lion-adversarial-distillation-of-proprietary-large-language-models/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - Knowledge Distillation 관련 연구",
        "headline": "Lion: Adversarial Distillation of Proprietary Large Language Models",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/lion-adversarial-distillation-of-proprietary-large-language-models/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Lion: Adversarial Distillation of Proprietary Large Language Models</h1> <p class="post-meta"> Created on January 30, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/knowledge-distillation"> <i class="fa-solid fa-hashtag fa-sm"></i> knowledge distillation</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-01-30</li> <li> <strong>Reviewer</strong>: 전민진</li> <li> <strong>Property</strong>: Knowledge Distillation</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>지금까지 제안된 knowledge distillation 방법론은 student model의 답변과 teacher model의 답변 align되도록하는, unidirectional knowledge(teacher → student)에 초점을 둠</p> </li> <li> <p>본 논문에서는 이러한 방법론들이 “feedback”을 학습 과정에 포함시킬 가능성을 간과했다고 지적</p> <ul> <li> <p>feedback이란 student model의 성능을 저하시키는 어려운 데이터(instruction)을 식별하는 것을 의미</p> </li> <li> <p>feadback을 바탕으로 반복적으로 학습한다면 student model의 성능을 끌어올릴 수 있을거라 봄</p> </li> </ul> </li> <li> <p>본 논문에서는 3단계(imitation, discrimination, generation)로 구성된 novel한 adversarial distillation framework를 제안</p> <ul> <li> <p>teacher model이 prompt를 통해 “hard” instruction을 식별하고 새로운 “hard” instruction을 생성하도록 하도록 함</p> </li> <li> <p>생성한 instruction을 바탕으로 다시 student model 학습, 이를 반복</p> </li> <li> <p>해당 프레임워크를 통해, ChatGPT로부터 knowledge를 transfer, 70k training data만을 사용</p> </li> </ul> </li> <li> <p>실험 결과, open-ended generation 능력이 ChatGPT에 상응할 뿐만 아니라, BIG-Bench Hard, AGIEval에서 기존의 instruction-tuned model를 능가하는 성능을 보여줌</p> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>최근, LLM이 새로운 task에 대한 zero-shot 성능을 보였지만, 대부분의 모델이 소유권이 있음</p> </li> <li> <p>proprietary “teacher” LLM으로부터 knowledge distillation을 하기 위해 teacher model의 답변과 student model의 답변을 aligning하는 방식을 사용</p> <ul> <li>이 때 사용되는 instruction은 manully or automatically generated</li> </ul> </li> <li> <p>그러나 지금까지는 학습 과정에 “feedback”을 활용하려는 시도가 없었음</p> <ul> <li> <p>학교 수업을 생각해보면, 학생의 성적을 저하시키는 포인트를 짚어서 feedback을 해줌</p> </li> <li> <p>즉, feedback은 학생 개개인에 적합한 맞춤형 학습을 제공</p> </li> </ul> </li> <li> <p>adversarial knowledge distillation(AKD)에 영감을 얻어, proprietary LLM을 compact student model로 distilling하는 adversarial framework를 제안</p> <ul> <li>AKD는 반복적으로 generated hard sample로 학습해서 student model의 성능을 끌어올리는 방법론</li> </ul> </li> <li> <p>하지만, 바로 AKD방법론은 teacher model의 weight나 gradient가 필요하기 때문에 현상황에 바로 적용할 순 없음</p> <ul> <li> <p>이러한 문제를 우회하기 위해, unparalleled role adaptability를 극대화</p> <ul> <li>teacher model이 referee(to discriminate hard instructions), generator(to produce new instructions)역할도 같이 함</li> </ul> </li> </ul> </li> <li> <p>해당 프레임워크는 다음과 같은 단계로 구성</p> </li> </ul> <ol> <li> <p>imitation stage : to align the student’s response with the teacher’s repponse</p> </li> <li> <p>discrimination stage : to identify hard instructions</p> </li> <li> <p>Generation stage : to produce new hard insturctions for escalating the challenges presented to the student model</p> </li> </ol> <ul> <li> <p>Alpaca’s training data(175개의 사람이 작성한 seed instruction으로 생성된 데이터셋)으로 기반으로 해당 프레임워크를 3번 반복, 총 70K data로 student model을 학습(iteration한번에 6K data얻음)</p> </li> <li> <p>실험 결과, instruction-tuned baseline model보다 뛰어난 성능 보임</p> <ul> <li>Vicuna보다 더!</li> </ul> </li> <li> <p>본 논문의 핵심 contribution은 다음과 같음</p> <ul> <li> <p>LLM에 adversarial knowledge distillation을 적용한 첫번째 논문</p> </li> <li> <p>human annotation이 없이 70k의 data로 instruction tuning한 Lion-13B 모델이 ChatGPT의 open-ended generation 성능에 필적함</p> <ul> <li>다른 SOTA모델(Vicuna—13B)보다 reasoing tasks에서 뛰어난 성능을 보임</li> </ul> </li> <li> <p>범용성이 좋음</p> <ul> <li>ChatGPT외의 다른 proprietary LLM에도 해당 학습 방법을 적용할 수 있음</li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>Knowledge Distillation</p> <ul> <li> <p>KD를 할 때, privacy, legality, security, confidentiality 등으로 인해 학습에 사용할 수 있는 data가 제한적</p> </li> <li> <p>이러한 문제를 해결하기 위해, student model을 teacher model에 align하는 data-free KD method가 제안됨</p> <ul> <li> <p>관련 있는 proxy data를 사용</p> </li> <li> <p>learnable generator로 생성된 synthetic data를 사용</p> </li> <li> <p>teacher model inversion</p> <ul> <li>학습 데이터의 공통된 특징을 재사용해서 다른 data를 생성(비전논문)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>** 이러한 방법론들은 teacher model의 gradient나 weight가 필요</p> <ul> <li>한편으로는, data-free model extraction(or stealing)이라 불리는 연구가 제안</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- generator를 update하기 위해 teacher model의 진짜 gradient을 근사하기 위한 zero-order estimation과 같은 방법론을 적용(빨간색 점선 부분)

- 이러한 방법론을 그대로 적용할 경우 크게 2가지 문제가 발생

  - **image-based classification task를 위해 설계된 방법론이기 때문에, teacher model에서 continuous softmax vector(activation function에 넣기 전)에 접근 가능하다고 가정**

    - 또한, zero-order gradient의 경우, responses가 sequence-oriented이기 때문에 문제가 됨 ⇒ 아마 각각의 단어 벡터를 생성할 때 직전 단어들에게 영향을 받아서 생성되기 때문에, x로 gradient를 흘리려면 이 방향도 고려해야하기 때문?

      - zero-order gradient

        - 0-order 정보로 gradient 근사 / 비전 논문에서는 input x를 기반으로 gradient를 근사한다고 함
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - teacher model의 학습 데이터 분포를 모방하는 diverse, high-quality instruction을 생성하는 효과적인 instruction generator를 개발하는 것이 image domain보다 훨씬 어려움
</code></pre></div></div> <h2 id="method">Method</h2> <ul> <li> <p>문제 정의</p> <ul> <li> <p>teacher model T(x;\theta^T)의 학습된 지식을 이용하려고 함</p> <ul> <li>\theta^T는 inaccessable</li> </ul> </li> <li> <p>목표는 student model S(x;\theta^S)에 teacher model의 지식을 넣는 것</p> <ul> <li>이상적으로, uniform data distribution에 대해 model discrepancy의 expectation이 최소화될 때, student model이 optimal</li> </ul> </li> <li> <p>본 논문의 저자들은 expectation의 upper bound를 낮추고자 함</p> </li> </ul> </li> </ul> <p>⇒ hard sample에 대한 model discrepancy를 낮추자!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 단, 학습 과정에서 hard sample를 그대로 student가 마스터(?)할 것이니, 곧 easy sample로 바뀔것
</code></pre></div></div> <p>⇒ 지속적으로 hard sample를 생성해서 학습하는 프레임워크 필요</p> <ul> <li>Initialization</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>4가지 역할과 2가지 data pool이 존재</p> <ul> <li> <p>Teacher, Student, Referee, Generator</p> <ul> <li> <p>Student로는 LLaMA와 같은 foundation LLM으로 초기화</p> </li> <li> <p>Teacher, Referee, Generator는 proprietary LLM을 사용</p> <ul> <li>각각의 역할은 다른 prompt template을 통해 수행 가능</li> </ul> </li> </ul> </li> <li> <p>Train pool, Cache Pool</p> <ul> <li> <p>Train Pool X^A = {x<em>i^A}</em>{i\in[1,N^A]}, 여기서 x는 i번째 instruction, N은 train data 수</p> </li> <li> <p>Cache pool X^A = {x<em>i^B}</em>{i\in[1,N^B]}</p> <ul> <li> <p>student와 teacher의 성능을 평가하기 위한 instruction으로 구성됨</p> </li> <li> <p>처음엔 train pool과 같지만, 반복할수록 cache pool은 데이터 누적, train pool은 반복마다 다른 데이터셋으로 교체됨</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p><strong>Imitation Stage</strong></p> <ul> <li> <p><strong>베이직한 knowledge distillation 단계</strong></p> </li> <li> <p>teacher의 knowledge를 student에게 전달하기 위해, Train pool에 있는 instruction을 forward, instruction-response data를 구축</p> </li> <li> <p>finetune시에는 autoregressive language modeling objective를 최적화하는 방법을 사용해 studunt model이 teacher모델의 response를 align하도록 함</p> </li> </ul> </li> <li> <p><strong>Discrimination Stage</strong></p> <ul> <li> <p><strong>student와 teacher의 성능 차이를 유발하는 “hard” instruction을 찾는 단계</strong></p> </li> <li> <p>이번 단계는 Cache Pool를 기반으로 진행</p> <ul> <li> <p>처음엔 Train Pool과 Cache Pool이 같지만, 반복할 때 Cache pool는 새로 생성한 데이터를 누적해서 저장, Train Pool은 교체</p> </li> <li> <p>Cache pool의 저장 용량을 늘리면 더 다양한 범위에서 teacher와 student의 performance gap을 평가 가능</p> </li> </ul> </li> <li> <p>이 단계에서는 proprietary LLM이 “referee”로서 활동, teacher와 student사이의 성능 갭을 정량화</p> <ul> <li>Cache pool안에 있는 각각의 instruction x_i^B를 student, teacher에 forward, 해당 instruction과 모델의 forward결과를 LLM에 넣어서 둘의 퀄리티 차이를 측정</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - helpfulness, relevance, accuracy, level of detail을 고려하여 모델 output의 점수를 각각 측정, 둘의 차이가 d_i

    - 단 positional bias를 고려하여 순서 바꿔서 2번 실행, 평균값을 최종 점수로 씀
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - 이 때, 점수 d_i 가 threshold \tau보다 크면 “hard” instruction으로 분류, 아닐 경우 “easy” instruction으로 분류
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    - a와 b를 비교해보면, hard instruction의 pool이 기존 학습데이터셋과 꽤 다르고, 좀 더 복잡한 math, coding과 같은 태스크에 초점을 두고 있다는 것을 알 수 있음
</code></pre></div></div> <ul> <li> <p>Generation Stage</p> <ul> <li> <p>hard instruction의 data distribution에 근사해 instruction을 생성하는 단계</p> </li> <li> <p>proprietary LLM을 generator로 사용하여, instruction을 생성</p> <ul> <li> <p>hard instruction에서 랜덤하게 instruction을 샘플링, 이를 바탕으로 prompt를 사용해 new instruction 생성</p> <ul> <li>새로운 instruction은 샘플링된 instruction과 task type, domain이 같아야함</li> </ul> </li> <li> <p>catastrophic forgetting문제를 줄이고 다양성을 높이기 위해서, easy instruction에서도 instruction을 샘플링, 새로운 instruction을 생성</p> <ul> <li>동일 도메인에 속하지만, 보다 long-tail distribution을 나타내는 instruction을 생성하도록 유도</li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>각 iteration마다, N개의 새로운 instruction을 생성하고, generated hard instruction과 generated easy instruction의 비율 r을 유지하도록 함</p> <ul> <li> <p>여기서 r이 1이면 generated hard instruction만 있는 경우</p> </li> <li> <p>또한, 다양성을 높이기 위해, new instruction이 기존 cache pool에 있는 instruction과 ROUGE-L기준 0.7보다 낮도록 함</p> </li> </ul> </li> <li> <p>마지막으로, generation stage에서 생성한 데이터로 train pool을 교체하고, cache pool에는 생성한 데이터를 누적해서 추가함</p> </li> <li> <p>Min-Max Game Interpretation</p> <ul> <li> <p>본 논문에서 제안하는 adversarial knowledge distillation framework는 dynamic min-max game으로 해석될 수 있음</p> <ul> <li> <p>imitation stage에서 student를 finetune, student와 teacher사이의 model discrepancy를 최소화</p> </li> <li> <p>discrimination, generation stage에서 model discrepancy를 최대화하기 위해 hard instruction을 분류, 생성</p> </li> </ul> </li> <li> <p>이러한 학습 방식은 student model이 커버하지 못하는 hidden knowledge쪽으로 모델이 학습되게 함</p> </li> <li> <p>여러 번의 iteration을 거치면, 시스템이 이상적으로 equilibrium에 도달해야함</p> <ul> <li>이 지점이 student가 모든 hard sample을 마스터, 모든 hard instruction에 대해서 student가 낸 답과 teacher가 낸 답을 referee가 구분할 수 없는 지점</li> </ul> </li> </ul> </li> </ul> <p>⇒ 기능적으로 S와 T를 구별할 수 없게 됨</p> <h2 id="experiments-settings">Experiments Settings</h2> <ul> <li> <p>Datasets</p> <ul> <li> <p>Open-ended Generation Datasets</p> <ul> <li> <p>Vicunna-Instructions</p> <ul> <li> <p>9개의 task category에 대한 80개의 질문으로 구성</p> </li> <li> <p>LLM의 capabilities를 평가하는데 광범위하게 사용됨</p> </li> <li> <p>본 논문에서는 2가지 세팅에서 LLM의 성능 측정</p> <ul> <li> <p>Setting 1 : Vicuna와 같이, GPT-4를 사용해서 teacher(ChatGPT)와 student 의responses quality를 측정(1-10점 사이로 평가), student model의 성능은 teacher model 대비 총 점수의 percentage로 계산</p> <ul> <li>(ChatGPT의 답변이 100일 때 student의 답변은 몇점인지)</li> </ul> </li> <li> <p>Setting 2 : GPT-4에 systematic bias가 있다는 최근 연구를 고려. 해당 연구에서 이를 줄이기 위해, Multiple Evidence Calibration과 Balanced Position Calibration을 제안, human judgements와 유사한 alignment를 얻음</p> <ul> <li>아마 이 방식으로 response 점수를 계산했다는 것인듯(논문엔 딱 저 말까지만 쓰여있음. 쓰다가 까먹었나보다..)</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Reasoning Dataset</p> <ul> <li> <p>AGIEval</p> <ul> <li> <p>대학 입학 시험을 포함해서 법학 적성 시험 등 human-centric standardized exam으로 구성</p> </li> <li> <p>영어 다지 선다 문제를 선택(8 tasks, 2546 sampels)</p> </li> </ul> </li> <li> <p>BIG-Bench Hard(BBH)</p> <ul> <li> <p>LLM의 능력과 한계를 평가하기 위해 설계된 데이터셋</p> </li> <li> <p>다지선다 문제로 바꿀 수 있는 모든 task를 선택(23 tasks, 5511 sampels)</p> </li> </ul> </li> <li> <p>Setting</p> <ul> <li> <p>CoT나 examplar를 사용하지 않고 zero-shot으로 성능 평가</p> </li> <li> <p>답변의 첫번째 대문자만 gold answer로 간주(exact match), accuracy로 성능 리포팅</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Baseline</p> <ul> <li> <p>LLaMA, Alpaca, WizardLM, Vicuna, ChatGPT</p> <ul> <li> <p>Vicuna가 현재 open-source LM중에 탑이라, Vicuna중심으로 비교 진행</p> </li> <li> <p>Vicuna는 공개 API를 사용하여 ShareGPT에서 수집한 약 70K의 사용자 대화로 LLaMA을 fine-tuning하여 생성</p> </li> <li> <p>Alpaca 모델은 fine-tuning에 필요한 데이터셋을 얻기 위해 사람이 작성한 instruction-output 쌍으로 구성된 self-instruct seed를 GPT-3(text-davinci-003)의 프롬프트로 입력하여 추가 instruction(52K 예제)을 생성</p> </li> <li> <p>WizardLM</p> <ul> <li>it uses an Evol-Instruct method to bootstrap the 52k instruction-following examples of Alapca into a larger set of 250k more intricate instructions. Out of this larger set, 70k examples were selected to fine-tune LLaMA</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Implementation Details</p> <ul> <li> <p>Training Details</p> <ul> <li> <p>student model은 pre-trained LLaMA로 initialized</p> </li> <li> <p>Train Pool과 Cache Pool은 Alpaca에서 사용한 52K의 automatically generated instruction으로 초기화</p> </li> <li> <p>iteration은 3으로 설정, 각 iteration마다 6k의 instruction 생성</p> <ul> <li> <p>전체 70K의 데이터로 student model 학습</p> <ul> <li>다른 모델과 fair한 비교를 위해!</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Inference Details</p> <ul> <li> <p>Lion, ChatGPT에서 inference할 때 Temperature로 0.7사용</p> </li> <li> <p>최대 생성 길이 1024</p> </li> <li> <p>다른 파라미터는 default setting</p> </li> <li> <p>baseline의 경우 각자 논문에서 소개된 setting대로 사용함</p> </li> <li> <p>GPT-3.5-turbo API를 쓸 때 각 role에 맞게 상이한 hyperparameter사용</p> </li> </ul> </li> </ul> </li> </ul> <h2 id="experimental-results">Experimental Results</h2> <ul> <li>Results for Open-ended Generation</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>ChatGPT를 teacher model로, GPT-4를 referee/rater로 사용</p> </li> <li> <p>baseline에 비해 Lion의 성능이 압도적</p> </li> <li> <p>generic, knowledge, common-sense, counterfactual에 대해선 ChatGPT보다 높은 성능을 보였지만, fermi, coding, math과 같은 task에서는 비교적 낮은 성능을 보임</p> </li> <li> <p>Results for Reasoning</p> <ul> <li> <p>AGIEval</p> <ul> <li> <p>다지선다 영어 문제에 대한 AGIEval benchmark의 성능이 나와있음</p> </li> <li> <p>baseline모델 중에선 대체로 뛰어난 성능을 보이지만, ChatGPT보다는 훨씬 낮은 성능을 보임</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>BIG-Bench Hard Results</p> <ul> <li> <p>standard zero-shot prompting상황에서의 성능이 나와있음</p> </li> <li> <p>Vicuna는 정교한 reasoning task에서는 낮은 성능을 보이지만, Lion은 상대적으로 높은 성능을 보임</p> </li> <li> <p>특히 Movie Recommendation, Snarks(두가지 거의 동일한 문장에서 비꼬는 문장 식별), Tracking Shuffled Objects에서는 Lion-13B가 ChatGPT를 능가함</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="analyses">Analyses</h2> <ul> <li> <p>Ablation study</p> <ul> <li> <p>hard와 easy instruction 사이를 가르는 threshold \tau</p> <ul> <li> <p>\tau를 0에서 2사이로 두고 실험한 결과, 1.0과 1.5사이가 가장 성능이 좋았음</p> </li> <li> <p>0에서 1로 증가시킬 때는, hard와 easy를 구별하는것이 효과적이어서 성능이 점차 증가헀지만, 1에서 2로 증가시킬 때는 hard instruction의 다양성이 떨어져서 성능이 점차 감소</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>generated hard와 easy instruction의 비율 r</p> <ul> <li> <p>r이 1이면(1:0) all hard, 0이면(0:1) all easy</p> </li> <li> <p>1:1일때가 성능이 가장 높았음</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>The Learning Dynamics of Lion</p> <ul> <li> <p>iteration마다 Lion의 성능을 측정</p> </li> <li> <p>본 논문에서 제안하는 방법론의 효과성을 보여주는 장표</p> <ul> <li>특히 첫번째 iteration에서 성능이 급격히 향상, challenging example pattern을 식별하는 것이 중요하다는 걸 알 수 있음</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-01-30-lion-adversarial-distillation-of-proprietary-large-language-models/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Case Studies</p> <ul> <li> <p>본 논문에서 제안한 모델과 타모델의 답변을 명확하게 분석하기 위해 만든 장표</p> </li> <li> <p>Vicuna-instruction, BBH, AGIEval에서 9개의 case를 sampling</p> </li> <li> <p>math instuction에선 ChatGPT와 Lion만이 맞는 답변과 풀이과정을 보임</p> </li> <li> <p>conterfactual case에서는 Lion이 더 디테일한 답변을 제시</p> </li> </ul> </li> </ul> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>본 논문에서는 proprietary LLM을 distill하기 위한 adversarial knowledge distillation framework을 제안</p> <ul> <li>feedback을 활용하여 데이터를 생성, 이를 학습에 이용하는 프레임워크</li> </ul> </li> <li> <p>학습 결과, open-ended generation에선 ChatGPT와 상응하는 성능을, Reasoing task에서는 기존 SOTA모델인 Vicuna와 비슷한 성능을 보임</p> </li> <li> <p>Limitation</p> <ul> <li> <p>학습데이터에 dialogues가 포함되지 않아서, Lion의 경우 multi-turn dialogues에는 취약함</p> </li> <li> <p>최대 시퀀스 길이가 1024로 제한됨</p> </li> <li> <p>student model 하나를 학습시키기 위해서 gpt-3.5-turbo API를 450K번 request, 이는 WizardLM의 사용량 624K의 70%에 육박, 비용도 900불 가까이 듦</p> </li> <li> <p>다른 방법론에 비해 iteration이 필요해서 학습 속도가 다소 느림</p> </li> <li> <p>LLM의 성능에 의존해서 학습하는 방법이기 때문에, LLM이 역할이 매우 중대함</p> </li> </ul> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>