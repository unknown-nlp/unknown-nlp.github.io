<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training Language Models to Self-Correct via Reinforcement Learning | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, AGI ê´€ë ¨ ì—°êµ¬"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2024/training-language-models-to-self-correct-via-reinforcement/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Training Language Models to Self-Correct via Reinforcement Learning</h1> <p class="post-meta"> Created on September 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> Â  Â· Â  <a href="/blog/tag/agi"> <i class="fa-solid fa-hashtag fa-sm"></i> agi</a> Â  <a href="/blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a> Â  <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a> Â  <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a> Â  <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a> Â  <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> Â  <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement learning</a> Â  <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a> Â  <a href="/blog/tag/vision"> <i class="fa-solid fa-hashtag fa-sm"></i> vision</a> Â  Â· Â  <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2024-09-23</li> <li> <strong>Reviewer</strong>: ì¤€ì› ì¥</li> <li> <strong>Property</strong>: Reinforcement Learning, AGI</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>ìµœê·¼ì— ë² í¬ëœ LLMì€ â€˜ì•Œê³ ë¦¬ì¦˜=ë¬¸ì œìˆ˜í–‰ëŠ¥ë ¥â€™ì„ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰í•˜ì§€ ëª»í•¨ â‡’ test time queryì— ëŒ€í•´ì„œ LLMì´ ìì²´ì ìœ¼ë¡œ ìƒì„±í•œ responseë¥¼ â€˜self-correctâ€™í•˜ê³  best-possible final responseì„ ìƒì„±í•˜ëŠ” actionì„ ì·¨í•˜ì§€ ëª»í•¨.</p> <ul> <li> <p>â€˜self correctionâ€™ì—ì„œ external inputì´ ì—†ëŠ” settingì„ <em>â€˜intrinsic self-correctionâ€™</em>ì´ë¼ê³  í˜„ì¬ LLMì€ í•´ë‹¹ ë¬¸ì œì— ì·¨ì•½í•˜ë‹¤ëŠ” ì„ í–‰ì—°êµ¬ê°€ ì¡´ì¬í•¨.</p> </li> <li> <p>ë…¼ë¬¸ì—ì„œëŠ” <strong><em>â€˜intrinsic self-correctionâ€™</em></strong>ì„ í•´ê²°í•˜ê³ ì í•¨.</p> </li> </ul> </li> <li> <p>ë…¼ë¬¸ì—ì„œëŠ” LLMì´ ìœ„ì™€ ê°™ì´ ì–´ë ¤ìš´ ë¬¸ì œë¥¼ í’€ë•Œ â€œon-the-flyâ€ settingìœ¼ë¡œ mistakeë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë¡ ì„ ì—°êµ¬í•˜ê³ ì í•¨.</p> <ul> <li>Snell et al., 2024ì— ë”°ë¥´ë©´, LLMì´ ë¬¸ì œì— ëŒ€í•œ ì˜¤ë‹µì„ ë‚´ë†“ì•˜ì„ë•Œ ì‹¤ì œë¡œëŠ” ì •ë‹µê³¼ ê´€ë ¨ underlying â€œknowledgeâ€ì„ ê°€ì§€ê³  ìˆì§€ë§Œ, ê·¸ê²ƒì´ í•„ìš”í• ë•Œ ì •í™•íˆ elicití•˜ê³  draw inferenceí•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ë‹¤ê³  í•¨</li> </ul> </li> <li> <p>ê·¸ë ‡ë‹¤ë©´ ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” ì–´ë–»ê²Œ LLMì—ì„œ self-correction abilitiesë¥¼ ì£¼ì…í–ˆì„ê¹Œ?</p> <ol> <li> <p>Prompt-engineering (Kim et al., 2023; Madaan et al., 2023)</p> </li> <li> <p>meaningful intrinsic self-correctionì„ ìˆ˜í–‰í•˜ëŠ”ë° í•œê³„ê°€ ì¡´ì¬í•¨</p> </li> <li> <p>Fine-tuning (Havrilla et al., 2024b; Qu et al., 2024; Welleck et al., 2023; Yuan et al., 2024)</p> </li> <li> <p>inferenceì‹œì— verifier, refinement model ê°™ì€ multiple modelì´ í•„ìš”í•¨</p> </li> <li> <p>self-correction training ì¤‘ oracle teacher modelì´ í•„ìš”í•¨</p> </li> </ol> </li> <li> <p>ì €ìë“¤ì€ ì‹¤í—˜ ê°€ëŠ¥í•œ 2ê°œì˜ baselineì˜ í•œê³„ë¥¼ ì œì‹œí•˜ë©´ì„œ ìƒˆë¡œìš´ ë°©ë²•ë¡ ì„ ì œì‹œí•¨</p> <ol> <li>Rejection Sampling ê¸°ë°˜ì˜ STaRëŠ” modelì´ correctionì„ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ biasë¥¼ ê°•í™”í•œë‹¤.</li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>minimal edit strategyëŠ” self correction ëŠ¥ë ¥ì„ ì£¼ì…í•˜ì§„ ì•Šì§€ë§Œ, â€˜ ë‘ë²ˆì§¸ generationì—ì„œ ì •ë‹µì„ ì˜ëª» ì…ë ¥í•˜ëŠ” í•™ìŠµì„ ì–µì œí•œë‹¤.â€™ â†’ suboptimalì´ë©°, distributional mismatch í™˜ê²½ì—ì„œ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•„ì§</li> </ol> <ul> <li> <p>ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì—ì„œëŠ”</p> <ul> <li>self generated dataë¡œ í•™ìŠµì´ ê°€ëŠ¥í•œ multi-turn RL ë°©ë²•ë¡ ì„ ì œì‹œ</li> </ul> </li> </ul> <h2 id="2-related-works">2. Related Works</h2> <p><strong>#### Prompting for intrinsic self-correction</strong></p> <ul> <li> <p>self-correction ê³¼ì •ì¤‘ì— oracle answerì„ í™œìš©í•˜ëŠ” í•œê³„</p> </li> <li> <p>first responseë¥¼ ìƒì„±í• ë•ŒëŠ” weak promptë¥¼ ì‚¬ìš©í•˜ê³ , self-correctionì‹œì—ëŠ” strong promptë¥¼ ì‚¬ìš©í•´ overestimate ë¬¸ì œ</p> </li> </ul> <p><strong>#### Fine-tuning for intrinsic self-correction</strong></p> <ul> <li>oracle feedback (revisions directly from human annotators &amp; human)ì„ SFTì— ì§ì ‘ì ìœ¼ë¡œ í™œìš©í•œë‹¤ëŠ” í•œê³„</li> </ul> <p>â‡’ ë³¸ ì—°êµ¬ëŠ” learnerê°€ ì§ì ‘ ìƒì„±í•˜ëŠ” training dataë§Œì„ í™œìš©</p> <h2 id="3-preliminaries-and-problem-setup">3. Preliminaries and Problem Setup</h2> <blockquote> <p><strong>Problem Define</strong>: External Feedbackì´ ì—†ëŠ” intrinsic self-correctionâ€™ setup</p> </blockquote> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Setting</p> <ul> <li> <p><strong>Multi-turn RL (2-turns)</strong></p> </li> <li> <p>Dataset: \mathcal{D} = {(\mathbf{x}<em>i, \mathbf{y}_i^*)}</em>{i=1}^{N}</p> </li> <li> <p>What we want to train: LLM Policy \ \pi<em>\theta(\cdot \mid [\mathbf{x}, \hat{\mathbf{y}}</em>{1:t}, \mathbf{p}_{1:t}])</p> </li> <li> <p>Reward Function: \text{verifier } r(\mathbf{y}, \mathbf{y}^*)</p> <ul> <li>ë‹¹ì—°íˆ, inference timeì—ëŠ” ì ‘ê·¼í•  ìˆ˜ ì—†ìœ¼ë©° ëª¨ë¸ì´ ì¶”ë¡ ì„ í†µí•´ì„œ mistakeê°€ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ íŒë‹¨.</li> </ul> </li> </ul> </li> <li> <p>Standard SFTë‚˜ ì¼ë°˜ì ì¸ RL tuningê³¼ëŠ” ë‹¬ë¦¬, ì—¬ëŸ¬ í„´ì„ ë™ì‹œì— í•™ìŠµ.</p> </li> <li> <p>ì¤‘ê°„ í„´ ì‘ë‹µ Å·_{1:t}ì€ final rewardì˜ intermediate contextë¥¼ ìœ„í•´ ê°„ì ‘ì ìœ¼ë¡œ supervised tuningë¨.</p> </li> <li> <p><strong>A base RL approach for fine-tuning LLMs</strong></p> <ul> <li> <p>on-policy gradient í•™ìŠµ ê¸°ë²•ì„ ì‚¬ìš©</p> </li> <li> <p>REINFORCE with KLD penalty (PPO)</p> </li> </ul> </li> <li> <p><strong>Metric</strong></p> <ul> <li> <p>Accuracy@t1: ì²« ë²ˆì§¸ ì‹œë„ì—ì„œì˜ LMì˜ ACC</p> </li> <li> <p>Accuracy@t2: ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œì˜ LMì˜ ACC</p> </li> <li> <p>Î”(t1, t2): ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì‹œë„ ì‚¬ì´ì˜ LMì˜ ACC ìˆœ ê°œì„ , self-correction íš¨ê³¼ë¥¼ ì¸¡ì •</p> </li> <li> <p>Î”^iâ†’c(t1, t2): ì²« ì‹œë„ì—ì„œ í‹€ë ¸ì§€ë§Œ ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œ ë§ì€ ë¬¸ì œì˜ ë¹„ìœ¨, self-correctionì´ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë¬¸ì œì˜ ìˆ˜ë¥¼ ì¸¡ì •</p> </li> <li> <p>Î”^câ†’i(t1, t2): ì²« ì‹œë„ì—ì„œ ë§ì•˜ì§€ë§Œ ë‘ ë²ˆì§¸ ì‹œë„ì—ì„œ í‹€ë¦° ë¬¸ì œì˜ ë¹„ìœ¨, ëª¨ë¸ì´ ì •ë‹µì„ ì´í•´í•˜ëŠ” ëŠ¥ë ¥ì„ ì¸¡ì • (ë§ì´ ë–¨ì–´ì§€ë©´ ëœ ì´í•´í•˜ëŠ”ê±´ê°€?)</p> </li> </ul> </li> </ul> <h2 id="4-supervised-fine-tuning-on-self-generated-data-is-insufficient-for-self-correction">4. Supervised Fine-Tuning on Self-Generated Data is Insufficient for Self-Correction</h2> <p>** #### â€˜Intrinsic Self-Correctionâ€™ Settingì´ë‹ˆ ì™¸ë¶€ modelì˜ feedback ì—†ì´ base-modelì„ ê°€ì§€ê³  self correctioní•˜ëŠ” SFTí•˜ëŠ” ë°©ë²•ë¡ ì„ ëŒë ¤ë³´ë©´ ì–´ë–¨ê¹Œ?**</p> <ul> <li> <p>Baselines SFT</p> <ul> <li> <p>STaR</p> </li> <li> <p>Pair-SFT</p> </li> </ul> </li> </ul> <h3 id="41-analysis-setup-methods-and-dataset-construction">4.1. Analysis Setup: Methods and Dataset Construction</h3> <ul> <li> <p>STaR â‡’ (1) base modelë¡œ two-turn self-correction tracesë¥¼ ìƒì„±í•¨. (2) second attemptsê°€ successfullyí•˜ê²Œ first attemptì˜ incorrect responsesë¥¼ reviseí•  ê²½ìš°ì—ë§Œ filter</p> <ul> <li> <p>\mathcal{D}<em>{\text{STaR}} := {(x_i, \hat{y}_i^{-}, \hat{y}_i^{+})}</em>{i=1}^{N}, \text{ where } \hat{y}_i^{-} \text{ and } \hat{y}_i^{+} \text{is incorrect and correct answer}</p> <ul> <li>ìœ„ì˜ ì„¤ëª… ê·¸ëŒ€ë¡œ ë°ì´í„°ì…‹ êµ¬ì¶•</li> </ul> </li> </ul> </li> </ul> <p>â‡’ Training 3 iterations of collecting and running SFT.</p> <ul> <li> <p>Welleck et al. (2023) â‡’ (1) base modelë¡œ two-turn self-correction tracesë¥¼ ìƒì„±í•¨. (2) first attemptsì—ì„œ pairing incorrect responses with correct ones í›„ generates â€œsyntheticâ€ repair traces</p> <ul> <li> <p>\mathcal{D}<em>{\text{SFT}} := {(x_i, \hat{y}_i^{-}, \tilde{y}_i^{+})}</em>{i=1}^{N}, \text{ where } \tilde{y}_i^{+} \text{ is a random correct response for problem ğ’™} \ \text {randomly sampled from the set of all first-turn and second-turn responses produced} \ \text{by the model.}</p> <ul> <li>two-turn self-correction tracesì—ì„œ ìƒì„±í•œ ì •ë‹µ ì¤‘ ëœë¤í•˜ê²Œ ë°°ì¹˜.</li> </ul> </li> </ul> </li> </ul> <p>â‡’ Training 1 epoch with SFT.</p> <h3 id="42-empirical-findings">4.2. Empirical Findings</h3> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Pair-SFTë¡œ í•™ìŠµí•˜ë©´ base-model ëŒ€ë¹„ ì˜ë„í•œ self-correction í–‰ë™ê²½í–¥ì„±ì´ ë„ì¶œë¨. (Î”(t1, t2)ì˜ ì¦ê°€, incorrect â†’ correctì˜ ì¦ê°€ correct â†’ incorrectì˜ ê°ì†Œ)</p> </li> <li> <p>STaRì˜ ê²½ìš° ì˜¤íˆë ¤ self-correctionì´ ì „í˜€ ì¼ì–´ë‚˜ì§€ ì•Šê³  ìˆëŠ”ë°, ì €ìë“¤ì€ â€˜revision trajectoriesâ€™ê°€ limitedëœ spaceì—¬ì„œ ê·¸ë ‡ë‹¤ê³  í•¨ (ì‚¬ì‹¤ ë­”ë§ì¸ì§€ ëª¨ë¥´ê² ..)</p> </li> <li> <p>Table 1ì—ì„œ ê°€ì¥ í° ë¬¸ì œëŠ” Correctë¥¼ Incorrectë¡œ ë°”ê¿”ë²„ë¦°ë‹¤ëŠ” ê²ƒ <strong>(ì¤€ì› ìƒê°: ì‚¬ì‹¤ìƒ self-correctionì´ ì•„ë‹ˆë¼ ê·¸ëƒ¥ ë¬´ì‘ìœ„ë¡œ alignmentë¥¼ í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì„)</strong>, ì €ìë“¤ì€ ì´ëŸ° ê²½í–¥ì„±ì„ ì–µì§€ë¡œ ì§€ì›Œë²„ë¦¬ê³ ì 2ë²ˆì§¸ attempts ëª¨ë‘ì— ëŒ€í•´ì„œ correct responseë¡œ í•™ìŠµí•˜ëŠ” Table2 ê²°ê³¼ë¥¼ ë‚´ë†“ìŒ</p> <ul> <li>STaRëŠ” self-correctionì„ ê±°ì˜í•˜ì§€ ëª»í•˜ê³ , Pair-SFTëŠ” ì•„ì˜ˆ ì •ë‹µì„ ë³€ê²½í•˜ì§€ ì•Šê³  ìˆìŒ.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>(ì œì•ˆí•œ ë°©ë²•ë¡  SCoReë„ ê±°ì˜ ë³€í™”ê°€ ì—†ê¸´í•œë°..) STaR D+, SFT DëŠ” base model ëŒ€ë¹„ edit distance (first vs second response)ì˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p> <ul> <li>base model ëŒ€ë¹„ edit distanceê°€ ê±°ì˜ ì—†ë‹¤ (ë§Œì•½ first attemptì˜ ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ ìœ„ì˜ í‘œê°€ ë‚©ë“ì´ ê°€ì•¼í•˜ëŠ”ë° ê·¸ë ‡ì§€ ì•Šìœ¼ë‹ˆ, ë¬¸ì œê°€ ìˆë‹¤ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.)</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>STaR D+ì™€ëŠ” ë‹¬ë¦¬ SFT DëŠ” trainingê³¼ evaluationì—ì„œ edit distance (first vs second response)ì˜ ë¶„í¬ ì°¨ì´ê°€ ë§ì´ ë‚¨.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Pair-SFTì— ëŒ€í•œ ablation</p> </li> <li> <p>fixed ~: first responseëŠ” ê° datasetì— ìˆëŠ” offline responseë¥¼ ê·¸ëŒ€ë¡œ conditionìœ¼ë¡œ ì“°ëŠ” setting</p> <ul> <li>trainingì€ ë‹¹ì—°íˆ datasetì— ìˆëŠ” ê±¸ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•´ì•¼ í•˜ë‹ˆ Fixed train</li> </ul> </li> <li> <p>self-generated: learnerê°€ first responseë¥¼ generatingí•˜ëŠ” setting</p> </li> <li> <p>Pair-SFTì—ì„œ Trainingë•Œì²˜ëŸ¼ fixed validation setìœ¼ë¡œ validationì„ ì§„í–‰í•˜ë©´ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ ìœ ì§€ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ</p> </li> </ul> <p><strong>â‡’ distributional shiftëŠ” self-correctionì— ìˆì–´ì„œ ì¤‘ìš”í•œ issueì´ë©°, fixed offline datasetì€ ì¢‹ì€ í•™ìŠµ ë°ì´í„°ì…‹ì´ ì•„ë‹ˆë‹¤.</strong></p> <h3 id="takeaways-insufficiency-of-sft-previous-method">Takeaways: Insufficiency of SFT (previous method)</h3> <ol> <li> <p>STaR D+ëŠ” distribution shiftì—ëŠ” ê°•ê±´í•œ ëŒ€ì‹  one mode of correctionì—ë§Œ collapseëœë‹¤ëŠ” í•œê³„ê°€ ì¡´ì¬í•¨.</p> </li> <li> <p>(i.e., correct â†’ correctì—ì„œë§Œ ê·¸ë‚˜ë§ˆ ì¡°ê¸ˆ ì‘ë™í•¨)</p> </li> </ol> <p>â‡’ what we have to solve: amplification of certain pathological behaviors that seem promising on the training data</p> <ol> <li>Pair-SFTëŠ” distribution shiftê°€ ìˆì–´ ì¸í•´ (explorationì€ ì¢‹ì•„ì§€ë‚˜: ì–´ì°Œì €ì°Œ ì„±ëŠ¥ì€ ì¢‹ì•„ì§€ë‚˜) ì •ì‘ LMì˜ first attemptsì— ëŒ€í•œ self correction ëŠ¥ë ¥ì´ ì €í•˜ëœë‹¤.</li> </ol> <p>â‡’ what we have to solve: distribution shift in training data</p> <h2 id="5-score-self-correction-via-multi-turn-reinforcement-learning">5. SCoRe: Self-Correction via Multi-Turn Reinforcement Learning</h2> <h3 id="key-challenges">Key Challenges</h3> <ul> <li> <p>ìœ„ì˜ mulit-turn RLì„ í™œìš©í•´ í•™ìŠµì„ ì§„í–‰í•˜ë©´ distributional shiftë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ”ê±´ ìˆ˜ì‹ì ìœ¼ë¡œ ë‹¹ì—°</p> </li> <li> <p>ì‹¤í—˜ì„ ì§„í–‰</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>(left) first responseê³¼ second responseì˜ train accê°€ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ê³„ì† ì›€ì§ì´ëŠ” ê±¸ ì•Œ ìˆ˜ ìˆìŒ</p> </li> <li> <p>(right) ì´ˆë¡ìƒ‰ ì„ ì„ ë³´ë©´ first responseëŒ€ë¹„ second responseê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ answerë¥¼ ìƒì„±í•˜ëŠ”ê°€?ì¸ë°, í•™ìŠµì„ ì§€ì†í•  ìˆ˜ë¡ couplingëœ ì •ë‹µì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ</p> </li> </ul> <h3 id="why-does-this-happens">Why does this happens?</h3> <ul> <li> <p>Data distributionë‚´ì—ì„œ LMì´ ì·¨í•  ìˆ˜ ìˆëŠ” optimal actionì€ 2ê°€ì§€.</p> <ol> <li> <p>first responseì—ì„œ second responseë¡œ reviseí•˜ë„ë¡ í•™ìŠµí•˜ê±°ë‚˜ â‡’ Unseen Test distributionì—ì„œ generalization ì‹œì¼œì•¼í•  ì •ì±….</p> </li> <li> <p>first responseì—ì„œ ìµœìƒì˜ ì‘ë‹µì„ ìƒì„±í•œ ë‹¤ìŒ second responseì—ì„œ ìˆ˜ì •í•˜ì§€ ì•Šë„ë¡ í•™ìŠµ</p> </li> </ol> </li> </ul> <p><strong>â‡’ Overparameterizationëœ LLMì€ (Data distributionë‚´ì—ì„œ 1,2ê°€ ë‘˜ë‹¤ ìµœì ì´ë¼ê³  ë¦¬ì›Œë“œê°€ ì„¤ê³„ë˜ì–´ìˆê³  ê·¸ê±¸ ê·¸ëŒ€ë¡œ í•™ìŠµì„ í•˜ë©´) 1.ì— ëŒ€í•œ ì •ì±…ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŒ.</strong></p> <p>**â‡’ ê³¼ê±°ì˜ ì£¼ì–´ì§„ attemptsì— ëŒ€í•´ì„œ self-correctingí•˜ëŠ” ìµœì ì˜ ë°©ë²•ì´ ì•„ë‹ˆë¼, í˜„ì¬ ì£¼ì–´ì§„ responseë¥¼ ê°œì„ í•˜ëŠ” next responseë¥¼ ìƒì„±í•˜ë¼ê³  modelì—ê²Œ í•™ìŠµì‹œì¼œì•¼ í•¨. **</p> <h3 id="method">Method</h3> <p>Objective</p> <ol> <li> <p>(ë¹„ìœ ë¥¼ ë“¤ë©´ prior distributionì„ ê¹”ì•„ì¤Œìœ¼ë¡œì¨) LMì´ first attempt distributioní•˜ì—ì„œ second distributionì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ</p> </li> <li> <p>reward-shapingì„ í†µí•´ bias model to self-correct</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="51-stage-i-training-a-model-initialization-to-prevent-collapse">5.1. Stage I: Training a Model Initialization to Prevent Collapse</h3> <ul> <li> <p>one-mode collapseë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ RLë¡œ í•™ìŠµ ì§„í–‰</p> </li> <li> <p>second responseì—ì„œ ë†’ì€ reward revisionì´ ì´ë£¨ì–´ì§€ë„ë¡ í•˜ëŠ” ë™ì‹œì— KLDë¥¼ ì‚¬ìš©í•˜ì—¬ first response distributionì´ base-model ìµœëŒ€í•œ ê°€ê¹ê²Œ ì œí•œí•¨</p> <ul> <li> <p>ìˆ˜ì •í•  ê²Œ ê±°ì˜ ì—†ëŠ” first responseì„ ë” ë‚˜ì€ second responseìœ¼ë¡œ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ ìµœì ì´ ì•„ë‹Œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ëª©ì ì€ first responseì™€ second responseë¥¼ decouplingí•˜ëŠ”ë° ìˆìŒ</p> <ul> <li>second responseì—ë„ KLD termì„ ë‘ëŠ”ë° weightë¥¼ 1/10ìœ¼ë¡œ ë‘ </li> </ul> </li> <li> <p>Untitled ì„ ë³´ë©´ Stage 1ì´ ë‹¹ì—°í•˜ê²Œë„ first responseì™€ second responseì— ëŒ€í•œ mode collapse ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ëŠ”ë° ë„ì›€ì´ ë¨</p> </li> </ul> </li> </ul> <h3 id="52-stage-ii-multi-turn-rl-with-reward-shaping">5.2. Stage II: Multi-Turn RL with Reward Shaping</h3> <ul> <li> <p>first responseì™€ second responseì— ëŒ€í•œ reward sumì„ maxí•˜ë„ë¡ training</p> </li> <li> <p>Reward shaping to incentivize self-correction</p> <ul> <li> <p>LMì´ self-correction solutionì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ biasë¥¼ ê±¸ì–´ì£¼ëŠ” ì‘ì—…ì„ ì§„í–‰</p> </li> <li> <p>second responseì— ì•„ë˜ì˜ bonusë¥¼ ë”í•´ì¤Œ. (Î”^iâ†’c(t1, t2)ì™€ Î”^câ†’i(t1, t2)ì— ìœ ë¦¬í•œ reward shapping)</p> <ul> <li>\hat{b}(y<em>2 \mid y_1, y^</em>) = \alpha \cdot (\hat{r}(y<em>2, y^</em>) - \hat{r}(y_1, y^*))</li> </ul> </li> <li> <p>training setì— mode collapseë¥¼ ì™„í™”í•´ì¤„ ê²ƒì„ ê¸°ëŒ€</p> </li> </ul> </li> </ul> <h2 id="6-experimental-evaluation">6. Experimental Evaluation</h2> <ul> <li> <p><strong>Tasks &amp;. Models</strong></p> <ul> <li> <p>MATH â‡’ Gemini 1.5 Flash</p> <ul> <li>training set with 4500</li> </ul> </li> <li> <p>HumanEval, MBPP-R (offline test: incorrect first-attempt is generated by PaLM2) â‡’ Gemini 1.0 Pro</p> <ul> <li>training with MBPP</li> </ul> </li> </ul> </li> <li> <p><strong>Results</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>base model ëŒ€ë¹„ Î”(t1, t2) 15.6%, Accuracy@t2 23.0% ì¦ê°€</p> </li> <li> <p>ê°€ì¥ ê³ ë¬´ì ì¸ê±´ ì˜ë„í•œ self-correctionì´ ë™ì‘í•œë‹¤ëŠ” ì </p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Pair-SFTê°™ì€ ê²½ìš° offline settingì—ì„œë§Œ ì„±ëŠ¥ì´ ì¢‹ì€ ë°˜ë©´, SCoReëŠ” self-generated settingì—ì„œ self-correction(12.2% intrinsic self-correction delta)ì„ í†µí•´ ì„±ëŠ¥ ì¦ê°€ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ</p> <ul> <li>Accuracy@t1ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ê±´ KLD ë•Œë¬¸ì´ ì•„ë‹ê¹Œ?</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-09-23-training-language-models-to-self-correct-via-reinforcement/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>(w/o m.t.t) single turnìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ë©´ ë‹¹ì—°íˆ Accuracy@t1ì€ ë†’ì§€ë§Œ ê·¸ ì™¸ì˜ ì§€í‘œëŠ” ë–¨ì–´ì§€ê³ ,</p> </li> <li> <p>(w/o s1, rs) stage 1ì´ë‚˜ reward shapingì„ ì œê±°í•˜ë©´ ì˜ë„í•œ self-correctionì„ ìˆ˜í–‰í•˜ì§€ ëª»í•´ Accuracy@t2ì™€ net increase acc ì—­ì‹œ í•˜ë½í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p> </li> <li> <p>(w STaR) ë§ˆì§€ë§‰ìœ¼ë¡œ stage 2ì—ì„œ on-policyê°€ ì•„ë‹Œ offline dataë¡œ í•™ìŠµí•˜ë©´ distributional shiftë•Œë¬¸ì— spurious solutionì„ í•™ìŠµí•´ ì„±ëŠ¥ì´ í•˜ë½í•œë‹¤ê³  ë…¼ë¬¸ì—ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤.</p> </li> </ul> <h2 id="7-discussion">7. Discussion</h2> <ul> <li> <p>ë…¼ë¬¸ì—ì„œë„ ì–¸ê¸‰í•˜ì§€ë§Œ 1 round ì´ìƒ iterative correction ëª»í•œ ê²ƒì„ limitationìœ¼ë¡œ ì´ì•¼ê¸°í•˜ê³  ìˆìŒ</p> </li> <li> <p>ChatGPTë‚˜ ì‹œì¤‘ì— ë‚˜ì˜¨ chatbotë“¤ì„ ì“°ë©´ì„œ ëª‡ turn ëŒ€í™”ë¥¼ ì´ì–´ì„œ ì§„í–‰í•  ë•Œ ê³„ì† ê°™ì€ í•´ë‹µì„ ë‚´ì£¼ëŠ”ê±°ì— ëŒ€í•´ì„œ ì™œ ê·¸ëŸ´ê¹Œ?ì— ëŒ€í•œ ëŒ€ë‹µê³¼ ê·¸ í•´ê²°ì±…ì„ ì œì‹œí•´ì¤€ ë…¼ë¬¸ì´ì—ˆë‹¤.</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>