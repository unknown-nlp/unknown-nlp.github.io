<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Retrieval 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks"> <meta property="og:url" content="https://unknown-nlp.github.io/blog/2024/search-in-the-chain-interactively-enhancing-large-language/"> <meta property="og:description" content="논문 리뷰 - Retrieval 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks"> <meta name="twitter:description" content="논문 리뷰 - Retrieval 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknown-nlp.github.io/blog/2024/search-in-the-chain-interactively-enhancing-large-language/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - Retrieval 관련 연구",
        "headline": "Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknown-nlp.github.io/blog/2024/search-in-the-chain-interactively-enhancing-large-language/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks</h1> <p class="post-meta"> Created on March 26, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> retrieval</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2024-03-26</li> <li> <strong>Reviewer</strong>: 상엽</li> <li> <strong>Property</strong>: Retrieval</li> </ul> <h1 id="introduction">Introduction</h1> <p>LLM의 여전한 한계</p> <ol> <li> <p>다양한 지식들을 합쳐서 추론하는 것에 대한 어려움.</p> </li> <li> <p>memorization of long-tail and real-time knowledge</p> </li> <li> <p>hallucination</p> </li> <li> <p>reference없이 context만으로 생성하는 것은 traceability가 떨어지면 신뢰하기 어려움.</p> </li> </ol> <p>→ Retrieval-augmented method는 위의 문제들을 외부 지식과 모델을 결함시킴으로써 일부 해결</p> <p>Retrieval-agumneted method를 도입할 때 역시 문제 존재</p> <p>C-1 : IR을 LLM reasoning process에 바로 이용하는 것은 LLM의 reasoning chain을 훼손할 때도 있음. (LLM이 local sub-question reason만 하기 때문에) ← IR의 결과물이 들어가기 때문에 발생하는 문제점 + sub-question에 집중한 reasoning 방식</p> <p>C-2 : IR 결과 vs LLM 자체 지식의 충돌 → 잘못된 결과를 만들 가능성이 존재.</p> <p>C-3 : reasoning direction을 수정할 방법이 없음. (조금 억지)</p> <p>Search-in-the-Chain (SearChain) 제안</p> <ul> <li> <p>IR-LLM interaction round를 여러 번 진행</p> </li> <li> <p>IR-LLM interaction : reasoning → (verification → completion) 반복 → tracing</p> </li> <li> <p>그림을 보자.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>CoQ를 만들기 위해 In-context learning 이용 복잡한 질문을 풀기 위한 IR-oriented query 형태로 분해해서 구성</li> </ol> <ul> <li> <p><strong>reasoning</strong></p> <ul> <li> <p><strong>노드</strong>(node)** :** IR-oriented query</p> </li> <li> <p><strong>정답</strong>(answer)** :** 쿼리에 대한 LLM의 답변</p> </li> <li> <p>flag : LLM이 추가 지식을 필요로 하는지 아닌지</p> </li> </ul> </li> </ul> <p>→ 기존에는 IR을 활용할 경우 노드당 한 번의 reasoning만 가능했지만 CoQ는 우선 완전한 체인을 먼저 생성 → C-1 해결</p> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>각 노드에 대해 IR과 상호작용 진행, 구체적으로 아래 두 단계를 선택적으로 반복</li> </ol> <ul> <li> <p><strong>verification</strong></p> <ul> <li>LLM의 정답과 retrieved 정보가 일치하지 않고 IR이 <strong>high confidence</strong>를 가질 때 → IR 피드백을 이용해 정답을 다시 생성</li> </ul> </li> <li> <p><strong>completion</strong></p> <ul> <li>flag → IR knowledge를 이용해 정답을 다시 생성</li> </ul> </li> <li> <p>위의 IR interaction을 라운드마다 반복해 CoQ를 수정 → IR에 의한 오류를 감소시킴 → C-2 해결</p> </li> </ul> <ol> <li> <strong>tracing</strong> : reasoning process를 만들고 각 reasoning step을 support하는 reference 선택</li> </ol> <ul> <li> <p>선택된 reference를 이용해 답변 생성 → traceability 향상 (RAG 방식은 공통이라고 생각.)</p> </li> <li> <p>IR interaction은 reasoning path를 chain에서 <strong>node-identify</strong> Depth-first search로 바꿈 (Tree-of-Reasoning, ToR) (뒤에 설명)</p> </li> <li> <p>IR을 이용해 node를 수정 → C-3 해결 (논문에서는 여기 적긴 했는데 verification에 좀 더 가까운듯)</p> </li> </ul> <p><strong>Main contribution</strong></p> <ul> <li> <p>We highlight the challenges in introducing IR into LLM from the perspectives of reasoning and knowledge. → ??? LLM에 IR을 도입할 때 있는 문제는 대체로 알던 거 아닌가?</p> </li> <li> <p>SearChain not only improves the knowledge-reasoning ability of LLM but also uses IR to identify and <strong>give the knowledge that LLM really needs</strong>. Besides, SearChain can <strong>mark references to supporting documents</strong> for the knowledge involved in the generated content.</p> </li> </ul> <p>→ reasoning 실험 + (Retrieval-augmented 방식 + confidence 개념)</p> <ul> <li>Interaction with IR in <strong>SearChain forms a novel reasoning path: node-identify Depth-first Search on a tree</strong>, which enables LLM to dynamically modify the direction of reasoning.</li> </ul> <p>→ ToR 제시</p> <ul> <li>Experiment shows that SearChain <strong>outperforms state-of-the- art baselines</strong> on complex knowledge-intensive tasks including multi- hop Q&amp;A, slot filling, fact checking and long-form Q&amp;A. → SOTA</li> </ul> <h1 id="related-work">Related work</h1> <ul> <li> <p>CoT prompting</p> <ul> <li> <p>CoT, self-consistency 등</p> </li> <li> <p>복잡한 질문을 작은 질문으로 나눈 후 각 질문을 locally 해결하는 것에만 집중</p> </li> </ul> </li> </ul> <p>→ Search plan을 통한 global reasoning을 고려할 수 있는 형태로 바꿈.</p> <ul> <li> <p>Retrieval-augmented LM</p> <ul> <li> <p>기존 방식들에서는 one-step reasoning만을 진행 → global chain을 만들어서 logical relationship을 더 잘 이해함.</p> </li> <li> <p>기존 IR은 정보 제공만 할뿐 수정에는 관여하지 않음. → IR iteraction은 missing knowledge를 가진 항목들에 대해서만 결과 수정을 제공하기 때문에 IR의 negative effect를 감소시키며 수정도 가능</p> </li> </ul> </li> </ul> <h1 id="method">Method</h1> <h3 id="chain-of-query-generation">Chain-of Query Generation</h3> <ul> <li> <p>In-context learning 이용</p> </li> <li> <p>global reasoning chain for complex question</p> <ul> <li> <p>branch of Tree-of-Reasoning</p> </li> <li> <p>q_i : IR-oriented query, a_i : its answer</p> </li> </ul> </li> <li> <p>Prompt</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><em>Construct a global reasoning chain</em></p> <ul> <li> <p>main task</p> </li> <li> <p>“Global” : 완전한 reasoning chain을 만들어라.</p> </li> </ul> </li> </ul> <blockquote> <p>“Global” means that LLM needs to plan a complete reasoning chain for the complex question, rather than answer the question directly or only solve “local” sub-questions (comparison shown in Figure 2)</p> </blockquote> <ul> <li> <p><em>generate a query to the search engine based on what you already know at each step of the reasoning chain</em></p> <ul> <li> <p>각 노드에서 IR-oriented query와 LLM만의 답변을 생성</p> </li> <li> <p>만약 답을 모르겠다면 [Unsolved Query] flag 이용 (missing knowledge)</p> </li> </ul> </li> <li> <p>이후 있을 round에서도 CoQ 생성은 위의 방법을 따름.</p> </li> </ul> <h3 id="interaction-with-ir">Interaction with IR</h3> <ul> <li> <p>CoQ의 각 노드에 대해 IR을 이용해 다음의 작업을 반복 - verification, completion</p> </li> <li> <p>IR의 결과를 바탕으로 CoQ의 결과를 수정 → new branch of ToR (Tree of Reasoning)</p> </li> <li> <p>Top-1 retreived document를 supporting document로 이용</p> </li> <li> <p>더 이상 노드에 대한 수정이 필요없어지면 라운드 종료</p> </li> <li> <p>correct reasoning path와 supporting document를 이용해 답변 생성</p> </li> <li> <p>Algorithm</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Verification</strong></p> <ul> <li>IR 결과를 바탕으로 CoQ 각 노드의 정답이 맞는지 확인</li> </ul> <ol> <li>retrieved Top-1 document d_i → ODQA 데이터셋으로 학습된 reader(DPR) 이용해 answer g 추출</li> </ol> <ul> <li>confidence f : predicted value that measures whether g can answer q_i</li> </ul> <p>\mathbf{H} \in \mathbb{R}^{L \times E} : input text “[\mathrm{CLS}] q_i[\mathrm{SEP}] d_i”의 last hidden state sequence (L : length, E : hidden dimension)</p> <p>\mathbf{H}_{[\mathrm{CLS}]} : last hidden state of [\mathrm{CLS}] token</p> <ol> <li>a_i와 g 비교</li> </ol> <ul> <li> <p>short-form generation task</p> <ul> <li>a_i 안에 g가 있는지 여부</li> </ul> </li> <li> <p>long-form generation task</p> <ul> <li>ROUGE between a_i and d_i &gt; threshold \alpha</li> </ul> </li> </ul> <ol> <li>a_i와 g의 결과가 일치하지 않음 &amp; f &gt; \theta (IR로 인한 성능 감소를 막기 위한 threshold)</li> </ol> <ul> <li>prompt</li> </ul> <blockquote> <p>According to the Reference, the answer for q_i should be g, you can change your answer and continue constructing the reasoning chain for [Question]: Q. Reference: d_i.</p> </blockquote> <ul> <li>a<em>i → a^{\prime}</em>{i} 변경 후, (q<em>i, a^{\prime}</em>{i})를 root 노드로 하여 새로운 CoQ 진행</li> </ul> <p><strong>Completion</strong></p> <ul> <li> <p>missing knowledge를 갖는 노드의 정답을 보충</p> </li> <li> <p>[Unsolved Query]에 대해 진행</p> </li> <li> <p>f값에 관계없이 다음 prompt 실행</p> </li> </ul> <blockquote> <p>According to the Reference, the answer for q^{\star}<em>{i} should be g^{\star}, you can give your answer and continue constructing the reasoning chain for [Question]: Q. Reference: d^{\star}</em>{i}.</p> </blockquote> <ul> <li>a<em>i → a^{\star}</em>{i} 변경 후, (q<em>i, a^{\star}</em>{i})를 root 노드로 하여 새로운 CoQ 진행</li> </ul> <p><strong>Tracing</strong></p> <ul> <li> <p>reasoning process 생성 및 각 노드에 대해 supporting document mark</p> </li> <li> <p>prompt</p> </li> </ul> <blockquote> <p>You can try to generate the final answer for the [Question] by referring to the [Query]-[Answer] pairs, starting with [Final Content]. [Query1]: q_1 [Answer1]: a_1 …[Query m]: q_m [Answer m]: a_m.</p> </blockquote> <p><strong>Node-identify Depth-first Search</strong></p> <ul> <li> <p>SearChain이 만드는 reasoing path</p> </li> <li> <p>Depth first search와 같이 정답이 해결되거나 unsolvable sub-question이 존재하면 계속 reasoning한다는 점에서 동일</p> </li> <li> <p>“node-identify” : 하나의 search direction이 끝날 때 parent node로 가는 것이 아니라 node verification과 completation을 통해 시작 node를 결정한다는 것이 다름.</p> </li> <li> <p>LLM - IR 상호작용을 통해 동적으로 reasoning direction을 결정</p> </li> </ul> <h1 id="experiments">Experiments</h1> <h3 id="experimental-setup">Experimental setup</h3> <ul> <li> <p>Datasets</p> <ul> <li> <p>multi-hop question-answering (HotpotQA (HoPo)</p> </li> <li> <p>Musique (MQ)</p> </li> <li> <p>WikiMulti-HopQA (WQA)</p> </li> <li> <p>StrategyQA (SQA)</p> </li> <li> <p>slotfilling (zsRE)</p> </li> <li> <p>T-REx</p> </li> <li> <p>fact checking (FEVER)</p> </li> <li> <p>long-form question-answering (ELI5)</p> </li> </ul> </li> <li> <p>Evaluation Metric</p> <ul> <li> <p>long and free-form : ROUGE-L</p> </li> <li> <p>cover-EM : 정답 포함 여부</p> </li> </ul> </li> <li> <p>Baselines</p> <ul> <li> <p>reasoning : CoT, CoT-SC, Auto-CoT, Recite-and-answer, Least-to-Most</p> </li> <li> <p>IR : Direct, Self-Ask, ToolFormer, React, DSP, Verify-and-Edit</p> </li> </ul> </li> <li> <p>implementation</p> <ul> <li> <p>LLM : gpt-3.5-turbo</p> </li> <li> <p>retrieval model : ColBERTv2 (V100 이용)</p> </li> <li> <p>최대 interaction 라운드 수 : 5</p> </li> <li> <p>\alpha : 0.35, \theta : 1.5</p> </li> </ul> </li> </ul> <h3 id="main-results">Main Results</h3> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Effect of Chain-of-Query</strong></p> <ul> <li> <p>outperform all baselines</p> </li> <li> <p>local perspective : step by step으로 sub-question을 생성하고 답변하는 것</p> </li> <li> <p>global perspiective : global reasoning chain of sub-questions</p> </li> </ul> </li> <li> <p><strong>Effect of interaction with IR</strong></p> <ul> <li> <p>again outperforms all the baselines.</p> </li> <li> <p>CoQ는 IR interaction을 통해 LLM reasoning과 IR 결과의 일관성을 유지하기 때문 (IR 결과를 사용할지 말지 threshold가 있음.)</p> </li> </ul> </li> </ul> <h3 id="analysis">Analysis</h3> <p><strong>KnowledgeDecoupling</strong></p> <ul> <li> <p>4 multi-hop QA datasets의 knowledge source에 대해 분석</p> <ol> <li> <p>LLM의 knowledge</p> </li> <li> <p>Corrected by IR (Verification) : IR과 일치하지 않은 결과로 인해 수정된 knowledge</p> </li> <li> <p>Completed by IR (Completion) : LLM의 [Unsolved Query]에 대해 수정된 knowledge</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 대부분은 LLM 자체 지식으로 해결되지만 IR 결과에 의해 수정되는 부분도 생각보다 많이 존재

- 데이터셋의 특징에 따라 그 비율은 많이 바뀌는 거 같음.
</code></pre></div></div> <p><strong>Positive and Negative Effects of IR on LLM</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>(a) Positive</p> <ul> <li> <p>\mathbb{S}_{I R} : IR 결과에 의해 수정, 보완된 answer들만 추출</p> </li> <li> <p>w/o IR (\mathbb{S}_{I R}) &lt; w/o IR (\mathbb{S}) : 실제로 LLM이 답변을 하는데 어려움을 겪음.</p> </li> <li> <p>w IR (\mathbb{S}<em>{I R}) &gt; w/o IR (\mathbb{S}) &gt; w/o IR (\mathbb{S}</em>{I R}) : IR을 통한 답변 성능 향상</p> </li> </ul> </li> <li> <p>(b) Negative</p> <ul> <li> <p>LLM이 맞았지만 IR이 다른 정답을 준 비율</p> </li> <li> <p>SearChain에서 confidence를 사용한 필터링이 효과적임.</p> </li> </ul> </li> </ul> <p><strong>CoQ vs Baselines in Reasoning</strong></p> <p>성능 외 reasoning을 평가하기 위한 두 가지 측면 추가적으로 제시</p> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>reasoning step의 수</p> <ul> <li> <p>복잡한 쿼리를 가지는 Musique 이용</p> </li> <li> <p>제안 모델이 reasoning step이 많고 정확도 또한 높다.</p> </li> </ul> </li> <li> <p>어려운 sub-question 해결력</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>local perspective (주어진 쿼리를 해결하는 것에 집중) 방식으로는 reasoning을 멈추는 경향 이 큼. (Figure 4)</p> </li> <li> <p>LLM gloobal chain reasoning을 유도함으로써 이런 현상이 줄어듦.</p> <ul> <li>상엽 생각 : global을 확인할 수 있는 부분이 prompt에서 첫 문장 뿐이고 global chain이라는 용어가 생소할텐데 이게 어떻게 되는지 이해가 잘 안됨.</li> </ul> </li> <li> <p>table 4의 결과 역시 더 많은 reasoning이 끊기지 않음을 뒷받침하는 근거</p> </li> <li> <p><strong>more case study</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>SearChain vs New Bing in Tracing</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>SearChain과 New bing search를 아래 두 가지 측면에서 비교</p> <ul> <li> <p>Scope of Knowledge Coverage (SKC) [0, +]:</p> <ul> <li> <p>답변에서 document에 의해 support 된다고 mark된 아이템의 수</p> </li> <li> <p>SearChain (2.882) is better than New Bing (1.143)</p> </li> </ul> </li> <li> <p>Accuracy of Marking Position (AMP) [0, 1]</p> <ul> <li> <p>reference mark의 위치의 정확도, 3명의 대학원생이 평가</p> </li> <li> <p>SearChain (0.80) is better than New Bing (0.45)</p> </li> </ul> </li> </ul> <p><strong>Efficiency Analysis</strong></p> <figure> <picture> <img src="/assets/img/posts/2024-03-26-search-in-the-chain-interactively-enhancing-large-language/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>n : LLM의 input</p> <p>m : LLM의 output</p> <p>r : IR-LLM interaction 수</p> <p>t : 응답 시간</p> <p>나의 생각</p> <p>[Unsolved Query]가 정말 잘 달릴까? Table 2에서 일부 확인 가능</p> <p>Appendix와 본문을 봤을 때 threshold와 alpha를 정하는 과정에서 validation이 아닌 test set을 이용했을 가능성도 있는 거 같다.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>