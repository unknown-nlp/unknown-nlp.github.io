<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Titans: Learning to Memorize at Test Time | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="ë…¼ë¬¸ ë¦¬ë·°"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Titans: Learning to Memorize at Test Time"> <meta property="og:url" content="https://unknown-nlp.github.io/blog/2025/titans-learning-to-memorize-at-test-time/"> <meta property="og:description" content="ë…¼ë¬¸ ë¦¬ë·°"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Titans: Learning to Memorize at Test Time"> <meta name="twitter:description" content="ë…¼ë¬¸ ë¦¬ë·°"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknown-nlp.github.io/blog/2025/titans-learning-to-memorize-at-test-time/",
        "@type": "BlogPosting",
        "description": "ë…¼ë¬¸ ë¦¬ë·°",
        "headline": "Titans: Learning to Memorize at Test Time",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknown-nlp.github.io/blog/2025/titans-learning-to-memorize-at-test-time/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Titans: Learning to Memorize at Test Time</h1> <p class="post-meta"> Created on February 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a> Â  <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a> Â  <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a> Â  <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a> Â  <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a> Â  Â· Â  <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2025-02-04</li> <li> <strong>Reviewer</strong>: ì¤€ì› ì¥</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li>Transformer</li> </ul> <p>â‡’ (1) key-value associationsì„ ì €ì¥ (2) queryë¥¼ í†µí•´ retrieveí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ</p> <p>â‡’ current context windowì— ì§ì ‘ì ì¸ dependenciesê°€ í˜•ì„±ë  ìˆ˜ ë°–ì— ì—†ìŒ</p> <ul> <li>Overcome the scalability issue of Transformers</li> </ul> <p>â‡’ linear transformer: softmaxëŒ€ì‹  kernel trickìœ¼ë¡œ attention ê³„ì‚°</p> <p>â‡’ dataê°€ matrix-valued statesë¡œ mapping/compressedì´ ë˜ê¸° ë•Œë¬¸ì— very long contextì—ì„œ íš¨ìš©X</p> <ul> <li> <p>Limitation of recurrent neural network</p> <ol> <li> <p>ë‹¨ê¸° ê¸°ì–µ, ì¥ê¸° ê¸°ì–µ, ë©”íƒ€ ê¸°ì–µ, í˜„ì¬ ë§¥ë½ì— ëŒ€í•œ attentionì„ ëª¨ë‘ ì ì ˆí•˜ê²Œ êµ¬í˜„í•œ architectureì˜ ë¶€ì¬</p> </li> <li> <p>êµ¬ì„± ìš”ì†Œê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆëŠ” ìƒí˜¸ ì—°ê²°ëœ ì‹œìŠ¤í…œì˜ ë¶€ì¬</p> </li> <li> <p>(LSTM, GRUê°€ ì–´ëŠì •ë„ëŠ” í•˜ì§€ë§Œ) ì—¬ì „íˆ ë°ì´í„°ë¥¼ í†µí•´ ì¶”ìƒí™”ëœ ê³¼ê±° ì—­ì‚¬ë¥¼ ì•”ê¸°í•˜ëŠ” ëŠ¥ë ¥ì´ ê²°ì—¬</p> </li> </ol> </li> <li> <p>Memory Perspective</p> </li> </ul> <p>â‡’ ë…¼ë¬¸ì—ì„œëŠ” Memory ê´€ì ì—ì„œ ê¸°ì¡´ì˜ Modelì„ ë…ìì—ê²Œ ì´í•´ì‹œí‚¤ê³ ì í•¨</p> <ul> <li><strong>memory: inputì— ì˜í•´ì„œ ë°œìƒí•˜ëŠ” neural update</strong></li> </ul> <ol> <li> <p>RNN</p> </li> <li> <p>ğ‘“ (M<em>{ğ‘¡ âˆ’1}, ğ‘¥</em>ğ‘¡ )</p> </li> <li> <p>ğ‘”(M<em>ğ‘¡, ğ‘¥</em>ğ‘¡ )</p> </li> </ol> <p>â†’ të²ˆì§¸ ì…ë ¥ì— ì˜í•´ì„œ â€˜vector-valued memory moduleâ€™ Mì´ ì—…ë°ì´íŠ¸ ë˜ê³ , retreiving ë˜ëŠ” ê²ƒì˜ ë°˜ë³µ</p> <ol> <li>Transformer</li> </ol> <p>â†’ RNNê³¼ ë‹¬ë¦¬ past key, valueë¥¼ ê³„ì† appendingí•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸</p> <p>â†’ matrix-valued memory Module</p> <ul> <li> <p>ìœ„ì˜ ë…¼ì˜ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¬¸ì—ì„œëŠ” 5ê°œ RQë¥¼ ë˜ì§</p> <ol> <li> <p>ì¢‹ì€ ë©”ëª¨ë¦¬ êµ¬ì¡°ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>ì ì ˆí•œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì´ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>ì¢‹ì€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>(ê¸°ì–µì´ ë‹¨ì¼ê³¼ì •ì´ ì•„ë‹ˆë©° ë‹¨ì¼ ê¸°ëŠ¥ë„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ê°ê° ë‹¤ë¥¸ ì‹ ê²½ êµ¬ì¡°ë¡œ ì„œë¡œ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ë©° ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ) ì„œë¡œ ë‹¤ë¥¸ ìƒí˜¸ ì—°ê²°ëœ ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•©í•˜ëŠ” íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>(ë°ì´í„°ë¥¼ linear mannerë¡œ ë²¡í„°ë‚˜ í–‰ë ¬ì— ì €ì¥í•œë‹¤ëŠ” ê°€ì •ì€ oversimplificationì¼ ìˆ˜ë„ ìˆë‹¤) long-term memoryë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥/ê¸°ì–µí•˜ë ¤ë©´ deep memory moduleì´ í•„ìš”í•œê°€?</p> </li> </ol> </li> </ul> <p>â‡’ ìœ„ì˜ ë¬¼ìŒì— ëŒ€í•œ í•´ë‹µì„ ì œì‹œí•˜ë©´ì„œ test timeì— memorizeê°€ ê°€ëŠ¥í•œ architecture ì œê³µ</p> <h2 id="2-preliminaries">2. Preliminaries</h2> <h3 id="notations">Notations</h3> <ul> <li> <p>Input: x âˆˆ â„^{(NÃ—d_m)}</p> </li> <li> <p>Neural Network Module: \mathcal{M}</p> </li> <li> <p>Attention Mask: M</p> </li> <li> <p>Segment</p> <ul> <li> <p>ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸: S^{(i)}</p> </li> <li> <p>ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ì˜ jë²ˆì§¸ í† í°, ë²¡í„°, hidden state: S_j^{(i)}</p> </li> </ul> </li> <li> <p>Neural Network</p> <ul> <li> <p>forward pass with weight adjustment: \mathcal{N}(x)</p> </li> <li> <p>forward pass without weight adjustment: \mathcal{N}^{*}(x)</p> </li> <li> <p>forward pass in k-th layer: \mathcal{N}^{(k)}</p> </li> </ul> </li> </ul> <h3 id="backgrounds">Backgrounds</h3> <ul> <li> <p>Transformers</p> <ul> <li> <p>y<em>i = \sum</em>{j=1}^i \frac{\exp(Q<em>i^T K_j/\sqrt{d_m}) V_j}{\sum</em>{l=1}^i \exp(Q_i^T K_l/\sqrt{d_m})}</p> </li> <li> <p>ğ‘ Ã— ğ‘‘ operationì´ í•„ìš”í•¨ â†’ ê¸´ ë©”ëª¨ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” larger memory consumption and lower-throughput</p> </li> </ul> </li> <li> <p>Efficient Attentions (linear attentions)</p> <ul> <li> <p>kernel function: \phi(x,y) = \phi(x)\phi(y)</p> </li> <li> <p>attention: y<em>i = \sum</em>{j=1}^i \frac{\phi(Q<em>i^T K_j)}{\sum</em>{l=1}^i \phi(Q<em>i^T K_l)} V_j = \sum</em>{j=1}^i \frac{\phi(Q<em>i)^T \phi(K_j)}{\sum</em>{l=1}^i \phi(Q<em>i)^T \phi(K_l)} V_j = \frac{\phi(Q_i)^T \sum</em>{j=1}^i \phi(K<em>j)V_j}{\phi(Q_i)^T \sum</em>{l=1}^i \phi(K_l)}</p> </li> <li> <p>kernel ì´ identity functionì´ë©´ ë‹¤ìŒê³¼ ê°™ì´ recurrent formatì„ ê°–ëŠ” transformerë¡œ ì „ê°œ</p> </li> </ul> </li> </ul> <p>M<em>t = M</em>{t-1} + K_t^T V_t</p> <p>y_t = Q_t M_t</p> <ul> <li> <p>RNN</p> <ul> <li> <p>hidden state = memory units</p> </li> <li> <p>recurrent processë¥¼ memory ê´€ì ì—ì„œ read/writeë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ</p> <ul> <li> <p>read(input â†’ hidden) : y_t = g(\mathcal{M}_t, x_t) \quad \text{Read Operation}</p> </li> <li> <p>write(hidden â†’ output): \mathcal{M}<em>t = f(\mathcal{M}</em>{t-1}, x_t) \quad \text{Write Operation}</p> </li> </ul> </li> </ul> </li> </ul> <p>â‡’ ì´ ê´€ì ì—ì„œ ë³´ë©´ <strong>Equation</strong>ì€ matrix-valued memoryì— keyì™€ valueë¥¼ ê³„ì†í•´ì„œ writeí•˜ëŠ” ê³¼ì •ì´ë¼ ë³¼ ìˆ˜ ìˆìŒ</p> <blockquote> <p>ê²°êµ­ sequenceê°€ ê¸¸ì–´ì§ì— ë”°ë¼ ëª¨ë¸ì´ forwarding í•˜ë©´ì„œ í’€ì–´ì•¼ í•˜ëŠ” ë¬¸ì œëŠ” 2ê°œë¡œ ì¢í˜€ì§ (memory moduleì„ ì˜ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ê±´ ì—¬ê¸°ì—ì„  ë‹¹ì—°í•œ ë¬¸ì œ)</p> </blockquote> <ol> <li> <p>forget mechanismì„ ì˜ ì¶”ê°€í•´ memory ì ì¬ë¥¼ ì¤„ì´ëŠëƒ? (xLSTM, Mamaba2)</p> </li> <li> <p>write operationë¥¼ improvingì‹œí‚¤ëƒ? (ë­ ë…¼ë¬¸ ì„¤ëª…ì„ ë³´ë©´ ì˜ ì§€ìš°ë©´ì„œ writeì‹œí‚¤ëƒ, ë³‘ë ¬ì²˜ë¦¬í•™ìŠµì´ ê°€ëŠ¥í•˜ëƒë¡œ ì„¤ëª…í•¨)</p> </li> </ol> <h2 id="3-learning-to-memorize-at-test-time">3. Learning to Memorize at Test Time</h2> <blockquote> <p><strong>ë…¼ë¬¸ì´ ì œì•ˆí•˜ê³ ì í•˜ëŠ”ê²ƒì€ â€˜inference timeâ€™ë•Œ long-term memoryë¥¼ ì˜ í™œìš©í•˜ëŠ” meta memory model â†’ neural network (e.g., LM)ì´ sequenceë¥¼ ì²˜ë¦¬í•  ë•Œ ì´ë¥¼ ì ì ˆíˆ â€˜ì €ì¥í•  í•¨ìˆ˜â€™ë¥¼ íŒŒë¼ë¯¸í„°ë¡œì¨ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ</strong></p> </blockquote> <h3 id="31-long-term-memory">3.1 Long-term Memory</h3> <p>â†’ memorizationì´ ê°€ëŠ¥í•œ learning function, ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ ë°ì´í„°ë¥¼ ëª¨ë“ˆì´ ì–´ë–»ê²Œ ì €ì¥í•˜ëŠ”ì§€ì— ëŒ€í•œ ë°©ë²•ì„ í•™ìŠµ</p> <ul> <li> <p><strong>Learning Process and Surprise Metric.</strong></p> <ul> <li>Online learningì„ ì°¨ìš©í•œ í›„ ë„ˆë¬´ë‚˜ ì§ê´€ì ì¸ ë°©ë²•ì„ í™œìš©í•´ current sequence input x_tì´ ê·¸ë™ì•ˆì˜ Memory Moduleì´ ì €ì¥í•´ì˜¨ dataì˜ patternê³¼ ë‹¤ë¥´ë©´ Memory Moduleì„ updateí•˜ëŠ” ì‹ìœ¼ë¡œ í•™ìŠµ</li> </ul> </li> </ul> <p>â†’ \nabla \ell(\mathcal{M}_{t-1}; x_t)ì„ <code class="language-plaintext highlighter-rouge">surprise</code> ë¡œ ì •ì˜í•˜ëŠ”ë° ì‚¬ì‹¤ìƒ past sequenceë‘ ë§ì´ ë‹¤ë¥´ë©´ Memory Moduleì„ ë§ì´ ì—…ë°ì´íŠ¸ í•˜ê² ë‹¤.ë¼ëŠ” ì „í˜•ì ì¸ DL ì—…ë°ì´íŠ¸</p> <p>â†’ gradient descent with momentumì˜ í˜•ì‹ê³¼ ë˜‘ê°™ì´ <code class="language-plaintext highlighter-rouge">surprise</code> S_të¥¼ ì •í•¨</p> <ul> <li> <p>\eta<em>{t}ì™€ \theta</em>{t}ê°€ ëª¨ë‘ function of input x_t</p> </li> <li> <p>data-dependent mannerë¡œ memory moduleì„ updateí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—</p> </li> </ul> <p>(e.g., ëª¨ë“  í† í°ì´ ê´€ë ¨ì„±ì´ ìˆê³  ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ì— ìˆì„ ê²½ìš°, recent past tokens ëŒ€ë¹„ input x<em>tê°€ \eta</em>{t} \rightarrow 1ë¡œ í•´ì•¼ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµì´ ë¨)</p> <ul> <li><strong>Objective.</strong></li> </ul> <p>â†’ past dataë¥¼ keyì™€ valueì˜ pairë¡œ ì €ì¥í•˜ëŠ” ì´ì „ modelë“¤ì˜ ê´€ì ì„ ë”°ë¼ ì•„ë˜ì˜ lossë¡œ memory moduleì„ í•™ìŠµ</p> <ul> <li> <p>input x_të¥¼ ì‚¬ì˜ì‹œí‚¨ ì´í›„ memory moduleì´ key â†” valueì˜ ê´€ê³„ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹</p> </li> <li> <p>ì´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê´€ì ì—ì„œ W_K, W_VëŠ” hyperparameter</p> </li> <li> <p><strong>Forgetting Mechanism.</strong></p> </li> </ul> <p>â†’ GRU, LSTMê°™ì´ forgetting ë¶€ë¶„ ë„ì…. í•´ë‹¹ weight decay/forgetting ë¶€ë¶„ë„ input x_tì— ì˜í•´ì„œ í•™ìŠµë˜ë„ë¡ ì„¤ê³„</p> <ul> <li> <p><strong>Memory Architecture.</strong></p> <ul> <li> <p>vector-valuedë‚˜ matrix-valuedë¥¼ í™œìš©í•´ memory moduleì„ ì„¤ê³„í•  ê²½ìš° â†’ \mathcal{M}_t =w_t</p> <ul> <li>ì´ ê²½ìš° ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ëª©í‘œë¥¼ í‘¸ëŠ”ê²Œ ë˜ê³ , ìµœì ì˜ ê°’ì€ ë°ì´í„°ì˜ ì¢…ì†ì„±ì´ ì„ í˜•ì´ë¼ëŠ” ê°€ì •ì„ í•´ì•¼í•¨</li> </ul> </li> <li> <p>ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ í¸ì˜ë¥¼ ìœ„í•´ í¸ì˜ìƒ í‘œê¸°ë¥¼ â€˜\mathcal{M}_t =w_tâ€™ë¡œ í•˜ì§€ë§Œ expressive powerë¥¼ ìœ„í•´ 2 layer MLPë¥¼ ì¼ë‹¤ê³  í•¨</p> </li> </ul> </li> <li> <p><strong>Retrieving a Memory.</strong></p> <ul> <li>ìœ„ì—ì„œ ì„¤ê³„í•œê±´ memory moduleì´ê¸°ì— informationì„ retrieveí•´ì„œ current sequenceí•´ concatí•´ processingì„ í•´ì•¼ ë¹„ë¡œì†Œ ì“¸ëª¨ê°€ ì™„ì„±ì´ ë¨</li> </ul> </li> </ul> <h3 id="32-how-to-parallelize-the-long-term-memory-training">3.2 How to Parallelize the Long-term Memory Training</h3> <p>â†’ long-term memory module í•™ìŠµì‹œì— ê¸´ sequenceë¥¼ parallelí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ì—¬ì¤€ ë¶€ë¶„</p> <ul> <li> <p>\mathcal{M}_0ì—ì„œ í•™ìŠµì‹œì‘</p> </li> <li> <p>tâ€™: 0</p> </li> <li> <p>t: b</p> </li> <li> <p>\beta<em>i = \prod</em>{j=1}^i(1-\alpha_j)</p> </li> </ul> <p>â†’ê° ì²­í¬(rank)ì— ê´€ë ¨ëœ í–‰ë ¬ì„ ì €ì¥í•¨ìœ¼ë¡œ ë¶„ì‚°í•™ìŠµ ê°€ëŠ¥</p> <p>â†’ ê° chunkì— ëŒ€í•œ u_të¥¼ êµ¬í•´ë†“ê³  recurrentí•˜ê²Œ <code class="language-plaintext highlighter-rouge">surprise</code> valueê°’ êµ¬í•˜ê¸° ê°€ëŠ¥</p> <h3 id="33-persistent-memory">3.3 Persistent Memory</h3> <p>â†’ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ input-independentí•œ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ task-related memoryë¡œ í™œìš©í•˜ê³ ì í•¨</p> <p>(ì—¬ê¸°ì„œë¶€í„° 2016-2019 ëª¨ë¸ë§ ì—°êµ¬ ëŠë‚Œ ë„ˆë¬´ ê°•í•¨;;;)</p> <ul> <li> <p>prefix/prompt tuningì²˜ëŸ¼ sequenceì•ì— task-specific learnable (inferenceì—ì„œëŠ” fixì¸) parameterë¥¼ ë„ì…</p> </li> <li> <p>ê·¸ëŸ¼ ì €ìë“¤ì€ ì´ moduleì„ ì™œ ë„ì…í–ˆëƒ?</p> <ol> <li> <p>memorization of the task knowledge (prefix/prompt tuningë‘ ë˜‘ê°™ìŒ)</p> </li> <li> <p>ì´ parameterë„ ê²°êµ­ attentionì˜ ëŒ€ìƒì´ ë˜ëŠ”ë°, input-independent attention weightsì´ í•„ìš”í•´ì„œ</p> </li> <li> <p>attention mapì„ ì‚´í´ë³´ë©´ initial biasê°€ ìˆëŠ”ë° input-independent parameterê°€ attention distribution redistributingí•´ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ (ê·¼ë° ë³´í†µ special tokenì— skewedê±¸ë¦¬ì§€ ì•Šë‚˜?)</p> </li> </ol> </li> </ul> <h2 id="4-how-to-incorporate-memory">4 How to Incorporate Memory?</h2> <blockquote> <p><strong>ìœ„ì—ì„œ ì†Œê°œí•œ â€˜neural memoryâ€™ë¥¼ neural networkì— incorporateí•˜ëŠ” 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•¨ â†’ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ neural networkë¥¼ short-term memory modulesë¼ê³  í‘œí˜„í•˜ë©´ì„œ, íŠ¹íˆ transformerëŠ”, key valueë¥¼ ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— long contextì—ì„œëŠ” í•œê³„ê°€ ìˆë‹¤ê³  ì–¸ê¸‰ â†’ í•˜ì§€ë§Œ ì œì•ˆí•˜ëŠ” memory moduleì„ memoryì— read/writeí•˜ë©´ì„œ current key/value representationì„ ê°•í™”</strong></p> </blockquote> <p>â†’ ì•„ë˜ ëª¨ë“  frameworkì—ì„œ coreë¥¼ neural network/lmì •ë„ë¡œ ìƒê°í•˜ê³  ë”°ë¼ê°€ë©´ ëœë‹¤.</p> <p>â†’ ë˜í•œ ì•„ë˜ì˜ ëª¨ë“  frameworkê°€ test timeì— ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë”°ë¼ê°€ì.</p> <h3 id="41-memory-as-a-context-mac">4.1 Memory as a Context (MAC)</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ S^{(i)} \ (i = 1,\ldots,N/C) : sequenceë¥¼ ê³ ì • í¬ê¸° ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ</p> <ol> <li> <p>h<em>t = \mathcal{M}</em>{t-1}^*(\mathbf{q}_t) : memory moduleì—ì„œ ê³ ì • ì„¸ê·¸ë¨¼íŠ¸ì™€ ìœ ì‚¬í•œ past information retrieve</p> </li> <li> <table> <tbody> <tr> <td>\tilde{S}^{(t)} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>h_t</td> <td>S^{(t)} \ y_t = \text{Attn}(\tilde{S}^{(t)}) : persistent memory, past information, ê³ ì • segmentë¥¼ neural networkì— forwardingí•´ì„œ attention</td> </tr> </tbody> </table> </li> </ol> <p>â†’ attentionì€ standard attentionê³¼ ë™ì¼í•¨</p> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ persistent memory, past informationê°€ ì¶”ê°€ë¨ì— ë”°ë¼ attention windowë§Œ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ</p> <ol> <li> <p>\mathcal{M}<em>t = \mathcal{M}</em>{t-1}(y_t) : attention outputì„ í™œìš©í•´ long-term memory moduleì„ update</p> </li> <li> <p>o_t = y_t \otimes \mathcal{M}_t^*(y_t) : updateë˜ memory moduleì— attention outputì„ í†µê³¼í•œ í›„ ì´ë¥¼ ê¸°ì¡´ attention outputê³¼ tensorê³± ì—°ì‚° í•´ ìµœì¢… output ê³„ì‚°</p> </li> </ol> <p>â‡’ í•´ë‹¹ êµ¬ì¡°ì˜ ê°€ì¥ í° ì¥ì ì€ attentionì´ current/longtermì— ë™ì‹œì— attentionì„ ì£¼ê¸° ë•Œë¬¸ì— ì–´ë–¤ ì •ë³´ê°€ ìœ ìš©í•œì§€ íŒŒì•… í›„ ë©”ëª¨ë¦¬ ìš©ëŸ‰ì„ ê´€ë¦¬í•˜ê¸°ì— ìš©ì´í•˜ë‹¤ëŠ” ê²ƒ</p> <h3 id="42-gated-memory-mag">4.2 Gated Memory (MAG)</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ ì´ì „ì²˜ëŸ¼ memory moduleì´ current inputì— ì˜í•´ updateë˜ê¸´ í•˜ì§€ë§Œ attentionì— í™œìš©ë˜ì§€ëŠ” ì•ŠìŒ</p> <ol> <li> <table> <tbody> <tr> <td>\tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>x</td> </tr> </tbody> </table> </li> <li>y = \text{SW-Attn}^*(\tilde{x}) : sliding window attentionìœ¼ë¡œ attention ì²˜ë¦¬</li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>o = y \otimes \mathcal{M}(\tilde{x})</li> </ol> <p>â†’ y \ ì™€ \mathcal{M}(\tilde{x})ì˜ outputì„ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„° ê°’ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”í•œ í›„ \sigma(\cdot) ì ìš© í›„ ë¹„ì„ í˜• ê²Œì´íŒ…ì„ í†µí•´ ìµœì¢… outputì„ ë§Œë“¦</p> <p>â†’ (ê·¸ë¦¬ê³  ì´ê²Œ ì•„ë§ˆ ëª‡ê°œì˜ layerë¡œ ìŒ“ì¼ ê²ƒì´ê¸° ë•Œë¬¸ì—) ê·¸ë¦¼ì—ì„œ long-term memoryëŠ” fadingì˜ í˜•íƒœë¡œ attention mapì— ë‚˜íƒ€ë‚¨. ë°˜ë©´, ë‹¤ë¥¸ memory moduleì€ preciseí•˜ê²Œ attentionì´ ë“¤ì–´ê°.</p> <h3 id="43-memory-as-a-layer-mal">4.3 Memory as a Layer (MAL)</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ attentionê³¼ neural memory moduleì´ ë³¸ì¸ë“¤ì´ ì„¤ê³„í•œ ì˜ë„ë¥¼ 100% í™œìš©í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°.</p> <ol> <li> <table> <tbody> <tr> <td>\tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>x</td> </tr> </tbody> </table> </li> <li> <p>y = \mathcal{M}(\tilde{x})</p> </li> <li>o = \text{SW-Attn}(y)</li> </ol> <p>â†’ attentionì˜ ì¥ì ì„ í™œìš©ëª»í•˜ë‹ˆ attention ë¶€ë¶„ì„ neural memory moduleë¡œ ë°”ê¾¼ LMMë¡œ ë˜ ë‹¤ë¥¸ ì‹¤í—˜ì„ í•´ë´¤ë‹¤ê³  í•¨.</p> <h2 id="5-experiments">5. Experiments</h2> <h3 id="51-experimental-setup">5.1 Experimental Setup</h3> <ul> <li> <p>Models</p> <ul> <li> <p>scale: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. (memory module + coreë¥¼ ì˜ë¯¸í•˜ëŠ” ë“¯, MAC, MAG, MALì˜ í¬ê¸°ê°€ ìœ„ì™€ ê°™ë‹¤ê³  í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì•„)</p> </li> <li> <p>training dataset: FineWeb-Edu dataset (i)~(iii)- 15B / (iv) - 30B</p> </li> </ul> </li> <li> <p>Training</p> <ul> <li> <p>max_lengths: 4K tokens.</p> </li> <li> <p>batch_size: 0.5M tokens</p> </li> </ul> </li> </ul> <h3 id="52-results---language-modeling">5.2 Results - Language Modeling</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ attentionì´ ë“¤ì–´ê°„ ëª¨ë¸: hybrid model â†’ *í‘œê¸°</p> <p>â†’ attentionì„ ì•ˆì¼ëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <strong>model</strong></p> <p>â†’ attentionì„ í™œìš©í–ˆëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <strong>model</strong></p> <ul> <li> <p>Titanì´ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ë‹¤.</p> </li> <li> <p>Mamba, Mamba2, and Gated DeltaNetë„ gating mechanismì„ ì“°ì§€ë§Œ ë³¸ì¸ë“¤ì˜ neural &amp; deep memoryê°€ ë” íš¨ìš©ì´ ë†’ë‹¤ê³  í•˜ëŠ”ë° attention ë•Œë¬¸ì— ì˜ë‚˜ì˜¨ê²Œ ì•„ë‹Œê°€?ë¼ëŠ” ë“¦.</p> </li> </ul> <p>â†’ ê·¸ë˜ì„œ Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention)ë³´ë‹¤ë„ ì„±ëŠ¥ì´ ì¢‹ê¸° ë•Œë¬¸ì— ë³¸ì¸ë“¤ì´ powerful neural memory moduleë¥¼ ì˜ êµ¬ì¶•í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  ì£¼ì¥</p> <ul> <li>êµ¬ì¡°ìƒ ë©”ëª¨ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ attentioní•˜ê³  updateí•˜ëŠ” MACì´ long sequence dataì— ëŒ€í•œ dependencyê°€ ê°•í•˜ë‹¤ê³  í•¨</li> </ul> <h3 id="52-results---needle-in-a-haystack">5.2 Results - Needle in a Haystack</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>TTTì— ë¹„í•´ì„œëŠ” momentumê³¼ forgetting mechanismì´ ìˆì–´ì„œ ìœ ì—°í•œ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥</p> </li> <li> <p>forgetting mechanismê°€ ìˆëŠ” Mamba2ì— ë¹„í•´ì„œëŠ” deep non-linear êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë³´ë‹¤ ë” íš¨ìš©ì„± ë†’ì€ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥</p> </li> </ul> <p>â‡’ ê°€ì¥ long context handling ëŠ¥ë ¥ì„ ë§ì´ ìš”êµ¬í•˜ëŠ” S-HIAH taskì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p> <h3 id="52-results---the-effect-of-deep-memory">5.2 Results - The Effect of Deep Memory</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ memory moduleë¡œë§Œ êµ¬ì¡°ë¥¼ ì§œë„ Mambaë³´ë‹¤ long context ëŒ€í•œ pplì´ ë–¨ì–´ì§</p> <p>â†’ memory module depthë§Œ ì˜¬ë ¤ë„ pplì´ ë–¨ì–´ì§€ë©°, ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í‚¤ìš¸ìˆ˜ë¡ ê¸´ ê¸¸ì´ì— ëŒ€í•œ pplì´ ëœì–´ì§</p> <h3 id="52-results---time-series--dna-modeling">5.2 Results - Time Series &amp; DNA Modeling</h3> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-02-04-titans-learning-to-memorize-at-test-time/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>â†’ (ìœ„) Mamba moduleë¥¼ neural memoryë¡œ ëŒ€ì²´í–ˆë”ë‹ˆ ì˜ ë‚˜ì˜¤ë”ë¼</p> <p>â†’ (ì•„ë˜) DNA modeling taskì—ì„œë„ ì„±ëŠ¥ ì˜ ë‚˜ì˜¤ë”ë¼</p> <h2 id="5-conclusion">5. Conclusion</h2> <ul> <li> <p>test timeë•Œ memory moduleì„ read/writeí•˜ëŠ” meta in-context learnerë¥¼ ë§Œë“¤ê³ ì í–ˆë˜ê±° ê°™ìŒ</p> </li> <li> <p>attentionì„ í†µí•´ ëª¨ë“  knowledgeë¥¼ ê¸°ì–µí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ pre-trainingë•Œ ì¼ë¶€ knowledgeëŠ” í•™ìŠµì‹œì¼œë†“ê³ , inference/forwardingë˜ë©´ì„œ í•„ìš”í•œ memoryë¥¼ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ê³ ì í•˜ëŠ”ê²Œ ì´ ë…¼ë¬¸ì˜ ìµœì¢… ëª©í‘œê°€ ì•„ë‹ˆì—ˆì„ê¹Œ?ë¼ëŠ” ìƒê°ì´ ë“¦.</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>