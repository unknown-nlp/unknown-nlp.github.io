<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Titans: Learning to Memorize at Test Time | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content=" ë…¼ë¬¸ ë¦¬ë·° - Titans: Learning to Memorize at Test Time"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/al-folio/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alshedivat.github.io/al-folio/blog/2025/titans-learning-to-memorize-at-test-time/"> <script src="/al-folio/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/al-folio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/al-folio/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/al-folio/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Titans: Learning to Memorize at Test Time</h1> <p class="post-meta"> Created on February 04, 2025 </p> <p class="post-tags"> <a href="/al-folio/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/al-folio/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  Â· Â  <a href="/al-folio/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2025-02-04</li> <li> <strong>Reviewer</strong>: ì¤€ì› ì¥</li> </ul> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>Transformer</p> <p>â‡’ (1) key-value associationsì„ ì €ì¥ (2) queryë¥¼ í†µí•´ retrieveí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµ</p> <p>â‡’ current context windowì— ì§ì ‘ì ì¸ dependenciesê°€ í˜•ì„±ë  ìˆ˜ ë°–ì— ì—†ìŒ</p> </li> <li> <p>Overcome the scalability issue of Transformers</p> <p>â‡’ linear transformer: softmaxëŒ€ì‹  kernel trickìœ¼ë¡œ attention ê³„ì‚°</p> <p>â‡’ dataê°€ matrix-valued statesë¡œ mapping/compressedì´ ë˜ê¸° ë•Œë¬¸ì— very long contextì—ì„œ íš¨ìš©X</p> </li> <li> <p>Limitation of recurrent neural network</p> <ol> <li> <p>ë‹¨ê¸° ê¸°ì–µ, ì¥ê¸° ê¸°ì–µ, ë©”íƒ€ ê¸°ì–µ, í˜„ì¬ ë§¥ë½ì— ëŒ€í•œ attentionì„ ëª¨ë‘ ì ì ˆí•˜ê²Œ êµ¬í˜„í•œ architectureì˜ ë¶€ì¬</p> </li> <li> <p>êµ¬ì„± ìš”ì†Œê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆëŠ” ìƒí˜¸ ì—°ê²°ëœ ì‹œìŠ¤í…œì˜ ë¶€ì¬</p> </li> <li> <p>(LSTM, GRUê°€ ì–´ëŠì •ë„ëŠ” í•˜ì§€ë§Œ) ì—¬ì „íˆ ë°ì´í„°ë¥¼ í†µí•´ ì¶”ìƒí™”ëœ ê³¼ê±° ì—­ì‚¬ë¥¼ ì•”ê¸°í•˜ëŠ” ëŠ¥ë ¥ì´ ê²°ì—¬</p> </li> </ol> </li> </ul> <p><br></p> <ul> <li> <p>Memory Perspective</p> <p>â‡’ ë…¼ë¬¸ì—ì„œëŠ” Memory ê´€ì ì—ì„œ ê¸°ì¡´ì˜ Modelì„ ë…ìì—ê²Œ ì´í•´ì‹œí‚¤ê³ ì í•¨</p> <ul> <li><span style="color:blue_background"><strong>memory: inputì— ì˜í•´ì„œ ë°œìƒí•˜ëŠ” neural update</strong></span></li> </ul> <ol> <li> <p>RNN</p> <ol> <li> <p>$ ğ‘“ (M_{ğ‘¡ âˆ’1}, ğ‘¥_ğ‘¡ ) $</p> </li> <li> <p>$ ğ‘”(M_ğ‘¡, ğ‘¥_ğ‘¡ ) $</p> </li> </ol> <p>â†’ të²ˆì§¸ ì…ë ¥ì— ì˜í•´ì„œ â€˜vector-valued memory moduleâ€™ $ M $ì´ ì—…ë°ì´íŠ¸ ë˜ê³ , retreiving ë˜ëŠ” ê²ƒì˜ ë°˜ë³µ</p> </li> <li> <p>Transformer</p> <p>â†’ RNNê³¼ ë‹¬ë¦¬ past key, valueë¥¼ ê³„ì† appendingí•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸</p> <p>â†’ matrix-valued memory Module</p> </li> </ol> <p><br></p> </li> <li> <p>ìœ„ì˜ ë…¼ì˜ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¬¸ì—ì„œëŠ” 5ê°œ RQë¥¼ ë˜ì§</p> <ol> <li> <p>ì¢‹ì€ ë©”ëª¨ë¦¬ êµ¬ì¡°ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>ì ì ˆí•œ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜ì´ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>ì¢‹ì€ ë©”ëª¨ë¦¬ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ë€ ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>(ê¸°ì–µì´ ë‹¨ì¼ê³¼ì •ì´ ì•„ë‹ˆë©° ë‹¨ì¼ ê¸°ëŠ¥ë„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ê°ê° ë‹¤ë¥¸ ì‹ ê²½ êµ¬ì¡°ë¡œ ì„œë¡œ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ë©° ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•œë‹¤ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ) ì„œë¡œ ë‹¤ë¥¸ ìƒí˜¸ ì—°ê²°ëœ ë©”ëª¨ë¦¬ ëª¨ë“ˆì„ í†µí•©í•˜ëŠ” íš¨ìœ¨ì ì¸ ì•„í‚¤í…ì²˜ëŠ” ë¬´ì—‡ì¼ê¹Œ?</p> </li> <li> <p>(ë°ì´í„°ë¥¼ linear mannerë¡œ ë²¡í„°ë‚˜ í–‰ë ¬ì— ì €ì¥í•œë‹¤ëŠ” ê°€ì •ì€ oversimplificationì¼ ìˆ˜ë„ ìˆë‹¤) long-term memoryë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì €ì¥/ê¸°ì–µí•˜ë ¤ë©´ deep memory moduleì´ í•„ìš”í•œê°€?</p> </li> </ol> <p><br></p> <p>â‡’ ìœ„ì˜ ë¬¼ìŒì— ëŒ€í•œ í•´ë‹µì„ ì œì‹œí•˜ë©´ì„œ test timeì— memorizeê°€ ê°€ëŠ¥í•œ architecture ì œê³µ</p> </li> </ul> <p><br></p> <h2 id="2-preliminaries">2. Preliminaries</h2> <h3 id="notations">Notations</h3> <ul> <li> <p>Input: $ x âˆˆ â„^{(NÃ—d_m)} $</p> </li> <li> <p>Neural Network Module: $ \mathcal{M}Â  $</p> </li> <li> <p>Attention Mask: $ M $</p> </li> <li> <p>Segment</p> <ul> <li> <p>ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸: $ S^{(i)} $</p> </li> <li> <p>ië²ˆì§¸ ì„¸ê·¸ë¨¼íŠ¸ì˜ jë²ˆì§¸ í† í°, ë²¡í„°, hidden state: $ S_j^{(i)} $</p> </li> </ul> </li> <li> <p>Neural Network</p> <ul> <li> <p>forward pass with weight adjustment: $ \mathcal{N}(x) $</p> </li> <li> <p>forward pass without weight adjustment: $ \mathcal{N}^{*}(x) $</p> </li> <li> <p>forward pass in k-th layer: $ \mathcal{N}^{(k)} $</p> </li> </ul> </li> </ul> <p><br></p> <h3 id="backgrounds">Backgrounds</h3> <ul> <li> <p>Transformers</p> <ul> <li> <p>$ y_i = \sum_{j=1}^i \frac{\exp(Q_i^T K_j/\sqrt{d_m}) V_j}{\sum_{l=1}^i \exp(Q_i^T K_l/\sqrt{d_m})} $</p> </li> <li> <p>ğ‘ Ã— ğ‘‘ operationì´ í•„ìš”í•¨ â†’ ê¸´ ë©”ëª¨ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” larger memory consumption and lower-throughput</p> </li> </ul> </li> <li> <p>Efficient Attentions (linear attentions)</p> <ul> <li> <p>kernel function: $ \phi(x,y) = \phi(x)\phi(y) $</p> </li> <li> <p>attention: $ y_i = \sum_{j=1}^i \frac{\phi(Q_i^T K_j)}{\sum_{l=1}^i \phi(Q_i^T K_l)} V_j = \sum_{j=1}^i \frac{\phi(Q_i)^T \phi(K_j)}{\sum_{l=1}^i \phi(Q_i)^T \phi(K_l)} V_j = \frac{\phi(Q_i)^T \sum_{j=1}^i \phi(K_j)V_j}{\phi(Q_i)^T \sum_{l=1}^i \phi(K_l)} $</p> </li> <li> <p>kernel ì´ identity functionì´ë©´ ë‹¤ìŒê³¼ ê°™ì´ recurrent formatì„ ê°–ëŠ” transformerë¡œ ì „ê°œ</p> <p>$ M_t = M_{t-1} + K_t^T V_t $</p> <p>$ y_t = Q_t M_t $</p> </li> </ul> </li> <li> <p>RNN</p> <ul> <li> <p>hidden state = memory units</p> </li> <li> <p>recurrent processë¥¼ memory ê´€ì ì—ì„œ read/writeë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ</p> <ul> <li> <p>read(input â†’ hidden) : $ y_t = g(\mathcal{M}_t, x_t) \quad \text{Read Operation} $</p> </li> <li> <p>write(hidden â†’ output): $ \mathcal{M}<em>t = f(\mathcal{M}</em>{t-1}, x_t) \quad \text{Write Operation} $</p> </li> </ul> </li> </ul> <p>â‡’ ì´ ê´€ì ì—ì„œ ë³´ë©´ <strong><a href="/17feef51b0f08099a4a4f96f0b89ea72#17feef51b0f080019eccc61bc5940ee7">Equation</a></strong>ì€ matrix-valued memoryì— keyì™€ valueë¥¼ ê³„ì†í•´ì„œ writeí•˜ëŠ” ê³¼ì •ì´ë¼ ë³¼ ìˆ˜ ìˆìŒ</p> <p><br></p> </li> </ul> <blockquote> <p>ê²°êµ­ sequenceê°€ ê¸¸ì–´ì§ì— ë”°ë¼ ëª¨ë¸ì´ forwarding í•˜ë©´ì„œ í’€ì–´ì•¼ í•˜ëŠ” ë¬¸ì œëŠ” 2ê°œë¡œ ì¢í˜€ì§ (memory moduleì„ ì˜ ì¶”ê°€í•´ì•¼ í•˜ëŠ”ê±´ ì—¬ê¸°ì—ì„  ë‹¹ì—°í•œ ë¬¸ì œ)</p> </blockquote> <ol> <li> <p>forget mechanismì„ ì˜ ì¶”ê°€í•´ memory ì ì¬ë¥¼ ì¤„ì´ëŠëƒ? (xLSTM, Mamaba2)</p> </li> <li> <p>write operationë¥¼ improvingì‹œí‚¤ëƒ? (ë­ ë…¼ë¬¸ ì„¤ëª…ì„ ë³´ë©´ ì˜ ì§€ìš°ë©´ì„œ writeì‹œí‚¤ëƒ, ë³‘ë ¬ì²˜ë¦¬í•™ìŠµì´ ê°€ëŠ¥í•˜ëƒë¡œ ì„¤ëª…í•¨)</p> </li> </ol> <p><br></p> <h2 id="3-learning-to-memorize-at-test-time">3. Learning to Memorize at Test Time</h2> <p>ğŸŒ <strong>ë…¼ë¬¸ì´ ì œì•ˆí•˜ê³ ì í•˜ëŠ”ê²ƒì€ â€˜inference timeâ€™ë•Œ long-term memoryë¥¼ ì˜ í™œìš©í•˜ëŠ” meta memory model â†’ neural network (e.g., LM)ì´ sequenceë¥¼ ì²˜ë¦¬í•  ë•Œ ì´ë¥¼ ì ì ˆíˆ â€˜ì €ì¥í•  í•¨ìˆ˜â€™ë¥¼ íŒŒë¼ë¯¸í„°ë¡œì¨ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ</strong></p> <p><br></p> <h3 id="31-long-term-memory">3.1 Long-term Memory</h3> <p>â†’ memorizationì´ ê°€ëŠ¥í•œ learning function, ë°ì´í„°ê°€ ë“¤ì–´ì˜¤ë©´ í•´ë‹¹ ë°ì´í„°ë¥¼ ëª¨ë“ˆì´ ì–´ë–»ê²Œ ì €ì¥í•˜ëŠ”ì§€ì— ëŒ€í•œ ë°©ë²•ì„ í•™ìŠµ</p> <p><br></p> <ul> <li> <p><strong>Learning Process and Surprise Metric.</strong></p> <ul> <li>Online learningì„ ì°¨ìš©í•œ í›„ ë„ˆë¬´ë‚˜ ì§ê´€ì ì¸ ë°©ë²•ì„ í™œìš©í•´ current sequence input $ x_t $ì´ ê·¸ë™ì•ˆì˜ Memory Moduleì´ ì €ì¥í•´ì˜¨ dataì˜ patternê³¼ ë‹¤ë¥´ë©´ Memory Moduleì„ updateí•˜ëŠ” ì‹ìœ¼ë¡œ í•™ìŠµ</li> </ul> \[\mathcal{M}_t = \mathcal{M}_{t-1} - \theta_t \nabla \ell(\mathcal{M}_{t-1}; x_t)\] <p>â†’ $ \nabla \ell(\mathcal{M}_{t-1}; x_t) $ì„ <code class="language-plaintext highlighter-rouge">surprise</code> ë¡œ ì •ì˜í•˜ëŠ”ë° ì‚¬ì‹¤ìƒ past sequenceë‘ ë§ì´ ë‹¤ë¥´ë©´ Memory Moduleì„ ë§ì´ ì—…ë°ì´íŠ¸ í•˜ê² ë‹¤.ë¼ëŠ” ì „í˜•ì ì¸ DL ì—…ë°ì´íŠ¸</p> \[\mathcal{M}_t = \mathcal{M}_{t-1} + S_t\] \[S_t = \eta_{t} \underbrace{S_{t-1}}_{\text{Past Surprise}} - \theta_t \underbrace{\nabla \ell(\mathcal{M}{t-1}; x_t)}_{\text{Momentary Surprise}}\] <p>â†’ gradient descent with momentumì˜ í˜•ì‹ê³¼ ë˜‘ê°™ì´ <code class="language-plaintext highlighter-rouge">surprise</code> $ S_t $ë¥¼ ì •í•¨</p> <ul> <li> <p>$ \eta_{t} $ì™€ $ \theta_{t} $ê°€ ëª¨ë‘ function of input $ x_t $</p> </li> <li> <p>data-dependent mannerë¡œ memory moduleì„ updateí•˜ëŠ” ë°©ë²•ì„ í•™ìŠµì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—</p> <p>(e.g., ëª¨ë“  í† í°ì´ ê´€ë ¨ì„±ì´ ìˆê³  ë™ì¼í•œ ì»¨í…ìŠ¤íŠ¸ì— ìˆì„ ê²½ìš°, recent past tokens ëŒ€ë¹„ input $ x_t $ê°€ $ \eta_{t} \rightarrow 1 $ë¡œ í•´ì•¼ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµì´ ë¨)</p> </li> </ul> <p><br></p> <p><br></p> </li> <li> <p><strong>Objective.</strong></p> <p>â†’ past dataë¥¼ keyì™€ valueì˜ pairë¡œ ì €ì¥í•˜ëŠ” ì´ì „ modelë“¤ì˜ ê´€ì ì„ ë”°ë¼ ì•„ë˜ì˜ lossë¡œ memory moduleì„ í•™ìŠµ</p> \[\mathbf{k}_t = x_t W_K, \quad \mathbf{v}_t = x_t W_V\] \[W_K, W_V \in \mathbb{R}^{d_{\text{in}} \times d_{\text{in}}}\] \[\ell(\mathcal{M}_{t-1}; x_t) = |\mathcal{M}_{t-1}(\mathbf{k}_t) - \mathbf{v}_t|_2^2\] <ul> <li> <p>input $ x_t $ë¥¼ ì‚¬ì˜ì‹œí‚¨ ì´í›„ memory moduleì´ key â†” valueì˜ ê´€ê³„ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ì‹</p> </li> <li> <p>ì´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê´€ì ì—ì„œ $ W_K, W_V $ëŠ” hyperparameter</p> </li> </ul> <p><br></p> </li> <li> <p><strong>Forgetting Mechanism.</strong></p> \[\mathcal{M}_t = (1 - \alpha_t)\mathcal{M}_{t-1} + S_t\] \[S_t = \eta_t S_{t-1} - \theta_t \nabla \ell(\mathcal{M}_{t-1}; x_t)\] <p>â†’ GRU, LSTMê°™ì´ forgetting ë¶€ë¶„ ë„ì…. í•´ë‹¹ weight decay/forgetting ë¶€ë¶„ë„ input $ x_t $ì— ì˜í•´ì„œ í•™ìŠµë˜ë„ë¡ ì„¤ê³„</p> <p><br></p> </li> <li> <p><strong>Memory Architecture.</strong></p> <ul> <li> <p>vector-valuedë‚˜ matrix-valuedë¥¼ í™œìš©í•´ memory moduleì„ ì„¤ê³„í•  ê²½ìš° â†’ $ \mathcal{M}_t =w_t $</p> <ul> <li>ì´ ê²½ìš° ì˜¨ë¼ì¸ ì„ í˜• íšŒê·€ ëª©í‘œë¥¼ í‘¸ëŠ”ê²Œ ë˜ê³ , ìµœì ì˜ ê°’ì€ ë°ì´í„°ì˜ ì¢…ì†ì„±ì´ ì„ í˜•ì´ë¼ëŠ” ê°€ì •ì„ í•´ì•¼í•¨</li> </ul> </li> <li> <p>ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ í¸ì˜ë¥¼ ìœ„í•´ í¸ì˜ìƒ í‘œê¸°ë¥¼ â€˜$ \mathcal{M}_t =w_t $â€™ë¡œ í•˜ì§€ë§Œ expressive powerë¥¼ ìœ„í•´ 2 layer MLPë¥¼ ì¼ë‹¤ê³  í•¨</p> </li> </ul> <p><br></p> </li> <li> <p><strong>Retrieving a Memory.</strong></p> <ul> <li> <p>ìœ„ì—ì„œ ì„¤ê³„í•œê±´ memory moduleì´ê¸°ì— informationì„ retrieveí•´ì„œ current sequenceí•´ concatí•´ processingì„ í•´ì•¼ ë¹„ë¡œì†Œ ì“¸ëª¨ê°€ ì™„ì„±ì´ ë¨</p> \[\mathbf{q}_t = x_t W_Q\] \[y_t = \mathcal{M}^*(\mathbf{q}_t)\] </li> </ul> </li> </ul> <p><br></p> <h3 id="32-how-to-parallelize-the-long-term-memory-training">3.2 How to Parallelize the Long-term Memory Training</h3> <p>â†’ long-term memory module í•™ìŠµì‹œì— ê¸´ sequenceë¥¼ parallelí•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ì—¬ì¤€ ë¶€ë¶„</p> \[\mathcal{M}_t = (1-\alpha_t)\mathcal{M}_{t-1} - \theta_t\nabla\ell(\mathcal{M}_{t-1}; x_t) = \beta_t\mathcal{M}_0 - \sum_{i=1}^t \theta_i \frac{\beta_t}{\beta_i}\nabla\ell(\mathcal{M}_{t'}; x_i)\] <ul> <li> <p>$ \mathcal{M}_0 $ì—ì„œ í•™ìŠµì‹œì‘</p> </li> <li> <p>$ tâ€™ $: 0</p> </li> <li> <p>$ t $: $ b $</p> </li> </ul> \[\nabla\ell(W_0; x_t) = (W_0x_t - x_t)x_t^\top \Rightarrow \sum_{i=1}^b \theta_i\frac{\beta_b}{\beta_i}\nabla\ell(W_0; x_i) = \Theta_b B_b(W_0X - X)X^\top\] <ul> <li>$ \beta_i = \prod_{j=1}^i(1-\alpha_j) $</li> </ul> <p>â†’ê° ì²­í¬(rank)ì— ê´€ë ¨ëœ í–‰ë ¬ì„ ì €ì¥í•¨ìœ¼ë¡œ ë¶„ì‚°í•™ìŠµ ê°€ëŠ¥</p> \[S_t = \eta_t S_{t-1} - \theta_t u_t\] <p>â†’ ê° chunkì— ëŒ€í•œ $ u_t $ë¥¼ êµ¬í•´ë†“ê³  recurrentí•˜ê²Œ <code class="language-plaintext highlighter-rouge">surprise</code> valueê°’ êµ¬í•˜ê¸° ê°€ëŠ¥</p> <p><br></p> <h3 id="33-persistent-memory">3.3 Persistent Memory</h3> <p>â†’ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ input-independentí•œ íŒŒë¼ë¯¸í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ task-related memoryë¡œ í™œìš©í•˜ê³ ì í•¨</p> <p>(ì—¬ê¸°ì„œë¶€í„° 2016-2019 ëª¨ë¸ë§ ì—°êµ¬ ëŠë‚Œ ë„ˆë¬´ ê°•í•¨;;;)</p> \[x_{\text{new}} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}] | x\] <ul> <li> <p>prefix/prompt tuningì²˜ëŸ¼ sequenceì•ì— task-specific learnable (inferenceì—ì„œëŠ” fixì¸) parameterë¥¼ ë„ì…</p> </li> <li> <p>ê·¸ëŸ¼ ì €ìë“¤ì€ ì´ moduleì„ ì™œ ë„ì…í–ˆëƒ?</p> <ol> <li> <p>memorization of the task knowledge (prefix/prompt tuningë‘ ë˜‘ê°™ìŒ)</p> </li> <li> <p>ì´ parameterë„ ê²°êµ­ attentionì˜ ëŒ€ìƒì´ ë˜ëŠ”ë°, input-independent attention weightsì´ í•„ìš”í•´ì„œ</p> </li> <li> <p>attention mapì„ ì‚´í´ë³´ë©´ initial biasê°€ ìˆëŠ”ë° input-independent parameterê°€ attention distribution redistributingí•´ì¤„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ (ê·¼ë° ë³´í†µ special tokenì— skewedê±¸ë¦¬ì§€ ì•Šë‚˜?)</p> </li> </ol> </li> </ul> <p><br></p> <h2 id="4-how-to-incorporate-memory">4 How to Incorporate Memory?</h2> <p>ğŸŒ <strong>ìœ„ì—ì„œ ì†Œê°œí•œ â€˜neural memoryâ€™ë¥¼ neural networkì— incorporateí•˜ëŠ” 3ê°€ì§€ ë°©ë²•ì„ ì œì‹œí•¨ â†’ ë…¼ë¬¸ì—ì„œëŠ” ê¸°ì¡´ neural networkë¥¼ short-term memory modulesë¼ê³  í‘œí˜„í•˜ë©´ì„œ, íŠ¹íˆ transformerëŠ”, key valueë¥¼ ëˆ„ì í•˜ê¸° ë•Œë¬¸ì— long contextì—ì„œëŠ” í•œê³„ê°€ ìˆë‹¤ê³  ì–¸ê¸‰ â†’ í•˜ì§€ë§Œ ì œì•ˆí•˜ëŠ” memory moduleì„ memoryì— read/writeí•˜ë©´ì„œ current key/value representationì„ ê°•í™”</strong></p> <p>â†’ ì•„ë˜ ëª¨ë“  frameworkì—ì„œ coreë¥¼ neural network/lmì •ë„ë¡œ ìƒê°í•˜ê³  ë”°ë¼ê°€ë©´ ëœë‹¤.</p> <p>â†’ ë˜í•œ ì•„ë˜ì˜ ëª¨ë“  frameworkê°€ test timeì— ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë”°ë¼ê°€ì.</p> <p><br></p> <h3 id="41-memory-as-a-context-mac">4.1 Memory as a Context (MAC)</h3> <p>â†’ $ S^{(i)} \ (i = 1,\ldots,N/C) $ : sequenceë¥¼ ê³ ì • í¬ê¸° ì„¸ê·¸ë¨¼íŠ¸ë§Œ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œ</p> <ol> <li> <p>$ h_t = \mathcal{M}_{t-1}^*(\mathbf{q}_t) $ : memory moduleì—ì„œ ê³ ì • ì„¸ê·¸ë¨¼íŠ¸ì™€ ìœ ì‚¬í•œ past information retrieve</p> </li> <li> <table> <tbody> <tr> <td>$ \tilde{S}^{(t)} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>h_t</td> <td>S^{(t)} \ y_t = \text{Attn}(\tilde{S}^{(t)}) $ : persistent memory, past information, ê³ ì • segmentë¥¼ neural networkì— forwardingí•´ì„œ attention</td> </tr> </tbody> </table> <p>â†’ attentionì€ <a href="/17feef51b0f08099a4a4f96f0b89ea72#17feef51b0f080598d4cf59d04e34647">standard attention</a>ê³¼ ë™ì¼í•¨</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> â†’ ì„¸ê·¸ë¨¼íŠ¸ë³„ë¡œ persistent memory, past informationê°€ ì¶”ê°€ë¨ì— ë”°ë¼ attention windowë§Œ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ
</code></pre></div> </div> </li> <li> <p>$ \mathcal{M}<em>t = \mathcal{M}</em>{t-1}(y_t) $ : attention outputì„ í™œìš©í•´ long-term memory moduleì„ update</p> </li> <li>$ o_t = y_t \otimes \mathcal{M}_t^*(y_t) $ : updateë˜ memory moduleì— attention outputì„ í†µê³¼í•œ í›„ ì´ë¥¼ ê¸°ì¡´ attention outputê³¼ tensorê³± ì—°ì‚° í•´ ìµœì¢… output ê³„ì‚°</li> </ol> <p><br></p> <p>â‡’ í•´ë‹¹ êµ¬ì¡°ì˜ ê°€ì¥ í° ì¥ì ì€ attentionì´ current/longtermì— ë™ì‹œì— attentionì„ ì£¼ê¸° ë•Œë¬¸ì— ì–´ë–¤ ì •ë³´ê°€ ìœ ìš©í•œì§€ íŒŒì•… í›„ ë©”ëª¨ë¦¬ ìš©ëŸ‰ì„ ê´€ë¦¬í•˜ê¸°ì— ìš©ì´í•˜ë‹¤ëŠ” ê²ƒ</p> <p><br></p> <h3 id="42-gated-memory-mag">4.2 Gated Memory (MAG)</h3> <p>â†’ ì´ì „ì²˜ëŸ¼ memory moduleì´ current inputì— ì˜í•´ updateë˜ê¸´ í•˜ì§€ë§Œ attentionì— í™œìš©ë˜ì§€ëŠ” ì•ŠìŒ</p> <ol> <li> <table> <tbody> <tr> <td>$ \tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>x $</td> </tr> </tbody> </table> </li> <li> <p>$ y = \text{SW-Attn}^*(\tilde{x}) $ : sliding window attentionìœ¼ë¡œ attention ì²˜ë¦¬</p> <ol> <li>$ o = y \otimes \mathcal{M}(\tilde{x}) $</li> </ol> <p>â†’ $ y \ ì™€ \mathcal{M}(\tilde{x}) $ì˜ outputì„ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„° ê°’ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”í•œ í›„ $ \sigma(\cdot) $ ì ìš© í›„ ë¹„ì„ í˜• ê²Œì´íŒ…ì„ í†µí•´ ìµœì¢… outputì„ ë§Œë“¦</p> <p>â†’ (ê·¸ë¦¬ê³  ì´ê²Œ ì•„ë§ˆ ëª‡ê°œì˜ layerë¡œ ìŒ“ì¼ ê²ƒì´ê¸° ë•Œë¬¸ì—) ê·¸ë¦¼ì—ì„œ long-term memoryëŠ” fadingì˜ í˜•íƒœë¡œ attention mapì— ë‚˜íƒ€ë‚¨. ë°˜ë©´, ë‹¤ë¥¸ memory moduleì€ preciseí•˜ê²Œ attentionì´ ë“¤ì–´ê°.</p> </li> </ol> <p><br></p> <h3 id="43-memory-as-a-layer-mal">4.3 Memory as a Layer (MAL)</h3> <p>â†’ attentionê³¼ neural memory moduleì´ ë³¸ì¸ë“¤ì´ ì„¤ê³„í•œ ì˜ë„ë¥¼ 100% í™œìš©í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ê³„ëœ êµ¬ì¡°.</p> <ol> <li> <table> <tbody> <tr> <td>$ \tilde{x} = [p_1 \quad p_2 \quad \cdots \quad p_{N_p}]</td> <td>x $</td> </tr> </tbody> </table> </li> <li> <p>$ y = \mathcal{M}(\tilde{x}) $</p> </li> <li>$ o = \text{SW-Attn}(y) $</li> </ol> <p><br></p> <p>â†’ attentionì˜ ì¥ì ì„ í™œìš©ëª»í•˜ë‹ˆ attention ë¶€ë¶„ì„ neural memory moduleë¡œ ë°”ê¾¼ LMMë¡œ ë˜ ë‹¤ë¥¸ ì‹¤í—˜ì„ í•´ë´¤ë‹¤ê³  í•¨.</p> <p><br></p> <h2 id="5-experiments">5. Experiments</h2> <p><br></p> <h3 id="51-experimental-setup">5.1 Experimental Setup</h3> <ul> <li> <p>Models</p> <ul> <li> <p>scale: (i) 170M, (ii) 340M, (iii) 400M, and (iv) 760M parameters. (memory module + coreë¥¼ ì˜ë¯¸í•˜ëŠ” ë“¯, MAC, MAG, MALì˜ í¬ê¸°ê°€ ìœ„ì™€ ê°™ë‹¤ê³  í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì•„)</p> </li> <li> <p>training dataset: FineWeb-Edu dataset (i)~(iii)- 15B / (iv) - 30B</p> </li> </ul> </li> <li> <p>Training</p> <ul> <li> <p>max_lengths: 4K tokens.</p> </li> <li> <p>batch_size: 0.5M tokens</p> </li> </ul> </li> </ul> <p><br></p> <h3 id="52-results---language-modeling">5.2 Results - Language Modeling</h3> <p>â†’ attentionì´ ë“¤ì–´ê°„ ëª¨ë¸: hybrid model â†’ *í‘œê¸°</p> <p>â†’ attentionì„ ì•ˆì¼ëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <span style="color:yellow_background"><strong>model</strong></span></p> <p>â†’ attentionì„ í™œìš©í–ˆëŠ”ë° ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ model â†’ <span style="color:blue_background"><strong>model</strong></span></p> <ul> <li> <p>Titanì´ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ë‹¤.</p> </li> <li> <p>Mamba, Mamba2, and Gated DeltaNetë„ gating mechanismì„ ì“°ì§€ë§Œ ë³¸ì¸ë“¤ì˜ neural &amp; deep memoryê°€ ë” íš¨ìš©ì´ ë†’ë‹¤ê³  í•˜ëŠ”ë° attention ë•Œë¬¸ì— ì˜ë‚˜ì˜¨ê²Œ ì•„ë‹Œê°€?ë¼ëŠ” ë“¦.</p> <p>â†’ ê·¸ë˜ì„œ Samba (Mamba + attention) and Gated DeltaNet-H2 (Gated DeltaNet + atttention)ë³´ë‹¤ë„ ì„±ëŠ¥ì´ ì¢‹ê¸° ë•Œë¬¸ì— ë³¸ì¸ë“¤ì´ powerful neural memory moduleë¥¼ ì˜ êµ¬ì¶•í•œ í”„ë ˆì„ì›Œí¬ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  ì£¼ì¥</p> </li> <li> <p>êµ¬ì¡°ìƒ ë©”ëª¨ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ attentioní•˜ê³  updateí•˜ëŠ” MACì´ long sequence dataì— ëŒ€í•œ dependencyê°€ ê°•í•˜ë‹¤ê³  í•¨</p> </li> </ul> <p><br></p> <h3 id="52-results---needle-in-a-haystack">5.2 Results - Needle in a Haystack</h3> <ul> <li> <p>TTTì— ë¹„í•´ì„œëŠ” momentumê³¼ forgetting mechanismì´ ìˆì–´ì„œ ìœ ì—°í•œ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥</p> </li> <li> <p>forgetting mechanismê°€ ìˆëŠ” Mamba2ì— ë¹„í•´ì„œëŠ” deep non-linear êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— ë³´ë‹¤ ë” íš¨ìš©ì„± ë†’ì€ memory module ê´€ë¦¬ê°€ ê°€ëŠ¥</p> </li> </ul> <p>â‡’ ê°€ì¥ long context handling ëŠ¥ë ¥ì„ ë§ì´ ìš”êµ¬í•˜ëŠ” S-HIAH taskì—ì„œ ì„±ëŠ¥ì´ ì¢‹ë‹¤.</p> <p><br></p> <h3 id="52-results---the-effect-of-deep-memory">5.2 Results - The Effect of Deep Memory</h3> <p>â†’ memory moduleë¡œë§Œ êµ¬ì¡°ë¥¼ ì§œë„ Mambaë³´ë‹¤ long context ëŒ€í•œ pplì´ ë–¨ì–´ì§</p> <p>â†’ memory module depthë§Œ ì˜¬ë ¤ë„ pplì´ ë–¨ì–´ì§€ë©°, ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ í‚¤ìš¸ìˆ˜ë¡ ê¸´ ê¸¸ì´ì— ëŒ€í•œ pplì´ ëœì–´ì§</p> <p><br></p> <h3 id="52-results---time-series--dna-modeling">5.2 Results - Time Series &amp; DNA Modeling</h3> <p>â†’ (ìœ„) Mamba moduleë¥¼ neural memoryë¡œ ëŒ€ì²´í–ˆë”ë‹ˆ ì˜ ë‚˜ì˜¤ë”ë¼</p> <p>â†’ (ì•„ë˜) DNA modeling taskì—ì„œë„ ì„±ëŠ¥ ì˜ ë‚˜ì˜¤ë”ë¼</p> <p><br></p> <h2 id="5-conclusion">5. Conclusion</h2> <ul> <li> <p>test timeë•Œ memory moduleì„ read/writeí•˜ëŠ” meta in-context learnerë¥¼ ë§Œë“¤ê³ ì í–ˆë˜ê±° ê°™ìŒ</p> </li> <li> <p>attentionì„ í†µí•´ ëª¨ë“  knowledgeë¥¼ ê¸°ì–µí•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ pre-trainingë•Œ ì¼ë¶€ knowledgeëŠ” í•™ìŠµì‹œì¼œë†“ê³ , inference/forwardingë˜ë©´ì„œ í•„ìš”í•œ memoryë¥¼ ê´€ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ê³ ì í•˜ëŠ”ê²Œ ì´ ë…¼ë¬¸ì˜ ìµœì¢… ëª©í‘œê°€ ì•„ë‹ˆì—ˆì„ê¹Œ?ë¼ëŠ” ìƒê°ì´ ë“¦.</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/al-folio/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/al-folio/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/al-folio/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/al-folio/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/al-folio/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/al-folio/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/al-folio/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/al-folio/assets/js/search-data.js"></script> <script src="/al-folio/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>