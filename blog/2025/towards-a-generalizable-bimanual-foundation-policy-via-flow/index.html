<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Robotics 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2025/towards-a-generalizable-bimanual-foundation-policy-via-flow/"> <meta property="og:description" content="논문 리뷰 - Robotics 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction"> <meta name="twitter:description" content="논문 리뷰 - Robotics 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2025/towards-a-generalizable-bimanual-foundation-policy-via-flow/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - Robotics 관련 연구",
        "headline": "Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/towards-a-generalizable-bimanual-foundation-policy-via-flow/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction</h1> <p class="post-meta"> Created on June 10, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a>   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusion</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> robotics</a>   <a href="/blog/tag/vision"> <i class="fa-solid fa-hashtag fa-sm"></i> vision</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-06-10</li> <li> <strong>Reviewer</strong>: 전민진</li> <li> <strong>Property</strong>: Robotics</li> </ul> <h2 id="abstract">Abstract</h2> <ul> <li> <p>Generalizable bimaual policy를 만드는 것은 여러 challenge가 존재</p> <ul> <li> <p>large action space : 같은 task여도 한 팔로 할 때 보다 양팔로 할 때 더 다양하게 수행할 수 있음</p> </li> <li> <p>the need for coordinated arm movements : 양팔의 task를 조정, 충돌을 피하도록 action이 구성되어야 함</p> </li> </ul> </li> <li> <p>최근에 pretrained VLA를 기반으로 general bimanual policy를 만드는 연구들이 제안되어 왔으나 효과적이진 않음</p> <ul> <li>bimanual data가 희소하고, 한 팔과는 근본적인 차이가 존재</li> </ul> </li> <li> <p>text-to-video model과 light diffusion policy로 구성된 프레임워크를 구축, 각각의 모델을 FT해서 사용하는 방법론을 제안</p> <ul> <li> <p>mitigate ambiguity of language in single-stage text-to-video prediction : text-to-video prediction시 중간에 flow video를 생성, 해당 영상을 기반으로 로봇 영상을 생성하도록 하여 언어의 모호성을 줄임</p> </li> <li> <p>reduce the robot-data requirement : text-to-video prediction의 경우 어느정도 pretrain되어있기 때문에 적은 bimanual dataset으로도 충분히 fitting가능</p> </li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>bimanual manipulation은 embodied agent가 양손이 모두 필요한 복잡한 task를 수행하기 위해서 중요한 분야</p> </li> <li> <p>single-arm manupulation과 달리, bimanual task의 경우 human-like coordination이 필요</p> <ul> <li> <p>양팔 움직임에 대한 action space가 상당히 커짐</p> </li> <li> <p>양팔이 충돌 되지 않고, 전체 task에 맞게 각 팔의 역할이 적절히 분배되어야 함</p> </li> </ul> </li> <li> <p>이전 bimanual policy는 다음과 같은 방법으로 학습</p> <ul> <li> <p>simulation을 사용해서 학습</p> </li> <li> <p>small scale real-world data를 활용해 FT</p> </li> <li> <p>human-objective primitives</p> </li> <li> <p>RL with policy tranfer to learn a bimanual policy</p> </li> </ul> </li> <li> <p>하지만 이전 방법론은 다음과 같은 한계가 존재</p> <ul> <li> <p>high-quality bimanual data가 희소해, generalization capabilities가 제한적</p> </li> <li> <p>sim-to-real gap 존재 + (민진 경험) simulator속도가 개느림</p> </li> </ul> </li> <li> <p>VLA을 기반으로 한 방법론은 일반화가 가능하지만..</p> <ul> <li> <p>cross-embodied data를 섞어서 하나의 모델을 학습 ⇒ RDT, OpenVLA등 대부분의 VLA모델</p> </li> <li> <p>unified action space를 정의, 한팔, 양팔 다 같이 pretraining ⇒ RDT</p> </li> <li> <p>latent action space 사용, shared codebook 형태 ⇒ GROOT N1 &amp; GO-1</p> </li> </ul> </li> </ul> <p>⇒ unified action space로 표현하기 때문에, scratch부터 학습 필요</p> <p>⇒ bimnaual의 multi-modality를 고려했을 때 data coverage가 낮음</p> <ul> <li> <p>본 논문의 저자들은 heterogenous action을 직접적으로 다루지 않고, foundation model을 써서 bimanual policy를 구축해보고자 함</p> <ul> <li>이전에 영상을 통해서 trajectory를 uniformly하게 표현할 수 있다는 연구 결과가 존재</li> </ul> </li> </ul> <p>⇒ 자연어 기반 영상 처리 framework로 bimanual foundation모델을 가능하게 해보자!</p> <p>(기존 text-to-video모델은 instruction following, generation 능력이 훌륭하다고 함 + motion semantic을 포착할 수 있고, 내재적으로 temporal dependency도 포착 가능)</p> <ul> <li> <p>본 논문에서는 CogRobot를 소개, 기존 SOTA T2V model CogVideoX를 활용해서 bimanual policy를 구축</p> <ul> <li> <p>T2V모델에 초기 이미지와 task instruction을 넣어서 로봇 움직임에 대한 영상을 생성, 해당 영상의 프레임 goal state로 주고 diffusion policy로 구체적인 action space를 생성하는 구조</p> </li> <li> <p>CogVideoX를 작은 bimanual dataset에 바로 FT할 경우 suboptimal한 결과가 나올 수 있음</p> </li> </ul> </li> </ul> <p>⇒ 이를 위해서 중간에 optical flow를 생성, 이를 기반으로 최종 영상을 생성하도록 하는 구조를 사용</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- text-to-flow : kinematic behavior과 로봇과 물체 사이의 interaction에 집중하여 optical flow를 생성

- flow-to-video : flow video,  instruction를 기반으로 detailed video를 생성
</code></pre></div></div> <p>⇒ language instruction에서의 모호함을 줄이고(?), FT에 필요한 데이터의 양을 효과적으로 줄임</p> <ul> <li> <p>본 논문의 contribution은 다음과 같음</p> <ul> <li> <p>T2V model을 활용한 bimanual foundation policy를 학습하는 프레임워크를 제안</p> </li> <li> <p>T2V시, optical flow를 concise video representation으로 활용하여 적은 데이터로 FT이 가능한 two-stage paradigm 소개</p> </li> <li> <p>high-quality bimanual manipulation dataset을 구축, simulation과 real robot에서 우리 방법론을 평가할 수 있는 dual-arm platorm을 구축</p> </li> </ul> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <ul> <li>본 논문에서는 7 DoF realman robotic arm과 external camera를 사용해 dual-arm system을 구축</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>bimanual manipulation task T를 goal-conditined Partially Observable Markoc Decision Process(POMDP)로 formulate</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>VR device를 통해서 expert data를 수집 (Open-Television을 활용)</p> </li> <li> <p>수집된 데이터셋은 episodic data를 포함</p> <ul> <li>video v, action sequence a, lanague description l</li> </ul> </li> </ul> <h2 id="proposed-methods">Proposed Methods</h2> <ul> <li> <p>CogRobot에서, instruction-conditioned bimanual policy의 학습을 두 가지 스텝으로 분해</p> <ul> <li> <p>future obsercation trajectories를 예측 : o<em>{t+1:t+N}=o</em>{t+1},…,o_{t+N}</p> <ul> <li>current observation o_t를 기반으로 specified goal l을 달성하기 위한 로봇 움직임 영상</li> </ul> </li> <li> <p>predicted observation sequence(o<em>{t+1:t+N}=o</em>{t+1},…,o_{t+N})를 기반으로 executable low-level action을 생성</p> </li> </ul> </li> <li> <p>최근 T2V model은 고품질의 realistic video를 생성하는 능력이 아주 굿 ⇒ 하지만 바로 bimanual에 FT하기엔 한계가 존재</p> <ul> <li> <p>dual-arm system은 coordination이 고려되어야 함</p> </li> <li> <p>데이터가 너무 적음 (부정확한 영상 생성 확률이 높음)</p> </li> </ul> </li> </ul> <p>⇒ T2f, f2V로 나눠서 FT</p> <ul> <li> <p>T2f: pixel-level motion encode, future optical flow 예측</p> </li> <li> <p>f2V : future flow로 video생성</p> </li> <li> <p>양팔 학습 데이터로는 RDT, ROBOMIND사용</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="text-to-flow-generation">Text-to-Flow Generation</h3> <ul> <li> <p>vanilla CogVideoX로 로봇팔 초기 이미지 넣고 생성해달라고 하면 사람 손을 생성함</p> <ul> <li>초기 이미지보다 자연어 prompt에 더 집중하는 경향 존재</li> </ul> </li> <li> <p>본 논문에서 CogVideoX 2B, 5B를 단순하게 SFT하는 버전 혹은 2 stage(T2f, f2V)로 나눠서 학습하는 버전 두 가지 모두 실험</p> </li> <li> <p>vanilla 모델을 그대로 쓰거나 단순 SFT만 할 경우엔 다음과 같은 문제 발생</p> <ul> <li> <p>physical hallucination : 로봇 팔을 사람 손으로 바꿔버림</p> </li> <li> <p>task confusion : long-horizon task일 때(A-B-C), 다음 task가 B인지 C인지 헷갈려 함</p> </li> <li> <p>vague instruction : 현실적으로 로봇팔이 작동할 수 없는 agressive traejctory를 생성(precision부족)</p> </li> </ul> </li> <li> <p>이러한 문제를 해결하기 위해서 optical flow를 활용하는 방법론을 제안</p> <ul> <li>RGB observation pair가 있을 때, 그 둘의 optical flow를 pixel단위의 displacement field로 계산</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>즉, f는 각 픽셀별로 (u,v) 2차원의 벡터로 표현됨</p> </li> <li> <p>보통 로봇에서 두 obsercation의 차이는 robot arm의 움직임에서 기인하기 때문에, 해당 정보를 활용하면 kinematics, interaction with arm and object등에 대해 모델링이 가능</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>하지만 2-optical flow와 3-channel RGB image사이의 modality차이 때문에, t2v모델로 raw optical flow를 바로 예측하게 학습하는건 어려움</p> <ul> <li>이전 논문에서는 추가적인 flow VAE를 scatch부터 학습하는 방법론이 필요했지만, 현재 세팅에서는 데이터 자체가 적기 때문에 적합하지 않음</li> </ul> </li> <li> <p>optical flow를 flow video format으로 변환해 사용</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>위의 transformation을 통해, flow generation task는 flow video에 대한 distribution을 학습하는 것으로 reformulated될 수 있음</p> <ul> <li> <p>pretrained CogVideoX를 사용, 학습 시 flow video를 latent z로 encode하는 VAE를 freeze</p> <ul> <li> <p>latent는 점점 noise를 더하면서 perturb됨</p> </li> <li> <p>각 denoising step k에서 모델은 noisy latent z를 받아서 상응하는 noise eps를 예측하도록 함</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- **CogVideoX architecture**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="flow-to-video-generation">Flow-to-Video Generation</h3> <ul> <li> <p>text-to-flow model을 기반으로, flow-to-video model을 구축</p> <ul> <li>이전엔 low-level action input에 의존했다면, 해당 방법은 high-level semantic 정보를 받기 때문에, 로봇팔과 물체 식별에 도움이 됨</li> </ul> </li> <li> <p>해당 방식의 우수성을 보이기 위해서, instruction의 특정 단어를 선택, 해당 단어에 대한 cross-attention map을 추출해서 영상과 잘 mapping이 되는지를 봄</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>해당 그림을 보면, language-only model의 경우(그냥 SFT) meaningful region을 식별하는데 실패하는 것을 볼 수 있음</p> </li> <li> <p>flow video를 중간에 생성, 이를 기반으로 detailed vidoe를 생성함으로써, intruction과 visual input사이의 더 나은 alignment를 달성</p> </li> <li> <p>첫번째 단계에서 생성한 flow video를 잘 활용하여 detailed video를 생성할 수 있도록, flow video와 RGB vidoe를 channel dimension에 따라서 concat하는 방법론을 제안</p> <ul> <li> <p>학습 동안, flow video v_F과 dual-arm trajectory video v는 VAE에 각각 encoding되고, z_f, z_v^0으로 나옴</p> </li> <li> <p>flow generation의 절차와 동일하게, video latent z_v^0은 noise로 perturb되고(z_v^k), noisy latent는 flow latent와 content되어서 z^k=[z_v^k,z_f] 모델로 들어감</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="diffusion-policy-from-videos">Diffusion policy from Videos</h3> <ul> <li> <p>predicted video의 각 프레임을 target observation으로 사용해서 excutable low-level action을 생성</p> </li> <li> <p>학습 동안, goal step을 랜덤하게 추출, 노이즈를 활용해서 progressively perturb</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="related-works">Related works</h2> <ul> <li> <p>Bimnaual의 challenge</p> <ul> <li> <p>data scarcity</p> </li> <li> <p>expanded action space</p> </li> <li> <p>diverse collaboration modality</p> </li> <li> <p>simulation fidelity한계</p> </li> <li> <p>가격이 괜찮은 realj-world interface부재</p> </li> </ul> </li> <li> <p>이전에 나온 연구</p> <ul> <li> <p>저비용 teleoperation system(mobile aloha)</p> </li> <li> <p>data augmentation(dexmimicgen)</p> </li> <li> <p>human-object interaction primitive(YOTO)</p> </li> <li> <p>key point를 통해 geometric constraint추출</p> </li> <li> <p>attention으로 dual-arm mechanism 모델림</p> </li> </ul> </li> <li> <p>이전 video generation 활용 연구</p> <ul> <li> <p>VidMan : OpenSORA를 OXE에 학습</p> </li> <li> <p>다른 애들은 robot action을 기반으로 world model를 구축</p> </li> </ul> </li> </ul> <p>** optical flow를 썼다는게 본 논문의 핵심 차별점</p> <h2 id="experiments">Experiments</h2> <h3 id="experiment-setup">Experiment setup</h3> <ul> <li> <p>Simulation setup</p> <ul> <li>RoboTwin으로 평가</li> </ul> </li> <li> <p>Real-world setup and data collection</p> <ul> <li> <p>양팔을 책상 사이드에 마운트, front-facing camera만 사용해서 세팅</p> </li> <li> <p>Vision Pro로 tele-operation</p> </li> </ul> </li> <li> <p>Architecthre Detail</p> <ul> <li> <p>text-to-flow, flow-to-video mdoel은 pretrained CogVideoX-2B로 초기화</p> </li> <li> <p>먼저 bimanual dataset 2개(RDT, RoboMIND)로 초벌 FT, 이후 각 평가 task의 demonstration에 대해서 추가 FT</p> </li> <li> <p>모든 비디오는 256x256, 17프레임으로 구성됨</p> </li> <li> <p>text-to-flow model의 경우 FlowFormer++를 사용해서 각 비디오 클립에서 ground-truth optical flow를 추출</p> </li> </ul> </li> <li> <p>Baseline</p> <ul> <li> <p>DP(diffusion policy)(86M) : conditional denoising diffusion모델로 action을 생성하는 방식</p> </li> <li> <p>RDT(2B) : 양팔 데이터셋으로 pretrain된 VLA모델</p> </li> <li> <p>DP3 : point cloud기반 표현을 활용한 3d 기반 방법론</p> </li> <li> <p>text-to-flow model, flow-to-video model, RDT는 mixed multi-task dataset에 학습되지만, downstream goal-conditioned policy와 다른 baseline은 single-task dataset에만 학습</p> <ul> <li> <p>각 태스크는 100개의 demonstration으로 구성</p> </li> <li> <p>RDT에서는 카메라로 D435사용(default), 나머지는 L515 사용</p> </li> </ul> </li> </ul> </li> </ul> <h3 id="main-result-in-simulation-setup">Main result in simulation setup</h3> <ul> <li>각 태스크 별로 10개의 random seed를 활용해서 평가, 각 시드마다 10번 돌리고 평균냄</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>single camera를 사용하기 때문에 vanilla DP는 다른 추가적인 visual input(3d representation이나 multi-view images)을 사용하는 baseline보다 낮게 나옴</p> </li> <li> <p>하지만 future state를 예상하는 능력이 추가된 CogRobot의 경우 제한적인 image input으로도 높은 성능을 보임</p> </li> </ul> <h3 id="real-world-experiments">Real-World experiments</h3> <ul> <li> <p>2가지 task로 평가</p> <ul> <li> <p>Lift bag : 양팔로 가방 드는 task</p> </li> <li> <p>Pull box : 박스 안의 밧줄을 들어서 그걸로 박스를 가까이 가져오는 task(multi-stage task)</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>각 task마다 사람이 100개의 demonstration을 직접 수집</p> </li> <li> <p>우리 방법론의 view adaptabilites를 평가하기 위해 각 태스크는 다양한 camera viewpoint에서 기록됨</p> </li> <li> <p>이 demonstration을 활용해서 RDT와 RoboMIND에 1차로 FT된 CogVideoX-2B모델을 추가로 FT</p> </li> <li> <p>baseline으로는 DP만 활용</p> </li> <li> <p>실험 결과, 확실히 어려운 task(pull box)에서 CogRobot의 성능이 높게 나옴</p> <ul> <li> <p>video prediction model 을 high-level planner로 사용하는 것에 대한 장점을 보임</p> </li> <li> <p>위의 그림을 봤을 때, 실제 teleopration한 영상과 predicted video가 큰 차이가 나지 않음</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="visualization-and-ablation">Visualization and Ablation</h3> <ul> <li> <p>Visualization</p> <ul> <li> <p>optical flow와 manipulation video를 시각화</p> </li> <li> <p>우리 모델 optical flow 잘 예측하더라!</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Ablation</p> <ul> <li> <p>flow-guided video prediction과 그냥 SFT의 영상 생성 능력을 평가</p> </li> <li> <p>CogVideoX-2B,5B를 같은 해상도, 비디오 길이를 사용해서 RDT와 RoboMIND를 합쳐서 학습</p> </li> <li> <p>평가시에는 RoboMIND validation set 5346개, RDT validation set 1757개로 평가</p> </li> <li> <p>4가지 metric을 사용</p> <ul> <li> <p>PSNR : 재생성된 이미지/영상이 얼마나 원본에 가깝게 복원되었는지를 픽셀 단위에서 측정</p> </li> <li> <p>SSIM : 두 이미지간의 구조적 유사성을 측정(밝기, 대비, 구조) - patch단위로 비교</p> </li> <li> <p>LPIPS : 두 이미지 간의 지각적 유사성을 딥러닝 feature space에서 측정(사전 학습된 CNN의 여러 레이어에서 feature 추출, 둘의 차이를 L2 norm으로 계산)</p> </li> <li> <p>FVD : 영상 전체 시퀀스의 품질을 측정 - 영상 시퀀스를 feature space에 mapping, 분포간 거리를 측정</p> <ul> <li>I3D등의 video model에서 feature추출, 실제와 생성 비디오의 gaussian 분포 간의 FVD계산</li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-06-10-towards-a-generalizable-bimanual-foundation-policy-via-flow/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="conclusion">Conclusion</h2> <ul> <li> <p>bimanual policy를 구축할 때 T2V 모델을 활용</p> </li> <li> <p>적은 데이터에서 효과적으로 T2V를 FT하기 위해 flow-guided framework를 제안</p> </li> <li> <p>방법론은 신박한데 평가가 아쉽다</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>