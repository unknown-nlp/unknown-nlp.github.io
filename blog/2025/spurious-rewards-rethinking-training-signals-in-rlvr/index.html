<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Spurious Rewards: Rethinking Training Signals in RLVR | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - RLVR 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/spurious-rewards-rethinking-training-signals-in-rlvr/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Spurious Rewards: Rethinking Training Signals in RLVR</h1> <p class="post-meta"> Created on August 19, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   <a href="/blog/tag/rlvr"> <i class="fa-solid fa-hashtag fa-sm"></i> rlvr</a>   <a href="/blog/tag/vision"> <i class="fa-solid fa-hashtag fa-sm"></i> vision</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-08-19</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: RLVR</li> </ul> <h3 id="key-takeaways">Key Takeaways</h3> <ol> <li> <p><strong>Pretraining is important for RLVR:</strong> RLVR outcomes depend heavily on reasoning strategies already learned during pretraining.</p> </li> <li> <p><strong>Weak or spurious rewards can still work:</strong> Even random or incorrect rewards can improve performance by amplifying useful pre-existing behaviors (e.g., code reasoning).</p> </li> <li> <p><strong>Limited generalization:</strong> Gains observed in Qwen models do not necessarily transfer to other model families like Llama or OLMo.</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <h2 id="1-introduction">1. Introduction</h2> <p>최근에 RLVR이 language model reasoning을 향상시킴에 있어 큰 도움이 되었는데, 역설적으로 본 연구에서 처음으로 <strong><em>spurious rewards</em></strong>만으로도 특정 모델에서 강력한 mathematical reasoning을 이끌어낼 수 있음을 보여줌.</p> <ul> <li> <p>MATH-500에서 Incorrect labels을 이용해 학습하면 24.1% 성능 향상을 보여주고, 이는 ground truth reward를 사용했을때 29.1% 향상과 비슷한 수준임</p> </li> <li> <p>format reward, random reward → 13.8%, 21.4% performance increases, respectively</p> </li> </ul> <p>→ 위 발견은 RLVR이 성능을 개선하는 정확한 메커니즘을 아직 완전히 이해하지 못하고, 많은 경우 RLVR은 reward signal과는 별개로, <strong>pretraining 중에 학습된 모델의 innate abilities를 드러내는 방식으로 작동할 수 있다고 볼 수 있음</strong>.</p> <hr> <p>weak and spurious rewards가 주는 성능 향상을 측정하기 위해 cross-model analysis를 진행했고, 그 결과 non-Qwen models (OLMo2, Llama3 variants)는 성능 향상이 거의 없거나 오히려 악화된 결과를 보여줌.</p> <p>→<strong>RLVR 성능 차이가 적어도 일부는 pretraining에서의 차이에서 비롯됨을 시사함</strong></p> <hr> <p>본 연구는 <strong><em>pretraining 과정에서 주입된 reasoning patterns이 RLVR 학습에 큰 영향을 끼치는 것</em></strong>을 알아야한다는 것을 강조함.</p> <ul> <li> <p>Qwen models은 open weight and high performance이기에, RLVR 연구의 de facto choice가 되버림 (최근 RLVR 연구 대부분이 Qwen2.5-Math-7B 중심의 실험)</p> </li> <li> <p>본 연구에서 Qwen 계열 models은 spurious rewards 만으로도 큰 성능 향상을 얻을 수 있음을 보여줌</p> </li> </ul> <p>→ 향후 RLVR 연구는 가능하다면 <strong>다른 계열의 models에서 결과가 재현되는지</strong> 확인하는 것을 제안함.</p> <hr> <h2 id="2-spurious-rewards-yield-significant-rlvr-gains">2. Spurious Rewards Yield Significant RLVR Gains</h2> <h3 id="21-experimental-setup">2.1 Experimental Setup</h3> <p>최근 RLVR 연구들을 따라, GRPO를 사용하여 Qwen2.5-Math를 finetune 수행하고 rollouts에 대해 binary reward를 부여함.</p> <p>해당 standard ground-truth reward를 대체하기 위해 점진적으로 약화된 reward functions을 다음과 같이 설계함 (<strong>얼마나 적은 supervision으로도 효과적인 RLVR 학습이 가능한지 한계를 확인하기 위함</strong>)</p> <ul> <li> <p><strong>Ground truth reward</strong>: standard RLVR approach (upper bound for reward supervision quality)</p> </li> <li> <p><strong>weak rewards</strong></p> <ul> <li> <p><strong>majority vote reward</strong>: ground truth labels을 사용하지 않고, RLVR 학습을 하지 않은 model을 활용해 pseudo-label 진행 (각 prompt에 대해 64개 response 추출후 majority answer를 선택하여, online RLVR training에 활용)</p> </li> <li> <p><strong>format reward</strong>: response의 수학적 정답 여부를 전혀 고려하지 않고, 최소한 하나의 non-empty \boxed{} expression이 있으면 reward 부여함. (<strong>괄호 안의 정답 여부 상관없음</strong>)</p> </li> </ul> </li> <li> <p><strong>spurious rewards</strong></p> <ul> <li> <p><strong>random reward</strong>: <em>no guidance</em> in the rewarding process를 해도 수학 성능이 향상되는지 확인함. 보상을 fixed probability \gamma로 설정하여 \gamma 확률로 reward 1을 부여함 (\gamma=0.5 사용)</p> </li> <li> <p><strong>incorrect reward</strong>: 의도적으로 incorrect supervision을 제공하여 incorrect answer에 대해서만 reward를 부여함 (majority voting으로 training data를 labeling한 뒤에, incorrect label이 붙은 subset만 선택하여 training data로 사용함 → 이렇게 얻어진 incorrect label은 model이 산출할 가능성이 높은 출력)</p> </li> </ul> </li> </ul> <hr> <ul> <li>Training data: DeepScaleR (40,000 unique math problem-answer pairs)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Benchmark dataset (+Metric)</p> <ul> <li> <p>MATH-500 w/ pass@1 accuracy</p> </li> <li> <p>AMC w/ average@8 accuracy</p> </li> </ul> </li> </ul> <hr> <h3 id="23-results">2.3 Results</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>MATH, AMC 모든 벤치마크에서 reward 종류와 상관없이 untuned baseline 대비 학습 초반 50 steps이후부터 유의미한 성능 개선을 보여줌</p> <ul> <li>(예외) Qwen2.5-Math-1.5B에서 Random reward는 비교적 느리게 학습되며, AMC에서는 4.9% 제한적 성능 향상을 보여줌</li> </ul> </li> <li> <p>MATH-500과 AMC에서 모두 Spurious rewards에 의한 성능 향상은 ground truth 기반의 RLVR과 차이가 크게 나지 않음을 보여줌</p> <ul> <li> <p><strong>**Additional Results</strong> (AIME 2024, 2025)**</p> <ul> <li> <p>AIME2024역시 ground truth와 spurious rewards랑 큰 차이를 보여주지 않음</p> </li> <li> <p>다만 AIME2025에서는 유의미한 차이가 나타나는데, 이는 AIME2025가 model의 지식 cutoff 이후에 작성된 문제들을 포함하기 때문 (그럼에도 불구하고 untuned baselines 대비 성능 향상 보임)</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong><em>→ 위 실험 결과는 적어도 open-source post-training pipeline 규모에서, RLVR은 새로운 reasoning capabilities를 가르치는 것이 아니라, 이미 base model의 latent capabilities를 trigger함을 보여줌</em></strong></p> <p>(뒤에 추가 실험 결과들은 위 가설을 뒷받침함)</p> <hr> <h2 id="3-lack-of-generalization-to-other-models">3. (Lack of) Generalization to Other Models</h2> <p>Section 2에서 보여준 현상이 다른 모델을 학습할 때에도 적용되는지 확인하는 실험을 함.</p> <p>Models</p> <ul> <li> <p>Qwen2.5-7B, Qwen2.5-1.5B (수학 특화 모델이 아닌 general-purpose variants)</p> </li> <li> <p>Llama3.1-8B(-Instruct), Llama3.2-3B(-Instruct)</p> </li> <li> <p>OLMo2-7B, OLMo2-7B-SFT</p> </li> </ul> <p><strong>Spurious rewards can benefit Qwen2.5 models, but nearly always fail to improve non-Qwen models.</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>같은 계열의 models에서는 일반적으로 유사한 경향을 보여줌</p> <ul> <li> <p>Qwen2.5 models에서 random reward를 제외하면, MATH와 ACC에서 명확한 성능 향상을 보여줌</p> </li> <li> <p>OLMo models은 ground truth reward에 대해서만 효과 있음 (spurious rewards X)</p> </li> </ul> </li> </ul> <p>**→ 저자들은 같은 계열의 model들이 유사한 경향을 보여주는 이유는, pretraining data의 distribution이 유사하기 때문이라고 추정함 **</p> <ul> <li>작은 models일수록 spurious rewards의 gain이 낮음</li> </ul> <p><strong>→ 저자들은 bigger models이 pretraining 단계에서 더 많은 knowledge를 학습했고, spurious rewards가 그 지식을 이끌어낼 수 있기 때문이라고 추정함</strong></p> <ul> <li> <p>어떤 model 계열에서 잘 작동하는 reward signals이 다른 계열의 모델로 일반화되지는 않음</p> <ul> <li>Spurious rewards는 Qwen 계열의 models에서 일관된 성능 향상을 보여주고, 다른 계열의 models에서는 그렇지 않음</li> </ul> </li> </ul> <hr> <p><strong>Practical warning : Proposed RLVR reward signals should be tested on diverse models!</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>최근 RLVR reasoning 연구들은 주로 Qwen model에 대해서 결론을 도출함 (test-time scaling, one-shot RL)</p> </li> <li> <p>TTRL과 One-Shot RL에 대해서 Qwen 계열 models을 포함하여 다른 계열의 models도 실험해본 결과,</p> <ul> <li> <p>proposed rewards는 Qwen 계열에서는 잘 작동함 (어떤 spurious reward 적용했는지 언급 x..)</p> </li> <li> <p>동일한 reward signal임에도 다른 models 계열에서는 성능 향상이 없음</p> </li> </ul> </li> </ul> <p><strong>→ 앞으로 Qwen-centric RLVR 연구는 non-Qwen models에 대해서도 validation이 필요함</strong></p> <hr> <h2 id="4-what-makes-rlvr-with-spurious-rewards-work">4. What Makes RLVR with Spurious Rewards Work?</h2> <p>Section 3에서는 동일한 reward function을 사용함에도 불구하고 model에 따라 결과가 달라지는 것을 보여줬고, 이번 Section에서는 왜 이러한 discrepancy가 발생하는지 알아봄.</p> <p><strong><em>hypothesis</em></strong><em>: RLVR 결과의 차이가 각 model이 pretraining하는 동안 학습한 특정 reasoning strategies의 차이에서 발생함. (어떤 strategy는 RLVR에 의해 쉽게 이끌어낼 수 있고, 다른 strategy는 그렇지 않음)</em></p> <ul> <li> <p>Qwen-Math는 효과적으로 활용하고 다른 model 계열은 그렇지 못하는, ‘generating code to assist in math reasoning’ strategy를 확인함 (Section 4.1)</p> </li> <li> <p>RLVR 학습 과정에서 code reasoning의 prevalence를 tracing하며, 위 hypothesis를 지지하는 evidence 발견함 (Section 4.2)</p> </li> <li> <p>Incorrect and Random rewards (Spurious rewards)의 reward signals의 origin에 대한 hypothesis를 제안함 (Section 4.4)</p> </li> </ul> <hr> <h3 id="41-different-models-exhibit-pre-existing-discrepancies-in-reasoning-strategies">4.1 Different Models Exhibit Pre-existing Discrepancies in Reasoning Strategies</h3> <p>Qwen2.5-Math-7B와 OLMo2-7B의 behaviors discrepancy를 이해하기 위해, MATH-500에 대한 reasoning traces를 평가함.</p> <ul> <li>Qwen2.5-Math-7B는 code execution environment가 아님에도 불구하고, 자신의 thinking process를 돕기 위해 자주 Python 코드를 생성함 (65.0% of all responses) → <strong><em>Code Reasoning</em></strong> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Code Reasoning</strong> 현상은 단순히 model의 memorization이라고 볼 수 없는게, 문제에서 숫자만 바꿔도 accuracy를 유지하는 현상을 보여줌.</p> </li> <li> <p>문제가 다른 서술 형태로 재구성되면 Code Reasoning 현상을 보여주지 않고, 틀린 정답을 도출함</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Qwen2.5-Math-7B가 pretraining 과정에서 많은 code-assisted math reasoning traces를 접했을 것이라 추정함</p> <ul> <li>Code 사용은 answer correctness를 강하게 예측하는 것을 보여줌</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Qwen2.5-Math-7B/1.5B 둘 다 Acc. w/Code가 Acc. w/Lang 보다 유의미하게 높은 성능 보여줌</p> <ul> <li>두 모델은 동일한 corpus로 pretraining 진행됨</li> </ul> </li> <li> <p>다른 계열의 models에서는 이러한 현상이 나타지 않음 → <strong>No-Code / Bad-Code</strong></p> <ul> <li> <p>No-Code: Llama, Qwen2.5-1.5B, OLMo2-7B는 Code Frequency가 0%</p> </li> <li> <p>Bad-Code: OLMo2-7B-SFT, Qwen2.5-7B는 Code Frequency가 90% 이상으로 매우 높지만, 오히려 성능 악화로 이어짐</p> </li> </ul> </li> </ul> <p><strong>→ Effective Code Reasoning은 RLVR 학습 이전에 Qwen2.5-Math models이 갖는 unique capability라고 볼 수 있음</strong></p> <hr> <h3 id="42-rlvr-with-spurious-rewards-can-upweight-pre-existing-reasoning-strategies">4.2 RLVR with Spurious Rewards Can Upweight Pre-existing Reasoning Strategies</h3> <p>Section 4.1의 결과에 따라 RLVR training에 걸친 model의 reasoning behavior를 다음과 같이 분석함</p> <ol> <li> <p><strong>Accuracy</strong>: MATH-500의 평균 accuracy</p> </li> <li> <p><strong>Code reasoning frequency</strong>: model 응답에서 “python” string을 포함하는 비율</p> </li> </ol> <hr> <p><strong>Performance is correlated with code reasoning frequency</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Qwen2.5-Math-7B는 RLVR training 이후에 초반 15 steps에 reward에 상관없이 code frequency가 대략 90%로 보여주며, accuracy improvements와 강한 상관관계를 보여줌 (뚜렷한가..?)</p> <ul> <li> <p>Random Reward는 비록 초반에 낮은 수치를 보여주지만, 후반부에 가서 95.6% 찍음</p> </li> <li> <p>Ground Truth를 reward로 RLVR을 수행할 때, code frequency는 급격히 증가하지만, model의 natural language reasoning accuracy가 올라감에 따라 감소하는 경향을 보여줌</p> </li> </ul> </li> </ul> <p><strong>→ RLVR 중에 model은 high-quality ground truth reward로부터 real knowledge를 학습함</strong></p> <ul> <li>Bad-Code 모델은 뚜렷한 상관관계를 보여주지는 않다고 주장하지만, 대체로 음의 상관관계 보여줌</li> </ul> <hr> <p><strong>Reasoning strategy switches during RLVR</strong></p> <ul> <li> <p>Qwen2.5-Math-7B의 accuracy 향상을 세분화하여 분석하기 위해, 각 reward signal로 학습한 model의 성능을 다음과 같이 설정하여 분석함 (test prompt를 4개의 베타적인 subset으로 나눔)</p> <ol> <li> <p><strong>Code → Code</strong>: RLVR 전후 모두 code reasoning 사용</p> </li> <li> <p><strong>Code → Lang</strong>: 초기에는 code reasoning 사용, 이후에 natural language reasoning 사용</p> </li> <li> <p><strong>Lang → Code</strong>: 초기에는 natural language reasoning 사용, 이후에 code reasoning 사용</p> </li> <li> <p><strong>Lang → Lang</strong>: RLVR 전후 모두 natural language reasoning 사용</p> </li> </ol> </li> </ul> <p>**Partial Contribution Score **C_d: test set D의 subset d에 대한 부분 기여 점수를 이용하여 각 subset이 성능 향상에 기여한 정도를 정량화 시킴</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Frequency</strong></p> <ul> <li> <p>각 reward signal 별로 Qwen2.5-Math-7B의 reasoning strategy switches를 보면,</p> <ul> <li> <p>weak/spurious rewards에서 RLVR 이후에 code reasoning을 많이 사용함 (C → L cases는 적지만, L → C cases는 상대적으로 많음)</p> </li> <li> <p>Ground truth reward는 상반된 결과를 보여줌 (L → C cases &lt; C → L cases)</p> </li> </ul> </li> <li> <p>Section 4.2 결과와도 일맥상 통하는 결론</p> <ul> <li> <p>Bad-Code 모델에서 (Qwen2.5-7B, OLMo2-7B-SFT) meaningful reward (ground truth, majority vote)는 model로 하여금 bad code reasoning으로부터 멀어지게 함 (<em>위에 Figure 참조</em>)</p> </li> <li> <p>(No-Code 모델에서 RLVR은 reasoning strategy에서 meaningful changes를 보여주지 못하는데, 이는 <strong>pretraining 과정에서 해당 capability 자체가 학습되지 않았기 때문</strong>)</p> </li> </ul> </li> </ul> <hr> <p><strong>Accuracy</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>위 Figure 보면, L → C subset의 accuracy가 모든 reward에서 RLVR 이후 큰 성능 향상을 보여줌</li> </ul> <p>→ Qwen2.5-Math-7B/1.5B의 성능 향상의 58.3% / 78.7%은 L→C case가 차지함</p> <ul> <li>Bad-Code model인 Qwen2.5-7B에서 성능 향상은 C → L case가 93.9% 차지함</li> </ul> <p>→ 이 model은 language reasoning accuracy가 code reasoning accuracy보다 높기 때문에, **RLVR training은 결국 model이 더 잘하는 strategy를 이용하도록 학습됨 **(즉, prior knowledge에 대한 dependency가 있다고 생각함)</p> <hr> <h3 id="43-intervening-explicitly-on-code-reasoning-frequency">4.3 Intervening Explicitly on Code Reasoning Frequency</h3> <p>Section 4.2와 4.3에서 RLVR 동안 code reasoning frequency가 증가하고, 이는 test performance의 향상과 상관관계가 있다는 것을 실험적으로 보여줌. 이번 Section에서는 Code reasoning의 빈도를 more / less 명시적으로 유도하며, causal impact를 분석함</p> <p><strong><em>Hypothesis</em></strong><em>: spurious reward로 학습할 때, code reasoning 증가가 Qwen2.5-Math-7B의 성능 향상의 주된 원인중 하나가 맞다</em></p> <p>→ 해당 hypothesis가 맞다면, code reasoning frequency에 intervention을 하는 것은 그에 상응하는 성능 향상 혹은 감소를 일으켜야함.</p> <hr> <p>Code reasoning을 유도하면 Qwen2.5-Math의 성능이 향상되는 반면, 다른 모델들은 반대 경향을 보여줌.</p> <ul> <li> <p>Prompting과 RLVR 학습을 통해 code reasoning을 유도함</p> <ul> <li> <strong>Prompting</strong>: “Let’s solve this using Python” 명시적으로 강제함</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- No-Code models은 성능 하락을 보이는데, 이는 해당 계열 models이 effective code reasoning behavior를 보여주지 않음 (Section 4.1 실험 결과와 일치)
</code></pre></div></div> <ul> <li> <strong>RLVR</strong>: response에 “python” string을 포함할때만 + reward 부여함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Qwen2.5-Math-7B는 20 training steps부터 code reasoning frequency가 99% 이상 비율

- Qwen2.5-Math에서만 유의미한 성능 향상을 보여주고, 다른 모델에서는 그렇지 않은 결과를 보임
</code></pre></div></div> <hr> <p>반대로 RLVR 중 code reasoning을 억제하면, Qwen2.5-Math-7B에서 성능이 줄어들고, 다른 models에서는 향상될 수 있음.</p> <p>위 Hypothesis에 대한 대우명제를 검증하는 실험을 진행함</p> <ul> <li> <p>대우: <em>“Code reasoning에 penalty를 주면 spurious reward로 인한 성능 향상이 감소할 수 있음”</em></p> </li> <li> <p>실험 세팅으로는 compound rewards를 설계함 (아래 두가지 조건 모두 만족할 때 reward 부여)</p> <ol> <li> <p>원래의 spurious rewards를 만족</p> </li> <li> <p>response에 “python” string이 없을 때</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>format reward와 no-code reward가 compound된 Figure (a)를 보면, Qwen2.5-Math-7B는 성능이 하락함 → <strong>hypothesis와 일치한 결과</strong> (<strong>웃긴게 여기만 제대로 일치함 ㅋㅋ</strong>)</p> </li> <li> <p>Incorrect reward에서 MATH-500은 Compound reward와 Original reward가 비슷한 수준으로 성능이 나오는 반면, 더 어려운 task인 AMC에서는 성능 향상의 폭이 줄어듬</p> </li> </ul> <p>→ code reasoning을 제거하면, spurious reward의 performance gain이 줄어듬</p> <ul> <li>ground truth reward에서는 성능 향상을 보여주는데, 이는 앞에서 code reasoning frequency가 줄어듬에 따라 real knowledge를 배워 성능이 개선되는 결과와 consistent함</li> </ul> <p>(<strong>???</strong>: 그런데, Incorrect reward와 GT reward에서 Qwen-Math-7B trend는 비슷한거 아닌가..? 최종 acc까지 비슷한 수준임. Format reward하고 일관된 trend가 보이지 않음..)</p> <ul> <li>Bad-code models (Qwen2.5-7B, OLMo2-7B-SFT)는 Compound reward가 original reward보다 더 높은 성능을 보여줌 → 이는, bad-code models이 weak code reasoning을 보이기 때문에, <strong>Compound reward가 model이 못하는 behavior의 weight를 명시적으로 낮춰 학습을 유도함</strong>.</li> </ul> <hr> <h3 id="44-the-curious-cases-training-signals-from-incorrect-rewards-and-random-rewards">4.4 The Curious Cases: Training Signals from Incorrect Rewards and Random Rewards</h3> <p>그러면 Spurious Rewards가 어떻게 RLVR에서 meaningful training signal을 만들어내는가?</p> <hr> <p><strong>Incorrect Rewards</strong></p> <p>저자들은 incorrect rewards가 effective training signals을 만드는 다음 두 가지 포인트를 가정함</p> <ol> <li> <p><em>많은 incorrect labels이 ground truth 값에 가까운 값으로 남아 있어, 대체로 올바른 reasoning에 positive reinforcement를 제공함</em></p> </li> <li> <p><em>Incorrect labels may function like format rewards</em></p> </li> </ol> <ul> <li>models은 생성된 response를 성공적으로 추출하고 평가하지 않으면 reward를 줄 수 없기 때문에, 일정 수준의 correct reasoning이 필요함</li> </ul> <p>→ 말로 가정만 하고… 정작 이를 검증하는 실험이 없네요….ㅋㅋ</p> <hr> <p><strong>Random Rewards</strong></p> <p>혹자는 Rewarded answers 다수가 correct하기 때문에, Qwen2.5-Math의 성능이 개선되었다 볼 수 있음.</p> <p>→ GRPO는 reward의 평균을 0으로 normalize하기 때문에, rewarded answers가 대부분 정답이라면, penalty answers도 대부분 correct하기 때문에 (penalized responses 중에도 correct 다수 포함), 위에 말은 틀림.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Random rewards with varying probabilities consistently improve performance</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>GRPO training에서 *Bernoulli<strong>(\gamma)</strong> *variable로 random rewards를 보여함</p> </li> <li> <p>\gamma 가 0이 아닐 때는, 유의미한 성능 향상을 보여줬고 (15~20% 향상), 0일 때는 constant rewards가 learning signal을 만들지 않아 개선이 없음.</p> <ul> <li>\gamma가 0이면, Reward가 0이고, 그럼 모든 rollout의 Advantage 역시 0이고 → gradient=0이라 학습x</li> </ul> </li> </ul> <p><strong>GRPO clipping bias can induce random reward training signals</strong></p> <p>GRPO는 gradient updates에서 reward에 대해 normalized group-relative advantage를 계산함. 이때, batch rollouts에 대해 normalize를 하기 때문에, advantage의 expectation은 0임.</p> <p>→ 그런데, GRPO의 clipping mechanism 때문에, 실제로 advantage의 expectation은 0이 아님.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>\rho_t=\pi_{\theta}(y)/\pi_{old}(y) → 1-\epsilon &lt; \rho_t &lt; 1 + \epsilon</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>확률이 높은 token에 대해서는 clip에 걸리는 경우가 거의 없어, update는 대부분 +방향으로 이루어짐</p> </li> <li> <p>확률이 낮은 token에 대해서는 범위가 좁아 clip에 쉽게 걸려, 조금만 확률을 늘리면 penalty (-gradient)가 들어옴 → 낮은 확률 token에 대해서는 키우기가 어려움</p> <ul> <li> <p><strong>Example</strong></p> <ul> <li> <p>\pi_{old}(y)=0.85, \epsilon=0.2</p> <ul> <li>clip range = [0.85 x 0.8, 0.85 x 1.2] = [0.68, 1.02]</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>→ prob은 1을 넘을 수 없어 실제 상한은 1.0이므로 구간의 폭은 0.32</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - \pi_{old}(y)=0.02,\epsilon=0.2

    - clip range = [0.02 x 0.8, 0.02 x 1.2] = [0.016, 0.024]
</code></pre></div></div> <p>→ 구간 폭은 0.008 (매우 좁기때문에, 조금만 확률이 늘어나도 clip에 걸려 gradient가 0이 되거나 penalty 발생 → 확률을 키우기 어려움)</p> <p>→ clipping으로 인해 비대칭이 생겨, advantage expectation은 0이 아니고, model이 원래 자주 생성하는 token들로 probability mass가 쏠리는 현상 발생 (<strong>model의 prior knowledge를 강화하는 방향으로 bias 생김</strong>)</p> <ul> <li> <p>높은 token 확률은 clipping에 잘 안걸려 → +gradient 누적</p> </li> <li> <p>낮은 token 확률은 clipping에 잘 걸려 → -gradient 누적</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 즉, random reward에서도 <strong>clipping bias가 prior knowledge에 기반한 behavior를 강화함</strong> (아래 실험 결과로도 보여줌)</p> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Dapo: An open-source llm reinforcement learning system at scale, 2025</p> <ul> <li>위 연구에서도 clipping bais가 RLVR에서 exploration을 줄이고 exploitation을 늘린다는 것을 발견함</li> </ul> <p>추가로 GRPO에서 clipping effect를 실험적으로 검증을 함</p> <ul> <li> <p>clipping 효과를 제거하기 위해, clipping bias를 없애는 설계를 구현함</p> <ol> <li> <p>loss calculation에서 clipping bias를 직접 제거</p> </li> <li> <p>training 및 rollout batch size를 조정하여 \pi_{\theta}=\pi_{old}를 보장하며, clipping이 발생하지 않도록 함</p> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-19-spurious-rewards-rethinking-training-signals-in-rlvr/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Random w. Clipping Enable (GRPO)에서 random reward가 code reasoning frequency를 높임.</p> </li> <li> <p>w. Clipping Disabled case를 보면, 이러한 reasoning pattern trend가 사라짐</p> </li> </ul> <p>→ Clipping을 사용하면, 이에 따라 성능 향상으로 이어짐</p> <hr> <h2 id="5-conclusion">5. Conclusion</h2> <p>본 연구에서는 Weak / Spurious Rewards를 사용한 RLVR이 Qwen-2.5-Math에서 유의미한 성능 개선을 보여주는데, 이는 existing reasoning patterns을 증폭함으로써 이루어진다는 것을 설명함.</p> <p>본 연구의 다양한 실험을 통해 아래 3가지 주요 포인트를 강조함.</p> <ol> <li> <p>Base model pretraining은 RLVR 결과에 큰 영향을 미침</p> </li> <li> <p>Spurious supervision도 유의미한 existing behaviors를 촉발할 때 reasoning을 향상시킬 수 있음</p> </li> <li> <p>특정 model 계열의 결과가 다른 계열의 model로 일반화 되지 않음</p> </li> </ol> <p>본 연구가 제시하는 기준은 다음과 같음</p> <ol> <li> <p>서로 다른 pretraining distribution을 가진 여러 model에 대해서 RLVR을 검증</p> </li> <li> <p>RL을 평가할 때, 다양한 reward를 baseline으로 두어 비교</p> </li> </ol> <h3 id="아쉬운점">아쉬운점..?</h3> <ul> <li> <p>전반적으로 잘 설계된 많은 실험과 흥미로운 결과들을 보여줘서 인정하지만, 몇몇 부분에서는 다소 비약이 있다고 생각함.</p> </li> <li> <p>Spurious Reward가 다른 계열의 모델에서는 잘 나타나지 않고, Qwen2.5-Math에서만 강하게 나타나는 것으로 보아, 해당 계열 model의 특수성일 수 있음. 즉, RLVR이 latent capability를 trigger한다는 주장은 일반화하기 어려움</p> </li> <li> <p>random reward가 왜 작동하는지 설명하는 부분에 있어서, 변인통제가 제대로 이루어지지 않음. 다른 bias로 인해 영향을 받았을 수도 있음</p> </li> <li> <p>Math 이외의 domain (ex. code, language reasoning) tasks에 대해서 어떻게 나타는지 분석이 아쉬움</p> </li> <li> <p>실험 전반의 RLVR training steps이 50~100 steps에서 saturation이 이러나는데, long-term stability 검증이 부족함 (catastrophic forgetting 혹은 overfitting issue에 어떻게 되는지)</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>