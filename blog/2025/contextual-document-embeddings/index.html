<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Contextual Document Embeddings | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Retrieval, Embeddings 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/contextual-document-embeddings/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Contextual Document Embeddings</h1> <p class="post-meta"> Created on March 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/embedding"> <i class="fa-solid fa-hashtag fa-sm"></i> embedding</a>   <a href="/blog/tag/embeddings"> <i class="fa-solid fa-hashtag fa-sm"></i> embeddings</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> retrieval</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-03-04</li> <li> <strong>Reviewer</strong>: 상엽</li> <li> <strong>Property</strong>: Retrieval, Embeddings</li> </ul> <h1 id="introduction">Introduction</h1> <ul> <li> <p><strong>Statistical approaches</strong>: BM25 → <strong>Neural method</strong>: <em>dual encoder</em></p> </li> <li> <p><strong>neural model</strong>에 없는 Statistical approach만이 가진 장점: <strong>prior corpus 통계치</strong>를 알 수 있다는 것</p> <ul> <li>prior term을 통해 <strong>context-dependence</strong>를 가짐.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Domain: Wikipedia, sports article, televised events

- IDF는 NFL, draft, annual 등에 더 높은 가중치를 부여할 수 있음.
</code></pre></div></div> <ul> <li><strong>연구 목표: dense encoder를 통한 contextualization of document embeddings</strong></li> </ul> <ol> <li><strong>Contextual training procudure</strong></li> </ol> <ul> <li> <p>Fast query-document clustering: contrastive learning 과정에서 배치 내 이웃 문서 (<strong>neighboring documents</strong>) 정의</p> </li> <li> <p>이웃 문서로만 배치학습 진행 ← <strong>most challenge contexts</strong>를 구별할 수 있게 하기 위함.</p> </li> </ul> <ol> <li><strong>Architecture</strong></li> </ol> <ul> <li> <p>임베딩 동안에 <strong>contextual document를 주입하는 새로운 encoder</strong> 설계</p> </li> <li> <p><strong>Contextual Document Embedding (CDE)</strong>: BERT-style encoder에 aggregated document-level information about neighboring documents를 제공</p> </li> <li> <p>사전에 계산된 corpus-level 통계치를 제공 → 동일한 임베딩 사이즈 유지</p> </li> </ul> <h1 id="background">Background</h1> <p><strong>Text retrieval</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Vector retrieval methods</p> <ul> <li>f(d,q) → \phi(d) \cdot\psi(q) (documenct 임베딩의 경우 사전에 계산→ fast computation 가능)</li> </ul> </li> <li> <p>Statistical Retrieval</p> <ul> <li>\phi(d;D) \cdot \psi(q;D): Document/Domain 정보를 활용 → Context 활용</li> </ul> </li> </ul> <h1 id="method">Method</h1> <p>일반적인 Retrieval 모델 학습은 여러 도메인을 가진 대량의 데이터를 활용하게 되므로 특정 도메인의 통계적 특성을 모델이 알 수가 없음.</p> <h3 id="contextual-training-with-adversarial-contrastive-learning">Contextual Training with Adversarial Contrastive Learning</h3> <ul> <li> <p>일반 도메인에서 NFL은 적은 문서에 등장하여 가치가 있는 단어일지라도 검색 대상 데이터가 Sports 도메인일 경우 해당 단어는 상대적으로 흔한 단어가 됨. → 가중치가 낮아짐.</p> </li> <li> <p><strong>Meta-learning-style objectives</strong>: 도메인 선정 → 관련 예시를 샘플링</p> <ol> <li>Training dataset (\mathcal{D}_T)를 각각의 pseudo-domain을 나타내는** (<strong>**\mathcal{B}^1, …, \mathcal{B}^B</strong><strong>) 그룹들로 분할</strong> </li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Hard negatives (\mathcal{H}) 없음!
</code></pre></div></div> <ol> <li>Group이 <strong>최대한 challenge</strong>하기 위해 다음의 최적화 문제를 풀 수 있음.</li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Zhang &amp; Stratos (2021) show that **increasing the partition term** **improves the contrastive approximation to the maximum likelihood the gradient.**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- target query의 d, q  pair가 아닌 Group 내 다른 d', q' pair와의 유사도가 최대가 되게 → 정답이 아닌 것도 유사도가 가장 높아지는 배치 그룹 구성 → 가장 Challenge한 그룹 구분

- 현재 수식은 Intractable → 클러스터링을 통해 근사

  1. **maximize → minimize 변환**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    - dot products를 L_2를 활용해 다음과 같이 변환 가능

    - 유사도를 높이자. == 거리를 줄이자. (normalized embeddings에서는 같은 의미)

  1. **유클리디안 거리는 any other pair ****m****에 대해 다음 ****triangle inequality를 만족**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  1. **이를 통해 Upper-bound를 얻을 수 있음.**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    - 이미 정해진 B에 대해 이건 **비대칭 K-Means 클러스터링** 문제로 볼 수 있음. (그룹에 속한 샘플들의 거리를 가장 줄이는 방향으로 그룹을 묶자.)

    - L_2 변환식을 볼 경우, m을 활용한 거리 계산은 \phi(d) \oplus \psi(q)와 같이 각각의 임베딩을 concat해서 빠르게 계산 가능!

    - 클러스터링 과정은 임베딩 모델 학습 이전에 사용하기 때문에 sparse 임베딩과 GTR 활용

  1. **Filtering Flase Negatives**

    - **False Negative가 동일 배치 내에 포함될 수 있으므로 특히나 민감함.**

    - MS Marco의 경우 70% 이상일 때도 있더라…

    - Equivalence class 계산

      - **다음의 surrogate function 활용**
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      - 정답보다 더 가까운 유사도를 보이는 다른 문서들을 제거

      - True negatives를 제거하게 될지라도 성능에 더 좋았음.

  1. **Packing**

    - equal-sized batches를 만들자.

    - 아래 두 방법을 고려, 큰 그룹은 나누고 작으며 가까운 그룹은 합침.

      1. random partioning

      1. cluster-level traveling salesman

    - 이 방법은 학습 때 마다 그룹 배치에 randomness를 줌으로써 도움이 되었음.
</code></pre></div></div> <h3 id="contextual-document-embedding-cde">Contextual Document Embedding (CDE)</h3> <ul> <li> <p><strong>contextualization을 architecture에 직접적으로 주입하자.</strong></p> </li> <li> <p>Sparse retrieval 모델과 같이 corpus에 직접적으로 접근할 수 있는 encoder 모델을 만들자.</p> </li> </ul> <ol> <li> <p>BM25와 같이 Corpus의 통계치만을 제공하자.</p> </li> <li> <p>전체 문서에 대한 접근 권한을 가지되 cross attention과 같은 형태로 문서를 선별하자. (Garnelo et al., 2018: small scale) ← Large dataset에서 한계</p> </li> </ol> <p>→ <strong>corpus statistics를 배우되 효과적으로 계산할 수 있는 middleground 방법을 제안</strong></p> <ul> <li> <p>Morris et al., 2023의 연구에서 documnet embeddings이 충분한 lexical inforamtion을 갖고 있음.</p> </li> <li> <p>corpus subset을 미리 임베딩해 만들어 활용한다면 lexical 정보를 encdoing에서 활용하는 것이 아닌가?</p> </li> <li> <p>Two-stage process를 통해 contextualized embedding을 생성</p> </li> </ul> <hr> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>*First stage: **</em>Gather and embed context*</p> <ul> <li>Context documents: d^1, …, d^J \in \mathcal{D}가 있을 때, 임베딩 모델을 사용해 만든 임베딩을 concat하여 Embedding sequence M_1(d^1)…M_1(d^J) 획득</li> </ul> <p><em>*Second stage: **</em>Embed document with additional context tokens*</p> <ul> <li>document d’의 임베딩을 일 계산하기 위해 contextual embedding sequence와 결합하여 다음을 계산</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>M_2: second-stage encoder model</p> </li> <li> <p>E: token embedding matrix of M_2</p> </li> <li> <p>Query도 유사하게 계산</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Contextual embedding implementation</p> <ul> <li> <p>매 배치에서 M_1 계산을 하는 것은 J에 비례해 많은 시간이 걸리는 작업</p> </li> <li> <p>배치 내에서 M_1(d^1)…M_1(d^J)을 공유함으로써 계산을 한 번만하고 caching해서 사용</p> </li> </ul> </li> </ul> <p><strong>Embedding without context</strong></p> <ul> <li> <p>Training 시, 모델의 generalization을 향상하기 위해 p 확률로 특정 context embedding M_1(d^*)을 null token으로 바꾸는 sequence dropout을 활용.</p> </li> <li> <p>Test 시, context를 활용할 수 없을 경우 null tokens을 활용</p> </li> </ul> <p><strong>Position-agnostic embedding</strong></p> <ul> <li> <p>Document의 순서는 무관하기에 모든 postionality를 제거</p> </li> <li> <p>\mathcal{D}에 상응하는 position에 positional embedding을 뺌.</p> </li> </ul> <p><strong>Two-stage gradient caching</strong></p> <ul> <li>GradCache의 two-stage version을 이용 → 더 큰 batch와 sequence를 활용 가능</li> </ul> <ol> <li> <p>first-stage와 second-stage를 gradient 없이 각각 계산 → Loss 계산</p> </li> <li> <p>Second-stage에 대해서만 gradient 계산</p> </li> <li> <p>Gradient 계산을 활성화하여 Second-stage 재실행 → Second-stage 업데이트</p> </li> <li> <p>Gradient 계산을 활성화하여 First-stage 재실행 → First-stage 업데이트</p> </li> </ol> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <p>적합한 세팅을 찾기 위해 BEIR의 truncate 버전을 활용해 <strong>small setting을 구성</strong></p> <ul> <li> <p>6-layer transformer</p> </li> <li> <p>maximum sequence length: 64</p> </li> <li> <p>maximum number of the additional contextual tokens: 64</p> </li> <li> <p>Batch size: {256, 512, 1024, 2048, 4096}</p> </li> <li> <p>Cluster size: {64, 256, 1024, 4096, …, 2097152, 4194304}</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Large setting에서도 small setting에서 찾은 하이퍼파라미터를 이용</p> </li> <li> <p><strong>Sequence length와 contextual token은 512개의 documents 사용</strong></p> </li> </ul> <p><strong>Training Data and Metrics</strong></p> <ul> <li> <p>텍스트 임베딩 학습: 웹 (레딧, 위키피디아)에서 크롤링한 24개의 datasets 활용</p> </li> <li> <p>Unsupervised training: 웹 (레딧, 위키피디아)에서 크롤링한 200M data 활용</p> </li> <li> <p>Supervised training: 1.8M human-written query-document pairs</p> </li> </ul> <p><strong>Implementation</strong></p> <p>배치 partioning 때,</p> <ul> <li> <p>GTR: documents와 queries 인코딩</p> </li> <li> <p>Faiss: clustering, 100 step, 3 attempts</p> </li> <li> <p>NomicBERT: pre-trained model backbone (137M) <strong>for filtering</strong></p> </li> </ul> <p><strong>Training</strong></p> <ul> <li> <p>M_1, M_2 : <strong>nomic-embed-text-v1</strong> ( Nussbaum et al., 2024) including flash attention</p> </li> <li> <p>Adam optimizer</p> <ul> <li> <p>warmup: 1000 steps</p> </li> <li> <p>lr: 2 x 10^{-5}</p> </li> <li> <p>3 epochs</p> </li> </ul> </li> <li> <p>Contrastive loss, \tau: 0.02</p> </li> <li> <p>sequence dropout: 0.005</p> </li> </ul> <h1 id="results">Results</h1> <p><strong>Contextual batching</strong></p> <ul> <li> <p>클러스터링을 통한 batch 그룹 생성과 false negative filtering 이후, batch의 difficulty와 NDCG의 강한 상관관계 확인</p> </li> <li> <p>batch의 document reoredering 역시 difficulty를 올렸으며 성능에 긍정적 영향을 줌.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Contextual architecture</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Archictecture에서도 Contextual을 넣는 것이 일반적으로 더 좋았음.</li> </ul> <h1 id="analysis">Analysis</h1> <p><strong>How hard are our clusters?</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>큰 배치는 더 쉬운 non-negative example을 가져옴 (난이도가 낮음.)</p> </li> <li> <p>cluster size를 감소시키는 것은 난이도를 높임.</p> </li> </ul> <p><strong>Which contextual documents help?</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-03-04-contextual-document-embeddings/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Contextual Documents 도메인을 다르게 해서 실험 진행. (Y-axis: input)</p> </li> <li> <p>NDCG 값이 기존 최고 점수와 1%p 이내의 경우, 하이라이트</p> <ul> <li> <p>당연히 도메인이 같을 경우 즉 동일한 도메인 문서를 context로 받을 경우, 성능이 더 높음.</p> </li> <li> <p>몇몇 도메인에서는 교차 상호작용도 있었음!</p> </li> </ul> </li> </ul> <hr> <ul> <li> <p>글에 에러가 좀 많음.</p> </li> <li> <p>앞부분에는 흥미로웠는데 뒤에 힘이 너무 빠진 느낌.</p> </li> <li> <p>이론과 성능을 다 얻기란 참 어렵다.</p> </li> <li> <p>MTEB은 너무 수렴해버린 거 같음.</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>