<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, Reward Model ê´€ë ¨ ì—°êµ¬"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward/"> <meta property="og:description" content="ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, Reward Model ê´€ë ¨ ì—°êµ¬"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective"> <meta name="twitter:description" content="ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, Reward Model ê´€ë ¨ ì—°êµ¬"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward/",
        "@type": "BlogPosting",
        "description": "ë…¼ë¬¸ ë¦¬ë·° - Reinforcement Learning, Reward Model ê´€ë ¨ ì—°êµ¬",
        "headline": "The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/the-accuracy-paradox-in-rlhf-when-better-reward/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models / What Makes a Reward Model a Good Teacher? An Optimization Perspective</h1> <p class="post-meta"> Created on August 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/blog/tag/alignment"> <i class="fa-solid fa-hashtag fa-sm"></i> alignment</a> Â  <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a> Â  <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement learning</a> Â  <a href="/blog/tag/reward-model"> <i class="fa-solid fa-hashtag fa-sm"></i> reward model</a> Â  <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf</a> Â  Â· Â  <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2025-08-12</li> <li> <strong>Reviewer</strong>: ì¤€ì› ì¥</li> <li> <strong>Property</strong>: Reinforcement Learning, Reward Model</li> </ul> <hr> <hr> <h1 id="the-accuracy-paradox-in-rlhf-when-better-reward-models-dont-yield-better-language-models">The Accuracy Paradox in RLHF: When Better Reward Models Donâ€™t Yield Better Language Models</h1> <h2 id="1-introduction">1. Introduction</h2> <ul> <li> <p>ìš°ë¦¬ëŠ” ì™œ RLì„ í†µí•´ alignmentë¥¼ í• ê¹Œ?</p> </li> <li> <p>SFT suffers from exposure bias</p> </li> <li> <p>SFT lacks the ability to optimize for sequence-level rewards</p> </li> <li> <p>RQ &amp; Our Common Myth</p> </li> <li> <p>ë” Accurateì„ ì¤„ ìˆ˜ ìˆëŠ” RMì´ ë” effectivenessí•œ RMì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤.</p> </li> <li> <p>ë…¼ë¬¸ì€ ì—¬ëŸ¬ ì‹¤í—˜ì„ í†µí•´ ì´ë¥¼ ë°˜ë°•í•˜ê³ ì í•¨.</p> </li> </ul> <p>â‡’ moderateí•œ accuracyë¥¼ ê°€ì§„ RMì´ the most accuracyë¥¼ ê°€ì§„ RMë³´ë‹¤ LM performanceë¥¼ ë” í–¥ìƒì‹œí‚¨ë‹¤.</p> <p>â‡’ RM accuracyë‘ LM final performanceëŠ” correlationì´ ì—†ë‹¤.</p> <h2 id="2-motivation-and-problem-setting--recap-rlhf-formula">2. Motivation and Problem Setting &amp; Recap RLHF Formula</h2> <h3 id="motivation">Motivation</h3> <ul> <li>LM performanceë¥¼ maximizeí•  ìˆ˜ ìˆëŠ” reward modelì˜ optimal accuracy rangeê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •</li> </ul> <h3 id="rlhf-formula">RLHF Formula</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>triplet: <code class="language-plaintext highlighter-rouge">(x, y_w, y_l)</code></p> <ul> <li> <p>accepted response score: <code class="language-plaintext highlighter-rouge">s_w = r_Î¸(x, y_w)</code></p> </li> <li> <p>rejected response score: <code class="language-plaintext highlighter-rouge">s_l = r_Î¸(x, y_l)</code></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="problem-setting">Problem Setting</h3> <ul> <li> <p>RM strengthê°€ LM performanceì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íƒìƒ‰</p> <ul> <li>metric</li> </ul> </li> <li> <p>factuality</p> </li> <li> <p>relevance</p> </li> <li> <p>completeness</p> </li> <li> <p>P<em>{LM} = f (S</em>{RM}, Ï„ )</p> <ul> <li> <p>P_{LM}: LM acc on task</p> </li> <li> <p>S_{LM}: RM acc on RM binary task</p> </li> <li> <p>Ï„: RL training time</p> </li> </ul> </li> </ul> <h2 id="3-experiment-and-results">3. Experiment and Results</h2> <h3 id="experimental-setting">Experimental Setting</h3> <ul> <li> <p>Models</p> <ul> <li> <p>LM: T5 (small, base, large)</p> </li> <li> <p>RM: Longformer-base-4096</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Datasets</p> <ul> <li>QA-FEEDBACK (3,853/500/948)</li> </ul> </li> </ul> <p>(Q, Gold, non-fact, â€¦)ê°€ ì¡´ì¬</p> <ul> <li> <p>Training</p> <ul> <li> <p>PPO</p> </li> <li> <p>RM list</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Critic LM: T5-base</p> </li> <li> <p>Reward hacking ë°©ì§€ë¥¼ ìœ„í•´ KLD(â†” ref LM)ê°€ ë„ˆë¬´ ì»¤ì§€ë©´ training interrupt (ì•½ê°„ì˜ íœ´ë¦¬ìŠ¤í‹±)</p> </li> </ul> <h3 id="results">Results</h3> <h3 id="are-high-accuracy-and-deeply-trained-reward-models-always-the-best">Are High-Accuracy and Deeply Trained Reward Models Always the Best?</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>moderate accuracyì™€ appropriate number of trained stepsë¡œ í•™ìŠµëœ RMì´ ë†’ì€ LM performanceë¡œ ì´ì–´ì§„ë‹¤.</p> <ul> <li> <p>relevance: mitigating the risk of overfitting</p> </li> <li> <p>factuality: prevent overfitting and ensure reliable outcomes</p> </li> </ul> </li> </ul> <p>consistent across the T5-base and T5-large models</p> <h3 id="how-do-best-and-most-accurate-reward-models-differ">How Do Best and Most Accurate Reward Models Differ?</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Relevance</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">high-variance</code></p> <ul> <li>Factuality</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">less-variance</code></p> <ul> <li>Completeness</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">low-score</code>, <code class="language-plaintext highlighter-rouge">high-variance</code></p> <p>â‡’ ê³µí†µì ìœ¼ë¡œ ëª¨ë“  taskì— ëŒ€í•´ì„œ best-performance RMì€ varianceê°€ ë†’ìŒ.</p> <p>ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´, ì´ ë§ì€ ê³§ RMì´ broader range of responsesì— ëŒ€í•œ í‰ê°€ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨ := exploration â‡’ improving the quality of the generated text</p> <p>(ê°œì¸ì ìœ¼ë¡œ, varianceê°€ í´ìˆ˜ ë°–ì— ì—†ëŠ” verifiable rewardê°€ ì´ë˜ì„œ LM performanceê°€ ì¢‹ì•˜ë‚˜..ë¼ëŠ” ìƒê°ì´ ë“¤ìŒ)</p> <h3 id="how-do-best-and-most-accurate-rewards-impact-models-ie-role-of-kld">How Do Best and Most Accurate Rewards Impact Models? (i.e., Role of KLD)</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Relevance</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">low-score</code>, <code class="language-plaintext highlighter-rouge">low-variance</code></p> <p>relevanceì¸¡ë©´ì—ì„œ stable alignment</p> <ul> <li>Factuality</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">high-score</code>, <code class="language-plaintext highlighter-rouge">low-variance</code></p> <p>factuality ì¸¡ë©´ì—ì„œ consistent yet varied alignment (refê°€ í‹€ë¦° ì‚¬ì‹¤ì„ ë§í•˜ê³  ìˆë‹¤ë©´)</p> <ul> <li>Completeness</li> </ul> <p>â†’ <code class="language-plaintext highlighter-rouge">high-score</code>,<code class="language-plaintext highlighter-rouge">high-variance</code></p> <p>flexible approach suitable for evaluating complex texts (ì´ê±´ í•´ì„ì´ ì¢€..)</p> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li>RMì„ í‰ê°€í• ë•Œ accuracy ìì²´ë¡œë§Œ í‰ê°€í•˜ëŠ” ê²ƒì˜ í•œê³„ë¥¼ ì‹¤í—˜ì ìœ¼ë¡œ ëª…í™•íˆ ë³´ì—¬ì¤€ ë…¼ë¬¸</li> </ul> <h1 id="what-makes-a-reward-model-a-good-teacher-an-optimization-perspective">What Makes a Reward Model a Good Teacher? An Optimization Perspective</h1> <h2 id="1-introduction-1">1. Introduction</h2> <ul> <li> <p>ì´ì „ ë…¼ë¬¸ì—ì„œ RL ê´€ì ì—ì„œ RMì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ accuracyë§Œìœ¼ë¡œ íŒë‹¨í•˜ëŠ” ê²ƒì˜ í•œê³„ë¥¼ ì§€ì í•˜ë©°, varianceì´ ë†’ì€ RMì´ ì˜¤íˆë ¤ ë” ë‚˜ì€ policy model performanceìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.</p> </li> <li> <p>ì´ ë…¼ë¬¸ì€ ì´ ë…¼ì˜ë¥¼ í™•ì¥í•´ ë‹¤ìŒì˜ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ í•˜ê³ ì í•¨.</p> </li> </ul> <p><strong><em>â€œwhat makes a reward model a good teacher for RLHF?â€</em></strong></p> <p>(ìˆ˜í•™ì ìœ¼ë¡œ ë§ì€ ì¦ëª…ë“¤ì´ ìˆì§€ë§Œ, ì°¨ì¹˜í•˜ê³  ë…¼ë¬¸ì—ì„œ ì´ì•¼ê¸°í•˜ê³  ì‹¶ì€ ë°”ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.)</p> <ul> <li> <p>\pi_{\theta} (policy)ì—ì„œ ì¶©ë¶„íˆ ë†’ì€ í™•ë¥ ë¡œ rolloutí•œ outputì— ëŒ€í•´ì„œ ì–¼ë§Œí¼ ì˜ êµ¬ë¶„í•˜ëŠ”ê°€ = reward variance</p> </li> <li> <p>r_G (ground truth reward: ìš°ë¦¬ê°€ ì˜¬ë ¤ì•¼í•˜ëŠ” reward)</p> </li> <li> <p>r_{rm} (proxy reward: policy modelì— ì˜í•´ í•™ìŠµë˜ëŠ” reward)</p> </li> </ul> <p>â‡’ low reward varianceëŠ” policy gradientë¡œ í•™ìŠµì‹œ r_{rm}ë¿ë§Œ ì•„ë‹ˆë¼ r_Gë„ êµ‰ì¥íˆ ëŠë¦¬ê²Œ updateí•˜ê²Œ ë§Œë“ ë‹¤.</p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="2-preliminaries">2. Preliminaries</h2> <p><strong>Reward model training or selection</strong></p> <ul> <li> <table> <tbody> <tr> <td>ìš°ë¦¬ì˜ ëª©í‘œ: E<em>{yâˆ¼Ï€</em>Î¸ (Â·</td> <td>x)}[r_G(x, y)]</td> </tr> </tbody> </table> </li> </ul> <p>maximize the exp. ground truth reward</p> <ul> <li>Proxy ëª©í‘œ: r_{RM} : X Ã— Y â†’ [âˆ’1, 1]</li> </ul> <p><strong>Reward maximization via policy gradient</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Accuracy in RM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>RMì—ì„œì˜ accuracyë€ r<em>Gë‘ r</em>{rm} ì´ ê°™ì€ rankingìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ìˆëƒ?ì´ë‹¤.</p> </li> <li> <p>ì¦‰,</p> </li> <li> <p>r_{G}: 0.9 (x1) &gt; 0.5 (x2) &gt; 0.2 (x3)</p> </li> <li> <p>r_{rm}: 0.54 (x1) &gt; 0.51 (x2) &gt; 0.49 (x3)</p> </li> </ul> <p>ì´ë©´ r_{rm}ì˜ accëŠ” 1.0ì´ë‹¤.</p> <ul> <li>ì¼ë°˜ì ìœ¼ë¡œ RMì˜ accëŠ” off-policy bencmark (e.g., HH test set)ì—ì„œ ì´ë£¨ì–´ì§. ê·¸ëŸ¬ë‚˜ RMì´ ì ìš©ë˜ëŠ” ì‹œì ì€ on-policy ì‹œì  â‡’ ë…¼ë¬¸ì€ ì´ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì„œ ë¶„ì„ì„ ì§„í–‰.</li> </ul> <p><strong>Reward Variance in RM</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>\pi_{\theta} (policy)í•˜ì—ì„œ ë°œìƒí•˜ëŠ” rolloutì„ RMì´ ì–¼ë§Œí¼ ì˜ êµ¬ë¶„í•˜ëŠ”ê°€?</li> </ul> <h2 id="3-theory-optimization-perspective-on-what-makes-a-good-reward-model">3. Theory: Optimization Perspective on What Makes a Good Reward Model</h2> <h3 id="technical-setting">Technical Setting</h3> <ul> <li>ë…¼ë¬¸ì—ì„œ ì¦ëª…ì„ ìœ„í•´ ì •ì˜í•œ policyì˜ generation ì‹</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="low-reward-variance-implies-slow-reward-maximization">Low Reward Variance Implies Slow Reward Maximization</h3> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>ì´ˆê¸° reward ëŒ€ë¹„ \gammaë§Œí¼ ê¸°ëŒ€ë³´ìƒì„ ì˜¬ë¦¬ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ tëŠ” reward varianceì˜ -1/3 ì œê³±ì— ë¹„ë¡€í•œë‹¤.</li> </ul> <p>(ë…¼ë¬¸ì— ì¦ëª…ìˆìŠµë‹ˆë‹¤!)</p> <ul> <li>reward variance ğŸ”½Â â†’ RLHF loss ê³ ì°¨ë¯¸ë¶„ ğŸ”½Â = hessian ğŸ”½Â â†’ gradient normì´ ì»¤ì§€ëŠ”ê±° ë°©ì§€ â†’ í•™ìŠµ ë°©í•´</li> </ul> <h3 id="more-accurate-reward-models-are-not-necessarily-better-teachers">More Accurate Reward Models Are Not Necessarily Better Teachers</h3> <ul> <li> <p>ì–´ë–¤ ì´ˆê¸° policy \pi_{\theta(0)}ì— ëŒ€í•´ì„œë„,</p> <ul> <li> <p>acc =1ì¸ ì™„ë²½í•œ ë³´ìƒ ëª¨ë¸ r_{\mathrm{RM}} ì¡´ì¬</p> </li> <li> <p>acc â‰¤2/âˆ£Yâˆ£ ì¸ ë§¤ìš° ë¶€ì •í™•í•œ ë³´ìƒ ëª¨ë¸ râ€™_{\mathrm{RM}} ì¡´ì¬</p> </li> </ul> </li> <li> <p>ê·¸ëŸ°ë° r<em>{\mathrm{RM}}ì„ ì“°ë©´ t</em>\gammaê°€ <strong>ë¬´í•œíˆ ì»¤ì§ˆ ìˆ˜ ìˆìŒ</strong> (í•™ìŠµì´ ê·¹ë„ë¡œ ëŠë¦¼)</p> </li> <li> <table> <tbody> <tr> <td>ë°˜ë©´ râ€™<em>{\mathrm{RM}}ì„ ì“°ë©´ t</em>\gamma = O(\pi_{\theta(0)}(y^\gamma</td> <td>x)^{-1})ë¡œ í›¨ì”¬ ì§§ì„ ìˆ˜ ìˆìŒ</td> </tr> </tbody> </table> </li> </ul> <p>(ì „ì¬ ì¡°ê±´ì€ ê·¸ë˜ë„ râ€™<em>{\mathrm{RM}}(x,y^\gamma) &gt; râ€™</em>{\mathrm{RM}}(x,y))</p> <h2 id="4-experiments">4. Experiments</h2> <ul> <li> <p>Ground truth reward.</p> </li> <li> <p>ArmoRMë¼ëŠ” ëª¨ë¸ì´ ì£¼ëŠ” rewardê°€ gt rewardë¼ê³  ê°€ì •</p> </li> <li> <p>Data.</p> </li> <li> <p>UltraFeedback (80: RM tr / 20: policy gradient)</p> </li> <li> <p>Ref.</p> </li> <li> <p>Pythia2.8B â†’ AlpacaFarm SFT</p> </li> <li> <p>Reward model</p> </li> <li> <p>On-Policy Data: 100%, 75%, 50%, 25%, 0% on-policy data samplingí•´ì„œ ArmoRMë¡œ labeling.</p> </li> <li> <p>Off-Policy Data: UltraFeedback</p> </li> <li> <p>Policy Gradient</p> </li> <li> <p>RLOO</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Reward varianceê°€ ë†’ì„ìˆ˜ë¡ proxy rewardëŠ” ë¹ ë¥´ê²Œ ì¦ê°€í•¨.</li> </ul> <p>í•˜ì§€ë§Œ, reward varianceê°€ ë†’ì€ RMì´ë¼ê³  í•˜ë”ë¼ë„ í•´ë‹¹ RMì˜ ë³¸ì§ˆì ì¸ ë¶ˆì•ˆì •ì„± (GTì˜ accë¥¼ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ì§€ëŠ” ëª»í•¨ = Reward Hacking)ì´ ìˆê¸°ì— Ground Truth rewardë§Œí¼ì„ True Rewardë¥¼ ëª»ì˜¬ë¦¼</p> <p>â‡’ Epoch3ê°€ Reward Hacking ì§€ì </p> <ul> <li> <p>Accuracyê°€ ë†’ê³  Reward varianceê°€ ë‚®ì€ RMì€ í•™ìŠµë„ ëŠë¦¬ë©° ì‹¤ì œ Ground Truth rewardë„ ê·¸ë ‡ê²Œ ë§ì´ ì˜¬ë¦¬ì§€ëŠ” ëª»í•¨</p> </li> <li> <p>ìš°ë¦¬ê°€ â€˜Ground Truth rewardâ€™ë¥¼ 100%ë¡œ ë°˜ì˜í•˜ëŠ” RMì„ ë°˜ì˜í•  ìˆ˜ ì—†ìœ¼ë‹ˆ ì‹¤ì œë¡œëŠ” reward varianceê°€ ë†’ì€ RMìœ¼ë¡œ on-policy trainingí•˜ê³  ëŠì–´ì£¼ëŠ”ê±´ ì¢‹ì€ RL optimizationì´ë‹¤.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Corrì„ ë³´ë©´, reward varianceëŠ” í•™ìŠµí•˜ê³ ì í•˜ëŠ” proxyë§ê³  ground truthë¥¼ targetí•˜ê¸°ì—ë„ ì¢‹ì€ feature</p> </li> <li> <p>ìš°ë¦¬ê°€ ìì£¼ ë´ì™”ë˜ off-policy acc (e.g., HH-test set)ì€ RLì‹œ RMì´ ë„ì›€ë˜ëŠ”ê°€?ì— ëŒ€í•œ ëŒ€ë‹µì´ ë˜ì§€ ëª»í•¨</p> </li> <li> <p>ë§ˆì§€ë§‰ ì§€í‘œëŠ” â€˜initial policyì— ëŒ€í•œ accuracyì™€ off-policy datasetì— ëŒ€í•œ accuracyë¥¼ reward varianceë¡œ í‰ê· ë‚¸ê±°â€™ë¼ëŠ”ë° í•´ì„ì€ ì˜ ëª»í–ˆìŠµë‹ˆë‹¤â€¦</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-08-12-the-accuracy-paradox-in-rlhf-when-better-reward/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>initial policyì˜ outputì— ëŒ€í•œ reward varianceë¥¼ ì¶©ë¶„íˆ í¬ê²Œ í•˜ëŠ” RMì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.</li> </ul> <h2 id="5-conclusion">5. Conclusion</h2> <ul> <li> <p>on-policy training ìƒí™©ì—ì„œ off-policy accuracyë§Œìœ¼ë¡œ RMì˜ effectivenessë¥¼ í‰ê°€í•˜ë©´ optimizationì— ì•…ì˜í–¥ì„ ë¼ì¹¨ì„ ë³´ì„</p> </li> <li> <p>reward varianceëŠ” RMì˜ optimizationë¥¼ ë¯¸ë¦¬ ê°€ëŠ í•´ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì§€í‘œ</p> </li> </ul> <p>â†’ ground truth rewardë¥¼ ë³´ì¥í•´ì£¼ì§€ëŠ” ì•ŠìŒ.</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>