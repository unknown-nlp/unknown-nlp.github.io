<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-tuning Vision-Language-Action Models: Optimizing Speed and Success | You R. Name </title> <meta name="author" content="You R. Name"> <meta name="description" content=" ë…¼ë¬¸ ë¦¬ë·° - Fine-tuning Vision-Language-Action Models: Optimizing Speed and Success"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/al-folio/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/al-folio/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/al-folio/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/al-folio/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alshedivat.github.io/al-folio/blog/2025/fine-tuning-vision-language-action-models-optimizing-speed-and-success/"> <script src="/al-folio/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/al-folio/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/al-folio/"> <span class="font-weight-bold">You</span> R. Name </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/al-folio/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/al-folio/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/al-folio/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/al-folio/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/al-folio/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-tuning Vision-Language-Action Models: Optimizing Speed and Success</h1> <p class="post-meta"> Created on April 22, 2025 </p> <p class="post-tags"> <a href="/al-folio/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/al-folio/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  Â· Â  <a href="/al-folio/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2025-04-22</li> <li> <strong>Reviewer</strong>: ì „ë¯¼ì§„</li> </ul> <p>ğŸ’¡ ê¸°ì¡´ openVLAì˜ í•œê³„(1. low frequency 2. bi-manual ë¶ˆê°€ 3. action discrete representation)ì„ í•´ì†Œí•˜ëŠ” FT ë°©ë²•ë¡  ì†Œê°œ</p> <h2 id="motivation">Motivation</h2> <ul> <li> <p>ê¸°ì¡´ openVLAëŠ” generalizationì—ëŠ” ì„±ê³µì (language following ability, semantic generalization)ì´ì—ˆì§€ë§Œ, ëª…ë°±í•œ í•œê³„ê°€ ì¡´ì¬</p> <ul> <li> <p>auto-regressive generation â‡’ low frequency</p> </li> <li> <p>FTí•´ë„ bi-manual ì„±ëŠ¥ì´ êµ¬ë¦¼</p> </li> </ul> </li> <li> <p>ìµœê·¼ì— ìƒì„± ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ì—°êµ¬ê°€ ì—¬ëŸ¬ê°€ì§€ ì†Œê°œë˜ì—ˆìœ¼ë‚˜, bi-manualì— ì·¨ì•½í•¨ â‡’ ì†ë„ë„ ë¹ ë¥´ë©´ì„œ ë§Œì¡±í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë°©ë²•ì€ ì—†ìŒ</p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” OpenVLAë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ íŒŒì¸íŠœë‹ ë ˆì‹œí”¼ Optimized Fine-Tuning(OFT) recipeë¥¼ íƒêµ¬</p> <ul> <li>different action decoding schemes, action representations, learning objectivesì— ëŒ€í•´ ë¶„ì„</li> </ul> </li> <li> <p>ì—¬ëŸ¬ ë””ìì¸ ì´ˆì´ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenVLA-OFTë¥¼ ì œì•ˆ, LIBERO simulation benchmarkì—ì„œ OpenVLAì˜ ì„±ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ ë†’ì´ë©´ì„œë„ 26x generation throughputë¥¼ ë‹¬ì„±</p> <ul> <li>real-world evaluationì—ì„œë„ OpenVLAë¥¼ bimanual ALOHA robot settingì— ë§ì¶°ì„œ í•™ìŠµ, ë‹¤ë¥¸ VLAë³´ë‹¤ í›¨ì”¬ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„</li> </ul> </li> </ul> <h2 id="introduction">Introduction</h2> <ul> <li> <p>ìµœê·¼ VLA ëª¨ë¸ì€ low-level robotic controlì„ ìœ„í•´ ëŒ€ëŸ‰ì˜ robot datasetì— pretrained VLMì„ í•™ìŠµí•´ êµ¬ì¶•, ë‹¤ì–‘í•œ ë¡œë´‡ê³¼ íƒœìŠ¤í¬ì—ì„œ ë†’ì€ ì„±ëŠ¥ê³¼ semantic generalization, language following abilityë¥¼ ë³´ì„</p> </li> <li> <p>fine-tuningì€ ìƒˆë¡œìš´ ë¡œë´‡ê³¼ íƒœìŠ¤í¬ì— ëŒ€í•´ VLAì˜ ë†’ì€ ì„±ëŠ¥ì„ ìœ„í•´ í•„ìˆ˜ì ì´ì§€ë§Œ, ì•„ì§ adapationì„ ìœ„í•´ ì–´ë–¤ ë””ìì¸ì´ ê°€ì¥ íš¨ê³¼ì ì¸ì§€ì— ëŒ€í•œ íƒêµ¬ëŠ” ë¶€ì¡±</p> <ul> <li>action headë¡œ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•  ê²ƒì¸ì§€, next token prediction lossê°€ ìµœì ì¸ì§€ ë“±ë“±</li> </ul> </li> <li> <p>ì´ì „ OpenVLA ë…¼ë¬¸ì—ì„œ LoRAë¥¼ í™œìš©í•œ FT adaptation strategyë¥¼ ì†Œê°œí•˜ì˜€ìœ¼ë‚˜, ëª‡ê°€ì§€ í•œê³„ê°€ ì¡´ì¬</p> <ul> <li> <p>autoregressive action generationì€ high-frequency control(25-50+Hz)ë¥¼ í•˜ê¸°ì—” ë„ˆë¬´ ëŠë¦¼(3-5Hz)</p> <ul> <li>hzëŠ” 1ì´ˆë‹¹ action íšŸìˆ˜</li> </ul> </li> <li> <p>LoRAì™€ full FT ëª¨ë‘ bi-manual manipulation taskì—ì„œ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ìš´ ì„±ëŠ¥ì„ ëƒ„</p> </li> </ul> </li> <li> <p>ìµœê·¼ì— ë” ì¢‹ì€ action tokenization schemesë¥¼ í†µí•´ì„œ efficiencyë¥¼ ë†’ì´ëŠ”(ì†ë„ë¥¼ 2ë°°ì—ì„œ 13ë°° ì •ë„ ë†’ì„) ì—°êµ¬ë“¤ì´ ì œì•ˆë˜ì—ˆìœ¼ë‚˜, action chunkì‚¬ì´ì— ìƒë‹¹í•œ latencyê°€ ìˆì–´(ìµœê·¼ ê°€ì¥ ë¹ ë¥¸ ë°©ë²• ê¸°ì¤€ 750ms) ì•„ì§ high-frequency bimanual robotì— ì ìš©í•˜ê¸°ì—” í•œê³„ê°€ ì¡´ì¬</p> <p>â‡’ ì†ë„ë„ ë¹ ë¥´ê³  ë§Œì¡±í•  ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ëŠ” ë°©ë²•ë¡ ì€ ì•„ì§ ë¶€ì¬</p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„  ì´ëŸ¬í•œ ì¸ì‚¬ì´íŠ¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ OFT recipeì˜ ì´ˆì•ˆ, OpenVLA-OFTë¥¼ ì œì•ˆ</p> <ul> <li> <p>parallel decodingê³¼ action chunkingì„ í†µí•©</p> </li> <li> <p>continuous action representation</p> </li> <li> <p>L1 regression objective</p> </li> </ul> </li> <li> <p>LIBERO simulation benchmarkì™€ real bimanul ALOHA robotìœ¼ë¡œ ì‹¤í—˜</p> <ul> <li> <p>LIBEROì—ì„œëŠ” 8 action chunkì‚¬ìš©, 26xë¹ ë¥¸ ì†ë„ë¡œ í‰ê·  97.1%ì˜ ì„±ëŠ¥ ë‹¬ì„±</p> </li> <li> <p>ALOHA taskì—ì„œëŠ” enchanced language groudingì„ ìœ„í•´ FiLMì„ ë ˆì‹œí”¼ì— ì¶”ê°€, í•´ë‹¹ ëª¨ë¸ì„ OFT+ë¼ ëª…ì‹œ</p> <ul> <li> <p>ì˜·ì ‘ê¸°, íƒ€ê²Ÿ ìŒì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¡°ì‘í•˜ëŠ” íƒœìŠ¤í¬ ë“±ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„</p> </li> <li> <p>25-timestep action chunckë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenVLAë³´ë‹¤ 43ë°° ë†’ì€ throughputë¥¼ ë³´ì„</p> </li> </ul> </li> </ul> </li> </ul> <h2 id="related-work">Related Work</h2> <ul> <li> <p>ì´ì „ì—” languageì™€ vision foundationëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ robotic capabilitiesë¥¼ ë†’ì´ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰</p> <ul> <li> <p>robotic policy learningì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ pretrained visual representationì„ ì‚¬ìš©</p> <ul> <li>robotic taskì—ì„œ object localizationì´ë‚˜ high-level planningê³¼ reasoning ë“±</li> </ul> </li> <li> <p>ë” ìµœê·¼ì—ëŠ” VLMì´ ë°”ë¡œ low-level robotic control actionì„ ì˜ˆì¸¡í•˜ë„ë¡ í•˜ëŠ” VLA ëª¨ë¸ì„ ë§Œë“œëŠ” ì—°êµ¬ë“¤ì´ ì œì•ˆë¨</p> <ul> <li> <p>íš¨ê³¼ì ì¸ OOD test conditionê³¼ unseen semantic conceptì— ëŒ€í•œ generalizationì„ ë³´ì„</p> </li> <li> <p>ë³´í†µ ëª¨ë¸ ê°œë°œì— ì´ˆì ì„ ë‘ê³  ì—°êµ¬</p> </li> </ul> <p>â‡’ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ëª¨ë¸ì„ FTí•  ë•Œì˜ ê°œë°œ ë ˆì‹œí”¼ì— ì´ˆì ì„ ì¤Œ</p> </li> </ul> </li> <li> <p>VLA ëª¨ë¸ ê°œë°œì‹œ FTì˜ ì—­í• ì´ ì¤‘ìš”í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , íš¨ê³¼ì ì¸ ë ˆì‹œí”¼ íƒêµ¬ëŠ” ë¶€ì¡±í–ˆìŒ</p> <ul> <li> <p>ì´ì „ì—ëŠ” full FTì™€ FT with LoRA ì •ë„</p> </li> <li> <p>í•˜ì§€ë§Œ ì´ì¡°ì°¨ë„ single-armì— í•œì •ì , low control frequenciesë¡œ bimanual robotì— í™•ì¥ì€ ë¶ˆê°€ëŠ¥</p> </li> </ul> </li> <li> <p>ìµœê·¼ì˜ ì—°êµ¬ì—ì„œëŠ” VLA íš¨ìœ¨ì„±ì„ ìœ„í•´ ìƒˆë¡œìš´ action tokenization ë°©ë²•ì´ ê³ ì•ˆë¨</p> <ul> <li> <p>vector quantization ì‚¬ìš©</p> </li> <li> <p>discrete cosine transform-based compression to represent action chunk with fewer tokens than simple per-dimension binning(RT-2ì™€ OpenVLAì—ì„œ ì‚¬ìš©ë˜ë˜ê±°)</p> </li> <li> <p>ì´ëŸ¬í•œ ë°©ë²•ë¡ ì€ ê¸°ì¡´ autoregressive VLAì— ëŒ€í•´ 2ë°°ì—ì„œ 13ë°° ì •ë„ì˜ ì†ë„ í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆìœ¼ë‚˜, iterative generation íŠ¹ì„± ë•Œë§¤ í•œì •ì </p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì˜ parallel decodingì€ action chunkì™€ í•¨ê»˜ ì‚¬ìš©í•  ê²½ìš° 26ë°°ì—ì„œ 43ë°°ì˜ throughputê³¼ í›¨ì”¬ ë‚®ì€ latency(0.07ms for single-arm task with one input image &amp; 0.321ms for bimanual tasks with three input images)ë¥¼ ë³´ì—¬ì¤Œ</p> </li> </ul> </li> <li> <p>ë‹¤ë¥¸ ì—°êµ¬ ë¼ì¸ì—ì„œëŠ” high-frequency, bimanual manipulationì„ ìœ„í•œ íš¨ê³¼ì ì¸ VLA FTë¥¼ ìœ„í•´ì„œ diffusion ì´ë‚˜ flow matchingì„ ì‚¬ìš©</p> <ul> <li> <p>diffusion ê¸°ë°˜ VLAëŠ” multi-step action chunksê°€ ë™ì‹œì— ê°€ëŠ¥í•´ throughputì€ ë†’ì§€ë§Œ í•™ìŠµ ì†ë„ëŠ” ëŠë¦¼</p> <ul> <li>ê°ì ëª¨ë¸ ë””ìì¸ì´ ìƒì´í•´ì„œ ì–´ë–¤ ìš”ì†Œê°€ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ëŠ” í™•ì¸í•˜ê¸° ì–´ë ¤ì›€</li> </ul> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í†µì œëœ ì„¸íŒ…ì—ì„œ ì‹¤í—˜, ì–´ë–¤ ìš”ì†Œê°€ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ë¼ì¹˜ëŠ”ì§€ë„ ë¶„ì„</p> </li> </ul> </li> </ul> <h2 id="preliminaries">Preliminaries</h2> <h3 id="original-openvla-formulation">Original OpenVLA formulation</h3> <ul> <li> <p>7B manipulation policy, Prismatic VLMì„ OXEì˜ 1M episodeì— FTí•´ì„œ ë§Œë“  ëª¨ë¸</p> </li> <li> <p>autoregressive predictionì„ í™œìš©, ê° timestepë§ˆë‹¤ 7 discrete robot action tokenì„ ìƒì„±</p> <ul> <li> <p>3ê°œëŠ” position control, 3ê°œëŠ” orientation control, 1ê°œëŠ” gripper control</p> </li> <li> <p>cross-entropy lossë¡œ Next-token predictionë°©ì‹ìœ¼ë¡œ í•™ìŠµ (ì–¸ì–´ëª¨ë¸ í•˜ë˜ê±° ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´)</p> </li> </ul> </li> </ul> <h3 id="action-chunking">Action chunking</h3> <ul> <li> <p>ì´ì „ ì—°êµ¬ëŠ” action chunking(ì¤‘ê°„ replanningì—†ì´ future action sequenceë¥¼ ì˜ˆì¸¡, ì‹¤í–‰)ì´ ì—¬ëŸ¬ manipulation taskì—ì„œ policy ì„±ê³µë¥ ì„ ë†’ì¸ë‹¤ëŠ” ê²ƒì„ ë³´ì„</p> </li> <li> <p>ê·¸ëŸ¬ë‚˜, OpenVLAì˜ autoregressive generation schemeì—ì„œëŠ” action chunkingì´ ì–´ë ¤ì›€</p> <ul> <li>single timestep actionë¥¼ ìƒì„±í•˜ëŠ”ë° 0.33ì´ˆê°€ ê±¸ë¦¼(A100ê¸°ì¤€)</li> </ul> </li> <li> <p>chunk sizeë¥¼ K, action dimensionalityë¥¼ Dë¼ê³  í•˜ë©´, OpenVLAëŠ” KDë²ˆ forwardë¥¼ í•´ì•¼í•¨</p> </li> </ul> <h2 id="proposed-method">Proposed Method</h2> <h3 id="studying-key-vla-fine-tuning-design-decisions">Studying key VLA Fine-Tuning Design Decisions</h3> <ul> <li> <p>ê¸°ì¡´ OpenVLAì˜ í•œê³„ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ì„œ ë‹¤ìŒ 3ê°€ì§€ ë””ìì¸ì— ëŒ€í•´ ì¡°ì‚¬</p> <ul> <li> <p>Action generation strategy</p> <ul> <li>ê¸°ì¡´ì—” action dimensionì´ 7ì´ë¼ê³  í•˜ë©´, í•˜ë‚˜ì˜ ì•¡ì…˜ì„ ìƒì„±í•˜ê¸° ìœ„í•´ 7ë²ˆ forward â‡’ low frequency</li> </ul> </li> <li> <p>Action representation</p> <ul> <li> <p>ê¸°ì¡´ì—” ì•¡ì…˜ì„ í† í°ìœ¼ë¡œ ìƒì„±, ì•¡ì…˜ì˜ ë²”ìœ„ë¥¼ 256ê°œë¡œ binning &amp; normalization, ê°ê°ì„ í† í°ìœ¼ë¡œ ì·¨ê¸‰</p> </li> <li> <p>ì¦‰, íŠ¹ì • ì•¡ì…˜ í† í°ì„ ìƒì„±í•˜ë©´ ê·¸ê±¸ ë‹¤ì‹œ ì •í•´ì§„ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ continous valueë¡œ mapping â‡’ precisionì´ ë–¨ì–´ì§ˆ ìˆ˜ë°–ì— ì—†ìŒ</p> </li> </ul> </li> <li> <p>Learning objective</p> <ul> <li>next-token predictionìœ¼ë¡œ íŒŒì¸íŠœë‹</li> </ul> </li> </ul> <p>â‡’ ë³¸ ì—°êµ¬ì—ì„œëŠ” OpenVLAë¥¼ base modelë¡œ ì‹¤í—˜, OpenVLAë¥¼ 500ê°œì˜ demonstrationì— ëŒ€í•´ LoRA FTë¡œ adaptation</p> </li> </ul> <h3 id="implementing-alternative-design-components">Implementing Alternative Design Components</h3> <ul> <li> <p>Parallel decoding with action chunking</p> <ul> <li> <p>ì—¬ëŸ¬ action sequenceë¥¼ í•œë²ˆì˜ forwardë¡œ ìƒì„±í•˜ê¸° ìœ„í•´ì„œ, ê¸°ì¡´ì˜ auto-regressiveë°©ì‹ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©</p> </li> <li> <p>ëª¨ë¸ì´ empty action embeddingì„ input tokenìœ¼ë¡œ ë°›ë„ë¡ ìˆ˜ì •, caual attention maskë¥¼ bi-directional attentionìœ¼ë¡œ ë³€ê²½, decoderê°€ ëª¨ë“  actionì„ ë™ì‹œì— predictioní•˜ë„ë¡ í•¨</p> <p>â‡’ ì´ ë°©ì‹ìœ¼ë¡œ D(action dimension) sequential passë¥¼ í•œë²ˆì˜ single passë¡œ ì¤„ì¼ ìˆ˜ ìˆìŒ</p> </li> <li> <p>Parallel decodingì—ì„œ action chunkingìœ¼ë¡œ í™•ì¥í•˜ëŠ”ê±°ëŠ” ì‰¬ì›€</p> <ul> <li> <p>empty action embeddingì„ decoderì˜ inputì— ì¶”ê°€, ì´ëŠ” future actionì˜ chunkë¡œ mappingë¨</p> </li> <li> <p>chunk size Kì— ëŒ€í•´ì„œ, ëª¨ë¸ì€ ì´ì œ KD actionsì„ í•œë²ˆì˜ forwardë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ , ìµœì†Œí•œì˜ latencyë¡œ thoughput K-foldë¥¼ ì¦ê°€</p> <ul> <li> <p>thoughput K-fold: Kì˜ rolloutì„ ë³‘ë ¬ë¡œ ìƒì„±í–ˆì„ ë•Œì˜ ì²˜ë¦¬ëŸ‰</p> <ul> <li>kë²ˆì— ê±¸ì³ action sequenceë¥¼ ìƒì„±, í‰ê°€í•´ ê°€ì¥ ì¢‹ì€ê±¸ ê³ ë¥´ëŠ” ë°©ë²•ë¡ (?)</li> </ul> </li> </ul> </li> <li> <p>parallel decodingì€ autoregressive approachì— ë¹„í•´ì„  ì´ë¡ ì ìœ¼ë¡œ ëœ expressiveí•˜ì§€ë§Œ ì—¬ëŸ¬ íƒœìŠ¤í¬ì— ì‹¤í—˜ ê²°ê³¼, ì„±ëŠ¥ í•˜ë½ì„ ë³´ì´ì§„ ì•ŠìŒ</p> </li> </ul> </li> </ul> </li> <li> <p>Continuous action space</p> <ul> <li> <p>OpenVLAëŠ” ì›ë˜ ê° action dimensionì´ [-1,+1]ë¡œ normalizedë˜ê³ , 256 binìœ¼ë¡œ uniformí•˜ê²Œ discretizedëœ discrete action tokenì„ ì‚¬ìš©</p> <ul> <li>ì´ ë°©ë²•ë¡ ì€ ê¸°ì¡´ì˜ VLMì„ ìˆ˜ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œëŠ” í¸ë¦¬í•˜ì§€ë§Œ, fine-grained action detailì„ ë–¨ì–´ëœ¨ë¦¼</li> </ul> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” í¬ê²Œ 2ê°€ì§€ ë°©ì‹ìœ¼ë¡œ continuous action spaceë¥¼ êµ¬í˜„</p> <ul> <li> <dl> <dt>1ì•ˆ) L1 regression êµ¬í˜„</dt> <dd> <p>LLMì˜ ëë‹¨ì— ìˆë˜ output embedding layerë¥¼ ë¹¼ê³ , ìƒˆë¡œ 4-layer MLP action headë¥¼ ì¶”ê°€, action tokenì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì •í™•í•œ action ê°’ ìì²´ë¥¼ ìƒì„±í•˜ë„ë¡ í•¨</p> </dd> </dl> </li> <li> <dl> <dt>2ì•ˆ) conditional denoising diffusion modeling</dt> <dd> <p>ëª¨ë¸ì´ forward diffusionì—ì„œ action sampleì— ì¶”ê°€ëœ noiseë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ, inferenceì‹œ noisy action sampleì„ ì ì°¨ denoisingí•˜ë©´ì„œ real actionì„ ì˜ˆì¸¡</p> </dd> </dl> <ul> <li> <p>ì´ ë°©ì‹ì€ ë” expressive action modelingì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì§€ë§Œ, inferenceì‹œ ì—¬ëŸ¬ë²ˆì˜ forward passê°€ í•„ìš”(50 diffusion steps in our implementation)</p> <ul> <li>Parallel decodingì„ ì‚¬ìš©í•´ì„œ latencyê°€ ì¢€ ë” ì»¤ì§</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Additional model inputs and outputs</p> <ul> <li> <p>orignal OpenVLAì—ì„œëŠ” single camera viewë§Œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í–ˆìœ¼ë‚˜, ëª‡ëª‡ ë¡œë´‡ ì„¸íŒ…ì—ì„œëŠ” ì—¬ëŸ¬ viewpointì™€ ì¶”ê°€ì ì¸ robot state informationì´ í¬í•¨ë¨</p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” flexible input processing pipelineì„ êµ¬í˜„</p> <ul> <li> <p>camera imageëŠ” OpenVLAì™€ ê°™ì€ dual vision encoderë¥¼ í†µí•´ viewë§ˆë‹¤ 256ê°œì˜ patch embeddingìœ¼ë¡œ ì¶”ì¶œ, language embedding spaceìœ¼ë¡œ projection</p> </li> <li> <p>low-dimensional robot state inputì— ëŒ€í•´ì„œëŠ” separate projection networkë¥¼ ì‚¬ìš©í•´ì„œ ê°™ì€ embedding spaceë¡œ mapping, í•˜ë‚˜ì˜ input embeddingìœ¼ë¡œ ë³€í™˜í•´ ì‚¬ìš©</p> </li> </ul> </li> <li> <p>ëª¨ë“  input embedding(visual features, robot state, language token)ì„ concatí•´ì„œ decoderì— ë“¤ì–´ê°</p> <p>â‡’ ì´ëŸ¬í•œ unified latent representationì€ ëª¨ë¸ì´ actionì„ ìƒì„±í•  ë•Œ, ëª¨ë“  ì´ìš©ê°€ëŠ¥í•œ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•¨</p> </li> </ul> </li> </ul> <h3 id="augmenting-openvla-oft-with-film-for-enchanced-language-grounding">Augmenting OpenVLA-OFT with FiLM for Enchanced Language Grounding</h3> <ul> <li> <p>bimanualì—ì„œëŠ” input imageê°€ 3ê°œ(3rd person view, wrist-mounted cameras)</p> <p>â‡’ visual inputê°„ì˜ spurious correlationë•Œë¬¸ì— language followingì— ì–´ë ¤ì›€ì„ ê²ªìŒ</p> </li> <li> <p>ê·¸ë˜ì„œ bi-manualì—ì„œëŠ” language following abilityí–¥ìƒì„ ìœ„í•´ì„œ ViTì— FiLMì„ ì¶”ê°€</p> <ul> <li> <p>ê° image patchë§ˆë‹¤ ë‹¤ë¥¸ scaling, shift factorë¥¼ ì ìš©í–ˆë”ë‹ˆ langauge followingì´ ì•ˆì¢‹ì•„ì„œ ê°™ì€ scaling, shift factorë¥¼ ì‚¬ìš©í•œë‹¤ê³  í•¨</p> <p>â‡’ ëª¨ë“  patchì— ê°™ì€ $ \gamma,\beta $ê°€ ì ìš©ë˜ì§€ë§Œ ê° ì°¨ì›ë³„ë¡œëŠ” ë‹¤ë¥¸ scalingê³¼ shiftê°€ ì ìš©</p> <ul> <li> <p>scaling, shift factorì˜ í¬ê¸°ëŠ” D_ViT</p> <ul> <li>D_ViTëŠ” visual patch embeddingì—ì„œ hidden dimensionì˜ í¬ê¸°</li> </ul> </li> </ul> </li> <li> <p>FiLMì€ ê° ViT blockì—ì„œ self-attention layerì™€ FFNì‚¬ì´ì— ì ìš©, ê° ë¸”ë¡ë§ˆë‹¤ ë‹¤ë¥¸ proejctorì‚¬ìš©</p> </li> <li> <p>FiLMì€ ALOHA experimentì—ì„œë§Œ ì‚¬ìš©!</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>## Experiment
</code></pre></div> </div> </li> </ul> </li> </ul> <h3 id="research-question">Research Question</h3> <p>RQ1. How does each design decision affect the fine-tuned policyâ€™s success rate on downstream tasks?</p> <p>RQ2. How does each design decisions affect model inference efficiency (action generation throughput and latency)?</p> <p>RQ3. How do alternative fine-tuning formulations affect flexibility in model input-output specifications?</p> <h3 id="libero">LIBERO</h3> <ul> <li> <p>Experiment setup</p> <ul> <li> <p>Franka Emika Panda armê¸°ë°˜ì˜ simulator, camera images, robot state, task annotation, and delta end-effector pose actionsì´ í¬í•¨ë˜ì–´ ìˆìŒ</p> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 4ê°€ì§€ taskë¥¼ ì‚¬ìš© - LIBERO-Spatial, Object, Goal, Long</p> <ul> <li> <p>policy generalizationì„ í‰ê°€í•˜ê¸° ìœ„í•´ ê°ê° 500ê°œì˜ expert demonstrationì„ ì œê³µ</p> <ul> <li>OpenVLAì—ì„œëŠ” unsucessful demonstrationì„ í•„í„°ë§í•˜ê³  ê° taskë§ˆë‹¤ ë”°ë¡œ LoRAë¡œ FT</li> </ul> </li> </ul> </li> <li> <p>ë³¸ ë…¼ë¬¸ì—ì„œëŠ” non-diffulsion methodëŠ” 50-150K gradient stepí•™ìŠµ, diffusion methodëŠ” 100-250K gradient stepì„ í•™ìŠµ</p> <ul> <li>batch size 64-128, A100/H100 8ëŒ€ ì‚¬ìš©</li> </ul> </li> <li> <p>íŠ¹ë³„í•œ ì–¸ê¸‰ì´ ì—†ìœ¼ë©´, policyëŠ” í•˜ë‚˜ì˜ third-person imageì™€ language instructionì„ inputìœ¼ë¡œ ë°›ìŒ</p> </li> <li> <p>action chunk size K=8ë¡œ ì„¤ì •, replanningì „ì— full chunk ì‹¤í–‰</p> </li> </ul> </li> <li> <p>Task performance comparisons</p> <ul> <li> <p>ìš°ì„  LIBERO benchmakrì— ì˜í–¥ì„ ë¼ì¹˜ëŠ” design decisionì— ëŒ€í•´ í‰ê°€</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼ PD(parallel decoding)ê³¼ AC(action chunking)ë¥¼ ë™ì‹œì— ì¨ì•¼ high-frequency controlì´ ê°€ëŠ¥í–ˆìŒ.</p> </li> <li> <p>ì‹¤í—˜ ê²°ê³¼, PD&amp;ACëŠ” ì“°ëŠ” ê²ƒì´ ë” ì¢‹ì•˜ê³ , Cont-L1ê³¼ diffusionì„ ë¹„êµí•˜ë©´, ì„±ëŠ¥ì´ ìœ ì‚¬í•˜ë‚˜ latencyì¸¡ë©´ì—ì„œ Cont-L1ì´ ë” ì¢‹ìŒ(ì•„ë˜ ë‚˜ì˜´)</p> <ul> <li>íŠ¹íˆ PD&amp;ACì˜ íš¨ê³¼ëŠ” LIBERO-Longì—ì„œ ê°€ì¥ ë›°ì–´ë‚¨</li> </ul> </li> <li> <p>Inference Efficiency comparisons</p> </li> <li> <p>í•˜ë‚˜ì˜ robot action í˜¹ì€ action chunkë¥¼ ìƒì„±í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„ì„ ì¸¡ì •</p> </li> <li> <p>Cont-L1ì˜ ê²½ìš° ê·¸ëƒ¥ PD&amp;ACì™€ ê±°ì˜ ì°¨ì´ê°€ ì•ˆë‚¨(MLP action headëŠ” ìµœì†Œí•œì˜ computational costê°€ ì¶”ê°€ë¨)</p> </li> <li> <p>diffusionì˜ ê²½ìš° denoising processê°€ ìˆì–´ ì‹œê°„ì´ ì¢€ ë” ê±¸ë¦¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ê¸°ì¡´ OpenVLAë³´ë‹¤ëŠ” 2-3ë°° ëŠë¦¼</p> </li> <li> <p>Model input-output flexibility</p> </li> <li> <p>ìœ„ì˜ í‘œ(table2)ë¥¼ ë³´ë©´ additional inputì„ ì¶”ê°€í•´ë„ ê¸°ì¡´ ëª¨ë¸ë³´ë‹¨ í›¨ì”¬ ë¹ ë¦„</p> </li> <li> <p>table 1ì„ ë³´ë©´ additional inputì„ ë„£ì—ˆì„ ë•Œ, ëª¨ë“  ëª¨ë¸ë“¤ì´ ì „ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ í–¥ìƒë˜ì—ˆìœ¼ë‚˜, ê° ëª¨ë¸ì˜ ë³µì¡í•œ í•™ìŠµ êµ¬ì¡°($ \pi_0 $ì€ MDT(multi-modal diffusion transformer)ì™€ flow-matching ì‚¬ìš©)ë¥¼ ê³ ë ¤í•´ë´¤ì„ ë•Œ, openvla-oftê°€ êµ‰ì¥íˆ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì„</p> </li> </ul> </li> <li> <p>Optimized Fine-tuning recipe</p> <ul> <li>ìœ„ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë´¤ì„ ë•Œ, ìµœì ì˜ design choiceëŠ” ë‹¤ìŒê³¼ ê°™ìŒ</li> </ul> <ol> <li> <p>parallel decoding with action chunking</p> </li> <li> <p>continuous action representation</p> </li> <li> <p>L1 regression objective</p> </li> </ol> <p>â‡’ ì´ì œ ì´ êµ¬ì¡°ì˜ ëª¨ë¸ì„ OpenVLA-OFTë¼ ëª…ëª…</p> </li> <li> <p>Additional experiments</p> <ul> <li> <p>FT formulationì„ ë°”ê¾¸ë©´ì„œ, base VLA pretrainê³¼ finetuningì‚¬ì´ì— í° distribution shiftê°€ ì¼ì–´ë‚¬ì„ ìˆ˜ë°–ì— ì—†ìŒ</p> <ul> <li> <p>ì •ë§ base VLAì˜ ì§€ì‹ì´ ë„ì›€ì´ ëì„ê¹Œ?</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>	- í¬ì§„ ì•Šì§€ë§Œ ë„ì›€ì´ ë¨!
</code></pre></div> </div> </li> </ul> </li> </ul> </li> </ul> <h3 id="real-world-aloha-robot">Real-World ALOHA Robot</h3> <dl> <dt>: ì´ì „ê¹Œì§€ëŠ” simulationì—ì„œì˜ ì„±ëŠ¥ì„ ë´¤ë‹¤ë©´, ì´ë²ˆ íŒŒíŠ¸ëŠ” ì‹¤ì œ ë¡œë´‡ì— ì ìš©í•´ë´¤ì„ ë•Œì˜ ì„±ëŠ¥</dt> <dd> <p>OpenVLAì˜ ê²½ìš° pretrainingë•Œ bimanual dataë¥¼ ë³¸ ì ì´ ì—†ìŒ</p> </dd> <dd> <p>ì—¬ê¸°ì„œëŠ” OpenVLA-OFTì— FiLMì„ ì¶”ê°€í•œ OpenVLA-OFT+ë¡œ ì„±ëŠ¥ í‰ê°€</p> </dd> </dl> <ul> <li> <p>Setup</p> <ul> <li> <p>ALOHA platformì€ 2ëŒ€ì˜ ViperX 300 S arms, 3ëŒ€ì˜ ì¹´ë©”ë¼ viewpoint(í•˜ë‚˜ëŠ” top-down, ë‘ëŒ€ëŠ” wrist-mounted), robot state input(14 dimensional joint angles)</p> </li> <li> <p>25Hzë¡œ ì‘ë™, ê° actionì€ target absolute joint angleë¥¼ í‘œí˜„</p> </li> </ul> <p>â‡’ ì´ ì„¸íŒ…ì€ OpenVLA pretrainingë•Œì™€ ë§¤ìš° ë‹¤ë¦„</p> <ul> <li> <p>í¬ê²Œ 4ê°œì˜ taskë¥¼ ì„ íƒ</p> <ul> <li> <p>fold shorts : í° ë°˜ë°”ì§€ ì ‘ê¸°, 20ê°œì˜ demonìœ¼ë¡œ í•™ìŠµ, í‰ê°€ëŠ” 10ë²ˆ</p> </li> <li> <p>fold shirt : í° í‹° ì ‘ê¸°, 30ê°œì˜ demonìœ¼ë¡œ í•™ìŠµ, í‰ê°€ëŠ” 10ë²ˆ</p> </li> <li> <p>scoop X into bowl : ì™¼ì†ìœ¼ë¡œ tableì˜ ì¤‘ì•™ìœ¼ë¡œ ê·¸ë¦‡ì„ ì›€ì§ì´ê³ , ì˜¤ë¥¸ì†ìœ¼ë¡œ íŠ¹ì • ì¬ë£Œë¥¼ í¼ì„œ ë‹´ìŒ, 45ê°œ demon(ì¬ë£Œë‹¹ 15ê°œ)ìœ¼ë¡œ í•™ìŠµ, 12ë²ˆ(ì¬ë£Œë‹¹ 4ë²ˆ) í‰ê°€</p> </li> <li> <p>put X into pot : ì™¼ì†ìœ¼ë¡œ ëƒ„ë¹„ ëšœê»‘ ì—´ì–´ì„œ ì˜¤ë¥¸ì†ìœ¼ë¡œ íŠ¹ì • ì•„ì´í…œì„ ë„£ìŒ, 300 demonìœ¼ë¡œ í•™ìŠµ(ì•„ì´í…œë‹¹ 100ë²ˆ), 24ë²ˆ í‰ê°€(12ë²ˆëŠ” í•™ìŠµ ì•„ì´í…œ, 12ë²ˆì€ ood)</p> </li> </ul> </li> <li> <p>OpenVLAë¥¼ ê° taskì— ë”°ë¡œë”°ë¡œ 50-150K gradient stepì •ë„ FT</p> <ul> <li>action chunk size K=25, inferenceì‹œì— ë°”ë¡œ í’€ action chunkì‹¤í–‰</li> </ul> </li> </ul> </li> <li> <p>Method in Comparison</p> <ul> <li> <p>ALOHAëŠ” OpenVLAê°€ adaptationí•˜ê¸°ê°€ ì–´ë ¤ì›€, ê·¸ë˜ì„œ ë” ìµœê·¼ VLA-RDT-1B, $ \pi_0 $ê³¼ ë¹„êµ</p> <ul> <li>ë‘˜ì€ bimanual manipulation dataë¡œ pretrainë¨</li> </ul> </li> <li> <p>ê° ëª¨ë¸ì€ ê° ì €ìì˜ ë ˆì‹œí”¼ë¡œ FTí•˜ê³  í‰ê°€í•¨</p> </li> <li> <p>computational efficiencyë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì˜ imitation learning baseline, ACTì™€ Diffusion policyë¥¼ scratchë¡œ ê° taskì— í•™ìŠµí•´ ì‚¬ìš©</p> </li> <li> <p>ê° baseline methodì˜ language followingì„ ìœ„í•´ì„œ, language-conditioned implementationì„ ì‚¬ìš©</p> <ul> <li>ACTëŠ” EfficientNet-B0ë¥¼ ìˆ˜ì •, Diffusion PolicyëŠ” DROID dataset êµ¬í˜„ì„ ì‚¬ìš©(DistillBERT language embeddingì— ê¸°ë°˜í•´ action denoisingì§„í–‰)</li> </ul> </li> </ul> </li> <li> <p>ALOHA Task Performance Results</p> <ul> <li> <p>ì‹¤í—˜ ê²°ê³¼, pretrainë•Œ ë³¸ ë°ì´í„°ì™€ ì „í˜€ ë‹¤ë¦„ì—ë„ ë§¤ìš° ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ</p> <ul> <li> <p>scratchë¶€í„° í•™ìŠµí•œ ACT, Diffusion policyì˜ ê²½ìš° language understandingì´ í•„ìš”í•œ Scoop X, put X taskì—ì„œ ì €ì¡°í•œ ì„±ëŠ¥ì„ ë³´ì„</p> </li> <li> <p>finetuend-VLAëŠ” ë¹„êµì  ì˜í•˜ë‚˜, visual feedbackì— ëŒ€í•´ ê³¼í•˜ê²Œ ì˜ì§€í•˜ëŠ” ê²½í–¥ì„ ë³´ì„</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>- FiLMì´ language followingì„ ìœ„í•´ ë§¤ìš° ì¤‘ìš”í•¨ì„ ë³´ì—¬ì¤Œ
</code></pre></div> </div> </li> <li> <p>ALOHA Inference Efficiency Comparison</p> </li> </ul> </li> <li> <p>ë‹¤ë¥¸ finetuned VLAê³¼ ë¹„êµí•´ì„œ í›¨ì”¬ ë†’ì€ efficiencyë¥¼ ë³´ì„</p> </li> </ul> <h2 id="limitations">Limitations</h2> </li> <li> <p>Handling multimodal demonstrations</p> <ul> <li>L1 lossë¡œëŠ” robot dataíŠ¹ìœ ì˜ multi-modalë¥¼ í¬ì°©í•  ìˆ˜ ì—†ìŒ</li> </ul> </li> <li> <p>Pretraining versus fine-tuning</p> <ul> <li>ë³¸ ë…¼ë¬¸ì€ FTì—ë§Œ ì§‘ì¤‘, pretrainingì˜ íš¨ê³¼ì— ëŒ€í•œ ë¶„ì„ì€ ë¶€ì¡±</li> </ul> </li> <li> <p>Inconsistent language grounding</p> <ul> <li> <p>LIBEROì—ì„œëŠ” ë¬¸ì œê°€ ì—†ì—ˆìœ¼ë‚˜, ALOHAì—ì„œëŠ” FiLMì—†ì´ëŠ” ì„±ëŠ¥ì´ í•˜ë½</p> <ul> <li>ì¼ê´€ì„±ì´ ë–¨ì–´ì§</li> </ul> </li> </ul> </li> </ul> <p><br></p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 You R. Name. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/al-folio/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/al-folio/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/al-folio/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/al-folio/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/al-folio/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/al-folio/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/al-folio/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/al-folio/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/al-folio/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/al-folio/assets/js/search-data.js"></script> <script src="/al-folio/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>