<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DeepSeek R1 | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/deepseek-r1/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DeepSeek R1</h1> <p class="post-meta"> Created on January 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-01-02</li> <li> <strong>Reviewer</strong>: 건우 김</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>https://substack.com/home/post/p-153314921</p> <h1 id="introduction">Introduction</h1> <p>최근에 post-training은 reasoning tasks, align/adapt to user preference 등 성능을 올리기 위해 많이 사용되고 있음. 특히 Reasoning capability 측면에서 보면 GPT-4-o1은 CoT의 reasoning process의 Length를 늘리며 <strong>inference-time 관점에서 scaling-out한</strong> 방법으로 처음 소개된 모델임. test-time scaling을 효과적으로 수행하기 위한 방법으로 다양한 선행 연구들이 있었지만, 이중에 어느 것도 o1과 같은 reasoning model의 성능을 달성하진 못했음.</p> <p>본 연구에서는 Reinforcement Learning(RL)을 활용해서 Language Model의 reasoning 성능을 끌어올리는 방법을 타묵함. <strong>연구의 주된 목표는 SFT 없이 RL만 활용해서 LLM의 reasoning capabilities를 키우는 것임.</strong></p> <p>DeepSeek-V3-Base을 backbone model로 두고 GROP (RL)로만 학습시켜 <strong>DeepSeek-R1-Zero</strong>를 만들었고, 이것의 reasoning 성능은 AIME2024에서 GPT-4-o1의 86.7% 수준의 성능을 보여줌.</p> <p>하지만 DeepSeek-R1-Zero의 output은 가독성이 떨어지기 때문에, 이를 해결하고자 multi-stage로 학습되는<strong>DeepSeek-R1</strong> 모델을 개발함. 본 모델의 학습 과정은 아래와 같음</p> <ol> <li> <p>Cold start</p> </li> <li> <p>Reasoning-oriented RL</p> </li> <li> <p>Rejection Sampling and SFT</p> </li> <li> <p>RL for all Scenarios</p> </li> </ol> <p>더 나아가서 DeepSeek-R1을 활용하여 여러 Open-source models을 대상으로 distillation을 진행시켰고, large model에서 보이는 reasoning pattern이 small models들에 transfer가 어느 정도 되는 것을 보여줌.</p> <p>본 논문의 주된 <strong>Contributions</strong>은 아래와 같음</p> <ol> <li> <p><strong>Post-Training: Large-Scale Reinforcement Learning on the Base Model</strong></p> </li> <li> <p><strong>Distillation: Smaller Models Can Be Powerful Too</strong></p> </li> </ol> <h1 id="approach">Approach</h1> <p>(논문의 내용과 jay alammar의 시각 자료를 활용함)</p> <h2 id="overview">Overview</h2> <p>DeepSeek-R1은 DeepSeek-V3에서 소개된 모델을 backbone으로 활용함. (이때, DeepSeek-V3의 final model이 아니고 base model임)</p> <p>DeepSeek-v3-Base에 SFT를 진행한 뒤에 RL을 적용하면 ⇒ DeepSeek-R1 !!</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="1-long-chains-of-reasoning-sft-data">1. Long chains of reasoning SFT Data</h2> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>SFT를 진행하기 위해서는 많은 양의 labeled data가 필요한데, 이 만큼의 양을 사람으로부터 얻기는 어렵다. 따라서, 이런 high quality의 long chain-of-thought reasoning examples을 얻기 위해서 “<strong>Interim reasoning LLM</strong>”을 별도로 학습시켜서 활요함.</p> <h2 id="2-an-interim-high-quality-reasoning-llm-but-worse-at-non-reasoning-tasks">2. An interim high-quality reasoning LLM (but worse at non-reasoning tasks)</h2> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Interim reasoning model을 별도로 학습시켜서 SFT data를 구축하는데 사용함. 이때, Interim reasoning model은 DeepSeek-R1-Zero과 비슷하게 구축이 됨. (DeepSeek-R1-Zero를 먼저 만들고 난 뒤에, 여기서 영감을 얻어 DeepSeek-R1을 개발함)</p> <h2 id="3-creating-reasoning-models-with-large-scale-rl">3. Creating reasoning models with large-scale RL</h2> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="31-large-scale-reasoning-oriented-rl-r1-zero"><strong>3.1 Large-Scale Reasoning-Oriented RL (R1-Zero)</strong></h3> <p>여기서 RL은 interim model을 학습시키기 위해 사용하고 해당 모델로 SFT reasoning samples을 구축함. 아래 table에서 보이는 바와 같이 R1-Zero는 o1과 비슷한 수준의 Reasoning 성능을 보여줬기에, 이에 영감을 받아 SFT reasoning samples을 구축하고자 함.</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>기존의 ML 모델들과 다르게, 더 이상 새로운 데이터를 추가하지 않아도 성능이 향상될 수 있음을 보여줌</p> <ol> <li> <p>14.8 Billion high-quality tokens으로 학습한 base model이 있는데, 이미 많은 데이터로 학습시켰기 때문에, 단순히 더 많은 데이터를 추가한다고 해서 성능이 획기적으로 좋아지는 단계는 지났을 가능성이 큼</p> </li> <li> <p>Reasoning 문제는 데이터 없이도 성능이 개선될 수 있음.</p> </li> <li> <p>일반적인 chat, wrtining은 사람이 직접 quality를 평가해야 하지만 reasoning 문제는 (수학, 코딩) 자동으로 정답을 확인할 수 있음. (ex. 2+2=5가 틀렸다는 것을 자명하게 알 수 있음)</p> </li> </ol> <p><strong>Example</strong></p> <blockquote> <p>Write python code that takes a list of numbers, returns them in a sorted order, but also adds 42 at the start.</p> </blockquote> <p>위 문제는</p> <ul> <li> <p>linter와 같은 software를 활용하여 생성된 코드가 python 문법을 따르는지 확인할 수 있음</p> </li> <li> <p>코드가 실제로 실행되는지 테스트할 수 있음 (실행되지 않으면 잘못된 코드임을 확인함)</p> </li> <li> <p>실행 속도(퍼포먼스) 측정 가능.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>이때 사용한 System message는 아래와 같음 (명시적으로 <think> tag 사이에 reasoning prcoess를 작성하라고 되어있지만, 논리 과정은 자율 형식)</think></p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>RL과정에서 사용된 두 가지 rewards는 <strong>accuracy reward</strong> (답변이 맞는지 확인)과 <strong>format rewards</strong>( think, answer tag를 잘 사용하고 있는지)가 있고, GRPO 알고리즘을 활용하여 model을 update함</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="group-relative-policy-optimization-grpo">Group Relative Policy Optimization (GRPO)</h3> <p><strong>PPO</strong></p> <p>Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 참조</p> <p><strong>Proximal Policy Optimization (PPO): Actor-Critic RL algorithm</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>PPO Advantage를 뜯어보면 아래와 같음</p> <ul> <li>Advantage는 Rt - V(st)로 산출됨</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>상태 st에서 얻을 수 있는 실제 return Rt^hat과 현재의 value function V(st)의 차이.</p> <ul> <li> <p>advantage 양수 (Rt &gt; V(st)): 현재 policy가 기존 policy 보다 나음</p> </li> <li> <p>advantage 음수 (Rt &lt; V(st)): 현재 policy가 기존 policy 보다 별로</p> </li> </ul> </li> <li> <p>Return: 총 보상을 나타내는 return 값.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- rt는 실제 보상이고 gamma discount를 적용하여 미래 보상 반영
</code></pre></div></div> <ul> <li>reward signal: reward model이 산출한 값과 policy model과 reference model의 ratio를 KL penalty로 적용</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Gradient Update process</p> <ol> <li> <p>Actor update (PPO Objective function)</p> </li> <li> <p>Actor policy model weight update</p> </li> <li> <p>Advantage를 활용하여 좋은 policy 방향으로 update 유도</p> </li> <li> <p>Critic update (Value function)</p> </li> <li> <p>Critic Loss를 사용해서 V(st)를 Rt에 가깝게 만들도록 MSE</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>critic model의 역할은 현재 state의 value를 예측하고, 실제 reward와 비교하여 advantage를 계산하는 것을 담당</li> </ul> <p>Actor update → Critic update</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>PPO에서는 별도의 critic model (=value function)을 사용하여 policy model의 행동을 평가하는데, critic model의 크기는 policy model과 동일하거나 비슷하기에 computational cost가 높은 단점이 있음</p> </li> <li> <p>critic model → group scores로 대체하여 baseline을 추정하는 방식</p> </li> <li> <p><strong>Core Ideas</strong></p> <ul> <li> <p>query q에 대해 기존 policy model의 여러 개의 responses (o1, o2, … oG)을 sampling함</p> </li> <li> <p>그룹 내에서 relative compairson을 통해 baseline을 추정함</p> </li> <li> <p>new policy model은 위 정보를 기반으로 <strong>objective를 maximize</strong>하도록 학습</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(1) PPO의 Clip Trick을 활용함</p> <ul> <li> <p>new policy model과 기존 policy model의 ratio 비교</p> </li> <li> <p>Ai: 각 response oi에 대한 advantage</p> </li> <li> <p>Clip Trick: +- eps 범위에서 ratio를 제한하여 큰 update 방지</p> </li> </ul> <p>→ 이전 policy model 대비 new policy model이 크게 변화하지 않도록 제한하면서 advantage을 극대화 시킴</p> <p>(2) KL Divergence</p> <ul> <li>policy model이 reference model 대비 분포의 차이를 최소화 시켜줌</li> </ul> <p>(3) Advantage (Ai)</p> <ul> <li> <p>ri: response oi의 reward 값</p> </li> <li> <p>group 내 mean rewards를 빼고 std로 나누어 정규화 시킴</p> </li> </ul> <p>→ <strong>각 응답의 상대적인 보상을 group 내에서 정규화하여 advantage를 구함</strong></p> <p>GRPO의 알고리즘은 아래와 같음</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>rule-based reward signal을 활용하여 model을 update하고 학습이 진행됨에 따라 성능이 올라가는 것을 볼 수 있음. (<strong>Step이 커짐에 따라 성능도 비례하며 올라감</strong>)</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>위와 같이 학습한 R1-Zero는 reasoning tasks에서 높은 성능을 보여주긴 하지만, poor readability와 같은 문제점이 존재함. (<strong>Step이 커짐에 따라 모델이 생성하고자 하는 text length도 길어짐</strong>)</p> <p>따라서 RL로만 학습하는 방식이 아닌 SFT 학습과정을 포함시킴 R1을 소개함.</p> <h3 id="32-creating-sft-reasoning-data-with-the-interim-reasoning-model"><strong>3.2 Creating SFT reasoning data with the interim reasoning model</strong></h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li><strong>Cold Start</strong></li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>R1-Zero와 다르게 초기 불안정한 학습을 방지하고자 수천개의 long CoT samples을 활용하여 SFT를 진행하여 initial RL actor를 구축함.</p> </li> <li> <p>samples을 구축할 때는 few-shot prompting으로 구축하고 human annotators가 post-processing 진행함</p> </li> <li> <p>Cold Start으로 데이터를 구축하면 되는데, RL 과정을 하는 이유는 <strong>scale of data</strong>!!</p> </li> </ol> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Cold Start는 5,000개 데이터만 있으면 되지만 **R1을 학습시키기 위해서는 600,000개가 필요함**
</code></pre></div></div> <ol> <li><strong>Reasoning-oriented Reinforcement Learning</strong></li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Interim model을 활용하여 600,000개 samples을 만들고 난 뒤에 DeepSeek-V3에서 사용한 SFT dataset samples 200,000개와 결합하여 최종적으로 <strong>800,000 SFT samples</strong> 구축</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_025.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="33-general-rl-training-phase">3.3 General RL training phase</h3> <p>위와 같이 SFT로 학습된 R1은 reasoning 및 non-reasoning tasks를 잘 수행하지만, 더 다양한 applications에 확장시키기 위해서 Llama2에서 사용한 helpfulness / safety reward model을 활용하여 일반적인 RL 학습을 진행함</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_026.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>여기서는 기존에 사용하던 reasoning modeling을 위한 reward verifier와 llama2에서 사용한 reward model을 <strong>동시에 같이 사용함</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_027.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="distillation-empower-small-models-with-reasoning-capability">*Distillation: Empower Small Models with Reasoning Capability</h3> <p>DeepSeek-R1은 671B params를 갖고 있기 때문에, 일반 hardware에서 사용하기에 어려움이 있음. 본 연구에서 R1의 reasoning quality를 Qwen-32B와 같은 open source model로 distillation을 진행함.</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_028.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>800,000개의 reasoning 및 non-reasoning samples로 학습을 진행함</p> <h1 id="experiment">Experiment</h1> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_029.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_030.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="discussion">Discussion</h1> <h2 id="distillation-vs-reinforcement-learning">Distillation vs. Reinforcement Learning</h2> <p><em>Can the model achieve comparable performance through the large-scale RL training discussed in the paper without distillation?</em></p> <p>→ Qwen32B-base로 실험한 결과 Zero모델은 R1에 비해 다소 떨어진 성능을 보이고 QwQ32B와 비슷한 성능을 보여줌</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-deepseek-r1/image_031.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>powerful models을 small model로 distillation하는 것은 효과적이나, **small model이 자체적으로 large-scale RL을 진행하는 것은 computaitonal cost가 큰것에 비해 성능이 좋지 **않음</p> </li> <li> <p>distillation이 effective하지만 intelleigence 넘어 도달하는 것은 아직도 <strong>powerful base model과 larger-scale RL 알고리즘이</strong> 필수적임</p> </li> </ol> <h2 id="unsuccessful-attempts">Unsuccessful Attempts</h2> <p>(SKIP)</p> <h1 id="conclusion">Conclusion</h1> <ul> <li> <p><strong>Reinforcement Learning Approach</strong>: DeepSeek-R1-Zero는 SFT없이 RL만으로 강력한 성능을 달성했으며, DeepSeek-R1은 초기 데이터와 RL을 활용하여 OpenAI-o1-1217 수준의 성능을 보여줌.</p> </li> <li> <p><strong>Distillation to Smaller Models</strong>: DeepSeek-R1이 80만 개의 학습 샘플을 생성하여 small model을 tuning한 결과, DeepSeek-R1-Distill-Qwen-1.5B는 수학 벤치마크에서 GPT-4o 및 Claude-3.5-Sonnet보다 높은 성능을 보여줌.</p> <ul> <li>Distilled-Qwen-32B는 한국어 매우 못함..+ 답변 길이가 매우 김</li> </ul> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>