<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content=" ë…¼ë¬¸ ë¦¬ë·° - DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/dra-grpo-exploring-diversity-aware-reward-adjustment-for-r1-zero-like-training-of-large-language-models/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models</h1> <p class="post-meta"> Created on June 10, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a> Â  Â· Â  <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a> Â  <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a> Â  Â· Â  <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>ë…¼ë¬¸ ì •ë³´</strong></p> <ul> <li> <strong>Date</strong>: 2025-06-10</li> <li> <p><strong>Reviewer</strong>: ê±´ìš° ê¹€</p> </li> <li> <p>ìµœê·¼ì— post-trainingì„ ìœ„í•œ RLì—ì„œ <strong>GRPO</strong>ì™€ ê°™ì´ low-resource settingsì—ì„œ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤Œ</p> <p><strong>â†’ GRPOëŠ” solution-levelì˜ scalar reward signalsì— ì˜ì¡´í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„, samplingëœ ë¬¸ì¥ë“¤ê°„ì˜ semantic diversityë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•¨</strong></p> <p>â†’ ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ reasoning pathë¥¼ ê°–ëŠ” responseë“¤ì´ êµ¬ë¶„ë˜ì§€ ì•ŠëŠ” ë™ì¼í•œ rewardë¥¼ ë°›ëŠ” (<strong>diversity-quality inconsistency</strong>) ë¬¸ì œê°€ ìˆìŒ</p> </li> <li> <p>ìœ„ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ reward computation ê³¼ì •ì—ì„œ semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ” ë°©ë²•ì¸ <strong>Diversity-aware Reward Adjustment (DRA)</strong>ë¥¼ ì œì•ˆí•¨</p> </li> <li> <p>DRAëŠ” Submodular Mutual Information (SMI)ë¥¼ í™œìš©í•˜ì—¬</p> <ol> <li> <p>ì¤‘ë³µëœ responseì˜ rewardëŠ” ê°ì†Œì‹œí‚´</p> </li> <li> <p>diverse responseì˜ rewardëŠ” ì¦í­ì‹œí‚´</p> </li> </ol> </li> <li> <p>5ê°œ Mathematical Reasoning benchmarkì—ì„œ recent methods ëŒ€ë¹„ outperform ì„±ëŠ¥ ë³´ì—¬ì¤Œ</p> <p>(ë‹¨ 7,000ê°œ sampleë¡œë§Œ fine-tuningì„ í•˜ê³ , $55 training costë¡œ í‰ê·  acc 58.2% sota ë‹¬ì„±)</p> </li> </ul> <p>DeepSeek-R1-Zero (Guo et al., 2025)ì—ì„œ ê¸°ì¡´ LLMì— SFTë¥¼ ì ìš©í•˜ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜, base LMì— ë°”ë¡œ RLì„ ì ìš©í•  ìˆ˜ ìˆëŠ” R1-Zero training pipelineì„ ì œì•ˆí•¨.</p> <p>â†’ Group Relative Policy Optimization (GRPO) ì•Œê³ ë¦¬ì¦˜ ë•ë¶„ì— ê°€ëŠ¥í•œ ë°©ë²•</p> <p>GRPOëŠ” PPOì™€ ë‹¤ë¥´ê²Œ critic model ì—†ì´ ì£¼ì–´ì§„ promptì— ëŒ€í•´ ì—¬ëŸ¬ samplingëœ completionsì˜ relative performanceì— ëŒ€í•œ advantageë¥¼ í‰ê°€í•¨.</p> <p><br></p> <p>í•˜ì§€ë§Œ ìµœê·¼ì— ê³µê°œëœ GRPO ë° ê·¸ variants (e.g,. DR. GRPO)ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì •ë‹µ ì—¬ë¶€ì™€ ê°™ì€ <span style="color:red"><strong>solution-levelì˜ scalar reward signalsì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²½í–¥ì´ ìˆì–´, ê°™ì€ ì •ë‹µì´ë¼ë„ diverse reasoning pathì˜ ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•¨</strong></span>.</p> <p>â†’ ì´ëŠ” semanticí•˜ê²Œ ë‹¤ë¥¸ completionsë“¤ì´ ì˜¬ë°”ë¥´ê±°ë‚˜ í‹€ë¦° ê²½ìš° ëª¨ë‘ ê±°ì˜ ë™ì¼í•œ rewardsë¥¼ ë°›ì•„, ì˜ë¯¸ ìˆëŠ” reasoning ì°¨ì´ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” <strong>indistinguishable advantage estimates</strong>ë¥¼ ìƒì„±í•˜ëŠ” ë¬¸ì œê°€ ìˆìŒ</p> <ul> <li> <p><strong>Example1 (Correct Reward)</strong></p> <p>GRPO training processì˜ examplesì„ ì¤€ë¹„í•¨. â†’ ì €ìë“¤ì˜ key motivation of research</p> <ul> <li> <p>LLMì€ diverse answerë¥¼ ìƒì„±í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŸ° answersë“¤ì€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ reward scoreë¥¼ ë°›ìŒ</p> <p>â†’ ì¦‰, solution-level judgementsëŠ” different reasoning pathsë¥¼ êµ¬ë³„í•˜ì§€ ëª»í•¨</p> </li> </ul> <p>ì•„ë˜ëŠ” questionì— ë™ì¼í•œ ì •ë‹µì„ ìƒì„±í–ˆì§€ë§Œ reasoning pathê°€ ì™„ì „íˆ ë‹¤ë¥¸ ë‘ê°€ì§€ ì‘ë‹µì— ëŒ€í•œ ì¼€ì´ìŠ¤ (ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  rewardëŠ” ë¹„ìŠ·í•¨)</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>	- **Example2 (Incorrect Reward)**
</code></pre></div> </div> <p>ì´ë²ˆ ì˜ˆì‹œëŠ” Questionì— ëŒ€í•´ ë‘ê°€ì§€ ì‘ë‹µì´ ëª¨ë‘ Incorrectì¸ ë°˜ë©´, reasoning pathëŠ” ì„œë¡œ ë‹¤ë¦„ â†’ ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  reward scoreëŠ” ë‘˜ ë‹¤ ë¹„ìŠ·í•˜ê²Œ ë‚®ìŒ</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>			â†’ ë˜í•œ, ì´ëŠ” resource-constrained settingsì—ì„œ ë” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŒ
</code></pre></div> </div> <ul> <li> <p>ê° promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” completionsì˜ ê°œìˆ˜ê°€ ì ê¸° ë•Œë¬¸ì—, <span style="color:orange_background"><strong>rewardê°€ ë†’ì€ outputsì— ëŒ€í•œ exploitationë§Œ reinforceí•˜ë©°, alternativeí•˜ê³  potentially validí•œ reasoning pathsì— ëŒ€í•œ explorationì„ ìœ ë„í•˜ì§€ ëª»í•¨.</strong></span></p> </li> <li> <p>(ë¹„ìœ ) ì„ ìƒë‹˜ì´ ìˆ˜í•™ ë¬¸ì œë¥¼ ëª¨ë‘ ì •í™•í•˜ê²Œ í‘¼ í•™ìƒë“¤ì—ê²Œ 100ì ì„ ì£¼ëŠ” ì¼€ì´ìŠ¤. ì •ë‹µì´ ë§ë”ë¼ë„, í•™ìƒì˜ ì´í•´ë„ë‚˜ ì‚¬ê³  ë°©ì‹ì„ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆëŠ” ë¬¸ì œë¥¼ í‘¸ëŠ” ë‹¤ì–‘í•œ ë°©ì‹ì€ í‰ê°€ë˜ì§€ ì•Šê³ , ì˜¤ë‹µì¼ ê²½ìš°ì—ë„ ê·¸ ê³¼ì •ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ë‹¤ì–‘í•œ ì¶”ë¡  ì ‘ê·¼ì„ í‰ê°€í•˜ì§€ ì•Šê³  ë‹¨ìˆœíˆ ê°™ì€ ê°ì ì„ ë°›ìŒ.</p> </li> </ul> </li> </ul> <p><br></p> <p>ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ì €ìë“¤ì€ <span style="color:orange_background"><strong>Diversity-aware Reward Adjustment (DRA)</strong></span>ë¥¼ ì œì•ˆí•¨.</p> <p>ì´ëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ samplingëœ completions ê°„ì˜ <em>semantic diversityë¥¼ ì§ì ‘ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ</em> ê·¸ë£¹ ë‚´ ë‹¤ë¥¸ <em>completionsê³¼ì˜ semantic similarityë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° outputì˜ rewardë¥¼ reweight</em>í•¨.</p> <ul> <li><span style="color:orange_background"><strong>diverse completionsì—ëŠ” ë” ë†’ì€ weight, ì¤‘ë³µëœ completionì—ëŠ” ë” ë‚®ì€ weight ë¶€ì—¬</strong></span></li> </ul> <h3 id="preliminary">Preliminary</h3> <p>LMì˜ generationì€ token-level Markov Decision Processë¡œ ë³¼ ìˆ˜ ìˆìŒ. ê° generation step $ t $ì—ì„œ state $ s<em>t $ëŠ” input question $ q $ì™€ ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ partial output sequence $ o</em>{&lt;t} $ì˜ concatenationì´ê¸°ì—, satesëŠ” ë‹¤ìŒê³¼ ê°™ìŒ $ s<em>t=[q;o</em>{&lt;t}] $.</p> <table> <tbody> <tr> <td>policy $ \pi*{\theta}(.</td> <td>s_t) $ëŠ” vocab set $ A $ì—ì„œ next token $ o_t $ë¥¼ ì„ íƒí•˜ê³ , ì´ëŠ” deterministic transitionì„ ìœ ë„í•˜ì—¬ next state $ s*{t+1}=[s_t;o_t] $ë¡œ ì´ë™í•¨.</td> </tr> </tbody> </table> <p>GRPOëŠ” ê° question $ q $ì— ëŒ€í•´ ì—¬ëŸ¬ ê°œì˜ responses $ C= ${$ o_1,â€¦o_G $}ë¥¼ samplingí•˜ê³ , ê° responseì— ëŒ€í•´ rewardë¥¼ ê³„ì‚°í•¨ $ R= ${$ R(q,o_1), â€¦ , R(q,o_G) $}</p> <p>ê³„ì‚°ëœ reward $ R $ì„ ì´ìš©í•´ advantage $ A_{i,t} $ë¥¼ ì•„ë˜ì™€ ê°™ì´ ê³„ì‚°í•¨ (normalize)</p> <p>GRPOì˜ objective function $ J<em>{GRPO}(\pi</em>{\theta}) $ë¥¼ optimizeí•¨</p> <p><br></p> <p>ì´í›„ ì—°êµ¬ì¸ DR.GRPO (Liu et al., 2025)ì—ì„œëŠ” token efficiencyë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ <strong>GRPOì˜ objective functionì—ì„œ â€˜response lengthâ€™ termê³¼ Advantageì—ì„œ stdë¡œ normalizeí•´ì£¼ëŠ” termì„ ì§€ì›€</strong></p> <ul> <li> <p>DR.GRPO (Zichen Liu, et al 2025)</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>GRPOì˜ ë‘ ê°€ì§€ í¸í–¥
</code></pre></div> </div> <ol> <li> <p><strong>Response-level length bias</strong></p> <ol> <li> <table> <tbody> <tr> <td>ê° responseì— ëŒ€í•´ í‰ê· ì„ êµ¬í• ë•Œ, $ 1/</td> <td>o_i</td> <td>$ê°€ ê³±í•´ì§€ëŠ”ë°,</td> </tr> </tbody> </table> <ol> <li> <p>Correct responseì¸ ê²½ìš° advantageê°€ ì–‘ìˆ˜ì´ë©´ shorter responseì— ëŒ€í•´ì„œ greater gradient updatesë¥¼ ì•¼ê¸°í•¨ â†’ ì´ PolicyëŠ” correct answerì— ëŒ€í•´ brevity favorìˆìŒ</p> </li> <li> <table> <tbody> <tr> <td>Incorrect responseì¸ ê²½ìš° advantageê°€ ìŒìˆ˜ longer responseëŠ” $</td> <td>o_i</td> <td>$ê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— penalizedë¥¼ ë” ë°›ìŒ â†’ ì´ PolicyëŠ” Incorrect answerì— ëŒ€í•´ ê¸¸ê²Œ ë§í•˜ëŠ” favor ìˆìŒ</td> </tr> </tbody> </table> </li> </ol> <table> <tbody> <tr> <td>â‡’ ì‰½ê²Œ ë§í•˜ë©´ GRPOì˜ ê°œë³„ advantageë¥¼ $ A_{i,t}/</td> <td>o_i</td> <td>$ë¡œ ë³´ë©´</td> </tr> </tbody> </table> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>- positive advantage ì— ëŒ€í•´ì„œëŠ” ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ ì‘ì„ìˆ˜ë¡ updateê°€ ì»¤ì§€ê¸°ì— ì§§ì€ responseì— ê°•í•œ signalì„ ì£¼ê³ 

- negative advantage ì— ëŒ€í•´ì„œ ì—­ì‹œ ë™ì¼í•œ rewardë¼ë„ $ |o_i| $ê°€ í´ìˆ˜ë¡ updateê°€ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— ê¸´ responseì— ê°•í•œ signalì„ ì¤Œ (=ê¸´ ì˜¤ë‹µì€ under-penalized)

&lt;span style='color:orange_background'&gt;**â†’ ì¦‰, GRPOëŠ” ì •ë‹µì€ ì§§ê²Œ, ì˜¤ë‹µì€ ê¸¸ê²Œ ë§í•˜ê²Œë” biased policyë¥¼ ìœ ë„í•¨**&lt;/span&gt;
</code></pre></div> </div> </li> </ol> </li> <li> <p><strong>Question-level difficulty bias</strong></p> <ol> <li> <p>advantageë¥¼ íŠ¹ì • question ë‚´ì˜ group averageì™€ stdë¡œ normalizationì„ í•˜ê¸° ë•Œë¬¸ì—, stdê°€ ì‘ìœ¼ë©´ ìƒëŒ€ì ìœ¼ë¡œ í•´ë‹¹ questionì— ëŒ€í•œ training signal (weight update)ê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§</p> <p>â†’ ì¼ë°˜ì ì¸ RLì—ì„œëŠ” batch ë‹¨ìœ„ë¡œ normalizationë˜ì–´ biasë¥¼ ìƒì‡„ì‹œí‚¤ì§€ë§Œ, GRPOëŠ” question ë‹¨ìœ„ë¡œ ì²˜ë¦¬ë˜ì–´ ê·¸ë ‡ì§€ ëª»í•¨</p> </li> </ol> <p>â‡’ ìœ„ì™€ ê°™ì€ ë¬¸ì œëŠ” LMì˜ responseê°€ ê¸¸ì–´ì§€ëŠ” ì´ìœ ê°€ reasoning capability ë•Œë¬¸ì¸ì§€ ì•„ë‹ˆë©´ bias ë•Œë¬¸ì¸ì§€ êµ¬ë¶„í•˜ê¸°ê°€ ì–´ë ¤ì›Œì§. ì´ì— ë”°ë¼ <span style="color:orange_background"><strong>unbiased optimization methodì¸ DR.GRPO ì†Œê°œí•¨</strong></span></p> </li> </ol> <p><strong>Question1</strong>: ìœ„ì—ì„œ ë¶„ëª… Correct responseì— ëŒ€í•´ì„œëŠ” ì§§ì•„ì§€ëŠ”ë° ì™œ responseê°€ ê¸¸ì–´ì§„ë‹¤ê³  í‘œí˜„í•˜ëŠ”ì§€?</p> <p>â‡’ (ê±´í”¼ì…œ) Correct response ê°œìˆ˜ë³´ë‹¤ Incorrect responseì˜ ìˆ˜ê°€ ë” ë§ê¸°ì—, ëŒ€ë¶€ë¶„ì˜ responseëŠ” RL trainingì—ì„œ ì˜¤ë‹µì´ë¼ ê¸¸ê²Œ ìƒì„±í•˜ëŠ” ê²½í–¥ì´ ìƒê¹€</p> <p><strong>Question2</strong>: GRPOë¥¼ ë³´ë©´ í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ reasoning accuracyê°€ ì˜¬ë¼ê°€ëŠ”ë°, ê·¸ëŸ¬ë©´ biasì— ë”°ë¼ response lengthê°€ ì§§ì•„ì ¸ì•¼ í•˜ëŠ”ê±° ì•„ë‹Œê°€?</p> <p>â‡’ (ê±´í”¼ì…œ) complex tasksì—ì„œëŠ” ì•„ë¬´ë¦¬ ì˜¤ë˜ í•™ìŠµì‹œì¼œë„ ë†’ì€ accì— ë„ë‹¬í•˜ëŠ” ëª¨ë¸ì´ ì—†ì–´ì„œ ê·¸ë ‡ì§€ ì•Šì„ê¹Œ..?</p> <p><br></p> <p>DR.GRPOëŠ” GRPOì˜ optimization biasë¥¼ ì—†ì• ê¸° ìœ„í•´ ì•„ë˜ ë‘ê°€ì§€ termsì„ ì—†ì•°</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>R1-Zeroì™€ ë¹„ìŠ·í•œ minimal recipeë¡œ í•™ìŠµí•œ Oat-Zero-7B ì„±ëŠ¥
</code></pre></div> </div> <ul> <li> <p>minimal recipe</p> <ul> <li> <p>base model: Qwen2.5-Math-7B</p> </li> <li> <p>training dataset: MATH datasetì˜ level3~5</p> </li> <li> <p>GPU: 8xA100 27hrs</p> </li> </ul> <p>ì•„ë˜ (ìš°ì¸¡) ê·¸ë¦¼ì„ ë³´ë©´</p> </li> <li> <p>(solid lines) DR.GRPOëŠ” reasoning accuracyê°€ ì§€ì†ì ìœ¼ë¡œ ì˜¤ë¥´ëŠ” ë°˜ë©´ì—, GRPOëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ (ë¶ˆì•ˆì •í•¨)</p> </li> <li> <p>(dashed lines) DR.GRPOëŠ” response lengthê°€ ì§§ê³  ì•ˆì •ì ì¸ ë°˜ë©´ì—, GRPOëŠ” response lengthê°€ ê³„ì† ê¸¸ì–´ì§</p> </li> </ul> <p><br></p> </li> </ul> <p><br></p> <h3 id="diversity-quality-inconsistency">Diversity-Quality Inconsistency</h3> <p>GRPOì™€ DR.GRPOì˜ reward signalì€ <strong>solution-level correctness</strong>ë§Œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì—, ê° completionì— ëŒ€í•´ <strong>sparse scalar judgement</strong>ë¥¼ ê³„ì‚°í•¨.</p> <p>â†’ ì´ëŸ¬í•œ scalar rewardëŠ” ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ì‚°ì¶œí•˜ëŠ” diverse reasoning-pathë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, Diversity-Quality Inconsistencyê°€ ë°œìƒí•¨.</p> <p>ìœ„ì— Example ë§ê³ , ë³´ë‹¤ ì‹¤ì¦ì ì¸ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ statement (â€<strong><em>reward alone fails to reflect the underlying variability in reasoning strategies</em></strong>â€) ë¥¼ ê²€ì¦í•˜ê¸° ìœ„í•´ embedding distancesë¡œ ì¸¡ì •ëœ completionsì˜ structural dissimilarityë¥¼ ê³„ì‚°í•¨.</p> <ul> <li> <p>Spearmanâ€™s rank correlationì„ ì‚¬ìš©í•˜ì—¬ sampled completions ì‚¬ì´ì—ì„œ reward differenceì™€ semantic distanceë¥¼ ì¸¡ì •í•¨ â†’semantic distanceê°€ ì»¤ì§ˆìˆ˜ë¡ reward ì°¨ì´ë„ ì»¤ì§€ëŠ”ê°€?</p> <ul> <li>3000ê°œ prompt ë½‘ì•„ì„œ p-value ì¸¡ì •</li> </ul> </li> <li> <p>Figure2ëŠ” Spearmanâ€™s rank correlationì˜ p-valuesì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, ëŒ€ë¶€ë¶„ì˜ p-valueê°€ significance levelì¸ 0.05 ë³´ë‹¤ í° ê°’ì„ ë³´ì—¬ì£¼ë©°, ì‹¤ì œë¡œ 80% ì´ìƒì˜ promptì— ëŒ€í•´ statistically significant correlationì´ ì—†ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ</p> <p>â†’ ì¦‰, rewardê°€ semantic diversityì™€ ìƒê´€ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ</p> <p><br></p> <p>$ \tilde{a} \text{ì´ë ‡ê²Œ} $</p> </li> </ul> <h3 id="diversity-aware-reward-adjustment">Diversity-aware Reward Adjustment</h3> <p>Diversity-Quality Inconsistency ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, ê° sampleì˜ relative diversity/redundancyì— ë”°ë¼ rewardë¥¼ reweightí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨.</p> <p><span style="color:orange_background"><strong>â†’ diverse completionsì€ ë” ë†’ì€ weight, ì¤‘ë³µëœ responseëŠ” ë‚®ì€ weight</strong></span></p> <p>ë¨¼ì € ê¸°ì¡´ì˜ reward $ R(q,o_i) $ë¥¼ diversity-aware adjusted reward $ \tilde{R}(q,o_i) $ <span style="color:red">(í‹¸ë‹¤ í‘œì‹œ ì–´ë–»ê²Œ í•˜ë‚˜ìš”â€¦) </span>ìœ¼ë¡œ ëŒ€ì²´í•¨</p> <ul> <li> <p>SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” completion $ o_i $ì™€ ë‚˜ë¨¸ì§€ group $ C $ \ $ o_i $ ê°„ì˜ Submodular Mutual Informationì„ ë‚˜íƒ€ëƒ„</p> </li> <li> <p>Submodular functionsì€ diminishing returns íŠ¹ì„±ì„ ê°–ìœ¼ë©°, diversityì™€ redundancyë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŒ</p> <ul> <li> <strong>*diminishing returns**</strong>: ì–´ë–¤ ì§‘í•©ì— ìš”ì†Œë¥¼ í•˜ë‚˜ì”© ë” ì¶”ê°€í•  ë•Œ, ì´ë¯¸ ë¹„ìŠ·í•œ ìš”ì†Œê°€ ë§ì„ ìˆ˜ë¡ ê·¸ ìš”ì†Œê°€ ì¶”ê°€ë¡œ ê°€ì ¸ì˜¤ëŠ” ê°€ì¹˜(ì •ë³´ ê¸°ì—¬ë„)ëŠ” ì¤„ì–´ë“œëŠ” ì„±ì§ˆ*</li> </ul> </li> <li> <p>SMIëŠ” ë‘ ì§‘í•© ê°„ì˜ shared informationì„ ì •ëŸ‰í™”í•˜ë©° (Iyer et al., 2021a,b)ì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜í•¨</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>- $ s(o_i,j)=s(j,o_i) $ë¼ê³  ê°€ì •í•˜ë©°, SMI({$ o_i $},$ C $ \ {$ o_i $})ëŠ” $ o_i $ì™€ ë‚˜ë¨¸ì§€ elementsê°„ì˜ total semmetric simialrityë¥¼ ê³„ì‚°í•¨
</code></pre></div> </div> <ul> <li> <p>ì¦‰, $ o_i $ê°€ ë‚˜ë¨¸ì§€ completionsê³¼ ìœ ì‚¬í• ìˆ˜ë¡ SMIê°€ ì»¤ì ¸ reward ê°’ì´ ë‚®ê²Œ reweightë˜ê³ , $ o_i $ê°€ ë‹¤ë¥¼ìˆ˜ë¡ SMIê°€ ì‘ì•„ì ¸ reward ê°’ì´ í¬ê²Œ reweight ë¨.</p> </li> <li> <p>ê° completionì˜ embeddingì€ small LMìœ¼ë¡œ ì–»ê³ , $ s() $ëŠ” cosine similarityë¥¼ ì‚¬ìš©í•¨</p> </li> </ul> </li> </ul> <p><br></p> <ul> <li> <p>SMIë¥¼ ì‰½ê²Œ ë§í•˜ë©´ â€œ<span style="color:orange_background"><strong>íŠ¹ì • completion í•˜ë‚˜ê°€ group ë‚´ ë‹¤ë¥¸ completionê³¼ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ê°€</strong></span>â€ë¥¼ ìˆ˜ì¹˜ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê°’</p> <ul> <li> <p><strong>ì¤‘ë³µì´ ë§ìœ¼ë©´ (high redundancy) â†’ SMIê°€ í¼ â†’ rewardê°€ ì‘ì•„ì§</strong></p> </li> <li> <p><strong>ì¤‘ë³µì´ ì ìœ¼ë©´ (high diversity) â†’ SMIê°€ ì‘ìŒ â†’ rewardê°€ ì»¤ì§</strong></p> </li> </ul> </li> <li> <p>Submodular í•¨ìˆ˜ëŠ” ìˆ˜í•™ ê°œë…ìœ¼ë¡œ â€œìƒˆë¡œìš´ elementê°€ ê¸°ì¡´ì— ë¹„ìŠ·í•œê²Œ ë§ì„ìˆ˜ë¡ ê¸°ì—¬ë„ê°€ ì¤„ì–´ë“œëŠ” ì„±ì§ˆâ€ì„ ê°–ê³  ìˆìŒ</p> <p>ex) ìœ ì‚¬í•œ completionsì´ 9ê°œê°€ ìˆëŠ” ìƒí™©ì—ì„œ, 10ë²ˆì§¸ ë¹„ìŠ·í•œ completionì€ informationì„ ë³„ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ. ë°˜ë©´, ì™„ì „íˆ ë‹¤ë¥¸ completionì´ ë“±ì¥í•˜ë©´ information ê¸°ì—¬ë„ê°€ ì»¤ì§</p> </li> </ul> <p><br></p> <p>â†’ ì´ë ‡ê²Œ ìƒˆë¡œìš´ rewardë¥¼ êµ¬í•˜ëŠ” ì—°ì‚°ì€ Pytorchì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- $ \Sigma{L_{ij}} $ termì´ simiarlity matrix $ L $ì˜ $ i $th rowì˜ í•©
</code></pre></div></div> <ul> <li> <p>Pytorch Code for DAR</p> <h2 id="3-experiment">3. Experiment</h2> </li> </ul> <h3 id="31-experimental-setup">3.1 Experimental Setup</h3> <p>**Training Dataset: **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- s1 dataset + DeepScaleR dataset with mixed problem difficulties
</code></pre></div></div> <p>**Evaluation Dataset: **</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- five mathematical reasoning benchmarks (AIME24, MATH-500, AMC23, Minerva, OlympiadBench)
</code></pre></div></div> <p><strong>Baselines</strong>:</p> <ul> <li> <p>general purpose large model: Llama-3.1-70B-Instruct, o1-preivew</p> </li> <li> <p>Mathematics-focused 7B models: Qwen-2.5-Math-7B-Instruct, rStar-Math-7B, Eurus-2-7B-PRIME, Qwen2.5-7B-SimpleRL</p> </li> <li> <p>Mathematics-focused 1.5B models: DeepScaleR-1.5B-Preview, Still-3-1.5B-Preview, Open-RS</p> </li> </ul> <p><strong>Implementations:</strong></p> <ul> <li> <p>ë³¸ ì—°êµ¬ëŠ” DRAì˜ proof-of-conceptë§Œ ê²€ì¦í•˜ëŠ” ê²ƒì´ ëª©ì ì´ê¸°ì— DeepSeek-R1-Distill-Qwen-1.5Bë¥¼ base modelë¡œ ë‘ì–´ í•™ìŠµì‹œí‚´</p> </li> <li> <p>4 x A100 (40GB) GPUs</p> </li> </ul> <h3 id="32-empirical-analysis">3.2 Empirical Analysis</h3> <p><strong>Main Results</strong></p> <ul> <li> <p>DRA-DR.GRPOëŠ” avg accê°€ 58.2%ë¡œ ê°€ì¥ ë†’ê²Œ ë‚˜ì˜´ (DRA-GRPOì—­ì‹œ ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë†’ê²Œ ë‚˜ì˜´)</p> <ul> <li>AMC23ì—ì„œ íŠ¹íˆ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ (85.0%)</li> </ul> </li> <li> <p>DRA-GRPOì™€ DRA-DR.GRPOëŠ” fine-tuning samplesì„ 7,000ê°œ ë°–ì— ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  40,000ê°œ ì‚¬ìš©í•œ DeepScaleR-1.5B-previewë³´ë‹¤ ë†’ì€ ì„±ëŠ¥ ë³´ì—¬ì¤Œ</p> <p>â†’ Low-resource settingsì—ì„œë„ íš¨ê³¼ì ì„</p> <ul> <li> <p>DeepScaleR-1.5B-preview</p> <p>ë‚˜ë¦„ GOATê¸‰ì˜ ì„±ëŠ¥ ë³´ì—¬ì£¼ëŠ” ì˜ë‚˜ê°€ëŠ” ëª¨ë¸</p> <p>(<a href="https://www.notion.so/19681902c1468005bed8ca303013a4e2]" rel="external nofollow noopener" target="_blank">https://www.notion.so/19681902c1468005bed8ca303013a4e2</a></p> <p><a href="https://github.com/giterinhub/DeepScaleR-1.5B-Preview" rel="external nofollow noopener" target="_blank">https://github.com/giterinhub/DeepScaleR-1.5B-Preview</a></p> <p><strong>Ablation Study</strong></p> </li> </ul> </li> <li> <p>Base modelì¸ DeepSeek-R1-Distill-Qwen-1.5Bì™€ ë¹„êµí•˜ì—¬ DRA-GRPO, DRA-DR.GRPOëŠ” ê°ê° 7.8%, 9.3% ì„±ëŠ¥ í–¥ìƒë˜ê³  ë‹¨ìˆœ RL (GRPO, DR.GRPO) ëŒ€ë¹„ 1.9%, 2.2% í–¥ìƒ</p> <p>â†’ ì´ê²Œ ì™œ Ablation studyë¼ ë§í•˜ëŠ”ê±°ì§€ã…‹ã…‹</p> </li> </ul> <p><strong>Efficiency</strong></p> <p>DRAëŠ” completionsì„ encoding í•´ì•¼í•˜ê¸°ì— over-headê°€ ì¡´ì¬í•˜ì§€ë§Œ, ë³„ë¡œ í¬ì§€ ì•ŠìŒ.</p> <p>â†’ ì €ìë“¤ì´ ì‹¤í—˜ì— ì‚¬ìš©í•œ GPUìŠ¤í™ì¸ (A100-40GB)ì—ì„œëŠ” ì–´ì°¨í”¼ DRA ì—†ì´ë„ mini-batchë¥¼ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•´ì„œ DRA ì ìš©í•˜ëŠ” ê²ƒì´ ë³„ ë¬¸ì œê°€ ë˜ì§€ ì•Šë‹¤ê³  í•˜ëŠ”ë°â€¦. â†’ ğŸ¶Â ğŸ”ŠÂ ë¼ê³  ìƒê°í•©ë‹ˆë‹¤</p> <p><strong>Training Cost</strong></p> <p>500 steps í•™ìŠµì‹œì¼œ 12.5hr ì†Œìš”ë¨ â‡’ $55 ë¹„ìš©</p> <p>â†’ ë‹¤ë¥¸ ë°©ë²•ëŒ€ë¹„ íš¨ìœ¨ì ì„</p> <h3 id="33-discussion">3.3 Discussion</h3> <p><strong>Exploration-exploitation Balance</strong></p> <p>DRAëŠ” Exploration-exploitation balanceë¥¼ policy gradient ì•ˆì— ì§ì ‘ í†µí•©í•˜ì—¬ ì ìš©í•¨</p> <ul> <li> <p>Base rewardëŠ” high scoreë¥¼ ë°›ëŠ” completionì„ reinforceí•¨</p> <p>â†’ <strong>Exploitation ìœ ë„</strong></p> </li> <li> <p>Diversity weightingì€ semantically novel completionì— learning signalì„ amplify</p> <p>â†’ <strong>Exploration ìœ ë„</strong></p> </li> </ul> <p>ì´ëŸ¬í•œ íƒìƒ‰ì€ low-resource settings (promptë‹¹ samplingí•  ìˆ˜ ìˆëŠ” ì‘ë‹µ ìˆ˜ê°€ ì œí•œ ì ì¸ ê²½ìš°)ì—ì„œ ì¤‘ìš”í•¨</p> <p>â†’ DRAëŠ” mode collapseë¥¼ ë°©ì§€í•˜ê³  ë” ë„“ì€ reasoning strategiesë¥¼ ìœ ë„í•¨</p> <p><strong>Ad-hoc vs Post-hoc Diversity</strong></p> <p>generated completionsê°„ì˜ diversityë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ Ad-hoc, Post-hoc ë°©ì‹ì´ ìˆìŒ</p> <ol> <li> <p><strong>Ad-hoc</strong></p> <ol> <li> <p>generation ë‹¨ê³„ì—ì„œ ë‹¤ì–‘ì„±ì„ ìœ ë„í•¨ (temperature ì¡°ì ˆ, decoding ì„¤ì • ë³€ê²½)</p> </li> <li> <p>ì´ë ‡ê²Œ í•˜ë©´ ê° completionì´ ë…ë¦½ì ìœ¼ë¡œ samplingë˜ì–´ â†’ ì‘ë‹µ ê°„ correlationì„ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ì—†ìŒ (completionì´ ì„œë¡œ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ëª…ì‹œì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ìŒ)</p> </li> </ol> </li> <li> <p><strong>Post-hoc (ë³¸ ì—°êµ¬ì—ì„œ ì±„íƒí•œ ë°©ë²•)</strong></p> <ol> <li> <p>diversityë¥¼ reward signalì— ë°”ë¡œ í†µí•©</p> </li> <li> <p>Semantic redundancyë¥¼ í‰ê°€í•˜ì—¬ policyê°€ íš¨ìœ¨ì ìœ¼ë¡œ learningì„ ì¡°ì •í•  ìˆ˜ ìˆê²Œ í•´ì¤Œ</p> </li> </ol> </li> </ol> <h2 id="4-conclusion">4. Conclusion</h2> <ul> <li> <p>GRPO í˜•ì‹ì˜ RLì—ì„œ completions ê°„ì˜ semantic diversityë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” DRA ì•Œê³ ë¦¬ì¦˜ ì œì•ˆí•¨</p> <p>â†’ ê¸°ì¡´ scalar rewardì˜ ë¬¸ì œì¸ exploration-exploitation imbalance issueë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì™„í™”í•¨</p> </li> <li> <p>ë‘ê°€ì§€ í•œê³„ì ì´ ìˆìŒ</p> <ol> <li> <p>small-scale models (1.5B) / small group sizes (e.g 6 completions per prompt)</p> </li> <li> <p>diversityë¥¼ ì¸¡ì •í•  ë•Œ ì‚¬ìš©ëœ sentence embeddingsì€ ì™¸ë¶€ modelì— ì˜ì¡´í•˜ëŠ” êµ¬ì¡°</p> </li> </ol> </li> <li> <p>ì´ëŸ° ìª½ë„ ì¬ë°Œë‹¤!ã…‹ã…‹</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>