<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Reinforcement Learning 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</h1> <p class="post-meta"> Created on July 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement learning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-07-15</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: Reinforcement Learning</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="abstract">Abstract</h1> <ul> <li> <p>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</p> </li> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <strong>효과적으로 상호작용하는</strong> 능력이 필수적임</p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 **LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, 단순한 결과 기반 보상이 충분한지는 아직 불확실함.</p> </li> </ol> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <h1 id="2-related-works">2. Related Works</h1> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <h1 id="3-search-r1">3. Search-R1</h1> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine R을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>r_{\phi}: output quality를 평가하는 reward function</p> </li> <li> <p>\pi_\theta: policy LLM</p> </li> <li> <p>\pi_{ref}: reference LLM</p> </li> <li> <p>x: dataset D에서 추출된 input sample</p> </li> <li> <p>y: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>D_{KL}: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 \pi_\theta가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>\pi_{\theta}: current policy</p> </li> <li> <p>\pi_{old}: previous policy</p> </li> <li> <p>I(y_t): token loss masking 연산으로, y_t가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li> <p><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></p> </li> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Case Study</strong></p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008-480.webp 480w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008-800.webp 800w,/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="4-main-results">4. Main Results</h1> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> </li> <li> <p>Multi-Hop QA</p> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>