<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Reinforcement Learning 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/search-r1-training-llms-to-reason-and-leverage/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</h1> <p class="post-meta"> Created on July 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement learning</a>   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-07-15</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: Reinforcement Learning</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="abstract">Abstract</h1> <ul> <li>Reasoning과 text generation이 가능한 LLM에게 external knowledge와 최신 information을 효율적으로 삽입하는 것은 매우 중요함</li> </ul> <p>→ 하지만 기존 advanced reasoning ability를 가진 LLM에게 prompt 기반의 search engine을 활용하도록 하는 것은 suboptimal임 (LLM이 search engine과 어떻게 상호작용해야 하는지 완전히 이해 못함)</p> <ul> <li> <p>이 문제를 해결하기 위해 RL을 활용한 reasoning framework인 Search-R1을 소개함</p> <ul> <li>단계별 reasoning step에서 autonomously하게 multiple search queries를 생성하고 실시간으로 정보를 검색하도록 학습</li> </ul> </li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p>LLM은 natural language understanding과 generation에서 높은 성과를 보여줬지만, 여전히 external sources가 필요한 task에서 한계점을 보여줌.</p> <p>→ 즉, 최신 information을 잘 활용할 수 있도록 search engine과 <strong>효과적으로 상호작용하는</strong> 능력이 필수적임</p> <p>최근까지 LLM과 Search Engine을 결합하는 대표적인 방식은 두가지</p> <ol> <li> <p>Retrieval-Augmented Generation (RAG)</p> </li> <li> <p>search engine을 하나의 tool로 활용하는 방식</p> </li> </ol> <p>위 방법 덕분에 LLM이 external knowledge를 활용할 수 있긴 하지만, 최근 연구 (multi-turn, multi-query retrieval) 역시 본질적으로 **LLM이 search engine과 상호작용하는 방식을 최적화하지 못한 채 prompt에만 의존하는 한계점이 존재함. **</p> <p>다른 방법으로 LLM이 추론 과정에서 search engine을 포함한 여러 tool을 사용하도록 prompting하거나 training하는 방법들이 있지만</p> <ul> <li> <p>prompting 방법 역시 LLM의 pre-training 단계에서 경험하지 못한 작업에 generalize가 잘 안되는 문제</p> </li> <li> <p>training 기반 방식은 더 나은 adaptability를 보이지만 대규모 high quality annotated trajectories가 필요하고 search 연산이 미분이 불가능하기 때문에 end-to-end gradient descent로 최적화하기 어려움</p> </li> </ul> <p>한편으로 RL은 LLM의 reasoning capability를 높이는 robust 방법으로 최근에 주목 받는데, 이것을 **search-and-reasoning **scenarios에 적용하는 데는 3가지 문제가 있음</p> <ol> <li> <p><strong>RL framework and Stability</strong>: search engine을 어떻게 RL에 효과적으로 통합할지, 특히 검색된 context를 포함할 때 안정적인 최적화를 어떻게 보장할지 명확하지 않음</p> </li> <li> <p><strong>Multi-Turn Interleaved Reasoning and Search</strong>: 이상적으로는 LLM이 반복적으로 추론하고 search engine을 호출하며 문제의 난이도에 따라 검색 전략을 동적으로 조정할 수 있어야 함</p> </li> <li> <p><strong>Reward Design</strong>: Search와 Reasoning tasks에 의미 있고 일관된 검색 행동을 학습하게끔 유도할 수 있는 효과적인 reward function 설계가 필요하지만, 단순한 결과 기반 보상이 충분한지는 아직 불확실함.</p> </li> </ol> <p>→ 3번은 자기들도 모르면서 뭔가 싶네요 ㅋㅋ</p> <p>→ 이러한 문제를 해결하기 위해 <strong><em>Search-R1</em></strong>을 소개함. 이것은 LLM이 자체 추론 과정과 search engine을 interleaved하게 연계하여 사용할 수 있도록 설계가 됨.</p> <p>주요 특징은 다음과 같음</p> <ol> <li> <p>Search engine을 environment의 일부로 modeling하여, <strong>LLM의 token 생성과 검색 결과 호출이 혼합된 trajectory를 샘플링할</strong> 수 있음.</p> </li> <li> <p><strong>Multi-turn retrieval과 reasoning을 지원함</strong>. <search>와 </search> token으로 검색 호출을 트리거하고, 검색 결과는 <information>와 </information> 토큰으로, LLM의 추론 단계는 <think>와 </think> 토큰으로, 최종 답변은 <answer>와 </answer> 토큰으로 감싸 구조적이고 반복적인 의사결정이 가능함</p> </li> <li> <p>process-based rewards 대신 단순한 <strong>outcome-based reward function을 적용하여</strong> 복잡성을 줄임</p> </li> </ol> <h1 id="2-related-works">2. Related Works</h1> <p>2.1 Large Language Models and Retrieval</p> <p>(생략)</p> <p>2.2 Large Language Models and Reinforcement Learning</p> <p>(생략)</p> <h1 id="3-search-r1">3. Search-R1</h1> <p><strong>(1) extending RL to utilize search engines</strong></p> <p><strong>(2) text generation with an interleaved multi-turn search engine call</strong></p> <p><strong>(3) the training template</strong></p> <p><strong>(4) reward model design</strong></p> <h2 id="31-reinforcement-learning-with-a-search-engine">3.1 Reinforcement Learning with a Search Engine</h2> <p>Search-R1은 search engine R을 활용하는 RL의 objective function을 아래와 같이 정의함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>r_{\phi}: output quality를 평가하는 reward function</p> </li> <li> <p>\pi_\theta: policy LLM</p> </li> <li> <p>\pi_{ref}: reference LLM</p> </li> <li> <p>x: dataset D에서 추출된 input sample</p> </li> <li> <p>y: search engine calling 결과와 interleaved된 generated outputs</p> </li> <li> <p>D_{KL}: KL-divergence</p> </li> </ul> <p>기존 RL은 원래 \pi_\theta가 생성한 sequence만 학습하지만, Search-R1은 검색 호출과 추론이 교차된 (interleaved) 형태를 학습에 explicit하게 포함함.</p> <ul> <li> <table> <tbody> <tr> <td>retrieval interleaved reasoning via \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> <ul> <li>\bigotimes denotes interleaved retrieval-and-reasoning</li> </ul> </li> </ul> <p>즉, 추론 중 검색 결과를 반영하는 흐름을 통해 external information가 필요한 reasoning-intensive tasks에서도 더 효과적인 결정을 내릴 수 있게 해줌</p> <ul> <li>*<strong>*Formulation of RL with a Search Engine**</strong> </li> </ul> <p>LLM에서 자주 사용하는 원래 기존 RL의 objective는 아래와 같이 정의됨</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>그런데, 위 formulation은 entire output sequence y가 \pi_{\theta}로부터 생성되었다는 가정이 있음. 이 가정은 model behavior가 internal reasoning과 external information retrieval을 모두 포함하는 상황에서 적용할 수 없음.</p> <p>따라서, RL objective를 serach engine R과 통합시키기 위해 아래와 같이 수정함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>위 수정된 objective에서는 trajectory y 는 interleaved reasoning steps과 retrieved content를 포함</p> <p><strong>Loss Masking for Retrieved Tokens</strong></p> <p>PPO와 GRPO에서는 token-level loss를 전체 rollout sequence에 대해 계산함. 하지만 Search-R1의 rollout sequence는 LLM이 직접 생성한 token과 external knowledge에서 가져온 token이 함께 포함됨.</p> <p>LLM이 직접 생성한 token에 대해 손실을 최적화하는 것은 model이 search engine과 효과적으로 상호작용하고 추론하는 능력을 높이는데 도움됨. 그러나, 동일한 최적화를 검색된 token에까지 적용하면 원치 않는 학습 효과가 발생할 수 있음.</p> <p>따라서, Search-R1은 <strong>검색된 token에 대한 loss masking을 적용하여</strong>, policy gradient objective은 LLM이 생성한 token에 대해서만 계산하고, <strong>검색된 content는 최적화 과정에서 제외됨</strong>.</p> <p>→ 검색 기반 생성의 유연성은 유지하면서 학습 안정성을 높임</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>PPO with Search Engine</strong></p> <p>Search-R1에서는 검색 호출이 포함된 시나리오에 맞춰 PPO를 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>\pi_{\theta}: current policy</p> </li> <li> <p>\pi_{old}: previous policy</p> </li> <li> <p>I(y_t): token loss masking 연산으로, y_t가 LLM이 생성한 token이면 1, 검색된 token이면 0으로 설정</p> </li> </ul> <p><strong>GRPO with Search Engine</strong></p> <p>GRPO 역시 PPO와 마찬가지로 Search Engine을 적용할때, 검색된 token은 masking 적용함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="32-generation-with-multi-turn-search-engine-calling">3.2 Generation with Multi-turn Search Engine Calling</h2> <p>Search-R1이 어떻게 multi-turn search와 text 생성을 interleaved하게 수행하는지 rollout process를 수식적으로 나타내면 다음과 같음</p> <ul> <li> <table> <tbody> <tr> <td>y ~ \pi*{\theta}(.</td> <td>x;R) =\pi*{ref}(.</td> <td>x)\bigotimes R</td> </tr> </tbody> </table> </li> </ul> <p>→ LLM은 x를 입력 받아 Search Engine R과의 interleaved 흐름을 통해 y를 생성</p> <p>Search-R1의 생성 과정은 iterative한 구조로 진행됨</p> <ul> <li> <p><strong>LLM은 text를 생성하다가 필요할 때마다 external search engine queries를 보낸 뒤 검색 결과를 다시 반영하여 다음 generation step을 수행하며 이어가는 방식</strong></p> </li> <li> <p>system instruction은 LLM에게 external retrieval이 필요할 때 search query를 <search>와 &lt;\search&gt; token으로 감싸도록 함</search></p> </li> <li> <p>generated sequence에 이러한 token이 감지되면, system은 query를 추출해 search engine에 전달하고 적절한 relevant results를 가져옴</p> </li> <li> <p>retrieved information은 <information>과 &lt;\information&gt; token으로 감싸져 현재 rollout 시퀀스에 추가됨. 이렇게 추가된 정보는 next generation step에 추가 context로 활용</information></p> </li> </ul> <p>위 과정이 반복적으로 이어가다가 아래 두 가지 조건 중 하나를 만족하면 종료함</p> <ol> <li> <p>사전에 정의된 최대 행동 횟수에 도달할 때</p> </li> <li> <p>모델이 최종 응답을 생성하여 이를 <answer>와 &lt;\answer&gt; token으로 감쌀때</answer></p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="33-training-template">3.3 Training Template</h2> <p>Search-R1을 학습시킬때 사용하는 prompt template</p> <ul> <li> <p>아래 template은 모델이 출력할 구조를 think → search → answer 순서로 명확히 나누도록 유도함</p> </li> <li> <p>다만 특정 해결 방식이나 반영 수준을 강제하지 않아 모델이 RL 과정에서 자연스럽게 학습하도록 설계함 (구조적 형식만 따르게 제한함)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Case Study</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="34-reward-modeling">3.4 Reward Modeling</h2> <p>Search-R1은 outcome-based rule-based reward function을 사용함</p> <ul> <li> <p>예를 들어, factual reasoning task에서 정답과 모델의 출력이 일치하는지 exact match로 평가함</p> </li> <li> <p>별도의 형식 보상이나 복잡한 과정 기반 보상은 사용하지 않고, 신경망 기반 보상 모델도 학습하지 않아 학습 복잡성을 줄임</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="4-main-results">4. Main Results</h1> <h2 id="41-datasets">4.1 Datasets</h2> <ol> <li> <p>General QA</p> </li> <li> <p>Natural Questions (NQ)</p> </li> <li> <p>TriviaQA</p> </li> <li> <p>PopQA</p> </li> <li> <p>Multi-Hop QA</p> </li> <li> <p>HotpotQA</p> </li> <li> <p>2WikiMultiHopQA</p> </li> <li> <p>Musique</p> </li> <li> <p>Bamboogle</p> </li> </ol> <h2 id="42-baselines">4.2 Baselines</h2> <ol> <li> <p>Inference w/o Retrieval</p> </li> <li> <p>Direct Inference</p> </li> <li> <p>Chain-of-Thought</p> </li> <li> <p>Inference w/ Retrieval</p> </li> <li> <p>RAG</p> </li> <li> <p>IRCoT (Information Retrieval CoT)</p> </li> <li> <p>Search-o1 (using search engine tool)</p> </li> <li> <p>fine-tuning methods</p> </li> <li> <p>SFT</p> </li> <li> <p>R1: search engine없이 RL fine-tuning (Search-R1과 fair한 비교를 위해 동일 데이터로 RL을 학습하되 검색은 사용하지 않음)</p> </li> </ol> <h2 id="43-experimental-setup">4.3 Experimental Setup</h2> <ul> <li> <p>LLMs: Qwen-2.5-3B, Qwen-2.5-7B (Base / Instruct)</p> </li> <li> <p>Retrieval</p> <ul> <li> <p>Knowledge Source: 2018 Wikipedia dump (using E5 as retriever)</p> </li> <li> <p>number of retrieved documents: 3</p> </li> </ul> </li> <li> <p>Dataset</p> <ul> <li> <p>training data: NQ + HotpotQA for Search-R1 and fine-tuning methods</p> </li> <li> <p>evaluation data: (in-domain, out-of-domain)</p> </li> </ul> </li> <li> <p>metric: EM</p> </li> <li> <p>Inference 설정</p> <ul> <li>Inference-style baseline은 Instruct 모델 사용 (Base 모델은 instruction을 따르지 못함)</li> </ul> </li> <li> <p>RL 설정</p> <ul> <li>별도 언급이 없으면 PPO 사용</li> </ul> </li> </ul> <h2 id="44-performance">4.4 Performance</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Search-R1은 baselines 대비 우수한 성능 보여줌</p> <ul> <li> <p>Qwen2.5-7B: 평균적으로 41% 향상</p> </li> <li> <p>Qwen2.5-3B: 평균적으로 20% 향상</p> </li> </ul> </li> </ul> <p>→ in-domain (NQ, HotpotQA)와 out-of-domain (TriviaQA, PopQA, 2Wiki, Musique, Bamboogle) <strong>모두 일관되게 높음</strong></p> <ul> <li>검색 없이 추론만하는 R1보다도 Search-R1이 우수함</li> </ul> <p>→ <strong>Search가 LLM 추론에 external knowledge를 추가함으로써 도움되는 것을 보임</strong></p> <ul> <li>Base와 Instruct model 모두 일관되게 Search-R1 효과적임</li> </ul> <p>→ DeepSeek-R1-Zero style의 단순 outcome-based reward가 순수 Reasoning 뿐만 아니라 <strong>search를 포함한 complex reasoning scenarios에서도 효과적임을</strong> 보여줌</p> <ul> <li>**Model size가 클 수록 검색 활용 효과가 더 큼 **</li> </ul> <h1 id="5-analysis">5. Analysis</h1> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="51-different-rl-methods-ppo-vs-grpo">5.1 Different RL methods: PPO vs. GRPO</h2> <p>Search-R1에서 RL 방법으로 PPO와 GRPO 두 가지를 모두 실험함</p> <ol> <li> <p><strong>GRPO는 PPO보다 수렴 속도가 빠름</strong> → Figure2 (a)</p> </li> <li> <p>PPO는 critic model에 의존하기 때문에 효과적인 학습이 시작되려면 여러 단계의 워밍업이 필요하지만, GRPO는 baseline을 여러 샘플 평균으로 잡아 더 빠르게 수렴함</p> </li> <li> <p><strong>PPO는 학습 안정성이 더 높음</strong> → Figure2 (a)</p> </li> <li> <p><strong>GRPO는 일정 단계 이후 reward collapse</strong>가 나타나지만, <strong>PPO는 학습이 더 안정적으로 유지됨</strong></p> </li> <li> <p><strong>최종 train reward는 PPO와 GRPO 모두 유사함</strong></p> </li> <li> <p>수렴 속도와 안정성은 다르지만 최종 성능과 train reward는 큰 차이가 없음. 그래도 GRPO는 나중에 불안정해질 수 있기에 더 안정적인 PPO가 느리지만 적합함.</p> </li> </ol> <p><em>(다른 세팅에서도 동일한 현상이 관찰됨)</em></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="52-base-vs-instruct-llms">5.2 Base vs. Instruct LLMs</h2> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Figure2 (b)에서 Instruction-tuned model은 Base model보다 더 빠르게 수렴하고 초기 성능도 더 높게 나오지만, <strong>최종 train reward는 두 모델 모두 거의 동일한 수준으로 수렴함</strong> </li> </ul> <p>→ 이는 사전 instruction tuning이 초기 학습을 가속화하는데 도움이 되지만, <strong>RL만으로도 Base model이 충분히 따라잡을 수 있음을 보임</strong></p> <p>(다른 세팅에서도 동일한 현상이 관찰됨)</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="53-response-length-and-valid-search-study">5.3 Response Length and Valid Search Study</h2> <p>Qwen2.5-7B-base 모델로 response length와 검색 호출 횟수 변화를 분석함</p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Figure2 (c)를 보면</p> <ul> <li> <p>초기 단계 (100 steps 전후)</p> <ul> <li> <p><strong>응답 길이가 급격히 줄고, train reward는 소폭 상승함</strong></p> </li> <li> <p>모델이 불필요한 군더더기 단어를 줄이고 task에 적응하기 시작함을 보여줌</p> </li> </ul> </li> <li> <p>후기 단계 (100 steps 이후)</p> <ul> <li> <p><strong>응답 길이와 train reward 모두 증가함</strong></p> </li> <li> <p>모델이 검색 호출을 더 자주 하면서 (Search Engine을 자주 호출하는 법 학습) 검색 결과가 추가되어 응답이 길어짐</p> </li> <li> <p>검색 결과를 효과적으로 활용하며 train reward도 크게 향상됨</p> </li> </ul> </li> </ul> </li> <li> <p>Figure2 (d)를 보면 <strong>학습이 진행될수록 LLM이 검색 엔진 호출을 더 많이 학습한다는 점이 드러남</strong></p> </li> </ul> <h2 id="54-study-of-retrieved-tokens-loss-masking">5.4 Study of Retrieved Tokens Loss Masking</h2> <p>Retrieved Token Loss Masking은 unintended optimization을 방지하기 위해 도입한 것임. Retrieved token loss masking의 효과를 추가로 분석해봄 (Qwen2.5-7B-base)</p> <ul> <li>Figure 3에 따르면, <strong>masking을 적용하면 원치 않는 최적화 효과를 줄이고 LLM 성능 향상이 더 커짐</strong> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>w. mask와 w.o. mask를 비교한 결과 **masking을 적용한 경우가 항상 더 높은 성능을 기록함 **</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>*<strong>*Appendix**</strong> </li> </ul> <p><strong>Number of Retrieved Passages Study in SEARCH-R1 Training</strong></p> <ul> <li>본 실험에서는 top-k를 3으로 설정했지만, 1,3,5 바꿔가며 이것의 effect를 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ top-k가 1,3,5 설정이 <strong>모두 training pattern이 비슷함</strong> (top-k=5가 초기 수렴 속도가 빠른 대신에 이후 train reward가 감소하며 학습 안정성 떨어짐을 보임)</p> <p><strong>Group Size Study in SEARCH-R1 (GRPO) Training</strong></p> <ul> <li>본 실험에서는 Search-R1 (GRPO)의 group size를 5로 설정했지만, group size가 어떤 영향을 미치는지 확인하고자 1,3,5로 분석함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Figure7을 보면 <strong>group size가 칼수록 수렴 속도 빨라지는 반면 RL의 불안정성 때문에 training collapse 위험도 증가</strong></p> <figure> <picture> <img src="/assets/img/posts/2025-07-15-search-r1-training-llms-to-reason-and-leverage/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ Table8을 보면 group size가 큰 경우 빠른 수렴과 더 높은 train reward가 있었지만, group size=1일 때 학습이 더 안정적이고 일반화 성능이 더 우수함 (out-of-domain에서 더 우수함)</p> <h1 id="6-conclusion">6. Conclusion</h1> <ul> <li> <p>본 연구에서는 LLM이 self-reasoning과 실시간 검색 엔진 상호작용을 교차적으로 수행할 수 있는 framework인 Search-R1 제안함</p> </li> <li> <p>기존의 multi-turn search를 위해 많은 prompt에 의존하는 RAG나 대규모 train data가 필요한 tool 사용 기반 접근법과 달리, <strong>Search-R1은 RL을 통해 모델이 자율적으로 검색 쿼리를 생성하고 검색된 정보를 전략적으로 활용할 수 있도록 최적화함</strong></p> </li> </ul> <p>Limitations</p> <ul> <li>Reward Design가 단순 결과 기반 보상이라 보다 디벨롭이 필요함</li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>