<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Diffusion Language Model-Mathematical foundations &amp; inference optimization | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - DiffusionLM, Pre-training 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2025/diffusion-language-model-mathematical-foundations-inference-optimization/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Diffusion Language Model-Mathematical foundations &amp; inference optimization</h1> <p class="post-meta"> Created on January 02, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusion</a>   <a href="/blog/tag/diffusionlm"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusionlm</a>   <a href="/blog/tag/embedding"> <i class="fa-solid fa-hashtag fa-sm"></i> embedding</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2025-01-02</li> <li> <strong>Reviewer</strong>: 김재희</li> <li> <strong>Property</strong>: DiffusionLM, Pre-training</li> </ul> <blockquote> <p>masked diffusion language model의 수학적 기초를 다진 논문에 대해 살펴보겠습니다.</p> </blockquote> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="preliminary">Preliminary</h1> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Forward process: 원본 데이터에 대해 일정 비율의 노이즈를 입력하여 훼손하는 과정</p> <ul> <li> <p>최대 T번 반복</p> </li> <li> <p>scherduler: step t에서 유지할 정보의 수준을 정함 or 입력할 노이즈의 수준</p> </li> <li> <p>prior: 노이즈가 샘플링되는 분포 or 생성 시 가장 첫 시점(t=T)의 데이터 분포</p> </li> </ul> </li> <li> <p>Backward process: t번 훼손된 데이터에 대하여 s번 훼손된 데이터로 복원하는 과정 (t &gt; s)</p> <ul> <li>실제 모델의 학습 대상 → 완전한 노이즈 데이터에서 실제 데이터를 생성(샘플링) 할 수 있게 됨</li> </ul> </li> <li> <p>training objective function</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>원래 objective function(- \log p_{\theta}(x_0))은 필요한 계산이 너무 많음</p> <ul> <li>완전한 노이즈(x_T)부터 T개의 모든 스텝에 대해 1) 노이즈를 가하고 2) 모델을 통해 복원하는 과정을 수행해야 함</li> </ul> </li> </ul> <p>⇒ 매우 연산량/메모리를 많이 차지하게 됨</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>원 목적함수의 ELBo를 변분추론(몰라요…)을 통해 산출하게 됨</p> <ul> <li>Expectation 항으로 구성되기 때문에 모든 step 계산이 필요 없어짐</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- x_0: 실제 데이터(그냥 배치로 가져오는 과정)

- t: uniform dist에서 샘플링 (매 데이터마다 하나의 step에 대해서만 학습 진행) ⇒ 원 목적함수 대비 연산량이 매우 줄어들게 됨.

- \eta: 실제 noise, 매 iter마다 다른 noise를 주입해야 하므로 새로 sampling
</code></pre></div></div> <ul> <li> <p>실제로는 stepwise denoising term만을 사용하여 학습이 진행되게 됨</p> <ul> <li>실제로 t-1 → t step에서 가해진 noise를 모델이 예측하도록 학습됨.</li> </ul> </li> </ul> <blockquote> <p>Diffusion Model의 주된 연구 분야인 이미지는 continuous한 변수(pixel)을 다루기 때문에 모든 변수 및 과정이 이를 기반으로 진행됨.</p> </blockquote> <h1 id="mdlmmasked-diffusion-language-model">MDLM(Masked Diffusion Language Model)</h1> <blockquote> <p>텍스트 도메인에 적절한 diffusion modeling은 무엇일까?</p> </blockquote> <h2 id="텍스트-도메인의-특징-뇌피셜">텍스트 도메인의 특징 (뇌피셜)</h2> <ol> <li>text: 매우 고밀도의 정보가 보존된 도메인. 이미지와 다르게 정보량이 거의 없는 변수가 적음</li> </ol> <p>⇒ embedding에 가하는 노이즈는 크기가 매우 작더라도 큰 의미변화를 만들어낼 수 있음</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>discrete: 단어는 존재하거나, 존재하지 않는 binary한 변수임</p> </li> <li> <p>continuous한 noise를 삽입하는 interpolation이 불가능함</p> </li> <li> <p>embedding을 이용한 diffusion 방법론이 있지만 아직까지는 MDLM이 더 나은 방법론으로 보임</p> </li> </ol> <h2 id="수식-전개-discrete-diffusion">수식 전개 Discrete Diffusion</h2> <p>\textit{V} = [사과, 존맛탱, mask]</p> <p>입력문장: 사과 존맛탱</p> <h3 id="forward-process-노이즈를-주입-actual-token--mask">forward process: 노이즈를 주입, actual token → mask</h3> <p>q\left(\mathbf{z}_t \mid \mathbf{x}\right)=\operatorname{Cat}\left(\mathbf{z}_t ; \alpha_t \mathbf{x}+\left(1-\alpha_t\right) \boldsymbol{\pi}\right)</p> <ul> <li> <p>Terms</p> <ul> <li> <p>x: 원본 텍스트(1-hot encoding), [1, 0, 0]</p> </li> <li> <p>\alpha_t: t 시점의 원본 텍스트 보존 비율, 1-\alpha_t: masking token 비율, scheduler를 통해 결정됨</p> </li> </ul> </li> </ul> <p>(\alpha<em>t &lt; \alpha</em>{t-1})</p> <ul> <li> <p>\pi: noise 분포, MDLM에서는 그냥 mask 토큰을 noise로 사용하므로 mask 토큰의 인덱스만 1인 1-hot vector로 볼 수 있음 [0,0,1]</p> </li> <li> <p>설명</p> <ol> <li> <table> <tbody> <tr> <td>q(\textbf{z}_t</td> <td>\textbf{x}): 원본 텍스트(x)에서 t step의 훼손된 텍스트(\textbf{z}_t) 가 산출될 확률 [0.7, 0, 0.3]</td> </tr> </tbody> </table> </li> <li> <p>\alpha_t\textbf{x}: 원본 텍스트가 보존될 확률, [0.7, 0, 0]</p> </li> <li>(1-\alpha_t)\pi: 마스크 토큰으로 전환될 확률 [0, 0, 0.3]</li> </ol> </li> </ul> <p>⇒ 매 시점마다 점차 많은 토큰들이 mask 토큰으로 전환됨</p> <h3 id="reverse-posterior-노이즈를-복원-mask--actual-token">reverse posterior: 노이즈를 복원, mask → actual token</h3> <p>reverse posterior</p> <p>q\left(\mathbf{z}<em>s \mid \mathbf{z}_t, \mathbf{x}\right)=\operatorname{Cat}\left(\mathbf{z}_s ;\frac{\left[\alpha</em>{t \mid s} \mathbf{z}<em>t+\left(1-\alpha</em>{t \mid s}\right) \mathbf{1} \boldsymbol{\pi}^{\top} \mathbf{z}_t\right] \odot\left[\alpha_s \mathbf{x}+\left(1-\alpha_s\right) \boldsymbol{\pi}\right]}{\alpha_t \mathbf{z}_t^{\top} \mathbf{x}+\left(1-\alpha_t\right) \mathbf{z}_t^{\top} \boldsymbol{\pi}}\right)</p> <ul> <li> <p>t step에서 이전 시점 s(&lt;t)까지 노이즈를 복원하기 위한 추정 확률</p> <ul> <li> <p>어떤 토큰이 masking되었는지 알 수 없으므로 확률적으로 추정해야 함</p> </li> <li> <table> <tbody> <tr> <td>0&lt;\alpha_{t</td> <td>s} = \frac{\alpha_t}{\alpha_s}&lt;1: s step과 t step 사이에서 보존되는 원본 토큰 비율</td> </tr> </tbody> </table> <ul> <li> <p>length: 100</p> </li> <li> <p>s(70) → t(50)</p> </li> <li> <table> <tbody> <tr> <td>\textbf{a}_{t</td> <td>s} = \frac{0.5}{0.7}=\frac{5}{7}</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>\alpha<em>{t \mid s} \mathbf{z}_t+\left(1-\alpha</em>{t \mid s}\right) \mathbf{1} \boldsymbol{\pi}^{\top}\textbf{z}^t: s step과 t step 사이에서 보존되었던 원본 토큰 확률(\alpha_{t \mid s} \mathbf{z}_t)과 masking으로 변해버렸던 토큰의 확률(\mathbf{1} \boldsymbol{\pi}^{\top} \mathbf{z}_t)</p> <ul> <li> <p>s step에서 t step으로 진행되면서 masking된 토큰을 추정하는 항</p> </li> <li> <p>어떤 토큰을 복원할지 정하는 항</p> </li> <li> <p>[0.7, 0.7, 0.7] [1, 0, 0] + [0.3, 0.3, 0.3] [1,1,1][1,0,0]</p> <ul> <li>[0.7, 0, 0] + [0, 0., 0.3] = [0.7, 0, 0.3]</li> </ul> </li> </ul> </li> <li> <p>\alpha_s\textbf{x} + (1-\alpha_s) \boldsymbol{\pi}: 원본 토큰(\textbf{x})의 확률과 masking 토큰으로 처리될 확률(\pi)</p> <ul> <li>s step에서는 \alpha_s 비율의 토큰만 원본 토큰이어야 하고, (1-\alpha_s) 는 마스크 토큰이어야 하므로 1) 어떤 위치를 원본 토큰으로 남길지 2) 해당 위치에 어떤 토큰으로 복원할지 정하는 항임</li> </ul> </li> </ul> </li> </ul> <blockquote> <p>사실 지금까지의 수식은 masked diffusion은 아닙니다. 지금까지의 수식은 아래와 같은 특성을 가정하고 전개되었습니다.</p> </blockquote> <ol> <li>discrete한 time step</li> <li>\pi의 noise distribution이 존재</li> </ol> <p>하지만 masked diffusion은 이를 좀 더 단순화해서 사용할 수 있습니다. 왜냐하면 \pi=m이기 때문입니다.</p> <p>⇒ 노이즈는 임의의 분포가 아니라 무조건 masking token이다! 그러므로 이전 step에서 masking된 토큰은 이후에도 무조건 masking되어 존재하게 된다!</p> <h2 id="masked-diffusion">Masked Diffusion</h2> <h3 id="forward-masking-process">forward masking process</h3> <p>q\left(\mathbf{z}_t \mid \mathbf{x}\right)=\operatorname{Cat}\left(\mathbf{z}_t ; \alpha_t \mathbf{x}+\left(1-\alpha_t\right) \boldsymbol{m}\right)</p> <ul> <li>discrete diffusion에서 \pi가 m으로 변한 것 외에 차이 없습니다.</li> </ul> <h3 id="reverse-posterior-실제-loss-식을-산출하기-위해-필요한-항">reverse posterior: 실제 loss 식을 산출하기 위해 필요한 항</h3> <p>q\left(\mathbf{z}_s \mid \mathbf{z}_t, \mathbf{x}\right)= \begin{cases}\operatorname{Cat}\left(\mathbf{z}_s ; \mathbf{z}_t\right) &amp; \mathbf{z}_t \neq \mathbf{m} \ \operatorname{Cat}\left(\mathbf{z}_s ; \frac{\left(1-\alpha_s\right) \mathbf{m}+\left(\alpha_s-\alpha_t\right) \mathbf{x}}{1-\alpha_t}\right) &amp; \mathbf{z}_t=\mathbf{m}\end{cases}</p> <ul> <li> <p>\textbf{z}_t \neq \textbf{m}: step t에서 원본토큰이라면 → 그대로 유지</p> </li> <li> <p>\textbf{z} = \textbf{m}: step t에서 masking token이라면 → step s에서 t 사이에서 masking되었을 확률 산출</p> <ul> <li>근데 실제 텍스트(\textbf{x})를 알아야 산출할 수 있는 항</li> </ul> </li> </ul> <p>⇒ 복원 단계에서는 알 수 없으니 MDLM이 등장</p> <p>⇒ \textbf{x}_\theta(\textbf{z}_t, t): \theta를 파라미터로 가지는 모델에 의해 t step에서 복원된 문장</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="mdlm의-상황에-맞춘-2가지-property">MDLM의 상황에 맞춘 2가지 property</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>Zero Masking Probabilities: &lt;\textbf{x}, \textbf{m}&gt;=0임. 즉, 원본 토큰 중에는 masking token이 사용되지 않음</li> </ol> <p>⇒ 모델을 통해 복원할 때, mask token에 대한 logit은 -\infin로 처리</p> <ol> <li> <p>Carry-Over Unmasking: step t에서 복원(unmasking)된 토큰은 이후 모델의 복원 과정에서 수정되지 않음</p> </li> <li> <p>t=4에서 “a”라고 복원되었다면, t=1에서 “b”로 바뀌지 않고 무조건 “a”로 고정</p> </li> </ol> <h3 id="rao-balckwellized-likelihood-bounds">Rao-Balckwellized Likelihood Bounds</h3> <p>diffusion loss를 산출하듯이 본래 학습할 discrete-time diffusion의 loss의 lower bound를 산출하면 아래와 같음</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="continuous-time-likelihood-bounds">Continuous-Time Likelihood Bounds</h3> <p>기존 연구에서 정리하기로 T \to \infin로 정의할 경우 더욱 tight한 lower bound를 산출할 수 있음</p> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="masked-diffusion-language-models">Masked Diffusion Language Models</h3> <p>앞에서 정의된 tight한 lower bound를 language modeling 상황으로 가져오기 위해 아래 가정들을 적용할 수 있음</p> <ol> <li> <p>\textbf{x}^{1:L}: L개의 token sequence</p> </li> <li> <p>\textbf{x}^\textit{l}: \textit{l}번째 토큰</p> </li> <li> <p>forward와 backward 모두 각 토큰들이 독립적으로 진행</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>\textbf{x}^\textit{l}_\theta(\textbf{z}_t^{1:L}, t): t번째 step에서의 시퀀스에 대한 모델의 예측 문장, masking된 토큰을 예측하여 복원한 문장문</p> </li> <li> <p>\log&lt;\textbf{x}^\textit{l}_\theta(\textbf{z}_t^{1:L}, t), \textbf{x}^\textit{l}&gt;: loglikelihood, 갑자기요…?!</p> </li> <li> <p>이때 \alpha_s 가 사라진 것을 확인할 수 있음</p> <ul> <li> <p>masking과 unmasking 시 scheduler의 필요성 X → 많은 연산을 생략할 수 있음, 추론 시 이점 존재</p> </li> <li> <p>실제 masking 비율에 따라 다른 loss 반영</p> <ul> <li> <p>많은 토큰이 masking된 경우(\alpha_t가 작은 경우): weight가 작아짐 ⇒ 모델이 불확실할 수 밖에 없는 상황에서 학습을 적게 반영</p> </li> <li> <p>적은 토큰이 masking된 경우(\alpha_t가 큰 경우): weight가 커짐 ⇒ 모델이 확실히 맞추어야 하는 데이터에 대해 학습을 많이 반영</p> </li> </ul> </li> </ul> </li> </ul> <blockquote> <p>MDLM은 결국 discrete-time diffusion model을 masking과 text domain에 적합한 가정을 도입한 결과 weighted sum of MLM이라는 결론에 도달</p> </blockquote> <p>BERT의 목적함수를 일부 변형하는 것으로 도입이 가능해짐!</p> <h3 id="training-algorithm">Training Algorithm</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li> <p>데이터 sampling</p> </li> <li> <p>step sampling</p> </li> <li> <p>이전 step 대비 추가적으로 \alpha_t 비율을 masking한 masked input 산출</p> </li> <li> <p>weighted sum of MLM loss 형식으로 update</p> </li> </ol> <h2 id="actual-inference">Actual Inference</h2> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>해당 수식을 통해 실제 생성이 이루어지게 됨</p> <ol> <li>\textbf{z}_t = \textbf{m}: t step에서 mask 토큰으로 입력된 위치에 대해서만 예측 수행</li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>\frac{(1-\alpha<em>s)\textbf{m} + (\alpha_s - \alpha_t)\textbf{x}</em>\theta(\textbf{z}<em>t, t)}{1-\alpha_t} = \frac{1-\alpha_s}{1-\alpha_t}\textbf{m} + \frac{\alpha_s-\alpha_t}{1-\alpha_t}\textbf{x}</em>\theta(\textbf{z}_t, t): 모든 mask 토큰 위치에서 예측된 확률분포 중에서 \frac{1-\alpha_s}{1-\alpha_t} 비율은 다시 masking으로 돌림</li> </ol> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>⇒ 매 iteration 마다 \frac{\alpha_s - \alpha_t}{1-\alpha_t}만큼의 토큰이 복원되면서 생성</p> <h1 id="experiments">Experiments</h1> <h3 id="1-perplexity-evaluation">1. Perplexity evaluation</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>동일한 corpus를 이용하여 autoregressive model과 MDLM을 학습시켜 비교</li> </ul> <p>⇒ 실제 입력되는 non-masked token 수의 차이를 없애기 위한 다른 update step 사용</p> <ul> <li> <p>autoregressive model: 0.5M step</p> </li> <li> <p>MDLM: 1M step</p> </li> </ul> <h3 id="2-training-nll">2. Training NLL</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>기존 DDLM(SEDD)보다 훨씬 안정적인 NLL을 보이며 학습</li> </ul> <p>⇒ 2가지 property를 이용하여 tight한 lower bound를 형성한 덕분</p> <h3 id="3-zero-shot-perplexity">3. Zero-shot Perplexity</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>다양한 분야에 대하여 AR과 근접한 수준의 성능 달성</p> <ul> <li>PubMed, Arxiv와 같은 특수 도메인의 경우에는 DDLM이 AR보다 나은 모습을 보임 → Unmasking 학습 방식 자체가 OOD에 robust할 수 있음</li> </ul> </li> </ul> <h3 id="4-downstream-task">4. Downstream Task</h3> <figure> <picture> <img src="/assets/img/posts/2025-01-02-diffusion-language-model-mathematical-foundations-inference-optimization/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>BERT를 MDLM으로 일부 finetune한 결과로 비교</p> <ul> <li> <p>5k step finetune 진행</p> </li> <li> <p>사실 큰 의미는 모르겠음…</p> </li> </ul> </li> </ul> <h1 id="conclusion">conclusion</h1> <ul> <li> <p>수식 전개를 통해서 weighted sum of MLM 형식의 diffusion model formulation</p> </li> <li> <p>기존 diffuion lm들보다 좋은 성능과 안정된 학습을 보임</p> <ul> <li>(여기서 다루지는 않았지만) 간단한 추론 caching 및 생성 방법론도 함께 제시하여 후속 연구로 이어짐</li> </ul> </li> <li> <p>해당 논문의 프레임워크가 이후 LLaDA 등에서 활용 ⇒ 아마 향후 MDLM의 표준이 되지 않을까…?</p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>