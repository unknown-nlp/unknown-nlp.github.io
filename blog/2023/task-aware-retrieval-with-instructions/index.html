<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Task-aware Retrieval with Instructions | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Retrieval, Instruction Tuning 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Task-aware Retrieval with Instructions"> <meta property="og:url" content="https://unknownnlp.github.io/blog/2023/task-aware-retrieval-with-instructions/"> <meta property="og:description" content="논문 리뷰 - Retrieval, Instruction Tuning 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Task-aware Retrieval with Instructions"> <meta name="twitter:description" content="논문 리뷰 - Retrieval, Instruction Tuning 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknownnlp.github.io/blog/2023/task-aware-retrieval-with-instructions/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - Retrieval, Instruction Tuning 관련 연구",
        "headline": "Task-aware Retrieval with Instructions",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/task-aware-retrieval-with-instructions/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Task-aware Retrieval with Instructions</h1> <p class="post-meta"> Created on January 26, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/generative"> <i class="fa-solid fa-hashtag fa-sm"></i> generative</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/instruction-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> instruction tuning</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> retrieval</a>   <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-01-26</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: Retrieval, Instruction Tuning</li> </ul> <h1 id="1-introduction">1. Introduction</h1> <p><strong>Information Retrieval: **the task of finding **_relevant _</strong>documents from a large colection of texts</p> <ul> <li> <p><strong>Relevance</strong></p> <ul> <li> <p>amorphous (체계나 통일성이 없는)</p> </li> <li> <p>a query alone may not fully caputre user information needs</p> </li> </ul> </li> </ul> <p>→ 아래 Figure 예시에서 보이는 바와 같이, <strong><em>‘Implementing batch normalization in Python’</em></strong> query가 주어질 때, query만 가지고는 &lt;(1) query의 뜻을 python 함수로 구현하는지, (2) query 자체에 대한 동일한 질문을 찾는지, (3) 단편적인 함수를 찾는지&gt; 알 수가 없다.</p> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>지금까지 연구된 Retrieval module은 이러한 <strong><em>Implicit Intent</em></strong>를 labeled data을 활용해서 독립적인 모델로부터 학습을 시킨다. 이렇게 하면 당연히 몇 가지 Limitations들이 존재하는데,</p> <ol> <li> <p>task-specific notion of relevance를 학습하기 위해서 model은 labeled data가 많이 필요</p> </li> <li> <p>하나의 task에 학습된 model은 다른 새로운 task에 쉽게 transfer 할 수 없음</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong><em>Retrieval with instructions</em></strong>: explicitly하게 User의 의도를 자연어로 구성된 description을 통해 모델링하는 task → 이 task의 목표는 query와 relevant하면서 instruction에 잘 반영된 document를 찾는 것이다.</p> <ul> <li> <p><strong>BERRI</strong>: 대략 40개의 retrieval dataset으로 다양한 Instruction을 포함하며 구성된 데이터셋 (10개 이상의 domain)</p> </li> <li> <p><strong>TART</strong>: BERRI dataset을 기반으로 학습 시킨 single multi-task retrieval system (task-aware retriever)</p> <ul> <li> <p>TART-dual: dual-encoder 구조</p> </li> <li> <p>TART-full: cross-encoder 구조 → (show state-of-the-art on <strong>BEIR</strong>, <strong>LOTTE</strong>-pooled)</p> </li> </ul> </li> <li> <p><strong>X^2 **</strong>-Retrieval** (Cross-task Cross-domain Retrieval) Evaluation 제시: 다양한 intents들이 있는 query들이 large-scale + cross-domain한 corpus에서 relevant documents를 찾을 수 있는지 평가하는 task</p> </li> </ul> </li> </ul> <h1 id="2-background-and-related-work">2. Background and Related Work</h1> <h3 id="zero-shot-training-of-retrievers">Zero-shot training of retrievers</h3> <ul> <li> <p><strong>최근에 Neural Netwrok를 활용해서 term-based retrievers (BM25)보다 retrieval task에서 높은 성능을 보인 연구들이 다수 존재하지만, 그에 맞는 training data가 많이 필요한 문제가 있음</strong></p> <ul> <li> <p>Dense Passage Retrieval for Open-Domain Question Answering (Karpukhin et al. 2020)</p> <ul> <li>Dual-encoder 구조로 ODQA task에서 처음으로 BM25보다 높은 성능을 보임</li> </ul> </li> <li> <p>One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval (Asai et al. 2021)</p> <ul> <li>DPR의 multilingual version</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>KILT: a Benchmark for Knowledge Intensive Language Tasks (Petroni et al. 2021)</p> <ul> <li>KILT benchmakr 제안 → Wiki 기반으로 여러가지 Knowledge Intensive task 합친 것</li> </ul> </li> <li> <p><strong>따라서, zero-shot setting에서 Neural Network 기반 Retrieval을 연구하는 시도가 많아짐</strong></p> <ol> <li> <p>Unsupervised approach</p> </li> <li> <p>Contriever</p> </li> <li> <p>Train a single retreival on large-scale supervised dataset</p> </li> <li> <p>MS MARCO에 학습을 한 뒤 다른 dataset에 적용</p> </li> <li> <p>Train customized retrievers for each task on unlabeled corpora</p> </li> <li> <p>GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval (Wang et al. 2022)</p> </li> </ol> </li> </ul> <h3 id="instruction-tuning">Instruction tuning</h3> <ul> <li> <p><strong>최근에 instructions에 LLMs에 더해 다양한 task에서 zero-shot, few-shot 성능을 높인 연구들이 많이 존재함.</strong></p> <ul> <li> <p>FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS (Wei et al. 2022)</p> <ul> <li>FLAN 제시한 논문: Instruction tuning (LMs을 instruction이 있는 dataset에 finetuning)</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>MULTITASKPROMPTEDTRAININGENABLES ZERO-SHOTTASKGENERALIZATION (Sanh et al. 2022)</p> <ul> <li> <p>T0 제시한 논문: zero-shot generalization을 multitask learning으로 explicit하게 학습해서 구현</p> </li> <li> <p>NLP tasks를 human-readable prompted format으로 만들어 multi-task training으로 Enc-Dec 모델을 학습하고, test할 때 zero-shot으로 진행</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Training language models to follow instructions with human feedback (Ouyang et al. 2022)</p> <ul> <li> <p>InstructGPT 제시한 논문: 사전학습된 GPT-3를 RLHF를 통해 강화시킨 모델</p> <ul> <li> <p>Sampling된 prompt에 대해 GPT-3가 출력한 결과에 대해 사람이 직접 Ranking 매김</p> </li> <li> <p>PPO를 통해 Reward Model training</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>Scaling Instruction-Finetuned Language Models (Chung et al. 2022)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>하지만 large-sacle instruction-annotated datasets은 retrieval task를 포함하지 않는다….정말 대부분의 task에 대한 instruction-annotated dataset이 있는데 retrieval만 부재</strong></p> <ul> <li> <p>PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts (Bach et al. 2022)</p> <ul> <li> <p>Open-source system을 직접 구축해서, community에 있는 다양한 사람들로 하여금 Natural Language Prompt 구축</p> </li> <li> <p>2022년 1월 기준으로 170개 데이터셋에 대해 2000개의 prompts가 구축되어 있음. (필요할 때, 사용 가능 😀)</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks (Wang et al. 2022)</p> <ul> <li> <p>Super-NaturalInstruction이라는 benchmark dataset 제시 (1616개 NLP task + expert-written instructions)</p> </li> <li> <p>해당 데이터셋에 학습시킨 Tk-INSTRUCT를 제시하며, InstructGPT보다 높은 성능을 보임</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>대부분의 Instruction을 따른 LLMs들은 generation이 목적인 architecture를 주로 갖고 있는데 (encoder-decoder, decoder-only), 수 백만 건의 documents를 encoding을 해야하는 retrieval task에는 부적절함</li> </ul> <h3 id="retrieval-with-descriptions">Retrieval with descriptions</h3> <ul> <li> <p>이전에 retrieval module에 description을 활용한 연구는 title을 활용한 baseline에 비해 살짝 좋은 성능을 보임</p> </li> <li> <p>최근에 와서 BERT 기반 LMs들이 등장하면서 더 풍부한 linguistic context를 잡아낼 수 있음</p> <ul> <li> <p>Context-Aware Document Term Weighting for Ad-Hoc Search (Dai and Callan 2020)</p> </li> <li> <p>Deeper Text Understanding for IR with Contextual Neural Language Modeling (Dai and Callan 2019)</p> </li> </ul> </li> </ul> <h1 id="3-task-formulation">3. Task Formulation</h1> <ul> <li><strong><em>Retrieval with instructions (New Task)</em></strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Given,</p> <ul> <li> <p>N documents: D={d_1,…d_N}</p> </li> <li> <p><strong>Task Instruction: **</strong>t**</p> </li> <li> <p>Query: q</p> </li> </ul> </li> </ul> <p><strong>→ Find out optimal document **</strong>d** (instruction t를 잘 따르며 q와 relevant가 높은 문서)</p> <p>Retrieval with instruction으로 하여금 general하고 task-aware한 retrieval을 만들어보자. 이렇게 retrieval task를 새롭게 정의하면, 다른 데이터셋들이 하나의 retriever를 학습시키는데 같이 사용될 수 있는 <em>(LLM의 Instruction tuning에서 보인 것과 마찬가지로 Cross-task interdependence에 있어 성능 향상 가능)</em> 변화가 있다.</p> <p>→ zero-shot transfer이 가능하고 multi-task instruction-following retriever는 다양한 종류의 task-specific retrievers를 사용하지 않아도 된다.</p> <h1 id="4-berri-collections-of-instruction-annotated-retrieval-tasks">4. BERRI: Collections of Instruction-annotated Retrieval Tasks</h1> <ul> <li> <p>BERRI (Bank of Explicit RetRieval Instructions): retrieval dataset + other NLP datasets</p> </li> <li> <p>BERRI에서 각 task는 하나의 corpus와, k개의 query 그리고 하나의 instruction으로 이루어진다. 각 task의 하나의 instance는 query (q), gold documents (d^+), negative documents (d^-)과 하나의 explicit한 intent t가 있다.</p> </li> <li> <p>Instruction Tuning에서 informative + diverse한 instructions들이 주된 성공 요인으로 꼽히는데, retrieval task에서 다음을 만족하는 instruction을 설계하기 위해 다음과 같은 scheme을 만듬.</p> <ul> <li> <p>임의의 retrieval task를 설명하는 instruction은: <strong>*intent****</strong>, <strong>**</strong>domain<strong>**</strong>, <strong>**</strong>unit***을 포함한다.</p> <ul> <li> <p><strong>_intent _</strong>: retrieved text가 query와 얼마나 관련이 있는지 확인</p> </li> <li> <p><strong>_domain _</strong>: retrieved text의 expected source (ex. Wikipedia, PubMed articles)</p> </li> <li> <p><strong>_unit _</strong>: retrieved text의 text block (ex. sentence, paragraph)</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Dataset Collection</strong></p> <ul> <li>BERRI는 retrieval-centric datasets + non-retrieval datasets 사용해서 구축됨</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Selecting source datasets</strong></p> <ul> <li>Wikipedia와 같은 몇몇 domain을 제외한 나머지 데이터셋은 retrieval dataset이 없는 문제가 있다. 그래서 non-retrieval task에 사용되는 데이터셋을 retrieval task에 맞게끔 변형 시켜서 사용함.</li> </ul> </li> <li> <p><strong>Unification and instruction annotations</strong></p> <ul> <li>MS MARCO와 같은 retrieval dataset은 query q에 대한 annotated gold document를 d^+로 설정하지만, non-retrieval dataset에 대해서는 input sequence를 query, output sequence를 gold document로 설정했다.</li> </ul> </li> </ul> <p>ex) Summarization: x: source, y: summary → x: query, y: label</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 각 데이터셋에 대하여 저자들이 직접 instruction을 작성함 (avg: 3.5개)
</code></pre></div></div> <ul> <li> <p><strong>Negative documents selection</strong></p> <ul> <li> <p>기존에 사용되는 negative document와 다르게 이번 논문에서는 새로운 negative document도 정의함.</p> <ul> <li> <p><strong>Random negative documents</strong></p> </li> <li> <p><strong>Denoised hard negative documents: **</strong>d^{HD}**</p> <ul> <li>Contriever와 같은 off-the-shelf retriever를 이용해서 target corpus에서 k개의 top documents (False Negative)를 추출한 다음에 cross-encoder model (ms-marco-MiniLM-L-12-v2)와 같은 off-the-shelf reranker를 이용해서 normalized score가 0.1보다 낮은 값을 d^{HD}로 설정</li> </ul> </li> <li> <p><strong>Instruction-unfollowing negative documents: **</strong>d^{UF}**</p> <ul> <li> <p>retrieval (TART)가 instruction을 제대로 학습하기 위해 다음과 같은 negative sample을 새롭게 정의함.</p> </li> <li> <p>Instruction을 따르지 않는 negative document를 추출하기 위해서, 다른 task의 target corpus에서 off-the-shelf Contriever를 이용해 k 개의 문서들을 추출하면, 추출된 모든 문서들이 instruction 자체를 따르지 않았기 때문에 d^{UF}를 만족한다.</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="5-tart-multi-task-instructed-retriever">5. TART: Multi-task Instructed Retriever</h1> <ul> <li> <p>TART (TAsk-aware ReTriever): BERRI dataset을 기반으로 multi-task instruction tuning을 통해 학습한 single unified retriever</p> </li> <li> <p><strong>TART-dual</strong>: DPR과 동일한 구조를 갖고 있어 DPR이 갖는 장/단점 동일</p> <ul> <li>한 가지 다른 점은, query와 instruction이 concat되어 query encoder 통과함.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Training</strong></p> <ul> <li>In-batch negative 기법 적용해서 학습 (Contrastive learning)</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>TART-full</strong>: 다들 아시다시피 dual-encoder는 query와 document가 독립적으로 처리되기 때문에, limited interactions이 있음. 다른 cross-encdoer 구조의 retriever와 마찬가지로 query와 document를 함께 입력하여 relevance 계산. 그런데 수 백만 건의 document에 대해 학습하기에 매우 cost가 비싸기 때문에,</p> <ul> <li>off-the-shelf retrieval (bi-encoder)를 사용해 k개를 추출하고, 추출된 k개를 TART-full에 태워서 similarity score 계산 진행 (TART-dual과 마찬가지로 instruction + query + document를 concat한 구조로 입력)</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>사전학습된 T0-3B, FLAN-T5를 TART-full의 backbone으로 사용함</p> </li> <li> <p>T5를 non-autoregressive task에 접목시킨 EncT5와 동일하게 사용</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Training</strong></p> <ul> <li>cross-entropy loss로 학습 진행</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Knowledge distillation from TART-full to TART-dual</strong></p> <ul> <li> <p>BERRI에서 default hard negative는 MS MARCO에 fine-tuned된 off-the-shelf retrievals을 기반으로 정해지는데, 다른 몇몇 domain에서는 이렇게 뽑힌 hard negative가 성능이 안 좋을 수 있다.</p> </li> <li> <p>TART-full을 먼저 training을 시키고 나서, denoising step (hard negative d^{HD}얻는 과정)을 다시 진행해서, TART-full을 새로운 d^{HD} 를 기반으로 다시 학습시키면 성능이 오른다…</p> </li> <li> <p>굳이..? 실험인 것 같음..;;</p> </li> </ul> </li> </ul> <h1 id="6-experiments">6. Experiments</h1> <ul> <li> <p><strong>Zero-shot retrieval</strong></p> <ul> <li> <p>BEIR: NQ, MS MARCO, HotpotQA, FEVER, CQADupStack은 평가에서 제외</p> </li> <li> <p>LOTTE-pooled: 다양한 domain의 dataset을 합쳐서 평가 시에 instruction 안에 (<em>domain</em>)이 들어가게 설정 (ex. “Retrieve a *cooking *StackExchange forum ~”)</p> </li> </ul> <p>→ BERRI랑 겹치는 dataset은 없음</p> <ul> <li> <p>Metrics</p> <ul> <li> <p>BEIR → NDCG@10 (랭킹기반 추천시스템에서 주로 사용되는 지표인데, retrieval task에서 자주 사용)</p> </li> <li> <p>LOTTE-pooled → Recall@5</p> </li> </ul> </li> </ul> </li> <li> <p><strong>X^2 **</strong>-Retrieval (Cross-task Cross-domain Retrieval)**</p> <ul> <li> <p>Normal benchmark: 하나의 intent와 하나의 corpus 갖고 retrieval을 시행함 (oversimplify real-world)</p> </li> <li> <p>X^2-Retrieval은 모델로 하여금 새로운 task에서 zero-shot으로 수행 가능하고 user의 intent를 이해하는 것을 요구한다</p> </li> <li> <p>3가지 domains (Wikipedia, Science, Technical)을 포함하는 6가지 데이터셋 사용</p> </li> <li> <p>Source document: 3.7 million → oracle setup과 비교하기 위해 *closed *setup도 사용 (기존 benchmark인 BEIR와 동일한 환경)</p> </li> <li> <p>Metrics</p> <ul> <li> <p>NDCG@10</p> </li> <li> <p>gap between *closed *and *pooled *setups → to check <strong><em>robustness</em></strong></p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Baselines</strong></p> <ul> <li> <p>Unsupervised models (trained only on unlabeled text / not trained)</p> <ul> <li> <p>BM25</p> </li> <li> <p>Contriever</p> </li> <li> <p>UPR (Contriever의 결과 값을 T0-3B로 reranking한 모델)</p> </li> </ul> </li> <li> <p>Train retrievers and rerankers on MS MARCO → 새로운 task에 adaptation없이 적용</p> <ul> <li> <p>MonoT5</p> </li> <li> <p>Contriever-MS MARCO</p> </li> <li> <p>Contriever-MS MARCO + CE</p> </li> <li> <p>ColBERT v2</p> </li> <li> <p>GTR</p> </li> <li> <p>SGPT-BE</p> </li> </ul> </li> <li> <p>각 task에 맞게 학습되고 + additional data로 target corpus에서 생성한 data 추가 학습</p> <ul> <li> <p>Promptagator (FLAN을 사용해서 in-domain data 생성)</p> </li> <li> <p>GPL (DocT5Query를 사용해서 in-domain data 생성)</p> </li> </ul> </li> </ul> </li> <li> <p><strong>Experimental Settings</strong></p> <ul> <li> <p><strong>TART-full</strong></p> <ul> <li> <p>T0-3B , FLAN-T5 의 encoder를 initialize하여 사용</p> </li> <li> <p>positive passages : negative passages = 1 : 4</p> </li> <li> <p>8 GPUs 사용</p> </li> <li> <p>Contriever-MS MARCO를 사용해서 initial document의 candidates 추출</p> </li> </ul> </li> <li> <p><strong>TART-dual</strong></p> <ul> <li> <p>Contriever-MS MARCO를 initialize하여 사용</p> </li> <li> <p>positive passages : negative passages = 1 : 5</p> </li> <li> <p>Negative passages</p> <ul> <li> <p>random: 90%</p> </li> <li> <p>d^{HD}: 10%</p> </li> <li> <p>d^{UF}: 10%</p> </li> </ul> </li> <li> <p>64 GPUs 사용 + GPU 당 batch size: 16 (너무 크다..)</p> </li> </ul> </li> </ul> </li> </ul> <h1 id="7-results">7. Results</h1> <ul> <li> <p><strong>Zero-shot Evaluation Results</strong></p> <ul> <li> <p>3번째 row에 있는 GPL, Promptagator.. 친구들은 additional data사용해서 성능을 보여주지만, TART는 human-instruction만 있으면 된다</p> </li> <li> <p>그 와중에 BM 25의 성능이 정말 좋다 (학습을 x → 이 정도면 ..ㄷㄷ)</p> </li> <li> <p>BM25+MonoT5의 성능도 높지만 reranking할 때, passage 개수가 1000 (TART-full은 100)</p> </li> <li> <p>TART-dual과 Contriever-MS MARCO를 비교해보면 BEIR에서 (6/9)개가 높은 성능을 보이긴 하지만, Touche-2020, Climate-Fever에서 performance degradation이 너무 커서 평균 값이 낮게 선정됨</p> <p>→ 이에 대해 저자는 dual-encoder의 한계 (query와 document의 limited interaction)때문이라고 핑계를 대지만, Contriever도 동일 구조기 때문에 핑계가 과연 될까 의문..) + 다만, BERT (base) 작은 크기의 LM에 Instruction tuning을 접목 시킨 것이 이유 (이전 여러가지 Instruction tuning 연구들은 LLMs만 IT가 가능하고, 모델의 크기가 작으면 적용하기 힘들다는 것을 나타냄)</p> </li> <li> <p>LOTTE-pooled 결과를 보면 baselines들에 비해 TART가 큰 차이로 높은 성능을 보였는데, 이는 test시에 단순히 instruction을 추가하는 것은 크게 도움이 안된다는 것을 보여줌</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>X^2**</strong>-Retreival Evaluation Results**</p> <ul> <li>Contriever와 Contriever+CE는 closed setup에서 높은 성능을 보여주는데, pooled setup에서는 저조한 성능을 보임. (특히 Contriever+CE가 심함)</li> </ul> </li> </ul> <p>→ 그런데, Contriever 자체의 robustness는 또 강함…음..그냥 Contriever도 매우 강력한 retrieval인 거를 이 실험에서 확인했음</p> <ul> <li> <p>TART-full (T0)가 전체에서 pooled setup에서 높은 성능을 보여 매우 강력한 zero-shot adaptaion + cross-task ability를 갖고 있음을 보이는데, 위에 zero-shot evaluation에서는 TART-full (FLAN)이 더 좋긴 했음. 이거에 대한 언급은 따로 없는데..</p> </li> <li> <p>무엇보다 TART-dual 자체의 성능은 Contriever보다 closed setup에서도 낮게 나왔음.</p> </li> </ul> <p>→ 그런데, <strong>_robustness _</strong>척도가 가장 높게 나왔다고 해서, 저자가 또 말을 “even smaller models can be guided by instructions”이 가능하다고 하네…위에 실험이랑 다른 맥락의 말인데 이는 가볍게 무시. 그냥 작은 LM에는 instruction 적용하지 않는 것이 좋은 전략인듯</p> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="8-analysis">8. Analysis</h1> <ul> <li> <p><strong>Effects of Instruction at training and inference</strong></p> <ul> <li> <p>Red: TART-full (train with instruction + test with instruction)</p> </li> <li> <p>Green: train without instruction + test with instruction</p> </li> <li> <p>Blue: train with instructions + test without instruction</p> </li> <li> <p>Yellow: train without instruction + test without instruction</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>**Effects of dataset scale + Effects of model scale **</p> <p>→ 모델 크기 크면 좋고 + 데이터셋 number/domain/task 많으면 좋다</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_025.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Effects of carefully-designed negative samples</strong></p> <ul> <li> <p>d^{HD}, d^{UF}를 더 추가하면 (challenge negatives) TART-ful 성능 향상</p> </li> <li> <p>without instruction-folloiwng samples (w/o d^{UF})인 경우에서는 BEIR 평가할 때, 그냥 TART랑 동일 성능을 보인 반면에, X^2-Retrieval task에서는 큰 하락을 보임</p> <p>→ instruction을 끼고 학습 시키는 것이 모델의 robust task-aware retrieval ability 향상</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_026.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="9-dicussion-and-conclusion">9. Dicussion and Conclusion</h1> <ul> <li><strong>실제 동일 query에 있어 instruction을 다르게 주면 retrieve되는 text는 다름</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_027.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li><strong>정성 평가 예시 (물론 체리피킹)</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-01-26-task-aware-retrieval-with-instructions/image_028.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>NLP에서 최초로 Instruction Tuning을 retrieval task에 접목 시킨 paper</strong></p> </li> <li> <p><strong>dramatic한 performance gain이 있지는 않았지만, 방향을 제시한 것에 있어서 의의가 있음</strong></p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>