<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Do Prompt-Based Models Really Understand the Meaning of Their Prompts? | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - Instruction Tuning, ICL, Prompt Tuning 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/do-prompt-based-models-really-understand-the-meaning/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Do Prompt-Based Models Really Understand the Meaning of Their Prompts?</h1> <p class="post-meta"> Created on June 15, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/icl"> <i class="fa-solid fa-hashtag fa-sm"></i> icl</a>   <a href="/blog/tag/instruction-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> instruction tuning</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/prompt-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> prompt tuning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-06-15</li> <li> <strong>Reviewer</strong>: 준원 장</li> <li> <strong>Property</strong>: Instruction Tuning, ICL, Prompt Tuning</li> </ul> <h3 id="introduction">Introduction</h3> <ol> <li> <p>이라크에서는 아직 대량의 살상 무기가 발견되지 않았다.</p> </li> <li> <p>이라크에서 대량의 살상 무기가 발견되었다.</p> </li> </ol> <p>모델에게 1과 2가 동치인지 아닌지 판별하는 능력을 갖게 하기 위해서는 다양한 데이터셋이 필요하다.</p> <p>(PLM → Transfer Learning 관점)</p> <p>하지만<em>** ‘이라크에서는 아직 대량의 살상 무기가 발견되지 않았다.’라는 문장이 주어졌을때, ‘이라크에서 대량의 살상 무기가 발견되었다.’라는 문장은 옳은 문장인가?’ **</em>라는 식으로 질문을 바꾸어 버리면 인간은 한번에 빠르게 학습이 가능하다. (Motivation of Instruct Fine-Tuning)</p> <p>→ 이렇게 추가적인 Prompt가 Input에 결합되면 모델이 Input으로부터 유의미한 task instruction을 해석할 수 있기 때문에 빠르고 안정적인 학습이 가능하다고 알려져있다.</p> <p>→ 인간이 직접 쓴 Prompt가 자동으로 찾거나 만든 Prompt에 비해서 성능 향상에 도움이 된다고 알려져 있고 (report that Schick and Schütze (2021b)’s manually written prompts still on average outperform the automatically searched prompts across a range of SuperGLUE tasks (Wang et al., 2019)) 전문가에 의해서 작성된 Prompt에 의해서 유의미한 instruction을 작동시킬 수 있다고 알려져 있다.</p> <p>하지만, 본 연구는 위의 성능향상이 few-shot 및 zero-shot 상황에서 230M-175B까지의 다양한 모델들과 instruction tuning된 모델들이 과연 (사람처럼) prompt 내의 instruction 제대로 해석했기 때문일지에 대한 의문을 제기한다.</p> <h3 id="prompt-tuning--prompting">Prompt Tuning &amp; Prompting</h3> <p>앞으로 언급할 prompt tuning 및 prompting은 아래 3개 중 하나를 의미함</p> <ul> <li> <p><em>*Discrete Prompts: **</em>{sent} In summary, the restaurant is [prediction]*</p> </li> <li> <p><strong>Priming: ICL</strong></p> </li> <li> <p><strong>Continuous Prompts(prompt tuning, p-tuning)</strong>: In addition to discrete prompts, some models use continuous prompts that are generated using a separate language model. These continuous prompts are designed to be more flexible and can be tailored to specific tasks or domains. However, it is unclear whether these continuous prompts are better at conveying task-specific information than discrete prompts.</p> </li> </ul> <h3 id="experiment-setup">Experiment Setup</h3> <p><strong>Problem Situation</strong></p> <ul> <li> <p>Few shot 상황에서 Model이 Prompt내 Instruction의 의미를 얼마나 잘 이해하는지 실험</p> </li> <li> <p>k-shot = {0, 4, 8, 16, 32, 64, 128, 256}</p> </li> <li> <p>Prompt내에 있는 instruction의 범위를 ‘description of task’로 좁힘</p> </li> </ul> <p><strong>Baseline Setup</strong></p> <ul> <li> <p><strong>Weak Baseline</strong></p> </li> <li> <p><strong>Instruction-Tuned Model</strong></p> </li> <li> <p><strong>ICL</strong></p> </li> </ul> <p><strong>Data</strong></p> <ul> <li> <p>NLI (T0가 instruct tuning때 안봐서)</p> </li> <li> <p>Label Space는 Yes/No로 통일하고 실험 진행</p> </li> </ul> <p><strong>4개 Random Seed</strong></p> <h3 id="templates">Templates</h3> <p>논문의 목적이 모델이 Prompt내 Instruction의 Semantic을 제대로 이해했냐를 파악하기 위함이기에 아래 5개 종류의 Template를 제작함</p> <ul> <li>Instructive: how we would describe the NLI task to a human who has never seen this task before. (처음 NLI 문제를 보는 인간에게 설명하듯이 기술하기)</li> </ul> <p><strong>→ Prompt(Instruction) Tuning를 통해 모델이 인간이 Instruction을 보고 unseen task를 풀때와 같은 동작을 하기를 기대한다면 Instructive Prompt를 보았을때랑 아래 Prompt를 보았을 때 성능 차이가 뚜렸해야함</strong></p> <ul> <li> <p>Misleading-Moderate (적당히 속이기): instruct the models to perform a task related or tangential to NLI such that, if the model were to perform the task as explicitly instructed, it would perform poorly on NLI in general. (NLI랑 비슷한 Task를 수행하도록 기술함. 기술한 그대로 수행하면 NLI 성능은 좋지 않을 수 있음)</p> </li> <li> <p>Misleading-Extreme: instruct the models to perform a task unrelated to NLI. (NLI랑 무관)</p> </li> <li> <p>Irrelevant: concatenate the premise, a sentence unrelated to any NLP task, and the hypothesis. (무관한 문장을 premise랑 hypo사이에 끼워넣기)</p> </li> <li> <p>Null: concatenate the premise and the hypothesis without any additional text. (아무 정보도 넣지 않기)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-15-do-prompt-based-models-really-understand-the-meaning/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="results">Results</h3> <ul> <li> <p>**T0: Instructive Vs Irrelevant Template **</p> </li> <li> <p><strong>Misleading Template</strong></p> </li> <li> <p><strong>Null Template</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-15-do-prompt-based-models-really-understand-the-meaning/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ 일반적으로 제일 성능이 안좋으나 특정 order template의 경우 32 SHOT에서 성능 좋은 경우 있음 (뭐.. 이럴 수도 있지..)</p> <ul> <li><strong>Zero shot</strong></li> </ul> <p>→ Zero-shot에서 random보다 marginal하게 성능을 보인 model은 T0밖에 없어서 T0로 실험을 진행</p> <p>→ 3B모델은 Prompt 종류가 어떻든 간에 비슷한 수준의 performance를 보임</p> <p>→ 11B 모델은 통계적으로 유의미한 performance 차이를 보이지 못함. 11B++ 모델부터 유의미한 차이를 보이기 시작함 (instructicve prompt에서 성능이 제일 좋지만) (그럼에도 misleading-extreme prompt에 여전히 너무 잘 반응함)</p> <p><strong>→ GPT3도 비슷한 양상 보임 (instruct tuning은 안했지만 사이즈 키운다고 해서 해결되는 문제는 아님)</strong></p> <h3 id="label-space">Label Space</h3> <p>→ Label Space도 임의로 바꿔서 모델이 Label에 sensitive하게 반응하는지 실험함</p> <ul> <li> <p>Yes-no: Model is expected to predict the word “yes” for entailment and “no” for nonentailment. (기존 setting)</p> </li> <li> <p>Yes-no-like: Semantically equivalent to yesno but using superficially different words, e.g., “true”/“false”, “positive”/“negative”. (유의어)</p> </li> <li> <p>Arbitrary: Model is expected to predict arbitrary words that have no semantic relation to the entailment task, e.g., “cat” for entailment, “dog” for non-entailment. (임의 단어로 mapping)</p> </li> <li> <p>Reversed: Model is expected to predict the opposite of the (intuitive) yes-no and yes-nolike labels, e.g., (reverse)</p> </li> </ul> <p><strong>Results</strong></p> <figure> <picture> <img src="/assets/img/posts/2023-06-15-do-prompt-based-models-really-understand-the-meaning/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ ALBERT, T0 둘다 Best Illustrastive template로 실험했을때 Yes-No &gt; Arbitrary.Reversed</p> <ul> <li>추가 실험도 진행</li> </ul> <ol> <li> <p><strong>An irrelevant or misleading template + yes-no targets</strong>, e.g., {premise} Does the paragraph start with “the”? [yes/no] {hypothesis} :</p> </li> <li> <p><strong>An instructive template + arbitrary targets</strong>, e.g., {premise} Based on the previous passage, is it true that “{hypothesis}”? [cat/dog]</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2023-06-15-do-prompt-based-models-really-understand-the-meaning/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ <strong>An irrelevant or misleading template + yes-no targets의 성능이 더 높음. 인간이라면 몇개 shot만으로 Cat → Entatlilment / Dog → Not-Entailmenet라는 것을 빨리 Mapping할텐데 모델은 그렇지 못하고 있음. 오히려 잘못된 instruction을 전혀 해석하지 못하고 있음을 보여주고 있음.</strong></p> <h3 id="conclusion">Conclusion</h3> <p>→ Model이 instructive and irrelevant templates, misleading templates에 따라서 performance 차이가 다르게 나야하는데 그렇지 않음 (인간처럼 instruction을 해석하지는 않음)</p> <p>→ 반면 Target word에 따른 performance 차이는 consensus 존재</p> <p><strong>Additional Interpretation</strong></p> <ul> <li> <p><strong>Lack of Competence (너무 어려운 Task)</strong></p> </li> <li> <p><strong>Lack of Compliance (Instruction 무시)</strong></p> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>