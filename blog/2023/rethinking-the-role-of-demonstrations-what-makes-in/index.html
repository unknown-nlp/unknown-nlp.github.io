<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM, ICL 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/rethinking-the-role-of-demonstrations-what-makes-in/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</h1> <p class="post-meta"> Created on May 25, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/detection"> <i class="fa-solid fa-hashtag fa-sm"></i> detection</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/icl"> <i class="fa-solid fa-hashtag fa-sm"></i> icl</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-05-25</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: LLM, ICL</li> </ul> <h1 id="abstract">Abstract</h1> <p>Large language models (LLMs)들이 in-context learning을 통해 downstream task에서 좋은 성능을 보인다는 사실은 유명하지만, model이 <strong><em>어떻게 **</em>학습을 하고 **<em>어떤 점</em></strong>을 통해 성능을 내는지 확인하는 연구는 거의 없다. 본 논문에서 다양한 실험을 통해 처음으로 LLMs의 in-context learning이 <strong><em>어떻게 **</em>그리고 **<em>왜</em></strong> 작동을 하는 지에 대해서 수많은 실험을 통해 보여준다.</p> <ul> <li> <p><strong>Key Aspects</strong></p> <ul> <li> <p><strong>Label spcae</strong></p> </li> <li> <p><strong>Distribtuion of input text</strong></p> </li> <li> <p><strong>Overall format</strong></p> </li> </ul> </li> </ul> <h1 id="introduction">Introduction</h1> <ul> <li>LLMs은 few input-label pairs를 가지고 In-contex learning (ICL)을 통해 downstream tasks에서 꽤나 높은 성능을 보여줬지만, <strong><em>ICL이 왜 작동하고 어떻게 작동을 하는 지</em></strong>에 대한 연구는 거의 없다.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>ground turth demonstration이 실제로 ICL에 효과적이지 않음을 실험적으로 보임</p> <ul> <li>Labels in demonstration을 random labels로 바꾸어도 classification과 multi-choice task에서 성능 하락이 거의 없음. → Model은 task를 수행하는 것에 있어 input-label mapping에 크게 의존하지 않.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Demonstations의 어느 부분이 performance에 직접적인 기여를 하는지 알아보는 실험을 진행</p> <ol> <li> <p>Label space와 demonstration을 통해 특정된 input text의 distribution이 ICL에서 되게 중요한 역할을 함 (각각의 inputs에 대해 label이 올바른 것과 상관 없이)</p> </li> <li> <p>Overall format은 중요함.</p> </li> <li> <p>label space가 unknown일 때, label을 사용하지 않는 것 보다 random한 English 단어를 label로 사용하는 것이 더 좋음 → format 자체를 갖출 수 있</p> </li> <li> <p>ICL을 objective function으로 두고 학습을 하는 meta-training은 (1)과 (2)에서 언급한 점들을 극대화 시킴.</p> </li> <li> <p>MetaICL (Min et al., 2021)</p> </li> </ol> </li> <li> <p>본 논문에서 ICL에 사용되는 <strong><em>demonstration</em></strong>의 역할에 대해 이해하는 분석을 제시함.</p> </li> </ul> <h1 id="related-works">Related works</h1> <ul> <li> <p>LLMs, ICL 내용 생략</p> </li> <li> <p>본 논문에서 처음으로 ICL이 zero-shot 보다 왜 성능이 좋게 나오는지 실험적으로 분석함</p> </li> </ul> <h1 id="experimental-setup">Experimental Setup</h1> <ul> <li> <strong>Models</strong>: 6 종류의 LM을 사용함 (only-decoder model)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Inference method (Min et al., 2021) 2개 사용해서 총 12개의 models을 실험에 사용</p> <ul> <li> <p><strong>Direct</strong>: 우리가 흔히 알고 있는 방법. 출력을 추정하는 것에 초점을 둠</p> </li> <li> <p><strong>Channel</strong>: Bayes Rule을 통해 input과 label의 순서를 뒤집어 인과관계 파악</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Evaluation data: 26가지 datasets에 대해 평가 진행</p> <ul> <li> <p>(Sentiment analysis, paraphrase detection, NLI, hate speech detection …)</p> </li> <li> <p>dataset 선정 기준</p> <ol> <li> <p>low-resource dataset with less than 10K training examples</p> </li> <li> <p>연구에 많이 사용된 GLUE and SuperGLUE</p> </li> <li> <p>다양한 domains</p> </li> </ol> </li> </ul> </li> </ul> <h1 id="1-ground-truth-matters-little">1. Ground Truth Matters Little</h1> <h3 id="gold-labels-vs-random-labels">Gold labels vs Random labels</h3> <ul> <li> <p><strong>No demonstration</strong>s: zero-shot setting</p> </li> <li> <p><strong>Demonstrations w/ gold labels</strong>: ICL with *k *labeled examples</p> </li> <li> <p><strong>Demonstrations w/ ranodm lables</strong>: ICL with <em>k</em> labeled examples (gold → random labels), 여기서 사용한 random labels이란 gold label과 동일한 set을 공유하고 있음. (완전 random 아님x)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Results and analysis</strong></p> <ul> <li> <p>Demonstrations w/ gold labels (Yellow)는 No demonstrations (Blue)보다 항상 성능이 높음</p> </li> <li> <p>Labels을 random (Orange)과 Labels을 gold (Yellow)를 비교하면 비슷한 성능</p> </li> </ul> </li> </ul> <p>→ GT input-label pairs는 performance gain에 있어 필수적이진 않음</p> <p>→ 위 실험은 uniform sampling으로 random label을 뽑은 건데, true labels의 distribtuion에서 random하게 추출하면 성능의 간극을 더 줄</p> <p>→ label space를 공유하기 때문이라고 생각</p> <ul> <li>MetaICL은 0.1~0.9% 성능 하락만 있었는데, ICL을 목적 함수로 meta-training 시킨 것이 input-label mapping을 무시하 demonstrations의 다른 요소에 있어 영향을 받음</li> </ul> <h3 id="ablations">Ablations</h3> <ul> <li> <p><strong>Does the number of correct labels matter?</strong></p> <ul> <li> <p>Demonstrations w/ a% correct labels (0≤a≤100)</p> <ul> <li> <p>Correct pairs: k x (a/100)</p> </li> <li> <p>Incorrect pairs: k x (1-a/100)</p> </li> <li> <p>a=100 → equal to ICL (demonstrations w/ gold labels)</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Result and analysis</strong></p> <ul> <li> <p>실험 결과를 확인해보면 number of correct labels in demonstrations은 크게 중요 X</p> </li> <li> <p>GPT-J (Classification task) 경우에서는 10% 넘게 하락이 있긴 하지만, 대체로 비슷한 양상을 보임</p> </li> <li> <p>No demonstrations보단 그래도 incorrect labels이 항상 좋음</p> </li> </ul> </li> <li> <p><strong>Is the result consistent with varying k?</strong></p> <ul> <li>input-label pairs의 개수 ‘k’를 바꿔 감에 따라 실험을 진행</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>k가 4개일 경우만 보아도 No demonstrations (k=0)보다 성능이 좋음</p> </li> <li> <p>k가 8을 넘어서면 examples의 개수가 많아져도 performance 개선이 없음</p> <ul> <li>일반적인 sft 상황과 다른 양상을 보</li> </ul> </li> <li> <p><strong>Is the result consistent with better templates?</strong></p> <ul> <li> <p>minimal template을 default로 사용했지만, manual template을 적용해서 비교실험 진행</p> <ul> <li> <p><strong>minimal template</strong>: input의 단순 conatenation</p> </li> <li> <p><strong>manual template</strong>: dataset-specific 방식으로 작성한 방식</p> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>manual tempaltes을 사용하는 것이 항상 minimal tempalte 보다 좋지는 않음</p> </li> <li> <p>Gold labels → Random labels로 변경할 때, 성능 하락 폭이 작은 trend는 비슷하게 보임</p> <ul> <li> <ul> <li>No demonstrations보다 random labels이 더 좋은 trend도 비슷</li> </ul> </li> </ul> </li> </ul> <h1 id="2-why-does-in-context-learning-work">2. Why *does *In-Context Learning work?</h1> <ul> <li>위에 실험에서 demonstration에서 GT input-label이 ICL의 performance gain에 큰 영향이 없는 것을 보여줌. 이번 실험에서 demonstrations에서 다른 어떤 요소들이 ICL의 performance gain에 영향을 주는지 확인.</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Demonstraion의 4가지 aspects</strong></p> <ol> <li> <p><strong>Input-label mapping</strong>: input xi가 label yi와 올바르게 pair인지</p> </li> <li> <p>앞에서 다룬 내용이지만, 이것도 aspect 중 하나로 보고 변인 통제 진</p> </li> <li> <p><strong>Distribution of input text</strong>: x1~xk의 distribution</p> </li> <li> <p><strong>Label space</strong>: y1~yk의 space</p> </li> <li> <p><strong>Format</strong>: the use of input-label pairing as format</p> </li> </ol> </li> </ul> <h3 id="impact-of-the-distribtuion-of-the-input-text">Impact of the distribtuion of the input text</h3> <ul> <li> <p>out-of-domain distribtuion demonstrations 상황에서 실험</p> <ul> <li> <p>demonstration에 사용되는 k개의 x들을 아예 다른 corpus에서 샘플링해서 사용</p> </li> <li> <p>Demonstration의 label space와 format은 유지하며 input text의 distribtuion만 바꾸며 이에 대한 impact 평가</p> </li> </ul> </li> <li> <p><strong>Results</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Orange bar랑 Purple bar만 비교해보면, OOD input을 사용할 때 큰 성능 하락을 보임</p> <ul> <li>GPT-J 경우에는 No demonstration보다 낮은 성능</li> </ul> </li> <li> <p>Demonstrations의 in-distribtuion input은 performance gain에 큰 영향을 끼침</p> <ul> <li>근데, 너무 당연한 소리 아닌가 싶음. (분류 문제를 푸는데, 쌩뚱 맞은 text를 가져와서 example로 보여주면 성능하락이 당연히 있지 않을까..근데 그 당연할 걸 또 실험으로 보여주니 역시 대단함 😀)</li> </ul> </li> </ul> <h3 id="impact-of-the-label-space">Impact of the label space</h3> <ul> <li> <p>k개의 labels을 random한 English word를 사용해서 실험을 진행함.</p> <ul> <li> <p>원래의 label space 크기와 English word의 label space 크기는 동일하게 설정</p> </li> <li> <p>전체 English word space에서 random하게 label 추출하며 demonstration 구축</p> </li> </ul> </li> </ul> <p>→ 여기서도 역시 input text 및 format은 유지</p> <ul> <li><strong>Results</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Orange bar와 Green bar를 비교</p> </li> <li> <table> <tbody> <tr> <td>**Direct-models : P(y</td> <td>x)**</td> </tr> </tbody> </table> <ul> <li>random label을 사용할 때와, random Enlgish words를 사용할 때의 <strong>성능 차이가 명확함</strong> </li> </ul> </li> <li> <table> <tbody> <tr> <td>**Channel-models : P(x</td> <td>y)**</td> </tr> </tbody> </table> <ul> <li> <p>random label을 사용할 때와, random Enlgish words를 사용할 때의 <strong>성능 차이가 미미함</strong></p> </li> <li> <p>Channel-models이 label을 condition으로 두기 때문에, label space를 아는 것에 있어 gain을 얻을 수 없다고 함. great!</p> </li> </ul> </li> </ul> <h3 id="impact-of-input-label-pairing">Impact of input-label pairing</h3> <ul> <li> <p>Format의 형태를 바꿔가며 실험을 진행함. inputs과 labels의 pairing을 없애는 식으로 작은 변화만 줘서 format의 형태를 바꿈.</p> <ul> <li> <p>Demonstrations with no labels: concatenation of x1~xk</p> <ul> <li>Demonstrations with random English words의 format이 없는 경우와 대응</li> </ul> </li> <li> <p>Demonstrations with labels only: concatenation of y1~yk</p> <ul> <li>Demonstrations with OOD inputs의 format이 없는 경우와 대응</li> </ul> </li> </ul> </li> <li> <p><strong>Results</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Format의 형태를 바꾼 경우 → **진보라 **bar(label만 존재), **진녹색 **bar(input만 존재)</p> </li> <li> <p>Format을 유지하는 것은 중요하다.</p> <ul> <li> <p>연보라 vs <strong>진보라</strong>: (Format vs label은 있지만 input text x가 없는 경우)</p> </li> <li> <p>연녹색 vs <strong>진녹색</strong>: (Format vs input text은 있지만 label y가 없는 경우)</p> </li> </ul> </li> <li> <p>Format을 없애면 No demonstration보다 낮은 성능을 보임</p> <ul> <li>다른 case보다 Format을 없애는 것이 가장 큰 성능 하락을 보임</li> </ul> </li> </ul> <h3 id="impact-of-meta-training">Impact of meta-training</h3> <ul> <li> <p>다른 models들과 다르게 MetaICL은 ICL을 목적 함수로 두어 학습시킨 모델이다.</p> <ul> <li>MetaICL: 학습은 large collection of supervised dataset을 통해 multi-task learning으로 진행하고 Inference는 ICL과 동하게 진행함. (Unseen task에 대해서 generalizability를 높이기 위해 제안된 model)</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>위에 진행한 실험에서 MetaICL의 두드러진 몇 가지 특징이 보임</p> <ul> <li> <p>Format을 유지하는 것이 다른 models에 비해 더욱 중요</p> </li> <li> <p>GT Input-label mapping이 다른 models에 비해 덜 중요</p> </li> </ul> </li> </ul> <p>→** meta-training encourages the model to exculsively exploit simpler aspects of the demonstrations and to ignore others**</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- 이에 대해 저자들의 생각은 다음과 같음

  1. input-label mapping을 사용하는 것은 어렵다

  1. format을 사용하는 것은 비교적 쉽다

  1. model이 생성하도록 학습한 text의 space는 model이 condtion으로 둔 text의 space보다 사용하기 쉽다
</code></pre></div></div> <p>→ Direct model이 input distribtuion보다 label space를 잘 이용하고</p> <p>→ Channel model이 label space보다 input distribution을 잘 이용한다</p> <p>(이것도 당연한 얘기가 아닌가..그치만 대박 😃)</p> <figure> <picture> <img src="/assets/img/posts/2023-05-25-rethinking-the-role-of-demonstrations-what-makes-in/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="discussion">Discussion</h1> <h3 id="does-the-model-learn-at-test-time">Does the model learn at test time?</h3> <ul> <li> <p>Learning의 대한 엄밀한 정의를 다음과 같이 두면, ‘caputring the input-label correspondence given in the training data’</p> <ul> <li> <p>LMs은 test 시에 새로운 task를 학습하지는 않는다.</p> </li> <li> <p>저자들은 model이 demonstration에 언급된 task를 무시하고 pretraining에 사용된 prior를 사용하는 것과 같다고 생각함.</p> </li> </ul> </li> <li> <p>Learning을 넓은 의미로 해석하면</p> <ul> <li> <p>특정 input과 label의 distribtuions과 특정 format을 demonstration에 잘 녹여낼 때, model의 prediction 결과는 더 정확해질 수 있고, 이는 model이 demonstration을 통해 새로운 task를 학습한다고 볼 수 있음.</p> </li> <li> <p>test 시에 demonstration을 잘 구축하는 것이 새로운 task라고 보기보다는 사전학습된 weights를 잘 이용하는 것이니, 이것 역시 pretraining에 사용된 prior를 사용하는 거 아닐까?</p> </li> </ul> </li> </ul> <h3 id="capacity-of-lms">Capacity of LMs</h3> <ul> <li> <p>model은 input-label demonstration에 의존하지 않으며 downstream task를 수행한 것을 실험에서 보임. 이를 통해 input-label correspondence 자체를 langauge modeling (pretraining)할 때 학습을 한 것으로 볼 수 있음.</p> <ul> <li> <p>이는 langauge modeling 목적 함수가 zero-shot 성능의 주역이라고 봄</p> </li> <li> <p>반면에, ICL은 LM에서 학습하지 못한 input-label correspondence task는 수행할 수 없다고 봄. ICL이 풀 수 없는 NLP 문제들을 어떻게 발전시켜야 될 research question을 질문함.</p> </li> </ul> </li> </ul> <p>→ 이 점 역시 LM의 knowledge(task) injection 및 update와 관련이 있다고 생각</p> <h3 id="connection-to-instruction-following-models">Connection to instruction-following models</h3> <ul> <li> <p>(Instruction) natural language로 문제 설명을 설명하면 inference 과정에서 새로운 task를 수행 할 수 있다는 이전 연구들이 존재. → Demonstrations과 Instruction은 LM에게 있어 비슷한 역할 수행</p> </li> <li> <p>Instruction은 model로 하여금 model 갖고있는 capacity를 끌어올리는 것을 촉진 시킬 수는 있지만, 새로운 task를 학습 시키지는 못함.</p> </li> </ul> <p>###</p> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>