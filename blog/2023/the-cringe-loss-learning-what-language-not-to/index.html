<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The CRINGE Loss: Learning what language not to model | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LM 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/the-cringe-loss-learning-what-language-not-to/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The CRINGE Loss: Learning what language not to model</h1> <p class="post-meta"> Created on September 19, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/language-model"> <i class="fa-solid fa-hashtag fa-sm"></i> language-model</a>   <a href="/blog/tag/lm"> <i class="fa-solid fa-hashtag fa-sm"></i> lm</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-09-19</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: LM</li> </ul> <h1 id="introdution">Introdution</h1> <p>LM은 아직도 toxicicty, bias, lack of coherence, fail to user’s coherence와 같은 문제들이 있음. 이런 문제들을 해결하기 위해 objective function에 failure cases에 대한 정보를 주입하는 식으로 training objective를 설계하는 여러 시도들이 있음.</p> <p>본 연구에서는 일반적으로 자주 사용되는 <em>positive example sequences</em>와 *negative example sequences (model should not generate) *를 모두 포함하는 training data를 통해 해당 문제를 탐구함.</p> <p>다음 training data에 학습을 시키는 새로운 learning method, <strong>CRINGE loss,</strong>를 소개함.</p> <ul> <li>아이디어: positive examples들은 <strong>MLE</strong>로 학습이 되고, negative examples은 <strong>simple contrastive learning</strong>으로 학습이 이루어짐</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ negative example <strong>(dork)</strong> 의 output sequence에 penalizing을 함으로써, 학습이 이루어짐 (positive token <strong>(star)</strong>을 maximize시킴). Negative token output 각각에 대해, positive prediction은 LM이 생성한 top-k samples 중 sampled된 positive token과 contrast가 이루어짐.</p> <p>Positive와 negative training data를 갖는 3가지 task에 실험을 한 결과 existing methods보다 효과적인 성능을 보여줌</p> <ul> <li>Safe generation / Contradiction avoidance / Open-domain task-oriented conversation</li> </ul> <h1 id="related-works">Related works</h1> <h3 id="collecting-negative-examples">Collecting negative examples</h3> <p>최근에는 positive examples(e.g. human written text, websites etc.)을 수집하는 것 뿐만 아니라, 모델이 특정 ‘response’(e.g. contradictory, toxic, unhelpful responses)를 생성하지 않는 식으로 학습을 시키기 위해 negative examples을 수집하는 것에 대한 연구도 많이 진행됨.</p> <p>PPO에서 사용되는 human preference에 대한 ranked examples들도 있지만, 본 연구에서는 positive / negative examples만 고려해서 연구를 진행함.</p> <h3 id="training-with-negative-examples">Training with negative examples</h3> <ol> <li>Negative examples을 활용해서 LM을 학습하는 연구들도 다수 존재함.</li> </ol> <p><strong>(Neural Text **</strong>De*<strong>*Geneartion with Unlikelihood Training, ICLR 2020)</strong></p> <p>→ unlikelihood training을 소개하며, negative token들에 대한 probability를 낮추는 식으로 objective function을 새로 제안함. negative candidates들에 대해 모델의 probability를 감소시키는 아이디어.</p> <p><strong>Token-level unlikelihood objective</strong></p> <p>여기서 사용한 negative candidates는 previous context tokens으로 사용함.</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>incorerect repeating token을 제한</strong> (previous context는 potential repeat을 포함함)</p> </li> <li> <p><strong>frequent token이 덜 생성됨</strong> (frequent tokens들은 previous context에 존재하기 때문)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>하지만, Token-level unlikelihood objective는 token 단위로 penalty를 줄 수 있는 장점이 있지만, training sequence &amp; generated sequence 간의 <strong>distribution mismatch</strong>가 생기는 이슈 존재</p> <p><strong>Sequence-level unlikelihood objective</strong></p> <p>여기서 사용한 negative candidates는 sequence 내 repeating n-grams에 속한 token으로 사용</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>sequence-level UL은 주로 finetuning obejctive function으로 사용되고, token-level UL은 pretraining objective로 사용함</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>(A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration, arxiv 2022)</strong></p> <p>→ Text degeneration을 방지하기 위해 contrastive learning을 사용함. 앞선 M context tokens (negative candidates)들에 대해 positive label에 contrast를 가하는 것은 undesired token들이 생성되는 것을 방지 시켜줌.</p> <p>(Unlikelihood training은 undesired token을 생성하는 것이 문제): <em>아마도 ULS에서는 positive token에 대한 정보가 없기 때문</em></p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>해당 방법은 positive sequences에 대해서 repetition을 줄이는데 효과적이긴 하지만, 임의의 negative token에 대한 correct positive token의 knowledge가 필요하기 때문에 negative examples들에 대해 일반화 시키기는 어려움.</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>본 연구에서 제시하는 방법론은 위의 두 방법에 영감을 받고, <strong>negative example training setting</strong>에 대해서 <strong>generalize</strong>할 수 있는 장점이 있음.</p> <ol> <li>Negative examples을 objective function에 추가해서 training하는 것 말고, negative examples을 활용하는 방법으로 별도의 classifier 혹은 reranker model을 학습 시키는 것도 존재함.</li> </ol> <p>→ LM이 multiple candidates를 생성하고, 독립적인 model이 generations에 대해 scoring 진행 후 best-scoring candidate를 선정함.</p> <ul> <li> <p><strong>(Addressing Contradictions in Dialogue Modeling, 2021 ACL)</strong>: Reranker가 contradictory generation에 도움이 됨</p> </li> <li> <p><strong>(WebGPT: Browser-assisted question-answering with human feedback, arixv 2021)</strong>: 몇몇 case에서는 reranking이 RL보다 효과적임</p> </li> </ul> <ol> <li>독립적인 model이 final generation을 선택해주는 것 말고도 model-guiding에 관한 연구들도 다수 존재.</li> </ol> <ul> <li><strong>(FUDGE: Controlled Text Generation With Future Discriminators, NAACL 2021)</strong></li> </ul> <p>→ LM (Blue) 이외의 별도의 classifier (Red)를 두고, 각 token에 대해 rerank를 수행 한 뒤에 LM의 prob과 multiplication을 수행한 뒤 next token을 선택함.</p> <table> <tbody> <tr> <td>_X=x<em>{1:n}, P(X)=P(x</em>{1:n})= \prod<em>{i=1}^{n}P(x</em>{i}</td> <td>x_{1:i-1})**, attribute a를 condition으로 추가 P(X</td> <td>a)_</td> </tr> </tbody> </table> <p>→ 여기서 attirbute a는 <em>desired attribute</em> (e.g. formality)</p> <table> <tbody> <tr> <td>P(X</td> <td>a)=\prod<em>{i=1}^{n}P(x</em>{i}</td> <td>x_{1:i-1},a), 되고 Bayesian factorization을 적용하면 아래와 같이 전개</td> </tr> </tbody> </table> <p>이 식은, 아래 figure에서 Blue LM과 Red LM으로 구분된 것으로 볼 수 있음</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>하지만 별도의 LM이 또 필요하기에, inefficient하다는 단점이 존재함.</p> <ul> <li><strong>(Am I Me or You? State-of-the-art Dialouge Models Cannot Maintain an Identity, NAACL 2022)</strong></li> </ul> <p>→ <strong>PACER</strong>를 제시함. FUDGE의 variant로 token들을 모두 reranking하지 않고 sampling한 것에 대해서 일부만 reranking 진행하며 성능&amp;속도 up</p> <ul> <li><strong>(DIRECTOR: Generator-Classifiers For Supervised Language Modeling, ACL 2022)</strong></li> </ul> <p>→ Language modeling과 classification heads를 동일 architecture에서 공유함. Classifier head는 contradictions 혹은 repetitions과 같은 undesirable sequences를 생성하지 않도록 학습이 이루어짐 (아래 Figure 예시에서, ‘you’(toxic), ‘sports’(repetitions)의 class prob이 낮음)</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>하지만 대부분의 task에서 좋은 성능이 나왔지만, architecture 자체를 변경 시켜야 하기 때문에, existing models들에 바로 적용하는 것이 쉽지 않은 단점이 존재함</p> <ul> <li>밑에 실험에서 나오는 baseline으로 사용되는 DIRECTOR-shared는 ‘Head’의 Linear layer를 공유한 모델</li> </ul> <h1 id="method">Method</h1> <h3 id="cringe-loss-contrastive-iterative-negative-genearation">CRINGE Loss (ContRastive Iterative Negative GEnearation)</h3> <p>Positive와 negative sequence를 모두 포함하는 training data에 대해 학습을 진행함.</p> <ul> <li> <strong>positive examples</strong>: 일반적인 MLE 방법 사용 → <strong>Cross Entropy term</strong> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>negative examples</strong>: sequence 내의 각 token들을 LM의 top prediction과 contrasting하며 학습 → <strong>CRINGE term</strong></p> <ul> <li>training data에서 negative sequence가 주어지기는 하지만, negative sequence 내의 임의의 negative token에 대한 alternative positive token이 무엇인지 모름.</li> </ul> </li> </ul> <p>e.g) You are very stupid ididot ugly!</p> <p>→ You are very (negative sequence) → You are very (token) (token) (token)</p> <p>→ <strong>(A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration, arxiv 2022)</strong>의 <strong>문제점으로 지적</strong></p> <p>→ (arxiv 2022) paper에서는 token 단위로 contrastive learing을 수행할 수 있는 task들을 수행했는데, 본 실험에서 진행되는 task들은 sequence 단위로 negative example이 주어지기 때문에, 적용이 안되는거 같습니다…이거를 논문에 직접적으로 언급을 해줬으면 이해하기가 더 수월했을듯!</p> <ul> <li>이를 해결하기 위해, model의 현재 시점의 top-k prediction을 sample하여 alternative positive token으로 사용함. (negative token이 top-k 내에 존재하면, negative token이 positive example로 선택되면 안되므로 top-k에서 negative token을 지움)</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>아래 Pseudo code를 보면 직관적임</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>CRINGE Iterative Training</strong></p> <ul> <li>최종적인 CRINGE loss function은 아래와 같음</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>해당 objective function은 다음과 같은 방식으로 iteratively하게 적용되어 model의 성능을 올릴 수 있음</p> <ol> <li> <p>먼저 dataset <em>D</em> 로 model을 training 진행</p> </li> <li> <p>학습된 model이 기존 training contexts에 대해 additional seuqnece를 생성함</p> </li> <li> <p>model의 generation을 positive 혹은 negative로 labeling 진행한 뒤에, (1)에 있던 dataset <em>D</em>에 추가함</p> </li> </ol> <ul> <li>positive, negative를 포함하는 original training data로 별도의 classifier를 학습 시킨 뒤 model generation labeling을 진행함. (RLHF에서 reward model이랑 비슷)</li> </ul> <ol> <li>(1~3) process를 반복함</li> </ol> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h1 id="experiment">Experiment</h1> <h3 id="baselines"><strong>Baselines</strong></h3> <ul> <li> <p><strong>Transformer Baseline</strong>: 모든 baselines들이 아래 두개의 transformers를 backbone model로 사용</p> <ul> <li> <p>400M BlenderBot (BB1): enc-dec</p> </li> <li> <p>2.7B BlenderBot2 (BB2): intermediate step에서 search engine을 사용해서 FiD를 통해 생성을 진행함</p> </li> </ul> </li> <li> <p><strong>Reranking and Model Guiding</strong></p> <ul> <li> <p>Reranker, FUDGE, PACER는 별도의 300M Transformer-based classifier를 reranker/guiding model로 사용</p> <ul> <li> <p>Reranker는 model의 beam candidates들에 대해 rank를 진행</p> </li> <li> <p>FUDGE,PACER는 decoding 과정에서 token 단위로 reranking을 진행</p> </li> </ul> </li> </ul> </li> <li> <p><strong>Unlikelihood Loss</strong></p> <ul> <li>Unlikelihood loss는 unwanted token의 prob을 낮추지만, CRINGE Loss는 top-k prediction에 대해 contrast를 가하는 점이 다르다.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Director</strong></p> <ul> <li> <p>LM head는 positive sequence에 대해 CE Loss로 training이 되고,</p> </li> <li> <p>Classifier head는 각 token에 대해 positive, negative label로 BCE Loss로 training</p> </li> <li> <p>Inference 시에, two heads의 scores를 combined+normalize한 뒤에, 전체 vocab dist에서 final probability를 기반으로 수행</p> </li> </ul> </li> <li> <p><strong>SCONES (Sigmoid-only)</strong></p> </li> </ul> <p>**(Jam or Cream First? Modeling Ambiguity in Neural Machine Translation with SCONES, 2022 NAACL) **</p> <ul> <li>LM head의 softmax를 sigmoid로 대체함 → full vocab의 dist를 사용하는 대신에, 각 token에 대해 sigmoid를 적용한 뒤에 binary classification을 수행</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>SCONES term을 다음과 같이 변형 시킨 후 baseline으로 사용</p> <ul> <li>irrelevant하는 것을 처리해주는 term을 추가시킴</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_021.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="safe-generation-task">Safe Generation Task</h2> <p>모든 실험 setting은 DIRECTOR에서 진행한 실험과 동일하게 구성.</p> <p>Backbone으로 BB1 사용</p> <p>다음 두 가지 기준으로 평가 진행</p> <ol> <li> <p>동일 prompt가 주어질 때, ConvAI2 dataset의 gold respones에 대한 F1 score로 <strong><em>generation performance</em></strong> 측정</p> </li> <li> <p>WTC dataset의 toxic prompt에 대한* *<strong><em>safe generation</em></strong>의 비중 (DIRECTOR에서 사용한 classifier를 기준으로 평가 진행 → CRINGE training loop에서 사용되는 c도 동일하게 사용)</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_022.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>CRINGE loss (single iteration)은 DIRECTOR와 비슷한 수준을 보이고, unlikelihood training, Reranker, FUDGE, PACER보다 높은 성능 보임</p> </li> <li> <p>추가 iteration 진행한 CRINGE는 Geneartion perfromance (F1)을 거의 유지하며, Safety score가 거의 100% 수준을 보임</p> </li> <li> <p>아래는 WikiToxic prompt에 대한 정성 평가인데, CRINGE는 safe response를 잘 생성하는 반면에 DIRECTOR는 그렇지 못함</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_023.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="contradiction-avoidance-task">Contradiction Avoidance Task</h2> <p>사람이 contradictory / nonconctradictory로 label한 examples인 DECODE dataset을 사용</p> <p>Backbone으로 BB1 사용</p> <p>다음 두 가지 기준으로 평가 진행</p> <ol> <li> <p>ConvAI2 dataset의 gold respones에 대한 F1 score로 <strong><em>generation performance</em></strong> 측정</p> </li> <li> <p>DECODE에서 <strong><em>coherent</em></strong> 비중 (여기서도 Safe Generation Task와 유사하게 별도의 labeled dataset에 학습된 classifier 사용)</p> </li> </ol> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>CRINGE (single-iteration)과 DIRECTOR가 다른 baseline 보다 classifier accuracy 측면에서 큰 차이로 우수한 성능을 보임</li> </ul> <h2 id="open-domain-dialogue-fits-task">Open-domain Dialogue (FITS) Task</h2> <p>specific tasks가 아닌 practical한 상황에서 평가하기 위해 Feedback for Interactive Talk &amp; Search (FITS) benchmark사용해서 실험 진행</p> <ul> <li>FITS: diverse topic에 대해 human과 model 간의 conversation이 있고, model의 response에 대한 사람이 annotate binary feedback label이 존재함 (pos/neg)</li> </ul> <p>Backbone으로 BB2 사용 (search engine 사용 → FiD를 통해 top search results 사용)</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_025.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>valid(684), test(1453), test unseen(1366) 종류의 F1 score에 대한 weighted average</p> <ul> <li> <p>test unsee: training시에 등장하지 않은 topic</p> </li> <li> <p><em>valid를 포함시키는게 맞나…? 음………….?!</em></p> </li> </ul> </li> <li> <p>CRINGE (single-iteration)이 baseline 뛰어 넘고, 추가 iteration한 case가 가장 우수함</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_026.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>각 data별로 성능을 나누어 확인해보면, CRINGE는 iteration이 거듭될 수록 test unseen에서 성능이 하락하는 것을 볼 수 있음. 다소 큰 폭으로 성능이 하락하는데, 저자는 이를 overfitting이 원인일 수 있다고 방어함.</li> </ul> <p>→ <strong>뇌피셜</strong>: iterative하게 dataset을 scaling하는 과정에서 training에 봤던 topic에 대해서만 example이 쌓이기 때문에, overfitting이 존재할 수도 있을 것 같음</p> <h1 id="conclusion">Conclusion</h1> <p>본 연구에서는 LM을 iterative하게 학습시킬 수 있는 CRINGE Loss를 새롭게 제안함.</p> <p>그런데, 개인적인 생각으로 CRINGE Loss는 **(A Simple Contrastive Learning Objective for Alleviating Neural Text Degeneration, arxiv 2022) **에서 주장한 Contrastive Loss와 구조적으로 거의 동일하고, negative token에 대한 alternative positive token을 top-k prediction으로 사용해 개선한 점이 유일한 novelty라고 생각함.</p> <p>iterative하게 training data를 scale up 시키는 것에 대한 effectiveness를 실험적으로 잘 보여줬고, Safety task 와 Contradiction task에서 압도적으로 높은 성능을 보임</p> <p>다만, Open-domain Dialogue Task에서는 Unlikelihood training과 비슷하거나 못한 성능을 보임</p> <p>최근 Language Modeling objective function에 variant를 가한 아이디어 자체는 아래 범주에서 크게 벗어나지 않고 있는 것 같다고 느낌</p> <figure> <picture> <img src="/assets/img/posts/2023-09-19-the-cringe-loss-learning-what-language-not-to/image_027.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>