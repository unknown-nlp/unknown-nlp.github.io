<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="5EvH841dAH-gE3azIorT3dCfBA_7a3yppKdAm1JWne8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Code Llama: Open Foundation Models for Code | Unknown NLP Lab </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM 관련 연구"> <meta name="keywords" content="natural language processing, NLP, machine learning, artificial intelligence, research papers, academic collaboration, paper review, computational linguistics, deep learning, transformers, language models"> <meta property="og:site_name" content="Unknown NLP Lab"> <meta property="og:type" content="article"> <meta property="og:title" content="Unknown NLP Lab | Code Llama: Open Foundation Models for Code"> <meta property="og:url" content="https://unknown-nlp.github.io/blog/2023/code-llama-open-foundation-models-for-code/"> <meta property="og:description" content="논문 리뷰 - LLM 관련 연구"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Code Llama: Open Foundation Models for Code"> <meta name="twitter:description" content="논문 리뷰 - LLM 관련 연구"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Jeahee Kim"
        },
        "url": "https://unknown-nlp.github.io/blog/2023/code-llama-open-foundation-models-for-code/",
        "@type": "BlogPosting",
        "description": "논문 리뷰 - LLM 관련 연구",
        "headline": "Code Llama: Open Foundation Models for Code",
        
        "sameAs": ["https://inspirehep.net/authors/1010907","https://scholar.google.com/citations?user=qc6CJjYAAAAJ","https://www.alberteinstein.com/"],
        
        "name": "Jeahee Kim",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknown-nlp.github.io/blog/2023/code-llama-open-foundation-models-for-code/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Lab </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Code Llama: Open Foundation Models for Code</h1> <p class="post-meta"> Created on August 29, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/embedding"> <i class="fa-solid fa-hashtag fa-sm"></i> embedding</a>   <a href="/blog/tag/fine-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> fine-tuning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/pre-training"> <i class="fa-solid fa-hashtag fa-sm"></i> pre-training</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-08-29</li> <li> <strong>Reviewer</strong>: hyowon Cho</li> <li> <strong>Property</strong>: LLM</li> </ul> <h1 id="introduction">Introduction</h1> <p>domain-specific dataset을 이용해서 application에 특화된 모델을 만드는 것은 보편적인 방법이다. 이러한 추세는 언어모델을 이용하여 코드를 작성하는데까지도 이어졌다. 예를 들어, code completion, debugging, generating documentation과 같은 작업을 수행할 수 있다.</p> <p>이번 발표에서는 Meta AI에서 공개한 Code Llama에 대한 리뷰를 진행한다. 해당 모델의 의의는 크게 두 가지, 제일 성능이 좋다는 것과 repository-level의 긴 context를 받아서 처리할 수 있다는 것이다.</p> <p>실제 예시를 살펴보면 꽤나 좋은 성능을 내고 있다는 것을 확인할 수 있다.</p> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>요약된 특징은 다음과 같다:</p> <ol> <li>Code-training from foundation models</li> </ol> <ul> <li>AlphaCode (Li et al., 2022), InCoder (Fried et al., 2023), StarCoder (Li et al., 2023)과 같은 대다수의 최근 code LLM들은 모두 code로만 학습됨. 하지만 이번 모델은 Codex (Chen et al., 2021)와 같이 foundation model에서 출발. code로만 학습시켰을 때보다 더 좋은 성능을 내는 것을 보임</li> </ul> <ol> <li>Infilling</li> </ol> <ul> <li>일반적인 autoregressive 목적함수만을 사용하는 것이 아니라, multitask objective (autoregressive + causal infilling prediction) 사용</li> </ul> <ol> <li>Long input contexts.</li> </ol> <ul> <li>Llama 2은 4096 토큰 input. 하지만, repository-level reasoning을 하기 위해서는 이보다 더 긴 context를 받아야함. 이를 위해서, maximum context length을 4,096 tokens to 100,000 tokens로 늘리는 finetuning stage 제안. - modifying the parameters of the RoPE positional embeddings.</li> </ul> <ol> <li>Instruction fine-tuning.</li> </ol> <ul> <li>Code Llama - Instruct 에서 추가적인 Instruction fine-tuning 진행</li> </ul> <p>이 외에도 해당 paper에서는 다른 code-based LLM들과의 비교를 진행하고, 우리가 궁금할법한 다양한 ablation study를 진행한다.</p> <h1 id="technical-details">Technical Details</h1> <h3 id="the-code-llama-models-family">The Code Llama models family</h3> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Code Llama</p> <ul> <li> <p>7B and 13B models은 infilling objective를 이용해 진행</p> </li> <li> <p>34B model은 infilling objective 없이 auto-regressive만 진행</p> </li> <li> <p>최종적으로 trained on 500B tokens from a code-heavy dataset</p> </li> </ul> </li> <li> <p>Code Llama - Python:</p> <ul> <li> <p>Python에 특화된 버전</p> </li> <li> <p>100B tokens using a Python-heavy dataset</p> </li> </ul> </li> <li> <p>Code Llama - Instruct:</p> <ul> <li>human instructions and self-instruct code synthesis data을 사용하여 finetune</li> </ul> </li> </ul> <h3 id="dataset">Dataset</h3> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Natural Language Related to Code</p> <ul> <li>discussions about code and code snippets</li> </ul> </li> <li> <p>Natural Language</p> <ul> <li>natural language understanding skill을 보존하기 위해 포함시킴</li> </ul> </li> </ul> <h3 id="infilling">Infilling</h3> <p>Infilling을 위한 데이터들은 다음과 같이 구성된다.</p> <p>먼저 모든 데이터들은 prefix-middle-suffix로 나눠진다. (splitting locations are sampled independently from a uniform distribution over the document length.)</p> <p>이후, 전체 데이터의 절반은 prefix-suffix-middle (PSM) format, 나머지 절반은 suffix-prefix-middle (SPM) format으로 구성한다.</p> <p>suffix, prefix, middle의 시작과 infilling span의 끝을 표기하기 위한 토큰이 추가된다.</p> <p>이렇게 재배열된 데이터에 대한 auto-regressive training을 수행한다.</p> <p>전체 데이터의 90퍼센트는 Infilling, 나머지는 일반 auto-regressive수행한다.</p> <h3 id="long-context-fine-tuning">Long context fine-tuning</h3> <p>Code Llama에서는 최대 16,384 tokens을 다루기 위한 long context fine-tuning (LCFT) stage를 제안한다.</p> <p>이들이 제안하는 방법은 다음과 같다:</p> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Problem</p> <ul> <li> <p>RoPE: RoPE applies a rotation operation to the input embeddings based on their positions.</p> </li> <li> <p>weak extrapolation properties!</p> </li> </ul> </li> <li> <p>Solution: Change base period</p> <ul> <li> <p>Rotation frequencies are computed as θ_i = θ^{−2i/d},</p> </li> <li> <p>기존에는 10,000으로 설정하여 relative position이 멀어지면 inner product값을 감소하는 효과를 가짐</p> </li> <li> <p>base period θ를 기존 10,000에서 1,000,000으로 올림</p> </li> <li> <p>이를 통해 보다 먼 거리까지 고려하도록 함.</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Attention expectations over relative distances between key and value embeddings</p> </li> <li> <p>far-away tokens도 현재의 prediction에 더 기여하도록 바뀌었다는 것을 확인할 수 있음.</p> </li> </ul> <h3 id="instruction-fine-tuning">Instruction fine-tuning</h3> <p>instruction fine-tuned models Code Llama - Instruct은 Code Llama에 다음의 데이터셋을 이용하여 finetuning한 모델이다.</p> <ol> <li>Proprietary dataset</li> </ol> <ul> <li> <p>instruction tuning dataset collected for Llama 2</p> </li> <li> <p>multi-turn dialogue between a user and an assistant.</p> </li> <li> <p>few examples of code-related tasks</p> </li> </ul> <ol> <li>Self-instruct</li> </ol> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ol> <li>Rehearsal.</li> </ol> <ul> <li>코딩과 언어 이해 능력을 보존하기 위해서 Code Llama - Instructs는 code dataset (6%)과 our natural language dataset (2%)을 포함함.</li> </ul> <h3 id="training-details">Training details</h3> <ul> <li> <p>batch size of 4M tokens</p> </li> <li> <p>Long context fine-tuning</p> <ul> <li> <p>batch size is set to 2M tokens</p> </li> <li> <p>10,000 gradient steps</p> </li> </ul> </li> </ul> <h1 id="results">Results</h1> <h3 id="code-generation">Code generation</h3> <ul> <li> <p>description-to-code generation benchmarks for Python:</p> <ul> <li> <p>HumanEval (Chen et al., 2021),</p> </li> <li> <p>MBPP (Austin et al., 2021)</p> </li> <li> <p>APPS (programming interviews and competitions, Hendrycks et al., 2021).</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>The value of model specialization.</strong></p> <ul> <li> <p>Llama 2 vs Code Llama</p> <ul> <li>Llama 2 70B은 Code Llama 7B과 성능 비슷</li> </ul> </li> <li> <p>Code Llama vs Code Llama - Python</p> <ul> <li>Code Llama - Python 7B가 Code Llama 13B outperform</li> </ul> </li> </ul> <p><strong>Unnatural model.</strong></p> <ul> <li> <p>unnatural instructions (Honovich et al. (2023))을 이용해 finetune</p> <ul> <li>여러 개의 seed로 generate -&gt; rephrase</li> </ul> </li> <li> <p>indicative of the improvements that can be reached with a small set of high-quality coding data.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>APPS benchmark는 위에서 소개된 벤치마크보다 더 어려운 태스크.</p> <ul> <li> <p>Code Llama - Python models은 introductory and interview level 태스크에서 성능 저하를 보였다. 즉, 프롬프트 자체를 이해하는 것이 solution을 내는 것보다 어렵다는 것을 Imply.</p> </li> <li> <p>Code Llama - Python은 competition-level problems, solution 자체를 더 구하기 어려운 태스크, 에서 더 좋은 성능을 보였다.</p> </li> </ul> <p><strong>Scaling of specialized models.</strong> specialized 모델들을 기준으로, 크기가 크면 언제나 성능이 더 좋았다.</p> <h3 id="multilingual-evaluation">Multilingual evaluation</h3> <p>Python 이외에 Multilingual 성능 또한 평가한다.</p> <ul> <li> <p>MultiPL-E (Cassano et al., 2022)</p> <ul> <li>C++, Java, PHP, C#, TypeScript (TS), and Bash.</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>mono-lingual setting과 비슷한 결과를 확인할 수 있다.</p> <ul> <li> <p>Code Llama models clearly outperform Llama 2 models of the same size on code generation in any language,</p> </li> <li> <p>Code Llama 7B outperforms Llama 2 70B.</p> </li> <li> <p>다른 publicly available models과 비교해도 확실한 성능</p> </li> <li> <p>Code Llama -Python 30B는 Code Llama 30B보다 성능이 살짝 좋지 않았으나, Code Llama - Python 7B and 13B는 더 나았음.</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>multilingual pre-training의 영향력을 확인하기 위해 언어별 correlations을 찍어본 결과이다.</p> <ul> <li> <p>C++, C#, Java, PHP 사이 high correlation on model performance.</p> </li> <li> <p>Python, Bash 사이 high correlation on model performance.</p> </li> <li> <p><strong>Lastly, as expected the bigger and more expressive the models, the higher the correlation between the performance across all different languages.</strong></p> </li> </ul> <h3 id="infilling-evaluations">Infilling evaluations</h3> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="long-context-evaluations">Long context evaluations</h3> <p>perplexity, synthetic retrieval task, code completion with long source code files</p> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Perplexity during extrapolation.</p> <ul> <li>16384 tokens 밑으로는 steady decrease</li> </ul> </li> <li> <p>Key retrieval.</p> <ul> <li> <p>prompt: 특정 위치에 있는 scalar를 Return하는 Python function code</p> </li> <li> <p>모델은 return value를 제대로 맞추는 <code class="language-plaintext highlighter-rouge">assert</code> statement을 작성하도록 권유받음.</p> </li> <li> <p>거의 모든 모델은 strong retrieval performance</p> </li> <li> <p>with the exception of the 7B model for test cases in which the function is placed at the beginning of the prompt.</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Single line completion.</p> <ul> <li> <p>Long Code Completion (LCC) benchmark</p> </li> <li> <p>long contexts are informative for code completion</p> </li> <li> <p>LCFT의 당위성 강조!</p> </li> </ul> </li> <li> <p>Performance impact on short contexts.</p> <ul> <li>LCFT을 하면 slightly hurts performance on standard code synthesis benchmarks consisting of short sequences.</li> </ul> </li> </ul> <h1 id="ablation-studies">Ablation Studies</h1> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="fine-tuning-llama-2-vs-training-from-scratch-on-code">Fine tuning Llama 2 vs. training from scratch on code</h3> <p>(4b)를 보면, 거의 절반 차이를 보이는 것을 확인할 수 있다. 즉, fineuning이 훨씬 낫다는 것 강조!</p> <h3 id="instruction-fine-tuning-1">Instruction fine-tuning</h3> <ul> <li> <p>Code Llama - Instruct vs Llama 2-Chat</p> <ul> <li>Code Llama improves its coding abilities for each model sizes, while preserving the general helpfulness performance inherited from Llama 2.</li> </ul> </li> </ul> <h3 id="impact-of-self-instruct-data">Impact of Self-instruct data</h3> <figure> <picture> <img src="/assets/img/posts/2023-08-29-code-llama-open-foundation-models-for-code/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="demo">Demo</h2> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>