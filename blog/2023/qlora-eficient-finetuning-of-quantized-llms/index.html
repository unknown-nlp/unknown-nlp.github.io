<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> QLoRA: Eficient Finetuning of Quantized LLMs | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - LLM, Efficient Training 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/qlora-eficient-finetuning-of-quantized-llms/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">QLoRA: Eficient Finetuning of Quantized LLMs</h1> <p class="post-meta"> Created on June 29, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> bert</a>   <a href="/blog/tag/efficient-training"> <i class="fa-solid fa-hashtag fa-sm"></i> efficient training</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/neural"> <i class="fa-solid fa-hashtag fa-sm"></i> neural</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/rlhf"> <i class="fa-solid fa-hashtag fa-sm"></i> rlhf</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-06-29</li> <li> <strong>Reviewer</strong>: 건우 김</li> <li> <strong>Property</strong>: LLM, Efficient Training</li> </ul> <h1 id="introduction">Introduction</h1> <ul> <li> <p>Finetuning LLMs is effective way to improve performance, and to add desirable or remove undersiable behaviors</p> <ul> <li> <p>Expensive to train</p> </li> <li> <p>16-bit finetuning LLaMA 65B: &gt;780GB GPU memory</p> <ul> <li>Recent quantization methods can reduce the memory, but only work for inference</li> </ul> </li> </ul> </li> <li> <p><strong>QLoRA</strong>: quantize PLM to 4-bit, and add learnable params LoRA weights (updated using quantized weigths) → <em>performance degradation x</em></p> <ul> <li> <p>LLaMA 65B: &gt;780GB → &lt;48GB (no degradation)</p> <ul> <li> <p>best model (Guanaco family of models) = 99.3% level of ChatGPT</p> <ul> <li>single GPU over 24 hours</li> </ul> </li> </ul> </li> <li> <p><strong>QLoRA main components</strong></p> <ol> <li> <p><strong>4-bit NormalFloat</strong>: 4-bit Integer/Float보다 더 좋은 성능을 보이는 데이터 타입.</p> </li> <li> <p>PLMs의 weights는 주로 정규분포를 따르는데, 이에 맞는 데이터 타입 사용(이론적으로 가장 optimal한 quantization data type)</p> </li> <li> <p><strong>Double Quantization</strong>: quantization constant를 quantize하는 기법.</p> </li> <li> <p>parameter 당 0.37 bits 크기 save</p> </li> <li> <p>**Paged Optimizer: **GPU memory가 최대치에 도달할 때, 일반 memory에 data를 저장시켜 연산에 필요한 memory를 확보하는 기법</p> </li> <li> <p><strong>LoRA</strong></p> </li> </ol> </li> </ul> </li> <li> <p>**Additional analysis **</p> </li> </ul> <p>→ regular finetuning으로는 분석할 수 없었던 실험들을, quantization을 통해 in-depth study of instruction finetuning과 chatbot performance를 model의 scale에 따라 실험 진행</p> <ul> <li> <p><strong>*Data quality**</strong> is far more important than *<strong><em>dataset size.</em></strong></p> <ul> <li>9k sample dataset (OASST1) outperformed a 450k sample dataset (FLAN v2) on chatbot performance</li> </ul> </li> <li> <p><em>MMLU benchmark performance does not imply strong Vicuna chatbot benchmark performance and vice versa</em></p> <ul> <li>dataset suitability matters more than size for a given task</li> </ul> </li> </ul> <h2 id="background">Background</h2> <ul> <li> <p>*<strong>*Basic Knowledge**</strong></p> <ul> <li>BLOOM-176B</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Inference: 8x 80GB A100 GPUs

- Finetuning: 72x GPUs
</code></pre></div></div> <ul> <li> <p><strong>Floating Point Formats</strong></p> <ul> <li> <p>Floating point example</p> <ul> <li> <p>(10진법) 5.6875 → (2진법) 101.1011 → (정규화) 1.011011 x 2^2</p> <ul> <li> <p>sign: 0 (positive)</p> </li> <li> <p>exponent (unadjusted): 2</p> </li> <li> <p>mantissa (unnormalized): 1.011011</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- **Float32 **(FP32)는 IEEE754에서 정한 표준 기준 (Full precision)

  - 1bit: sign (부호)

  - 8 bits: exponent (지수)

  - 23 bits: mantissa

  - FP16은 underflow/overflow 문제 有

- **FP16**: Half-precision, 16비트 부동 소수점 형식.

  - Memory를 적게 사용하므로 DL에서 자주 사용하지만, precision이 낮아서 model의 성능을 보장하지 못함

  - Training에 주로 FP32 사용, Inference에 주로 FP16사용

  - Example
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    - FP32를 통해 학습시킨 network의 gradient를 ranodm하게 sampling하여 찍은 graph.

    - Red line 왼쪽 graident는 FP16에서 표현이 불가능

- **BFLOAT16 **(BF16)은 FP16의 한계점을 보완하기 위해 소개됨

  - 8bit를 exponent를 사용하고, 7bit를 mantissa로 사용

  - FP32과 동일한 dynamic range를 갖으며 memory 사용량 적지만, precision이 더 낮음

- **Mixed Precision**는 FP16과 FP32를 혼합하여 사용

  - Memory 사용량을 최적화 + 학습 가속화하며 model의 성능유지 가능

    - FP32대비 1/2 memory 사용량 + 8배 연산 처리량 + 2배 memory 처리량

  - Deep Neural Network에서 학습에 관여하는 4가지 tensor 종류

    - Weights, Weight Gradients: FP16 범위 안에서 연산 가능

    - Activation, Activation Gradient: FP16에서 0으로 변환

  - **Process**

    1. FP32 weight를 FP16 copy weight 만들어 줌

      1. copy weight가 forward, backward에 사용

    1. FP16 copy weight를 통해 forward 진행

    1. Forward로 계산된 FP16 prediction을 FP32로 캐스팅 진행

    1. FP32 prediction을 이용해 FP32 loss 계산하고, scaling factor 곱함

    1. scaled FP32 loss를 FP16으로 캐스팅 진행

    1. scaled FP16 loss를 통해 backpropagation 진행후 gradeint 계산

    1. FP16 gradient를 FP32로 캐스팅 하고, 다시 scaling factor로 나눔

    1. FP32 gradient를 통해 FP32 weight update
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s"> define loss scaler for automatic mixed precision </span><span class="sh">"""</span>
<span class="c1"># Creates a GradScaler once at the beginning of training.
</span><span class="n">scaler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nc">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
  <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>

  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">amp</span><span class="p">.</span><span class="nf">autocast</span><span class="p">():</span>
    <span class="c1"># Casts operations to mixed precision
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

  <span class="c1"># Scales the loss, and calls backward()
</span>  <span class="c1"># to create scaled gradients
</span>  <span class="n">scaler</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">).</span><span class="nf">backward</span><span class="p">()</span>

  <span class="c1"># Unscales gradients and calls
</span>  <span class="c1"># or skips optimizer.step()
</span>  <span class="n">scaler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">)</span>

  <span class="c1"># Updates the scale for next iteration
</span>  <span class="n">scaler</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - 학습 전에 scaler 선언하고 → torch.cuda.amp.autocast()를 통해 캐스팅하고 forward진행

    - backward, optimization, weight update → scaler를 통해 진행

    - batch size 2배 사용 가능 + OOM 해결 가능

- BLOOM-176B

  - BF16 사용: 176 x 10^9 x 2 bytes = 352GB 필요

  - **Quantization: **더 적은 memory를 사용해서 다른 data type으로 weight를 저장하는 방식
</code></pre></div></div> <ul> <li> <p><strong>Block-wise k-bit Quantization</strong></p> <ul> <li> <p><strong>8-bit quantization: quarter precision을 사용하며 model의 1/4 size만 사용</strong></p> <ul> <li> <p>Not jsut dropping half of the bits</p> </li> <li> <p>Example</p> <ul> <li> <p>1st range: 10진수 / 2nd range: 4진수</p> <ul> <li> <p>‘4’는 0~9에서 중앙에 위치하므로, 0~3에서 표현하면 중앙값인 ‘2’로 표현</p> </li> <li> <p>‘3’은 0~9에서 ‘2’와 ‘4’ 사이에 위치하므로, 상대적으로 0~3에서 ‘1’과 ‘2’사이에 위치하고 반올림하면 ‘2’로 표현</p> </li> <li> <p>다른 수를 동일한 값으로 표현하는 <strong>Information Loss</strong> 발생 (Lossy compression)</p> </li> </ul> </li> </ul> </li> <li> <p>Common techniques (8-bit quantization)</p> <ul> <li> <p>FP values를 더 compact한 int8 (1byte) values로 mapping시킴</p> </li> <li> <p><strong>Zero-point quantization</strong></p> <ul> <li> <p>Range: -1.0~1.0 (float) → Range: -127~127 (int8)</p> <ul> <li> <p>원래의 값을 다시 얻으려면, int8 value를 quantization factor (127)로 나눔</p> </li> <li> <p>Example) ‘0.3’은 0.3*127 = ‘38.1’이므로, 반올림 하면 ‘38’로 mapping 됨</p> <ul> <li>원래의 값을 얻으려면, 38/127 = ‘0.2992’를 얻게 되서, ‘0.008’ qunatization error 발생 → 누적되면 성능 저하의 원인</li> </ul> </li> </ul> </li> </ul> </li> <li> <p><strong>Absmax quantization</strong></p> <ul> <li> <p>Tensor의 absolute maximum 값을 활용하여 스케일</p> <ul> <li>no quantization error</li> </ul> </li> <li> <p>Example</p> </li> </ul> <p>23.5 = 127/5.4</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>quantizaing FP32 into Int8 [-127,127]</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>dequantizing</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>→ outlier 값을 다루기 위해 chunck 단위로 나누어 quantize 진행</p> <ul> <li><strong>Low-rank Adapters</strong></li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Memory Requirement of Parameter-Efficeint Finetuning</strong></p> <ul> <li> <p>LoRA가 PEFT로 소개가 (small memory footprint) 되긴 했지만, LLM을 finetuning할 때 발생되는 대부분의 memroy footprint는 LoRA params가 아닌 <strong>*Activation Gradients***</strong> **이다.</p> <ul> <li> <p>LLaMA-7B</p> <ul> <li> <p>LoRA input graidients: 567MB</p> </li> <li> <p>LoRA params: 26MB</p> </li> </ul> </li> <li> <p>Gradient checkpointing(GC)을 사용하면, input gradients는 18 MB로 줄어들 수 있다.</p> <ul> <li> <p>*<strong>*Gradient checkpointing (GC)**</strong></p> <ul> <li> <p>classic backpropagation: 당장 사용하지 않는 node의 값이라도 다 저장함. 연산 속도가 빠르다는 장점이 있지만, 저장해야할 weight가 늘어나 memory 사용량이 늘어나는 단점 존재</p> </li> <li> <p>weight를 저장하지 않는 backpropagation: 연산 속도를 생각하지 않으면, node의 weight를 저장할 필요 없음. 하지만 forward process가 2번씩 일어나서 O(N^2) 시간 복잡도 발생하는 단점 존재.</p> </li> <li> <p>GC: classic backpropagtaaion과 weight를 저장하지 않는 방식의 절충안. 일부 node만 선택하고 그 node의 gradient만 저장하는 방식. checkpoint 이후의 node까지 forward process를 빠르게 할 수 있음 O(N^{1/2})</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - LoRA 자체의 parameter를 줄이는 것은 중요하지 않으며, GC를 통해 더 많은 LoRA 사용가능 without increasing training memory footprint
</code></pre></div></div> <h2 id="qlora-finetuning">QLoRA Finetuning</h2> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>QLoRA의 <strong>storage data type은 4-bit</strong> / <strong>computation data type은 BFloat16</strong> </li> </ul> <p>→ QLoRA weight가 사용될 때, BFloat16으로 dequantize를 하고 16-bit에서 행렬 연산 진행</p> <ul> <li> <p><strong>4-bit NormalFloat(NF) Quantization</strong></p> <ul> <li> <p>NF data type은 이론적으로 가장 이상적인 data type이며 Quantile Quantization 기법에 사용</p> <ul> <li>Quantile Quantization: data를 동일한 크기의 quantile로 분류하는 방법. Empirical cumulative distribution function을 통해 tensor의 quantile을 추정하는 식으로 작동.</li> </ul> </li> </ul> </li> </ul> <p>(각 quantization bin에는 동일한 input tensor의 개수가 존재)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Limitation: quantile 추정이 expensive → 빠른 추정을 하기위해 SRAM quantile같은 approximation을 통해 추정하는 방법이 제안 되었지만, outliers에 대해 large quantization error가 존재해서 불안정

  - Input tensor들이 고정된 quantization constant의 분포로부터 나오면 앞선 문제들 해결 가능 → input tensors have the same quantiles (동일한 분포를 갖는다)
</code></pre></div></div> <ul> <li> <p>Pretrained neural network weights follow zero-centered normal distribtuion with standard deivation \sigma</p> <ul> <li>\sigma scaling을 통해 모든 weight를 single fixed distribution으로 변형 가능 → data type과 weights에 대응하는 quantiles들이 동일한 range [-1,1] 갖게끔 정규화 적용</li> </ul> </li> <li> <p>zero-mean normal-dist를 위한 data type을 구하는 Process</p> <ol> <li> <p>estimate 2^k+1 quantiles of a N(0,1) distribtuion to obtain a <em>k</em>-bit quantile quantization data type for norm-dist</p> </li> <li> <p>Take this data type and normalize its values into [-1,1] range</p> </li> <li> <p><strong>quantize an input weight tensor by normalizing it into [-1,1] through absolute maximum rescaling</strong></p> </li> </ol> </li> <li> <p>weight range와 data type range를 맞춘후, data type의 q_i 값들에 대한 quantize 진행 (2^k)</p> <ul> <li>위에 (3) 과정이 ‘\sigma scaling을 통해 모든 weight를 single fixed distribution으로 변형’하는 역할 수행</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>Double Quantization</strong></p> <ul> <li>추가적인 memory 사용량을 아끼기 위해 소개된 Quantization constant를 quantizing하는 방법</li> </ul> </li> <li> <p><strong>Paged Optimzier</strong></p> <ul> <li>CPU와 GPU간의 연동을 통해 GPU memory가 고갈되면, CPU RAM과 disk에서 memory paging하는 것과 비슷하게 동작. 자동으로 CPU RAM으로 backup해서 process가 끊임 없이 연산을 계속할 수 있게 함. (optimzier update step이 필요할 때, CPU → GPU 이동)</li> </ul> </li> <li> <p><strong>QLoRA</strong></p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>One storage data type (4-bit NF)</p> </li> <li> <p>One computation data type (16-bit BF)</p> <ul> <li>forward/backward process를 진행할 때, data type에 대해 dequantize를 수행</li> </ul> </li> </ul> <h2 id="qlora-vs-standard-finetuning">QLoRA vs Standard Finetuning</h2> <ul> <li> <p><strong>Default LoRA hyperparameters do not match 16-bit performance</strong></p> <ul> <li> <p>Alpaca (ours): full-finetuning → QLoRA-All과 비슷한 성능 보임</p> <ul> <li>Standford-Alpaca역시 full-finetuning인데 hyperparameters 조합에 대해 undertuned 되어 있음을 발견.</li> </ul> </li> <li> <p>Hyperparameters 중에 adapter의 개수만 성능에 영향을 미치고, LoRA의 projection dimension과 같은 다른 hyperparameter는 중요하지 않음</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>4-bit NormalFloat yields better performance than 4-bit Floating Point</strong></p> <ul> <li> <p>NF4가 FP4와 Int4 대비 높은 성능을 보임</p> <ul> <li>NF4 + Double Quantization (DQ)는 성능 하락 없이 memory 사용량을 줄여줌</li> </ul> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p><strong>k-bit QLoRA matches 16-bit full finetuning and 16-bit LoRA performance</strong></p> <ul> <li>최근까지 4-bit quantization에 대한 연구에서는, <em>inference</em>는* *가능 하지만 16-bit에 비해 performance degradation이 있다는 결과가 있음</li> </ul> </li> </ul> <p>→ 그러면 4-bit adatper finetuning을 하면 Loss performance를 되찾을 수 있나?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. **Small Models**

  - 16-bit, 8-bit, 4-bit adapter methods replicate the performance of fully finetuned 16-bit baseliens

    - BF16: fully finetuned

    - BF16 replication: LoRA training
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. **Large Models**

  - FP4는 16-bit LoRA baselines의 성능을 재현하지 못함.

  - NF4 + DQ는 16-bit LoRA finetuning 성능 재현 성공

    - NF4가 FP4보다 quantization precision 관점에서 더 적합함을 보임
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_017.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Summary</p> <ul> <li> <p>4-bit (NF) QLoRA는 16-bit full finetuning과 16-bit LoRA finetuning과 꾸준히 비슷한 성능 보임</p> </li> <li> <p>NF4가 FP4보다 더 효과적이고 DQ는 성능 저하가 없음</p> </li> </ul> </li> </ul> <h2 id="pushing-the-chatbot-state-of-the-art-with-qlora">Pushing the Chatbot State-of-the-art with QLoRA</h2> <ul> <li> <p>Experiment settings</p> <ul> <li> <p><strong>Data</strong></p> <ul> <li> <p>instruction-following datasets에 대한 comprehensive sutdy가 없어 다음 8개 사용</p> <ul> <li> <p>crwod-sourcing: OASST1, HH-RLHF</p> </li> <li> <p>distillation from instruction-tuned models: Alpaca, self-instruct, unnatural instructions</p> </li> <li> <p>corpora aggregations: FLAN v2, Chip2, Longform</p> </li> </ul> </li> </ul> </li> <li> <p><strong>Training setup</strong></p> <ul> <li> <p>QLoRA finetuning with CE loss (supervised) without RL (different resonse에 대한 human judgement가 있는 것도 RL 활용 x)</p> </li> <li> <p>NF4 QLoRA with DQ + Paged optimizers 적용</p> </li> </ul> </li> <li> <p><strong>Baselines</strong></p> <ul> <li> <p>Research models: Vicuna (LLaMA 13B finetuend), Open Assistant (LLaMA 33B finetuned with RLHF)</p> </li> <li> <p>Commercial models: GPT-4, GPT-3.5-turbo, BARD</p> </li> </ul> </li> <li> <p><strong>Evaluation</strong></p> <ul> <li> <p><strong>NLU task</strong>: MMLU (5-shot test accuracy)</p> </li> <li> <p><strong>NLG task</strong></p> <ul> <li> <p><strong>Benchmark data</strong></p> <ol> <li> <p>Vicuna prompts: 80 prompts from a diverse set of categories</p> </li> <li> <p>OASST1 validation: multilingual collection of crow-sourced multiturn dialogs between a user and an assitant (953 unqiue user queries)</p> </li> </ol> </li> <li> <p><strong>Evaluation</strong></p> <ul> <li>use GPT-4 to rate the performance of different systems against ChatGPT on Vicuna benchmark (model과 ChatGPT 비교)</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>→ query가 주어지면, ChatGPT의 response와 model의 response 각각을 생성하고, GPT-4가 각 response에 대해 점수 (10점 만점)를 매기고 explanation 생성</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      - model의 performance는 ChatGPT대비 얼만큼 나왔는지 percentage

      - GPT-4의 ordering effect가 존재: prompt 초반에 있는 response에게 더 높은 점수를 부여하는 경향이 있음 → 모든 orders를 고려해서 평균

    - Direct comparisons between system outputs: win / tie / lose 를 GPT-4가 정하게 시킴

    - Human evaluation

      - GPT-4의 reliability 수준은 human judgement와 아직 까지는 correlate하지 않기 때문에, 별도의  human evaluation 진행

    - Elo Rating

      - human과 automated pairwise comparison을 토대로 Elo rating 진행
</code></pre></div></div> <ul> <li> <p><strong>Results</strong></p> <ul> <li> <p>GPT-4 다음으로 Guannaco-65B이 높은 성능을 보임</p> </li> <li> <p>Guannaco-7B는 ALpaca 13B보다 20%p 높음 (5GB 용량으로 핸드폰에 들어갈 크기)</p> </li> <li> <p>Confidence Interval is wide: model 성능이 서로 overlapping하는 경우가 많음</p> </li> </ul> </li> </ul> <p>→ Elo 기반으로 평가 진행</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  - Guannaco-33B,65B는 GPT-4 다음으로 높음
</code></pre></div></div> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_018.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure> <picture> <img src="/assets/img/posts/2023-06-29-qlora-eficient-finetuning-of-quantized-llms/image_019.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="qualitative-analysis">Qualitative Analysis</h2> <ul> <li> <p>다른 LLMs과 비슷한 양상의 실수들이 존재하기 때문에 <strong>생략</strong></p> <ul> <li> <p>Factual Recall</p> <ul> <li>쉬운 질문은 잘 답하지만, 질문이 어려워지면 답변을 잘 못함 (with confident)</li> </ul> </li> <li> <p>Suggestibility</p> <ul> <li>말도 안되는 질문에 답변 회피. (e.g. ‘언제부터 지구는 평평해? → ‘그런적 없음’)</li> </ul> </li> <li> <p>Math</p> <ul> <li>math 종류의 reasoning task는 매우 못함</li> </ul> </li> <li> <p>etc.</p> </li> </ul> </li> </ul> <h2 id="limitations-and-discussion">Limitations and Discussion</h2> <ul> <li> <p>QLoRA를 통해 4-bit base model로 16-bit full finetuning 성능을 재현할 수는 있었지만, model의 scale이 33B 혹은 65B까지 가면 그렇지 못했다. → resource 부족으로 원인 발견 x</p> </li> <li> <p>Evaluation of instruction finetuning models</p> <ul> <li> <p>BigBench, RAFT, HELM과 같은 benchmark에 대해서 평가를 진행하지 않아, 앞에서 진행한 실험들이 generalize될지 모름</p> </li> <li> <p>finetuning data와 benchmark dataset이 similar한 정도에 성능이 dependent</p> </li> </ul> </li> <li> <p>다른 bit-precision (e.g 3-bit) base models 혹은 다른 adapter 방법론에 대한 실험 부재</p> </li> </ul> <h3 id="implementation">Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">facebook/opt-350m</span><span class="sh">"</span><span class="p">,</span> <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">##################
</span>
<span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>


<span class="n">nf4_config</span> <span class="o">=</span> <span class="nc">BitsAndBytesConfig</span><span class="p">(</span>
   <span class="n">load_in_4bit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="sh">"</span><span class="s">nf4</span><span class="sh">"</span><span class="p">,</span>
   <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
   <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">model_nf4</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">nf4_config</span><span class="p">)</span>

<span class="c1">##########
</span><span class="n">accelerate</span> <span class="n">library를</span> <span class="n">사용하기</span> <span class="n">때문에</span><span class="p">,</span> <span class="n">지원하는</span> <span class="n">models은</span> <span class="n">모두</span> <span class="n">사용</span> <span class="n">가능</span>
<span class="o">-</span> <span class="n">Llama</span><span class="p">,</span> <span class="n">OPT</span><span class="p">,</span> <span class="n">GPT</span><span class="o">-</span><span class="n">Neo</span><span class="p">,</span> <span class="n">GPT</span><span class="o">-</span><span class="n">NeoX</span>

<span class="p">[</span>
    <span class="sh">'</span><span class="s">bigbird_pegasus</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">blip_2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bloom</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">bridgetower</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">codegen</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">deit</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">esm</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">gpt2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gpt_bigcode</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gpt_neo</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gpt_neox</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gpt_neox_japanese</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gptj</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">gptsan_japanese</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">lilt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">llama</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">longformer</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">longt5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">luke</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">m2m_100</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mbart</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mega</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mt5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">nllb_moe</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">open_llama</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">opt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">owlvit</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">plbart</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">roberta</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">roberta_prelayernorm</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rwkv</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">switch_transformers</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">t5</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">vilt</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">vit</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">vit_hybrid</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">whisper</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">xglm</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">xlm_roberta</span><span class="sh">'</span>
<span class="p">]</span>
</code></pre></div></div> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>