<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning | Unknown NLP Papers </title> <meta name="author" content="Jeahee Kim"> <meta name="description" content="논문 리뷰 - ICL, In Context Learning, LLM 관련 연구"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://unknownnlp.github.io/blog/2023/label-words-are-anchors-an-information-flow-perspective/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Unknown NLP Papers </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning</h1> <p class="post-meta"> Created on December 12, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> classification</a>   <a href="/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt</a>   <a href="/blog/tag/icl"> <i class="fa-solid fa-hashtag fa-sm"></i> icl</a>   <a href="/blog/tag/in-context-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> in context learning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/paper-review"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-review</a>   ·   <a href="/blog/category/paper-reviews"> <i class="fa-solid fa-tag fa-sm"></i> paper-reviews</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>논문 정보</strong></p> <ul> <li> <strong>Date</strong>: 2023-12-12</li> <li> <strong>Reviewer</strong>: 김재희</li> <li> <strong>Property</strong>: ICL, In Context Learning, LLM</li> </ul> <hr> <hr> <h2 id="1-intro">1. Intro</h2> <ul> <li> <p>ICL 수행 시 각 demonstartion의 정보는 label words의 representation에 집중됨을 확인</p> </li> <li> <p>발견된 현상을 이용하여 ICL의 성능을 개선하는 방법론 제시</p> </li> <li> <p>얕은 레이어와 깊은 레이어에서 attention의 흐름(information flow)가 다르게 관측</p> <ul> <li> <p>얕은 레이어에선 label word에 정보가 집중됨</p> </li> <li> <p>깊은 레이어에선 label word에서 정보를 꺼내와 예측에 적극적으로 활용</p> </li> </ul> </li> </ul> <p>⇒ 이러한 현상을 활용하여 ICL 예측 성능 개선</p> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_000.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>얕은 레이어와 깊은 레이어의 역할</p> <ul> <li> <p>얕은 레이어 : 각 Demonstration의 정보를 label word가 취합</p> </li> <li> <p>깊은 레이어 : 실제 label word 예측 시 demonstration의 label word에 취합된 정보를 적극적으로 이용하여 예측 수행</p> </li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_001.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="2-analysis">2. Analysis</h2> <h3 id="21-가설-검증">2.1. 가설 검증</h3> <ul> <li> <p>본 논문이 관찰한 두가지 현상(deep/shallow layer)에 대한 직접적 검증 시도</p> </li> <li> <p>Saliency Score를 이용하여 예측에 각 단어의 영향력을 측정</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_002.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>⇒ Attention Map 상에서 각 토큰 간의 attention이 loss에 미친 영향력을 수치화</p> <ul> <li> <p>ICL 환경에서 입력을 세가지로 구분</p> <ul> <li> <p>w : Demonstration 상 Input Text(or 이전 입력 전체)</p> </li> <li> <p>p : Demonstration 상 Label Words</p> </li> <li> <p>q : 예측할 Input의 예측할 text(or “:”)</p> </li> </ul> </li> <li> <p>위 3가지 입력에 따라 Saliency Score를 계산하게 됨</p> </li> <li> <p>Input Text(w) → Label Words(p)</p> <ul> <li>Label Words를 예측하는데 이전 토큰이 미친 영향력, attention 정도</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_003.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Label Words(p) → Prediction(q)</p> <ul> <li>Demonstration 내 각 label words이 실제 예측 label에 미친 영향력, attention 정도</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_004.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Input Text(w) → Input Text(w)</p> <ul> <li>Label Words와 Label 예측을 제외한 나머지 Input Token 간의 영향력</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_005.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="22-실험-환경">2.2 실험 환경</h3> <ul> <li> <p>4개의 Text Classification Task 사용</p> <ul> <li> <p>평가 데이터 : 1,000개</p> </li> <li> <p>Demonstration : Class 당 1개 사용</p> </li> <li> <p>SST-2, TREC, AGNews, EmoC,</p> </li> </ul> </li> <li> <p>Model :</p> <ul> <li> <p>GPT2-XL(1.5B) : 저자들의 언급으로는 ICL이 충분히 가능하면서 실험 가능한 사이즈</p> </li> <li> <p>GPT-J(6B) : 쫄렸는지 갑자기 큰 모델도 해버림</p> </li> </ul> </li> </ul> <h3 id="23-information-flow-실험">2.3 Information Flow 실험</h3> <ol> <li>Layer 깊이에 따른 Information Flow</li> </ol> <ul> <li> <p>실험 결과 intro의 findings를 정량적으로 확인 가능</p> </li> <li> <p>초기 레이어 :</p> <ul> <li> <p>많은 정보(attn)이 label word로 집중되고 있음</p> </li> <li> <p>예측 할 토큰으로 흐르는 정보가 많지 않은 모습</p> </li> </ul> </li> </ul> <p>⇒ 초기 레이어는 각 Label Word로 정보를 모음</p> <ul> <li> <p>후기 레이어 :</p> <ul> <li> <p>대부분의 정보가 예측할 단어(”:”)로 흘러가고 있음(from label words)</p> </li> <li> <p>다른 토큰으로 정보가 거의 취합되지 않는 모습</p> </li> <li> <p>중반 이후 레이어에서 매우 꾸준한 경향</p> </li> </ul> </li> </ul> <p>⇒ 후기 레이어는 각 label word로부터 실제 예측에 사용될 정보를 취합하는데 집중</p> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_006.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="24-loyalty-실험">2.4 Loyalty 실험</h3> <ul> <li> <p>Isolation(Label Words) : Attention Map을 임의로 조정하여 Input Text → Label Word의 Attention을 막는 환경</p> </li> <li> <p>First/Last : 초기/후기 5개 레이어에 대해 Isolation을 수행</p> </li> <li> <p>Random : Attention Map 내 임의의 attention을 조정</p> </li> <li> <p>Loyalty : 2가지 측면에서 Isolation에 따른 영향력 평가</p> </li> </ul> <p>→ Isolation에 따른 예측값 변화량 측정</p> <ul> <li> <p>Label Loyalty : Isolation에 따른 Label 예측 변화량</p> </li> <li> <p>Word Loyalty : Isolation에 따라 예측된 top-5 token과 original 예측의 Jaccard 유사도</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_007.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>Label Word와 레이어 단위 별 영향력 평가</p> <ol> <li> <p>청록색 : 초기 레이어에서 Label Words에 대한 Isolation 수행 시 매우 큰 성능 저하 발생</p> </li> <li> <p>주황색 : 후기 레이어에서 Label Words에 대한 Isolation 수행 시 성능 저하 거의 발생 X</p> </li> </ol> </li> </ul> <p>→ Label Words에 대한 영향력은 초기 레이어에서 매우 큼</p> <ol> <li>빨간색/보라색 : 초기/후기 레이어에서 Random한 Isolation 수행 시 성능 저하 거의 발생 X</li> </ol> <p>→ 임의의 단어가 영향력을 가지지 않음</p> <p>⇒ Label Words의 정보가 초기에 수집되는 것이 ICL 성능에 큰 영향</p> <p>⇒ Label Words를 임의로 변경(A/B)로 하여도 비슷한 결과 관찰 가능</p> <p>(GPT-J, GPT-2의 경우 Label Words를 바꾸면 Random Guessing에 가까워져서 LLaMA-30B로 변경)</p> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_008.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="25-aucroc-실험">2.5 AUCROC 실험</h3> <ul> <li> <p>l번째 레이어의 예측 시점에 대한 각 Label Word와 실제 예측값 간의 높은 Correlation 관찰</p> <ul> <li> <p>예측 클래스에 해당하는 Label Word에 대해 높은 attn 부여</p> </li> <li> <p>…. positive, … negative …, : negative</p> </li> <li> <p>positive → : : 0.2</p> </li> <li> <p>negative → : : 0.8</p> </li> </ul> </li> <li> <p>위 가설에 대한 엄밀한 검증을 위해 AUROC 계산 실시</p> <ul> <li> <p>클래스 당 예측 확률 : 각 Label Word에 대한 attn값</p> </li> <li> <p>R_l=\frac{\sum_{i=1}^l\left(\mathrm{AUCROC}<em>i-0.5\right)}{\sum</em>{i=1}^N\left(\mathrm{AUCROC}_i-0.5\right)} : l번째 레이어의 Attention이 실제 예측에 영향을 준 정도에 대한 정량화</p> </li> </ul> </li> <li> <p>레이어가 깊어질 수록 AUROC 값이 커지는 경향성 확인</p> </li> </ul> <p>→ 모델의 실제 예측 class와 해당 Label Word에 대한 Attn Score가 깊은 레이어에서 높은 Correlation을 가짐</p> <ul> <li>레이어가 깊어질수록 R_l값이 점차 커지는 모습 확인</li> </ul> <p>→ 레이어가 깊어질수록 점차 Label Word가 최종 예측에 미치는 영향력이 커지는 모습</p> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_009.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="26-결론">2.6 결론</h3> <ul> <li>Label Word가 해당 Demonstration의 정보를 종합하고, 이를 예측에 전달하는 역할을 하는 모습 확인</li> </ul> <p>⇒ Label Word가 정보 흐름의 관점에서 anchor 역할로서 동작</p> <h2 id="3-proposed-method">3. Proposed Method</h2> <ul> <li> <p>앞선 분석을 바탕으로 직관적인 ICL 개선 방안 제안</p> <ul> <li>ICL의 성능 개선 및 속도 개선</li> </ul> </li> <li> <p>앞선 분석의 결론</p> </li> </ul> <p>⇒ 모델 예측과 attention distribution 간 높은 상관관계 확인</p> <h3 id="31-anchor-re-weighting">3.1 Anchor Re-weighting</h3> <ul> <li>Attention Mechanism을 직접적인 예측 확률의 추정으로 간주</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_010.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>두가지 식을 치환하여, log prob으로 표현 가능(\textbf{q}<em>q / \sqrt{d} = \hat{\textbf{x}}, \textbf{k}</em>{p_i} - \textbf{k}_{p_C} = \beta_i )</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_011.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>위와 같은 식은 결국 logistic regression 식으로 표현이 됨</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_012.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>위 식에서 \beta_i는 original Attention Weight의 값, \beta_0가 직접적인 attention map의 수정이라고 볼 수 있음</li> </ul> <p>⇒ 예측 시점에서 Attention 계산 시 query의 representation(”:”의 representation)은 고정된 상태에서 각 Label Word의 representation을 수정하는 것으로 보는듯</p> <p>⇒ original attention 값을 re-weighting하는 parameter를 도입하여 이를 통해 해결하려함</p> <ul> <li> <p>우리는 학습 파라미터 \beta_0 에 대해서 아래와 같은 closed form을 통해 해결 가능한 최적화 가능</p> <ul> <li>맞습니다요… ICL인데 학습 데이터가 필요해지는 상황이 오져</li> </ul> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_013.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>⇒ ICL을 Attention Matrix를 이용한 Logistic Regresseion으로 접근하여, LLM의 능력에 온전히 의존하지 않고, 외부에서 ICL을 수행하는 듯한 방법론</p> <p>⇒ 추론과정 수행 이전에 train sample들에 대해 별도로 수행하여 진행할 수 있게 됨</p> <h3 id="32-anchor-only-context-compression">3.2 Anchor-Only Context Compression</h3> <ul> <li>앞선 내용을 요약하면 <strong><em>모델이 예측 수행 시 Demonstration 중 Label Word에서만 정보를 취합하고 있음</em></strong> </li> </ul> <p>⇒ ICL 시 각 Demonstration의 Label Word의 Hidden Representation만 이용하면 안되나?</p> <ol> <li> <p>각 Demonstration에 대해서 별도의 Inference 수행</p> </li> <li> <p>Label Word의 모든 Layer에 대한 Hidden Reprensetation Caching</p> </li> <li> <p>Inference 시 Caching한 Representation을 입력의 앞에 concat하여 attention 수행</p> </li> </ol> <p>→ Inference 시 Demonstration이 입력되지 않으므로 속도/메모리 개선 가능</p> <h2 id="4-experiments">4. Experiments</h2> <h3 id="41-anchor-re-weighting-실험-환경">4.1 [Anchor Re-Weighting] 실험 환경</h3> <ul> <li> <p>ICL 시 사용될 데이터 : Class 당 1개씩 샘플링</p> </li> <li> <p>Re-weighting을 위해 사용할 학습 데이터 : Class 당 4개씩 샘플링</p> <ul> <li> <p>Re-Weighting을 위한 \beta를 해당 데이터를 이용하여 학습</p> </li> <li> <p>향후 실제 ICL 수행 시 각 class 별 \beta를 적용하여 최종 예측 수행</p> </li> </ul> </li> <li> <p>비교 방법론 :</p> <ul> <li> <p>1-shot ICL</p> </li> <li> <p>5-shot ICL : Re-weighting에 사용된 샘플들을 ICL에 이용한 상황</p> </li> </ul> </li> </ul> <h3 id="42-anchor-re-weighting-결과">4.2 [Anchor Re-Weighting] 결과</h3> <ul> <li>개잘나옴;;</li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>ICL 방법론과 비교하고 있지만, 해당 방법론은 일반적인 ICL과 다르게 동작함</p> <ol> <li> <p>모델이 예측해야 할 데이터에 대해 직접 확률 분포 생성 X</p> </li> <li> <p>모델이 예측해야 할 데이터에 대한 각 Demonstration의 Label Word에 대한 attention weight을 이용한 연산 수행</p> </li> </ol> <ul> <li>Demonstration 구축을 위한 데이터셋을 효과적으로 활용하는 방안 제시</li> </ul> </li> </ul> <h3 id="43-anchor-only-context-compression-실험-결과">4.3 [Anchor-Only Context Compression] 실험 결과</h3> <ul> <li> <p>Text Anchor : Label Word를 직접 Inference Input 앞에 Concat</p> </li> <li> <p>Hidden Random : Label Word를 제외한 임의 토큰의 representation을 caching하여 활용</p> </li> <li> <p>Hidden Random-Top : Label Word를 제외한 임의 토큰을 20개 선별하고, 이 중 가장 좋은 성능을 낸 세팅 리포팅</p> </li> <li> <p>Hidden Anchor : Label Word의 Representation을 caching하여 이용(proposed method)</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_015.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li> <p>실험 데이터셋의 평균 결과치</p> </li> <li> <p>제안 방법론이 가장 높은 성능 리포팅</p> </li> </ul> <p>→ Demonstration을 그대로 활용한 것보다는 떨어지는 성능</p> <ul> <li> <p>Demonstartion의 정보 중 상당수가 Label Word에 포함되어 있음</p> </li> <li> <p>Demonstration의 정보 중 일부가 Label Word외 다른 토큰에 포함되어 있음. 해당 정보를 이용하는 것이 최종적이 성능 개선에 도움이 됨</p> </li> </ul> <figure> <picture> <img src="/assets/img/posts/2023-12-12-label-words-are-anchors-an-information-flow-perspective/image_016.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <ul> <li>하지만 속도 측면에서는 Caching 전략이 유효</li> </ul> <h2 id="7-conclusion">7. Conclusion</h2> <h3 id="pros">Pros</h3> <ul> <li> <p>ICL에 대한 심도있는 분석과 설득력있는 주장</p> </li> <li> <p>주장을 뒷받침하기 위한 다양한 분석</p> </li> <li> <p>분석 결과를 이용한 ICL 개선 방법론 제시 및 큰 성능 개선 달성</p> </li> <li> <p>분석 결과를 이용한 ICL 개선 방법론 제시 및 속도 개선 달성</p> </li> <li> <p>엄밀한 Fair Comparison을 위한 실험 설정</p> </li> <li> <p>주장하는 바를 뒷받침하는 Metric 제안 및 활용</p> </li> <li> <p>깔끔한 논문 서술</p> </li> <li> <p>Appendix에 궁금할만한 결과물을 다 때려박음</p> </li> </ul> <h3 id="cons">Cons</h3> <ul> <li> <p>교묘하게 피해가는 Contribution</p> <ul> <li>속도 개선과 성능 개선이 동시에 달성되지 못함.</li> </ul> </li> </ul> <p>→ 이 부분을 그래서 강하게 주장하지 못함</p> <ul> <li> <p>복잡한 구현 방식</p> <ul> <li>Attention Matrix를 직접 이용하는 아이디어는 좋으나 너무 복잡해요 슨생님…</li> </ul> </li> </ul> </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <br> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'unknown-nlp/unknown-nlp.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jeahee Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>